---
layout: post
title: 'Pytorch 用户笔记'
date: 2020-03-23
author: 郑之杰
cover: ''
tags: Pytorch
---

> Notes about Pytorch.

1. model.eval()和with torch.no_grad()的比较
2. 更改pretrained model的网络结构

## 1. model.eval()和with torch.no_grad()的比较
```model.eval()```使模型进入eval测试模式（对应的，```model.train()```使模型进入train训练模式），batch normalization和dropout在训练和测试模式中表现不同。

- 对于batch normalization，训练时的mean和std是由每一个批次计算得到；测试时的mean和std是由训练时的参数滑动平均得到的。
- 对于(inverted) dropout，训练时每一个神经元都有一定概率被舍弃，并对该层的输出做修正；测试时不做处理。

```torch.no_grad()```关闭了自动求导机制，在```with torch.no_grad()```下会减少内存使用，提高计算速度，但是无法进行backprop。如果没有关闭autograd，即使没有进行backward，也会一直累计之前的gradient。

参考:[https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615](https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615)

## 2. 更改pretrained model的网络结构
继承自```torch.nn.Module```的model包含一个叫做[children()](http://s0pytorch0org.icopy.site/docs/stable/nn.html?highlight=children#torch.nn.Module.children)的函数，这个函数可以用来提取出model每一层的网络结构，在此基础上进行修改即可。

如去掉Resnet50的最后一层FC层：
```
class Net(torch.nn.Module):
    def __init__(self , model):
        super(Net, self).__init__()
        self.resnet_layer = torch.nn.Sequential(*list(model.children())[:-1])
    
    def forward(self, x):
        x = self.resnet_layer(x)
        return x.view(x.shape[0:2])  #压缩height和width轴
    
resnet = models.resnet50(pretrained=True)
model = Net(resnet)
```
