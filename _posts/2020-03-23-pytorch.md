---
layout: post
title: 'Pytorch 用户笔记'
date: 2020-03-23
author: 郑之杰
cover: ''
tags: Pytorch
---

> Notes about Pytorch.

1. model.eval()和with torch.no_grad()的比较

## 1. model.eval()和with torch.no_grad()的比较
```model.eval()```使模型进入eval测试模式（对应的，```model.train()```使模型进入train训练模式），batch normalization和dropout在训练和测试模式中表现不同。

- 对于batch normalization，训练时的mean和std是由每一个批次计算得到；测试时的mean和std是由训练时的参数滑动平均得到的。
- 对于(inverted) dropout，训练时每一个神经元都有一定概率被舍弃，并对该层的输出做修正；测试时不做处理。

```torch.no_grad()```关闭了自动求导机制，在```with torch.no_grad()```下会减少内存使用，提高计算速度，但是无法进行backprop。如果没有关闭autograd，即使没有进行backward，也会一直累计之前的gradient。

参考:[https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615](https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615)
