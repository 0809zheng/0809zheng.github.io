---
layout: post
title: '卷积神经网络 论文'
date: 2020-03-21
author: 郑之杰
cover: ''
tags: 深度学习 卷积神经网络
---

> Papers about Convolutional Neural Networks.

# Survey

### A Survey of the Recent Architectures of Deep Convolutional Neural Networks
- intro:
- arXiv:[https://arxiv.org/abs/1901.06032](https://arxiv.org/abs/1901.06032)


# CNN History

### Receptive fields, binocular interaction and functional architecture in the cat's visual cortex
- intro:Hubel,Wiesel
- pdf:[http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/additional/systems/JPhysiol-1962-Hubel-106-54.pdf](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/additional/systems/JPhysiol-1962-Hubel-106-54.pdf)

### Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Visual Pattern Recognition
- intro:neocognitron
- pdf:[https://www.cs.princeton.edu/courses/archive/spring08/cos598B/Readings/Fukushima1980.pdf](https://www.cs.princeton.edu/courses/archive/spring08/cos598B/Readings/Fukushima1980.pdf)

### 
- intro:
- arXiv:[]()


# CNN Components

### A Theoretical Analysis of Feature Pooling in Visual Recognition
- intro:pooling
- pdf:[https://www.di.ens.fr/sierra/pdfs/icml2010b.pdf](https://www.di.ens.fr/sierra/pdfs/icml2010b.pdf)

### Activation Functions: Comparison of trends in Practice and Research for Deep Learning
- intro:activation functions
- arXiv:[https://arxiv.org/abs/1811.03378](https://arxiv.org/abs/1811.03378)

### Mish: A Self Regularized Non-Monotonic Neural Activation Function
- intro:activation functions
- arXiv:[https://arxiv.org/abs/1908.08681](https://arxiv.org/abs/1908.08681)

### Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
- intro:batch norm
- arXiv:[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)

### Dropout: A Simple Way to Prevent Neural Networks from Overfitting
- intro:dropout
- pdf:[http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf](http://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)

### 
- intro:
- arXiv:[]()


# CNN Architectures

### Gradient-based learning applied to document recognition
- intro:LeNet5
- pdf:[http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)

### ImageNet Classification with Deep Convolutional Neural Networks
- intro:AlexNet
- pdf:[http://stanford.edu/class/cs231m/references/alexnet.pdf](http://stanford.edu/class/cs231m/references/alexnet.pdf)

### Visualizing and Understanding Convolutional Networks
- intro:ZFNet
- arXiv:[https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901)

### Network In Network
- intro:NiN
- arXiv:[https://arxiv.org/abs/1312.4400](https://arxiv.org/abs/1312.4400)

### Very Deep Convolutional Networks for Large-Scale Image Recognition
- intro:VGG
- arXiv:[https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556)

### Going Deeper with Convolutions
- intro:GoogLeNet
- arXiv:[https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)

### Highway Networks
- intro:Highway Networks
- arXiv:[https://arxiv.org/abs/1505.00387](https://arxiv.org/abs/1505.00387)

### Rethinking the Inception Architecture for Computer Vision
- intro:Inception-V3
- arXiv:[https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)

### Deep Residual Learning for Image Recognition
- intro:ResNet
- arXiv:[https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385)

### Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning
- intro:Inception-V4, Inception-ResNet
- arXiv:[https://arxiv.org/abs/1602.07261v1](https://arxiv.org/abs/1602.07261v1)

### Deep Networks with Stochastic Depth
- intro:Stochastic ResNet
- arXiv:[https://arxiv.org/abs/1603.09382](https://arxiv.org/abs/1603.09382)

### Wide Residual Networks
- intro:WideResNet
- arXiv:[https://arxiv.org/abs/1605.07146](https://arxiv.org/abs/1605.07146)

### Densely Connected Convolutional Networks
- intro:DenseNet
- arXiv:[https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)

### Xception: Deep learning with depthwise separable convolutions
- intro:Xception
- arXiv:[https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)

### Deep Pyramidal Residual Networks
- intro:Pyramidal ResNet
- arXiv:[https://arxiv.org/abs/1610.02915](https://arxiv.org/abs/1610.02915)

### Aggregated Residual Transformations for Deep Neural Networks
- intro:ResNeXt
- arXiv:[https://arxiv.org/abs/1611.05431](https://arxiv.org/abs/1611.05431)

### Squeeze-and-Excitation Networks
- intro:SENet
- arXiv:[https://arxiv.org/abs/1709.01507](https://arxiv.org/abs/1709.01507)

### A New Channel Boosted Convolutional Neural Network using Transfer Learning
- intro:Channel Boosted CNN
- arXiv:[https://arxiv.org/abs/1804.08528v4](https://arxiv.org/abs/1804.08528v4)

### Competitive Inner-Imaging Squeeze and Excitation for Residual Network
- intro:Competitive SENet
- arXiv:[https://arxiv.org/abs/1807.08920v4](https://arxiv.org/abs/1807.08920v4)

### 
- intro:
- arXiv:[]()

### 
- intro:
- arXiv:[]()

### 
- intro:
- arXiv:[]()

# CNN Benchmarks

### ImageNet Large Scale Visual Recognition Challenge
- intro:ILSVRC
- arXiv:[https://arxiv.org/abs/1409.0575](https://arxiv.org/abs/1409.0575)