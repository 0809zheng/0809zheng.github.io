---
layout: post
title: '局部加权线性回归(Local Weighted Linear Regression)'
date: 2020-03-31
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/639da58cb1fccdcd3679836d.jpg'
tags: 机器学习
---

> Local Weighted Linear Regression.

在标准的线性回归中，所有参与训练的样本点权重都是相同的(视为$1$)。
而**局部加权线性回归(Local Weighted Linear Regression)**是指根据实际提供的测试样本，为每个训练样本赋予不同的权重，然后再计算回归参数。


局部加权线性回归模型的形式与线性回归相同，可以表示为矩阵形式：记样本矩阵$X \in \Bbb{R}^{N×d}$，标签向量$y\in \Bbb{R}^{N}$，待求解权重参数$w \in \Bbb{R}^{d}$，预测结果$\hat{y} \in \Bbb{R}^{N}$，则：

$$ \hat{y} = Xw $$

若给定测试样本$x^{(j)}$，计算测试样本与每个训练样本$x_i$之间的相似性，然后构造权重矩阵：

$$ C^{(j)} = \begin{bmatrix} c_1^{(j)} & & & \\ & c_2^{(j)} & & \\ & & \cdots & \\ & & & c_N^{(j)} \end{bmatrix} $$

其中$c_i^{(j)}$表示训练样本$x_i$和测试样本$x^{(j)}$之间的相似度，通过径向基函数构造：

$$ c_i^{(j)} = \exp(-\frac{||x^{(j)}-x_i||_2^2}{2k^2}) $$

其中超参数$k$越小，有效参与计算的训练样本越少；超参数$k$越大，有效参与计算的训练样本越多。

局部加权线性回归将权重矩阵$C$加入到损失函数中，用于加权调整每个训练样本的损失：

$$ L(w) = \sum_{i=1}^N c_i^{(j)}(\hat{y}_i-y_i)^2 = C^{(j)} (Xw-y)^T(Xw-y) $$

该目标函数是凸函数，可以直接求梯度令其为零，得到全局最小解：

$$ \nabla_wL(w) = \nabla_w C^{(j)}  (Xw-y)^T(Xw-y) = 2 X^TC^{(j)}Xw - 2 XC^{(j)}y = 0 $$

整理得到：

$$  X^TC^{(j)}Xw =  XC^{(j)}y $$

可以得到权重参数的解析解：

$$ w = (X^TC^{(j)}X)^{-1} XC^{(j)} y $$

```python
def local_weight_LR(test_point, X, Y, k=1.0):
    # 构建权重矩阵
    diff = np.tile(test_point, [N,1]) - X
    C = np.exp(-np.sum(diff**2, axis=1)/(2*k**2))
    C = np.diag(C)
    XTCX = np.dot(np.dot(X.T, C), X)
    XCY = np.dot(np.dot(X, C), Y)
    return np.dot(np.linalg.inv(XTCX), XCY)
```

下图给出了超参数$k$取值不同时，局部加权线性回归的拟合结果。如果$k$取值较大，则训练样本的权重分布越平均(当$k \to +\infty$退化为线性回归)，容易出现欠拟合；如果$k$取值较小，则仅有一小部分样本用于学习过程，容易出现过拟合。

![](https://pic.imgdb.cn/item/639da58cb1fccdcd3679836d.jpg)






