---
layout: post
title: '集成学习中的Bagging(Bootstrap Aggregation)方法'
date: 2020-03-17
author: 郑之杰
cover: 'https://pic.downk.cc/item/5eda28cac2a9a83be55e3374.jpg'
tags: 机器学习
---

> An Ensemble Learning Method：Bagging.

**Bagging(Bootstrap Aggregation)**是一种集成学习的方法，主要关注如何获得不同的训练模型。
若在数据集上一共训练了$T$个模型$$\{g_t,t=1,2,...,T\}$$；如何得到这些模型呢？可以采取的方法有：
- 选择不同的模型假设空间；
- 设置不同的训练超参数；
- 由算法的随机性得到（如感知机）；
- 选择不同的训练样本集。

**Bagging**通过不同的训练集生成不同的模型。具体地，先通过**bootstrapping**方法从已有数据集中生成若干子数据集，在每个子数据集上训练模型，再将模型集成起来，对于分类任务可以使用简单投票法，对于回归任务可以使用简单平均法。

**Bagging**主要关注降低**方差**，因此在决策树、神经网络等易受样本扰动的的学习器上效果更明显。**Bagging**方法适用于模型复杂但容易拟合的情形，如用决策树构造[随机森林](https://0809zheng.github.io/2020/03/20/random-forest.html)。

### ⚪ 自助法 Bootstrapping
**自助法(Bootstrapping)**又称为自举法或拔靴法，名称来源为“**pulling yourself up by your own bootstraps**”，即“自力更生”；是以统计学中的**自助采样(bootstrapping sampling)**为基础。

对于具有$N$个样本的训练集$\mathcal{D}$，**有放回**的随机采样$N$次，得到包含$N$个样本的数据集$\mathcal{D}'$，将该数据集作为训练集；$\mathcal{D}$中有一部分样本会在$\mathcal{D}'$中多次出现，而另一部分样本不会出现。
数据集$\mathcal{D}'$恰好能还原训练集$\mathcal{D}$的概率是$\frac{N!}{N^N}$。
某一个样本在$N$次采样中始终不会被采集到的概率是：

$$ \mathop{lim}_{N→∞}(1-\frac{1}{N})^N=\frac{1}{e} $$

因此自助法采样后大约有$\frac{1}{e}≈36.8\%$的样本没有出现在数据集$\mathcal{D}'$中。

### ⚪ 包外估计 Out-Of-Bag Estimate
通过自助法在训练集$\mathcal{D}$采样后大约有$36.8\%$的样本没有出现在数据集$\mathcal{D}'$中，这一部分样本($\mathcal{D}-\mathcal{D}'$)可以作为验证集对泛化性能进行**包外估计(out-of-bag estimate)**。以分类为例，则包外估计泛化误差为：

$$ \epsilon^{oob} = \frac{1}{\mathcal{D}-\mathcal{D}'} \sum_{(x,y) \in \mathcal{D}-\mathcal{D}'}^{} \Bbb{I}(f(x)≠y) $$

包外估计也是减缓过拟合的方法。当基学习器是决策树时，包外估计可以辅助**剪枝**；当基学习器是神经网络时，包外估计可以辅助**early stopping**以减少过拟合的风险。
