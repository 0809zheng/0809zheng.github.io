---
layout: post
title: '深度信念网络'
date: 2020-04-16
author: 郑之杰
cover: 'https://pic.downk.cc/item/5e97dbe0c2a9a83be538717b.jpg'
tags: 机器学习
---

> Deep Belief Networks.

**本文目录**：
1. 模型介绍
2. 生成
3. 学习

# 1. 模型介绍
**深度信念网络（Deep Belief Network，DBN）**是一种深层的概率有向图模型，图结构由多层的节点构成，特点如下：
1. 每层节点的内部没有连接，相邻两层的节点之间为**全连接**；
2. 最底层为**可观测变量**，其他层节点都为**隐变量**；
3. 最顶部的两层间的连接是**无向**的(可以看作受限玻尔兹曼机)，其他层之间的连接是**有向**的(可以看作Sigmoid信念网络)。

一个4层的深度信念网络如下图所示：

![](https://pic.downk.cc/item/5e97d0bec2a9a83be531bd68.jpg)

对于有$L$层隐变量的深度信念网络，令$v$表示最底层的可观测变量，$$h^{(1)},...h^{(L)}$$表示其余每层的隐变量。

除了顶部两层，每一层变量$$h^{(l)}(\text{记}v=h^{(0)})$$依赖于上一层$$h^{(l+1)}$$，即：

$$ p(h^{(l)} \mid h^{(l+1)},...,h^{(L)}) = p(h^{(l)} \mid h^{(l+1)}) $$

深度信念网络的联合概率分布如下：

$$ p(v,h^{(0)},...,h^{(L)}) = p(v \mid h^{(1)}) (\prod_{l=1}^{L-2} {p(h^{(l)} \mid h^{(l+1)})}) p(h^{(L-1)},h^{(L)}) $$

其中$$p(h^{(l)} \mid h^{(l+1)})$$为Sigmoid型条件概率分布：

$$ p(h^{(l)} \mid h^{(l+1)}) = Sigmoid(a^{(l)} + W^{(l+1)}h^{(l+1)}) $$

其中$$a^{(l)}$$是偏置参数，$$W^{(l+1)}$$是权重参数。

尽管深度信念网络作为一种深度学习模型已经很少使用，但其在深度学习发展进程中的贡献十分巨大，并且其理论基础为概率图模型，有非常好的解释性，依然是一种值得深入研究的模型。

# 2. 生成
**生成**任务是指给定模型的参数，生成符合特定分布的样本。

1. 在生成样本时，首先在最顶两层进行足够多次的Gibbs采样，生成$$h^{(L-1)}$$；
2. 然后依次计算下一层隐变量的分布。因为在给定上一层变量取值时，下一层的变量是条件独立的，所以可以独立采样；
3. 可以从第$L-1$层开始，自顶向下进行逐层采样，最终得到可观测层的样本。

# 3. 学习
**学习**任务是指根据一组可观测数据，学习模型的参数。

深度信念网络的学习过程可以分为**逐层预训练**和**精调**两个阶段。

### (1). 逐层预训练
**逐层预训练(layerwise pretraining)**是指将每一层的Sigmoid信念网络转换为受限玻尔兹曼机，自下而上依次训练每一层的受限玻尔兹曼机。

![](https://pic.downk.cc/item/5e97d6fec2a9a83be535646e.jpg)

### (2). 精调
**精调（fine-tuning）**是根据具体的任务对预训练的模型进行全局学习。

除了顶层的受限玻尔兹曼机，其他层之间的权重可以被分成向上的**认知权重（Recognition Weight）**$$W'$$和向下的**生成权重（Generative Weight）**$W$，认知权重用来计算后验概率，而生成权重用来定义模型。

深度信念网络一般采用**Contrastive Wake-Sleep算法**进行精调：
1. **Wake阶段**：认知过程，通过外界输入（可观测变量）和向上的认知权重，计算每一层隐变量的后验概率并采样。然后，修改下行的生成权重使得下一层的变量的后验概率最大；
2. **Sleep阶段**：生成过程，通过顶层的采样和向下的生成权重，逐层计算每一层的后验概率并采样。然后，修改向上的认知权重使得上一层变量的后验概率最大；
3. 交替进行Wake和Sleep过程，直到收敛。