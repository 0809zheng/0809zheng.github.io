---
layout: post
title: '多维缩放'
date: 2021-07-28
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/610107145132923bf86e6723.jpg'
tags: 机器学习
---

> Multiple Dimensional Scaling.

**多维缩放(Multiple Dimensional Scaling, MDS)**是一种常用的线性降维方法，其基本思想是**原始空间中样本之间的距离在低维空间得以保持**。

假设在维度为$d$的原始空间中的$N$个样本表示为$X \in \Bbb{R}^{d \times N}$，目标是获得样本在$k$维空间中的降维表示$Z \in \Bbb{R}^{k \times N}$，且$k<d$。为了便于讨论，令降维后的样本被中心化，即$\sum_{i=1}^{N}z_i=0$。

记样本在原始空间中的(欧式)距离矩阵$D \in \Bbb{R}^{N \times N}$，其$i$行$j$列元素$d_{ij}$表示样本$x_i$到$x_j$的距离。**多维缩放**要求任意两个样本$x_i$,$x_j$在降维后的空间中的欧式距离等于原始空间中的距离，即$\|\|z_i-z_j\|\|=d_{ij}$。

若记降维后的样本的内积矩阵$B=Z^TZ \in \Bbb{R}^{N \times N}$，其中$b_{ij}=z_i^Tz_j$，则上述距离约束可以表示为

$$ d_{ij}^2 = ||z_i||^2+||z_j||^2-2z_i^Tz_j = b_{ii}+b_{jj}-2b_{ij} $$

注意到$\sum_{i=1}^{N}z_i=0$，因此有$\sum_{i=1}^{N}b_{ij}=\sum_{j=1}^{N}b_{ij}=0$，并记矩阵$B$的迹$\text{tr}(B)=\sum_{i=1}^{N}b_{ii}=\sum_{i=1}^{N}\|\|z_i\|\|^2$，则有：

$$ \sum_{i=1}^{N}d_{ij}^2 = \sum_{i=1}^{N}b_{ii}+\sum_{i=1}^{N}b_{jj}-2\sum_{i=1}^{N}b_{ij} = \text{tr}(B)+Nb_{jj} $$

$$ \sum_{j=1}^{N}d_{ij}^2 = \sum_{j=1}^{N}b_{ii}+\sum_{j=1}^{N}b_{jj}-2\sum_{j=1}^{N}b_{ij} = Nb_{ii}+\text{tr}(B) $$

$$ \sum_{i=1}^{N}\sum_{j=1}^{N}d_{ij}^2 = \sum_{i=1}^{N}\sum_{j=1}^{N}b_{ii}+\sum_{i=1}^{N}\sum_{j=1}^{N}b_{jj}-2\sum_{i=1}^{N}\sum_{j=1}^{N}b_{ij} = 2N\text{tr}(B) $$

若记$d_{i\cdot}=\frac{1}{N}\sum_{j=1}^{N}d_{ij},d_{\cdot j}=\frac{1}{N}\sum_{i=1}^{N}d_{ij},d_{\cdot \cdot}=\frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N}d_{ij}$，则内积矩阵$B$中的元素$b_{ij}$计算为：

$$ b_{ij} = \frac{1}{2}(b_{ii}+b_{jj}-d_{ij}^2) \\ = \frac{1}{2}(\frac{1}{N}\sum_{j=1}^{N}d_{ij}^2-\frac{1}{N}\text{tr}(B)+\frac{1}{N}\sum_{i=1}^{N}d_{ij}^2-\frac{1}{N}\text{tr}(B)-d_{ij}^2) \\ = \frac{1}{2}(d_{i\cdot}+d_{\cdot j}-\frac{2}{N}\frac{1}{2N}\sum_{i=1}^{N}\sum_{j=1}^{N}d_{ij}^2-d_{ij}^2) \\ = \frac{1}{2}(d_{i\cdot}+d_{\cdot j}-d_{\cdot \cdot}-d_{ij}^2) $$

因此可根据距离矩阵$D$求得内积矩阵$B$。对内积矩阵$B$进行特征值分解：

$$ B=V \Lambda V^T, \quad \Lambda=\text{diag}(\lambda_1,\lambda_2,...,\lambda_d) $$

选择前$k$个最大的非零特征值构成对角矩阵$\tilde{\Lambda}=\text{diag}(\lambda_1,\lambda_2,...,\lambda_k)$，相应的特征向量矩阵为$\tilde{V}$，则可以求得样本的降维表示$Z$:

$$ Z = \tilde{\Lambda}^{\frac{1}{2}}\tilde{V}^T \in \Bbb{R}^{k \times N} $$

