---
layout: post
title: '极限学习机(Extreme Learning Machine, ELM)'
date: 2025-11-28
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/69295833b4e23f08080aafea.png'
tags: 机器学习
---

> Extreme Learning Machine.

- [Extreme learning machine: a new learning scheme of feedforward neural networks](https://ieeexplore.ieee.org/document/1380068)

神经网络是基于梯度下降和反向传播进行训练的。**极限学习机（Extreme Learning Machine, ELM）**是一种特殊的神经网络，对单隐藏层前馈神经网络进行了化简：输入层到隐藏层的权重直接随机生成；一旦输入权重被随机固定，整个神经网络就变成了一个线性系统，隐藏层到输出层的权重可以通过求解一个线性方程组的最小二乘解来一步解析得到，从而避免了耗时且易陷入局部最优的迭代优化过程。

**ELM**的训练速度可以比传统的反向传播（**BP**）算法快上千倍，并且在许多基准回归和分类任务上取得了与**BP**和**SVM**相当甚至更好的泛化性能。**ELM**的解是所有最小二乘解中范数最小的，更小的权重范数通常意味着更好的泛化能力。

# 1.  前馈神经网络

给定$N$个训练样本$(x_i, t_i)$，其中$x_i \in \mathbb{R}^n$是输入特征，$t_i \in \mathbb{R}^m$是目标输出。一个有$L$个隐藏层神经元的单隐藏层前馈神经网络可以被数学地表示为：

$$ \sum_{i=1}^{L} \beta_i g(w_i \cdot x_j + b_i) = o_j, \quad j = 1, \dots, N $$

-   $w_i \in \mathbb{R}^n$: 连接第$i$个隐藏神经元和输入层的输入权重。
-   $b_i \in \mathbb{R}$: 第$i$个隐藏神经元的偏置。
-   $\beta_i \in \mathbb{R}^m$: 连接第$i$个隐藏神经元和输出层的输出权重。
-   $g(\cdot)$: 激活函数，如**Sigmoid**函数。
-   $o_j$: 网络对输入$x_j$的预测输出。

训练一个前馈神经网络的目标，就是找到一组最优的$w_i, b_i, \beta_i$，使得预测输出$o_j$与真实目标$t_j$的误差最小，即 $\sum_{j=1}^N \|o_j - t_j\| \approx 0$。

我们可以将上述$N$个方程写成一个紧凑的矩阵形式：

$$ H\beta = T $$

其中：
-   **H (隐藏层输出矩阵)**:矩阵$H$的第$i$行第$j$列，代表了输入样本$i$在第$j$个隐藏神经元上的输出。

$$ H = \begin{pmatrix} g(w_1 \cdot x_1 + b_1) & \cdots & g(w_L \cdot x_1 + b_L) \\ \vdots & \ddots & \vdots \\ g(w_1 \cdot x_N + b_1) & \cdots & g(w_L \cdot x_N + b_L) \end{pmatrix}_{N \times L} $$
    
-   **β (输出权重矩阵)**:

$$ \beta = \begin{pmatrix} \beta_1^T \\ \vdots \\ \beta_L^T \end{pmatrix}_{L \times m} $$

-   **T (目标输出矩阵)**:

$$ T = \begin{pmatrix} t_1^T \\ \vdots \\ t_N^T \end{pmatrix}_{N \times m} $$

在传统的**BP**算法中，我们需要通过迭代优化来同时求解$w$、$b$和$β$。

# 2. ELM模型

**ELM**完全抛弃了前馈神经网络复杂的迭代学习过程；**ELM**的学习过程没有迭代，没有梯度，没有学习率，只有一次随机初始化和一次矩阵运算：
1. 输入权重**随机初始化**: 随机地生成输入权重$w_i$和隐藏层偏置$b_i (i = 1, ..., L)$，一旦生成这些参数就被固定，不再进行任何调整。由于$w$和$b$被固定了，对于给定的训练集$X$，隐藏层输出矩阵$H$变成了一个确定的矩阵。
2. 输出权重**解析求解**: 在$H$已知后，训练神经网络转化成了一个求解线性方程组的问题：
   
$$ H\beta = T $$

我们现在只需要找到一个$β$，使得$Hβ$尽可能地接近$T$。这正是一个经典的线性最小二乘问题。根据线性代数理论，这个最小二乘问题的最优解（特指**范数最小**的解）可以通过**Moore-Penrose广义逆**（记为$H⁺$）来直接求得：

$$ \hat{\beta} = H^\dagger T $$

用训练好的**ELM**进行预测时，对于新的输入$x_{new}$，预测输出为：

$$ y_{new} = h(x_{new})\hat{\beta} $$

其中，$h(x_{new}) = [g(w_1 \cdot x_{new} + b_1), \dots, g(w_L \cdot x_{new} + b_L)]$ 是新样本的隐藏层输出向量。

# 3. 实验分析

作者在一系列回归和分类任务上，将**ELM**与当时主流的**BP**算法和**SVM**进行了对比。

### 3.1 加州房价预测（回归任务）

采用的数据集是**California Housing**，包含20640个样本，8个输入特征，1个输出目标。

速度上**ELM**的训练时间为**0.272**秒。**BP**算法为**295.23**秒。**SVM**为**558.41**秒。**ELM**比**BP**快了**1085**倍，比**SVM**快了**2053**倍。

性能上**ELM**的测试集均方根误差（**RMSE**）为**0.1365**，与**BP（0.1426）**和**SVM（0.1275）**非常接近。

![](https://pic1.imgdb.cn/item/69295eb8b4e23f08080ac102.png)

在这个中等规模的数据集上，**ELM**以几个数量级的速度优势，取得了与精细调优的传统方法几乎相当的性能。这充分展示了其“极限”的效率。

### 3.2 Pima印第安人糖尿病诊断（分类任务）

**Pima**数据集是一个经典的医疗诊断二分类问题，包含768个样本。

速度上**ELM**训练时间为**0.015**秒。**BP**为**16.196**秒。**SVM**为**0.186**秒。**ELM**比**BP**快了近**1000**倍，比**SVM**快了**12**倍。

性能上**ELM**的测试准确率为**76.54%**。这不仅远高于**BP（63.45%）**，而且与精心调参的**SVM（77.31%）**非常接近，并且优于文献中报道的其他多种算法（如**bagging, boosting, C4.5**等）。

![](https://pic1.imgdb.cn/item/69295fcdb4e23f08080ac14c.png)
![](https://pic1.imgdb.cn/item/692960117f8e1c737822decb.png)

在这个小型数据集上，**ELM**再次展现了其速度优势，并且在分类性能上甚至超越了**BP**，达到了**SOTA**级别。

### 3.3 森林覆盖类型预测（大规模复杂分类任务）

**Forest Cover Type**是一个包含581,012个样本和54个特征的大规模数据集。作者将其简化为二分类问题。

速度上训练时间：**ELM**为**1.6**分钟，而**SVM**需要**693.6**分钟（近**12**个小时）。**ELM**快了超过**430**倍。测试时间：**ELM**对近**50**万测试样本的预测耗时不到**1**分钟，而**SVM**耗时超过**5.5**小时。

性能上**ELM**的测试准确率为**90.21%**，优于**SVM**的**89.90%**。

![](https://pic1.imgdb.cn/item/6929608eb4e23f08080ac164.png)

这是对**ELM**最有力的证明。在这个大规模、复杂的真实世界问题上，**ELM**不仅在训练和测试速度上都取得了压倒性的优势，在泛化性能上也实现了对**SVM**的超越。
