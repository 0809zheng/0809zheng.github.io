---
layout: post
title: '支持向量机'
date: 2020-03-14
author: 郑之杰
cover: 'https://pic.downk.cc/item/5ed75966c2a9a83be54cb0f5.jpg'
tags: 机器学习
---

> Support Vector Machine.

本文目录：
1. 线性支持向量机
2. 对偶支持向量机
3. 核支持向量机
4. 软间隔支持向量机
5. 序列最小最优化(SMO)算法
6. 概率支持向量机
7. 最小二乘支持向量机


# 1. 线性支持向量机
**线性支持向量机（Linear SVM）**也叫**硬间隔支持向量机（Hard-margin SVM）**。

### (1)问题分析
假设数据集$$\{(x^{(1)},y^{(1)}),...,(x^{(N)},y^{(N)})\}$$是线性可分的，其中$$x \in \Bbb{R}^d$$，$$y \in \{-1,+1\}$$。

特征空间中的一个超平面可以表示为$$w^Tx+b=0$$，希望能找到一个**分离超平面 separating hyperplane**对样本正确的分类：

$$ \begin{cases} w^Tx^{(i)}+b>0, & y^{(i)} = +1 \\ w^Tx^{(i)}+b<0, & y^{(i)} = -1 \end{cases} $$

这个条件记为：

$$ y^{(i)}(w^Tx^{(i)}+b) > 0, \quad i=1,2,...,N $$

仅仅区分样本是不够的，如果分离超平面距离某些样本点过近，当样本点受到噪声的扰动时，会使得分类结果出错，因此希望每一个样本点到该超平面得距离尽可能大：

![](https://pic.downk.cc/item/5ec4cf98c2a9a83be56e416f.jpg)

定义超平面到所有样本点的距离为**间隔 margin**，问题是找到间隔最大的超平面，下面解释如何求间隔。

### (2)函数间隔与几何间隔
称上面定义的$y^{(i)}(w^Tx^{(i)}+b)$为样本点到超平面的**函数间隔（functional margin）**，可以定性地判断样本点到超平面的距离。

超平面$$w^Tx+b=0$$中的$w$是其法向量，证明如下：

任取超平面上两点$x'$、$x''$，则满足$$w^Tx'+b=0$$、$$w^Tx''+b=0$$，两式做差得：

$$ w^T(x'-x'')=0 $$

即向量$w$正交于超平面。

空间中任意一点$x$到该超平面$$w^Tx+b=0$$的**距离 dis**是该点到超平面上任意一点$x'$的连线在法向量$w$方向上的投影：

![](https://pic.downk.cc/item/5ec4d1c3c2a9a83be57108ae.jpg)

$$ dis(x,w,b) = \mid\mid x-x' \mid cosθ \mid = \mid \frac{w^T}{\mid\mid w \mid\mid}(x-x') \mid = \frac{1}{\mid\mid w \mid\mid} \mid w^Tx+b \mid $$

为了去掉绝对值运算，上式修改为：

$$ dis(x,w,b) = \frac{1}{\mid\mid w \mid\mid} y(w^Tx+b) $$

定义超平面的**几何间隔（geometric margin）**为所有样本点到该超平面距离的最小值：

$$ margin(w,b) = min_{(i=1,...,N)}dis(x^{(i)},w,b) = min_{(i=1,...,N)} \frac{1}{\mid\mid w \mid\mid} y(w^Tx^{(i)}+b) $$

则线性支持向量机的问题列为：

$$ max \quad margin(w,b) \\ s.t. \quad y^{(i)}(w^Tx^{(i)}+b) > 0, \quad i=1,2,...,N $$

### (3)一些化简
注意到超平面$$w^Tx+b=0$$的系数$w$和$b$的数值可以放缩，不会影响该超平面，为简化计算，可通过选取合适的$w$和$b$使得：

$$ min_{(i=1,...,N)}y^{(i)}(w^Tx^{(i)}+b) = 1 $$

则原约束可以修改为：

$$ y^{(i)}(w^Tx^{(i)}+b) ≥ 1, \quad i=1,2,...,N $$

超平面的间隔为：

$$ margin(w,b) = \frac{1}{\mid\mid w \mid\mid} $$

目标函数变为：

$$ max \quad margin(w,b) = max \quad \frac{1}{\mid\mid w \mid\mid} $$

将目标函数转换为：

$$ max \quad \frac{1}{\mid\mid w \mid\mid} = min \quad \mid\mid w \mid\mid = min \quad \frac{1}{2}w^Tw $$

则线性支持向量机的问题转化为：

$$ min \quad \frac{1}{2}w^Tw \\ s.t. \quad y^{(i)}(w^Tx^{(i)}+b) ≥ 1, \quad i=1,2,...,N $$

观察这个最优化问题，特点：
1. 目标函数是二次函数
2. 约束条件是变量的一次函数

该问题是一个**凸二次规划（convex quadratic programming）**问题，可以直接使用对应的工具包求解：

![](https://pic.downk.cc/item/5ec4d895c2a9a83be57c7214.jpg)

由该方法得到的超平面称为这些样本点的**最大间隔分离超平面（large-margin separating hyperplane）**，若样本集线性可分，则该超平面是存在且唯一。

### (4)支持向量
线性支持向量机问题求解后，会有一些点$(x^{(i)},y^{(i)})$满足$$y^{(i)}(w^Tx^{(i)}+b) = 1$$，其余点$(x^{(j)},y^{(j)})$满足$$y^{(j)}(w^Tx^{(j)}+b) > 1$$,

采用反证法证明上述结论，即假设所有点均满足$$y^{(i)}(w^Tx^{(i)}+b) > 1$$，此时的$$w$$应满足$$min \quad \frac{1}{2}w^Tw$$，

对$w$和$b$同时缩小一点，仍然满足不等式，但打破了最小目标函数的条件，故假设不成立。

称满足$$y^{(i)}(w^Tx^{(i)}+b) = 1$$的样本点$(x^{(i)},y^{(i)})$为**支持向量（support vector）**，最大间隔分离超平面由支持向量唯一确定。

可以计算得到超平面的最大间隔为$\frac{2}{\mid\mid w \mid\mid}$。

![](https://pic.downk.cc/item/5ec8c491c2a9a83be5a0fce3.jpg)

# 2. 对偶支持向量机
在支持向量机问题中，数据集$$\{(x^{(1)},y^{(1)}),...,(x^{(N)},y^{(N)})\},x \in \Bbb{R}^d$$，即共有$N$个样本，其中每个样本的特征是$d$维的；

上一节得到的二次规划问题称为支持向量机的**原问题（prime problem）**，该优化问题有$d+1$个待求解的变量，有$N$个约束条件。

有时样本的特征维度远高于样本个数，即$d>>N$；当引入核方法之后，样本的特征维度将变成无限维，直接求解原问题是不可解的，引入**对偶支持向量机（dual SVM）**。

### (1)拉格朗日函数
原二次规划问题是一个求解参数$w,b$的**约束问题（constrained）**，通过引入**拉格朗日乘子 Lagrange Multiplier**将其转化成对参数$w,b$的**无约束问题（unconstrained）**。

记**拉格朗日函数（Lagrange function）**：

$$ L(w,b,α) = \frac{1}{2}w^Tw + \sum_{i=1}^{N} {α_i(1-y^{(i)}(w^Tx^{(i)}+b))}, \quad α_i≥0 $$

则将原问题转化为：

$$ min_{w,b} \quad max_{α} \quad L(w,b,α), \quad α_i≥0 $$

这个优化问题没有显式地给出对参数$w,b$的约束，但仍与原问题等价，分析如下：
- 对于满足原约束条件$$y^{(i)}(w^Tx^{(i)}+b) ≥ 1$$的$w,b$，为使拉格朗日函数最大需$α_i=0$，此时目标函数退化为$$\frac{1}{2}w^Tw$$;
- 对于使$$y^{(i)}(w^Tx^{(i)}+b) < 1$$的$w,b$，$α_i$可取$∞$，此时目标函数为$∞$；
- 最终的优化问题从上述两种情况中取较小者，即第一种情况，恰好就是原二次规划的问题。

![](https://pic.downk.cc/item/5ec61217c2a9a83be5640991.jpg)

### (2)拉格朗日对偶问题
由不等式：

$$ min_{w,b} \quad max_{α} \quad L(w,b,α) ≥ min_{w,b} \quad L(w,b,α) $$

上式左端恒大于等于右端，自然大于等于右端给定$α$的情况：

$$ min_{w,b} \quad max_{α} \quad L(w,b,α) ≥ max_{α} \quad min_{w,b} \quad L(w,b,α) $$

上式左端是引入拉格朗日函数的原问题，右端是其**拉格朗日对偶问题（Lagrange dual problem）**，具有**弱对偶关系（weak duality）**。

当优化问题是二次规划时，上式具有**强对偶关系（strong duality）**，即：

$$ min_{w,b} \quad max_{α} \quad L(w,b,α) = max_{α} \quad min_{w,b} \quad L(w,b,α) $$

最终得到对偶支持向量机的优化问题：

$$ max_{α} \quad min_{w,b} \quad \frac{1}{2}w^Tw + \sum_{i=1}^{N} {α_i(1-y^{(i)}(w^Tx^{(i)}+b))} \\ s.t. \quad α_i≥0 $$

### (3)求解对偶问题
对偶问题是$w,b$的无约束问题，令其偏导数为零：

$$ \frac{\partial L(w,b,α)}{\partial w} = 0 \\ \frac{\partial L(w,b,α)}{\partial b} = 0 $$

解得：

$$ w = \sum_{i=1}^{N} {α_iy^{(i)}x^{(i)}} \\ \sum_{i=1}^{N} {α_iy^{(i)}} = 0 $$

对拉格朗日函数进行化简：

$$ L(w,b,α) = \frac{1}{2}w^Tw + \sum_{i=1}^{N} {α_i(1-y^{(i)}(w^Tx^{(i)}+b))} \\ = \frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}{x^{(i)}}^Tx^{(j)}}} + \sum_{i=1}^{N} {α_i} - \sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}{x^{(i)}}^Tx^{(j)}}} - \sum_{i=1}^{N} {α_iy^{(i)}b} \\ = -\frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}{x^{(i)}}^Tx^{(j)}}} + \sum_{i=1}^{N} {α_i} $$

对偶问题转化为只与$α$有关的形式：

$$ max_{α} \quad -\frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}{x^{(i)}}^Tx^{(j)}}} + \sum_{i=1}^{N} {α_i} \\ s.t. \quad α_i≥0,i=1,...,N \\ \quad \quad \sum_{i=1}^{N} {α_iy^{(i)}} = 0 $$

该优化问题有$N$个待求解的变量，有$N+1$个约束条件。

这是一个二次规划问题，可以直接使用对应的工具包求解，注意其中的等式约束可以拆解成两个不等式约束：

![](https://pic.downk.cc/item/5ec61ac8c2a9a83be56e7964.jpg)

### (4)KKT条件
求解得到$α$后，可以用**KKT条件**得到$w,b$。

**KKT条件（Karush-Kuhn-Tucker condition）**是强对偶关系的等价条件，可用于求解优化问题；

对于对偶支持向量机问题，**KKT条件**包括：
- **原问题的约束条件**：$y^{(i)}(w^Tx^{(i)}+b) ≥ 1, \quad i=1,2,...,N$
- **对偶问题的约束条件**：$α_i≥0, \quad i=1,2,...,N$
- **拉格朗日函数的一阶导数为零**：

$$ w = \sum_{i=1}^{N} {α_iy^{(i)}x^{(i)}} \\ \sum_{i=1}^{N} {α_iy^{(i)}} = 0 $$

- **互补松弛条件（complementary slackness）**：

$$ α_i(1-y^{(i)}(w^Tx^{(i)}+b)) = 0, \quad i=1,2,...,N $$

当得到$α$后，可直接得到$w$:

$$ w = \sum_{i=1}^{N} {α_iy^{(i)}x^{(i)}} $$

由互补松弛条件，任取一个$$α_s≠0$$，则$$1-y^{(s)}(w^Tx^{(s)}+b) = 0$$，解得：

$$ b = y^{(s)}-w^Tx^{(s)} $$

称$$α_i≠0$$对应的样本点为**支持向量（support vector）**，支持向量机的模型参数仅由这些支持向量确定。

则支持向量机的判别函数为：

$$ y = sign(w^Tx+b) = sign(\sum_{i=1}^{N} {α_iy^{(i)}{x^{(i)}}^Tx} + y^{(s)} - \sum_{i=1}^{N} {α_iy^{(i)}{x^{(i)}}^Tx^{(s)}}) $$

# 3. 核支持向量机
对偶支持向量机把优化问题转化成$N$个待求解的变量、$N$个约束条件的问题，与样本的特征维度$d$无关。但是在求解问题时，需计算内积$${x^{(i)}}^Tx^{(j)}$$，仍需要$O(d)$的运算复杂度。

与此同时，线性支持向量机要求数据集是线性可分的，对于线性不可分的数据集，通常引入一个**特征变换**$$ z = φ(x) $$把输入空间映射为特征空间，此时的问题变为：

$$ max_{α} \quad -\frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}{φ(x^{(i)})}^Tφ(x^{(j)})}} + \sum_{i=1}^{N} {α_i} \\ s.t. \quad α_i≥0,i=1,...,N \\ \quad \quad \sum_{i=1}^{N} {α_iy^{(i)}} = 0 $$

问题的解：

$$ w = \sum_{i=1}^{N} {α_iy^{(i)}φ(x^{(i)})} $$

$$ b = y^{(s)}-w^Tx^{(s)} = y^{(s)}-\sum_{i=1}^{N} {α_iy^{(i)}φ(x^{(i)})^Tφ(x^{(s)})} $$

则支持向量机的判别函数为：

$$ y = sign(w^Tz+b) = sign(\sum_{i=1}^{N} {α_iy^{(i)}φ(x^{(i)})^Tφ(x)} + y^{(s)} - \sum_{i=1}^{N} {α_iy^{(i)}φ(x^{(i)})^Tφ(x^{(s)})}) $$

当特征空间维度较高时，求解特征变换以及特征变换的内积运算困难，因此引入核技巧。

### (1)核技巧
在求解支持向量机问题中，不显式地计算特征变换以及特征变换的内积，这种方法叫做**核技巧（kernel trick）**。

定义**核函数（kernel function）**：

$$ K(x,x') = φ(x)^Tφ(x') $$

把计算特征变换以及特征变换的内积转化为计算一个函数的值，则**核支持向量机**问题：

$$ max_{α} \quad -\frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}K(x^{(i)},x^{(j)})}} + \sum_{i=1}^{N} {α_i} \\ s.t. \quad α_i≥0,i=1,...,N \\ \quad \quad \sum_{i=1}^{N} {α_iy^{(i)}} = 0 $$

最终的判别函数为：

$$ y = sign(w^Tz+b) = sign(\sum_{i=1}^{N} {α_iy^{(i)}K(x^{(i)},x)} + y^{(s)} - \sum_{i=1}^{N} {α_iy^{(i)}K(x^{(i)},x^{(s)})}) $$

### (2)核函数
**正定核函数（positive definite kernel function）**的充要条件：
1. $K(x,x')$是对称函数，即$K(x,x')=K(x',x)$
2. 对任意$$x=\{x_1,x_2,...,x_d\}^T$$，$Gram$矩阵$K=\[K(x_i,x_j)\]_{d×d}$是半正定矩阵。




下面介绍几个常用的核函数：

**1. 线性核（linear kernel）**

线性核函数定义为：

$$ K(x,x') = x^Tx' $$

此时模型退化为线性支持向量机，其优缺点如下：
- 优点：模型简单、速度快、可解释性好；
- 缺点：无法解决线性不可分的数据集。

**3. 二项式核（binomial kernel）**

对于输入样本$$x = (x_1,x_2,...,x_d)^T$$，定义二阶多项式变换得到的特征向量：

$$ z = φ(x) = (1,x_1,...,x_d,x_1^2,x_1x_2,...,x_d^2)^T $$

由核函数的定义：

$$ K(x,x') = φ(x)^Tφ(x') = 1 + \sum_{i=1}^{d} {x_i{x'}_i} + \sum_{i=1}^{d} {\sum_{j=1}^{d} {x_i{x'}_ix_j{x'}_j}} \\ = 1 + x^Tx' + (x^Tx')^2 $$

对特征变换进行一些修改，可以得到对应的核函数：
- 若特征变换为$ φ(x) = (1,\sqrt{2}x_1,...,\sqrt{2}x_d,x_1^2,x_1x_2,...,x_d^2)^T $，则核函数为$ K(x,x') = (1+x^Tx')^2 $
- 若特征变换为$ φ(x) = (1,\sqrt{2γ}x_1,...,\sqrt{2γ}x_d,γx_1^2,γx_1x_2,...,γx_d^2)^T $，则核函数为$ K(x,x') = (1+γx^Tx')^2, \quad γ>0 $
- 若特征变换为$ φ(x) = (ζ,\sqrt{2ζγ}x_1,...,\sqrt{2ζγ}x_d,γx_1^2,γx_1x_2,...,γx_d^2)^T $，则核函数为$ K(x,x') = (ζ+γx^Tx')^2, \quad ζ≥0,γ>0 $

上面的这些变化对应的特征空间是相同的，但内积是不同的，最终得到的“最大间隔超平面”也是不同的。

![](https://pic.downk.cc/item/5ec7a3abc2a9a83be5cc7079.jpg)

**3. 多项式核（polynomial kernel）**

扩展二项式核得到多项式核：

$$ K(x,x') = (ζ+γx^Tx')^q, \quad ζ≥0,γ>0 $$

其优缺点如下：
- 优点：模型非线性；可以控制阶数$q$；
- 缺点：(由于幂运算)数值稳定性差；需要选择三个超参数。

**4. 高斯核（Gaussian kernel）**

高斯核又叫**径向基函数（radial basis function，rbf）**，是一种无限维度的特征变换。

由核函数分析其特征变换，其中用到了$Taylor$展开：

$$ K(x,x') = exp(-(x-x')^2) = exp(-x^2)exp(-{x'}^2)exp(2xx') \\ = exp(-x^2)exp(-{x'}^2) \sum_{n=0}^{∞} {\frac{(2xx')^n}{n!}} \\ = \sum_{n=0}^{∞} {exp(-x^2)exp(-{x'}^2) \sqrt{\frac{2^n}{n!}} \sqrt{\frac{2^n}{n!}} x^n{x'}^n} \\ = φ(x)^Tφ(x') $$

可以得到特征变换：

$$ z = φ(x) = exp(-x^2)(1,\sqrt{\frac{2^1}{1!}} x^1,\sqrt{\frac{2^2}{2!}} x^2,...,\sqrt{\frac{2^n}{n!}} x^n,...)^T $$

广义的高斯核函数为：

$$ K(x,x') = exp(-γ \mid\mid x-x' \mid\mid^2) $$

其中超参数$γ$控制高斯函数的“方差”，取值越大，径向基函数的“径向”越小，模型越容易过拟合：

![](https://pic.downk.cc/item/5ec7a799c2a9a83be5d2284b.jpg)

当超参数$γ$趋于无穷大时，高斯核趋近于：

$$ K(x,x') = \begin{cases} 0, & x≠x' \\ 1, & x=x' \end{cases} $$

高斯核的优缺点如下：
- 优点：模型更强大；数值稳定性好；只需选择一个超参数；
- 缺点：可解释性差；计算速度慢；容易过拟合。

# 4. 软间隔支持向量机
之前介绍的都是**硬间隔（hard margin）**支持向量机，在输入空间或特征空间中寻找一个最大间隔超平面。

对于线性不可分的数据，也可以适当放松条件，允许一些样本点不满足间隔条件甚至被错误分类，这种方法称为**软间隔（soft margin）**支持向量机。

定义**松弛变量（容忍度）**对每个样本点到分离超平面的最小距离放松$ξ_i$，并使这些放松条件尽可能小：

$$ min \quad \frac{1}{2}w^Tw + C\sum_{i=1}^{N} {ξ_i} \\ s.t. \quad y^{(i)}(w^Tx^{(i)}+b) ≥ 1-ξ_i, \quad i=1,2,...,N \\ \quad \quad ξ_i ≥ 0, \quad i=1,2,...,N $$

超参数**惩罚系数**$C$权衡间隔和分类错误点。
- $C$越大，样本点到超平面的距离越大，分类错误的点数减少，最大间隔减小；
- $C$越小，样本点到超平面的距离限制减小，最大间隔增大，但可能分类错误的点数增加。

### (1)对偶问题
软间隔支持向量机的优化问题也是二次规划问题，引入拉格朗日函数：

$$ L(w,b,ξ,α,β) = \frac{1}{2}w^Tw + C\sum_{i=1}^{N} {ξ_i} + \sum_{i=1}^{N} {α_i(1-ξ_i-y^{(i)}(w^Tx^{(i)}+b))} + \sum_{i=1}^{N} {-β_iξ_i}, \quad α_i≥0,β_i≥0 $$

则对偶问题可写作：

$$ max_{α,β} \quad min_{w,b,ξ} \quad L(w,b,ξ,α,β) \\ s.t. \quad α_i≥0,β_i≥0,i=1,2,...,N $$

令$$\frac{\partial L(w,b,ξ,α,β)}{\partial ξ_i} = 0$$，得：

$$ C - α_i - β_i = 0 $$

把$$β_i = C - α_i$$代入对偶问题并化简得：

$$ max_{α} \quad min_{w,b,ξ} \quad \frac{1}{2}w^Tw + C\sum_{i=1}^{N} {ξ_i} + \sum_{i=1}^{N} {α_i(1-ξ_i-y^{(i)}(w^Tx^{(i)}+b))} + \sum_{i=1}^{N} {-(C - α_i)ξ_i} \\ = \frac{1}{2}w^Tw + \sum_{i=1}^{N} {α_i(1-y^{(i)}(w^Tx^{(i)}+b))} \\ s.t. \quad C≥α_i≥0,i=1,2,...,N $$

化简该对偶问题：

$$ max_{α} \quad -\frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}{x^{(i)}}^Tx^{(j)}}} + \sum_{i=1}^{N} {α_i} \\ s.t. \quad 0≤α_i≤C,i=1,...,N \\ \quad \quad \sum_{i=1}^{N} {α_iy^{(i)}} = 0 $$

该优化问题有$N$个待求解的变量，有$2N+1$个约束条件；与硬间隔支持向量机问题相比，仅仅是对$α$多了上界条件。

通过二次规划方法或$SMO$算法（见第5章）解得$α_i$后，利用KKT条件求解$w$和$b$：

$$ w = \sum_{i=1}^{N} {α_iy^{(i)}x^{(i)}} $$

由互补松弛条件：
- $α_i(1-ξ_i-y^{(i)}(w^Tx^{(i)}+b))=0$
- $-β_iξ_i = (C - α_i)ξ_i = 0$

任选一个满足$0<α_s<C$的支持向量$α_s$，使得：

$$ b = y^{(s)}-w^Tx^{(s)} = y^{(s)}-\sum_{i=1}^{N} {α_iy^{(i)}{x^{(i)}}^Tx^{(s)}} $$

### (2)几何解释
根据$α_i$的取值不同，样本点可分为三类：
- $α_i=0$：此时$ξ_i = 0$，对应最大间隔超平面完全分类正确的样本点；
- $0<α_i<C$：此时$ξ_i = 0$，$y^{(i)}(w^Tx^{(i)}+b)=1$，对应恰好在间隔边界上的点，称为**free support vector**，对应图中正方形框的点；
- $α_i=C$：此时$ξ_i ≠ 0$，$y^{(i)}(w^Tx^{(i)}+b)=1-ξ_i$，对应放松了间隔条件的点，称为**bounded support vector**，对应图中三角形框的点，此时若$ξ_i < 1$则样本点分类正确，$ξ_i > 1$则样本点分类错误。

![](https://pic.downk.cc/item/5ec8b980c2a9a83be592f424.jpg)

### (3)超参数C的选择
通常用**验证集**选择超参数$C$。

对于$N$个样本采用**留一交叉验证（leave-one-out cross validation）**，其验证集误差满足以下不等式：

$$ error_{loocv} ≤ \frac{\#SV}{N} $$

此时验证误差不超过样本集中支持向量所占的比例，证明如下：
- 若其中某个样本点是支持向量，将其划分出来作为验证集时，会使最终超平面改变，此时该点可能被错误分类，故误差$≤1$；
- 若其中某个样本点不是支持向量，将其划分出来作为验证集时，对最终超平面没有影响，此时该点仍会被分类正确，故误差$=0$。

故支持向量的个数一定程度上可以作为超参数选择的依据：

![](https://pic.downk.cc/item/5ec8beb9c2a9a83be59a73f1.jpg)

### (4)合页损失函数
软间隔支持向量机的优化问题：

$$ min \quad \frac{1}{2}w^Tw + C\sum_{i=1}^{N} {ξ_i} \\ s.t. \quad y^{(i)}(w^Tx^{(i)}+b) ≥ 1-ξ_i, \quad i=1,2,...,N \\ \quad \quad ξ_i ≥ 0, \quad i=1,2,...,N $$

分析约束条件，$ξ_i$描述的是样本点$(x^{(i)},y^{(i)})$到分离间隔$y(w^Tx+b)=1$的距离：

$$ ξ_i = \begin{cases} 0, & y^{(i)}(w^Tx^{(i)}+b)≥1 \\ = 1-y^{(i)}(w^Tx^{(i)}+b), & y^{(i)}(w^Tx^{(i)}+b)<1 \end{cases} $$

将上式表达为：

$$ ξ_i = max(1-y^{(i)}(w^Tx^{(i)}+b),0) = [1-y^{(i)}(w^Tx^{(i)}+b)]_+ $$

则软间隔支持向量机的优化问题等价于目标函数为**合页损失函数（hinge loss）**的优化问题：

$$ min \quad \sum_{i=1}^{N} {[1-y^{(i)}(w^Tx^{(i)}+b)]_+} + λw^Tw $$

其中正则化系数$λ=\frac{1}{2C}$。

由此可以看出，支持向量机的出发点“间隔最大化”具有一定的$L2$正则化的作用。

对于软间隔支持向量机，之所以采用对偶方法求解约束规划而不是求解上面的无约束目标，主要原因如下：
1. 凸二次规划问题具有良好的解析性质，可以求得解析解；而上述无约束目标函数需要用梯度下降方法，且存在求最大值的操作，不利于微分；
2. 直接解上面的无约束目标函数无法直接使用核方法。

当标签为$±1$时，比较感知机、逻辑回归和支持向量机的损失函数：
- 感知机：**0/1损失**：$$E_{0/1}=[sign(wx)=y]=[sign(ywx)=1]$$
- 逻辑回归：以$2$为底的**交叉熵**：$$E_{scaled-ce}=log_2(1+exp(-ywx))=\frac{1}{ln2}E_{ce}$$
- 支持向量机：**Hinge Loss**：$$E_{svm}=[1-ywx]_+$$

经过换底后的交叉熵损失和支持向量机损失都是$0/1$损失的上界：

![](https://pic.downk.cc/item/5ed49547c2a9a83be5ad4673.jpg)

# 5. 序列最小最优化算法
**序列最小最优化算法(sequential minimal optimization，SMO)**是用来解支持向量机对偶问题的数值方法。

引入核函数的软间隔支持向量机的优化问题如下：

$$ min_{α} \quad \frac{1}{2}\sum_{i=1}^{N} {\sum_{j=1}^{N} {α_iα_jy^{(i)}y^{(j)}K(x^{(i)},x^{(j)})}} - \sum_{i=1}^{N} {α_i} \\ s.t. \quad 0≤α_i≤C,i=1,...,N \\ \quad \quad \sum_{i=1}^{N} {α_iy^{(i)}} = 0 $$

该优化问题需要求解$N$个参数$α_1,...,α_N$。

序列最小最优化算法的思想是，在循环中每次选择其中的两个参数$α_1,α_2$进行优化，固定其余参数，用解析的方法解两个变量的二次规划问题。

注意到子问题中只有一个变量是自由变量，另外一个变量可以被表示为：

$$ α_1 = -y^{(1)}\sum_{i=2}^{N} {α_iy^{(i)}} $$

### (1)两个变量的二次规划问题
当选择$α_1,α_2$作为变量时，子问题为：

$$ min_{α} \quad \frac{1}{2} α_1^2K_{11} + \frac{1}{2} α_2^2K_{22} + α_1α_2y^{(1)}y^{(2)}K_{12} \\ \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad + α_1y^{(1)} \sum_{i=3}^{N} {α_iy^{(i)}K_{1i}} + α_2y^{(2)} \sum_{i=3}^{N} {α_iy^{(i)}K_{2i}} - α_1 - α_2 \\ s.t. \quad 0≤α_i≤C,i=1,2 \\ \quad \quad α_1y^{(1)} + α_2y^{(2)} = -\sum_{i=3}^{N} {α_iy^{(i)}} $$

其中记$$K_{ij}=K(x^{(i)},x^{(j)})$$。

记$g(x)$为当前时刻支持向量机的判别函数，

$$ g(x) = \sum_{i=1}^{N} {α_iy^{(i)}K(x^{(i)},x)}+b $$

记当前时刻与真实标签的差值为$E(x^{(i)}) = g(x^{(i)})-y^{(i)}$；

若先求解变量$α_2$，将等式约束$α_1 = y^{(1)}(α_2y^{(2)}-\sum_{i=3}^{N} {α_iy^{(i)}})$代入目标函数后，对其求关于$α_2$的偏导数，令其为零，得：

$$ α_2^{new} = α_2^{old} +\frac{y^{(2)}(E(x^{(1)})-E(x^{(2)}))}{K_{11}+K_{22}-2K_{12}} $$

考虑不等式约束$$0≤α_1≤C,0≤α_2≤C$$，需要限制$α_2^{new}$的更新范围：

$$ L≤α_2^{new}≤H $$

当$y^{(1)} ≠ y^{(2)}$时:

$$ \begin{cases} L=max(α_2^{old}-α_1^{old},0) \\ H = min(C-α_1^{old}+α_2^{old},C) \end{cases} $$

当$y^{(1)} = y^{(2)}$时:

$$ \begin{cases} L=max(α_1^{old}+α_2^{old}-C,0) \\ H = min(α_1^{old}+α_2^{old},C) \end{cases} $$

![](https://pic.downk.cc/item/5ec8e88fc2a9a83be5cb3cdb.jpg)

因此更新后的$α_2^{new}$：

$$ α_2^{new} = \begin{cases} L, & α_2^{new}≤L \\ H, & α_2^{new}≥H \\ α_2^{new}, & otherwise \end{cases} $$

由约束条件$y^{(1)}α_1^{old} + y^{(2)}α_2^{old} = y^{(1)}α_1^{new} + y^{(2)}α_2^{new}$得：

$$ α_1^{new} = α_1^{old} + y^{(1)}y^{(2)}(α_2^{old}-α_2^{new}) $$

算法每一步迭代得到新的$α_1,α_2$后需要更新参数$b$:

若$0<α_1^{new}<C$，则：

$$ b_1^{new} = y^{(1)}-\sum_{i=1}^{N} {α_iy^{(i)}K(x^{(i)},x^{(1)})} \\ = -E(x^{(1)}) -y^{(1)}K_{11}(α_1^{new} - α_1^{old}) - y^{(2)}K_{21}(α_2^{new} - α_2^{old}) $$

若$0<α_2^{new}<C$，则：

$$ b_2^{new} = y^{(2)}-\sum_{i=1}^{N} {α_iy^{(i)}K(x^{(i)},x^{(2)})} \\ = -E(x^{(2)}) -y^{(1)}K_{12}(α_1^{new} - α_1^{old}) - y^{(2)}K_{22}(α_2^{new} - α_2^{old}) $$

取$b^{new}=\frac{b_1^{new}+b_2^{new}}{2}$。

### (2)变量的选择
每次循环更新当前时刻与真实标签的差值：

$$ E(x^{(i)}) = g(x^{(i)})-y^{(i)} = \sum_{i=1}^{N} {α_iy^{(i)}K(x^{(i)},x)}+b-y^{(i)} $$

- 外层循环选择第一个变量：选择误差$\mid E(x^{(i)}) \mid$最大的样本对应的$α_i$；
- 内层循环选择第二个变量：选择使$\mid E(x^{(i)})-E(x^{(j)}) \mid$最大的样本对应的$α_j$。

# 6. 概率支持向量机
**概率支持向量机（probabilistic SVM）**是将支持向量机和逻辑回归结合起来的方法。既具有支持向量机问题解析的优良性，又具有逻辑回归中样本概率极大似然的思想。

该方法先求解核函数支持向量机问题，得到（未经过符号函数）的模型输出：

$$ w_{SVM}^Tφ(x)+b_{SVM} $$

将该标量通过**scaling**参数$A$和**shifting**参数$B$后，喂入Logistic函数，得到预测概率：

$$ g(x) = σ(A(w_{SVM}^Tφ(x)+b_{SVM})+B) $$

该问题只有两个变量$A$和$B$。

概率支持向量机的熵损失函数（标签为$±1$）：

$$ L(w) = \sum_{i=1}^{N} {-ln(A(w_{SVM}^Tφ(x)+b_{SVM})+B)} = \sum_{i=1}^{N} {ln(1+exp(A(w_{SVM}^Tφ(x)+b_{SVM})+B))} $$

这是一个无约束的凸优化问题，可以使用梯度下降方法。

# 7. 最小二乘支持向量机
**最小二乘支持向量机（least-squares SVM，LSSVM）**就是将[kernel ridge regression]()用于分类任务。

由引入核方法的岭回归对数据拟合，得到回归方程：

$$ y = \sum_{n=1}^{N} {β_nK(x^{(n)},x)} $$

用该方程作为分类超平面，就得到最小二乘支持向量机模型。其中超平面是由回归的均方误差确定的，故称为“最小二乘”。

比较软间隔支持向量机和最小二乘支持向量机：

![](https://pic.downk.cc/item/5ed5fc84c2a9a83be564976b.jpg)

两者的分类边界是类似的，但前者的支持向量少（对应$α≠0$的样本），后者的支持向量多（对应$β≠0$的样本）计算量更大。


