---
layout: post
title: 't-SNE'
date: 2020-04-10
author: 郑之杰
cover: 'https://pic.downk.cc/item/5e901801504f4bcb0465ca1d.jpg'
tags: 机器学习
---

> t-SNE：一种非线性降维算法，主要用于可视化。

**t-SNE(t-distributed stochastic neighbor embedding)**是一种非线性降维算法，非常适用于高维数据降维到2维或者3维，进行可视化。

**本文目录**：
1. 问题引入
2. 技术细节
3. 算法分析
4. 代码实现
5. 参考文献

# 1. 问题引入
设想如下二维空间的数据点：

![](https://pic.downk.cc/item/5e9006bd504f4bcb04541bb3.jpg)

希望将其降维至一维空间：

![](https://pic.downk.cc/item/5e900725504f4bcb0454941b.jpg)

t-SNE的想法是，先将数据随机嵌入一维空间：

![](https://pic.downk.cc/item/5e900bee504f4bcb0459fd5b.jpg)

通过不断调整数据在一维空间的相对位置，使得这些数据在二维空间和一维空间的分布情况相似。

衡量不同空间内数据分布的相似情况，就要建立衡量相似度的“距离”指标。

衡量一个数据和其余数据的**相似度(Similarity)**，以二维空间中标黑的数据点为例：

![](https://pic.downk.cc/item/5e90098c504f4bcb04575f1e.jpg)

可以建立以该数据点为中心的**概率分布**（如正态分布），其余点到该点的距离（通常用Euclidean距离）转化为概率值：

![](https://pic.downk.cc/item/5e900a4b504f4bcb04583df2.jpg)

通过选择概率分布的**方差**，使得相似的点具有较高的概率，不相似的点具有较低的概率；将概率进行归一化，作为其余数据点对该点的相似度得分。

对每个数据点做上述操作，即可得到一个相似度得分的矩阵：

![](https://pic.downk.cc/item/5e900af9504f4bcb0458fb9c.jpg)

同理，可以建立一维空间中（随机嵌入的）数据点的相似度得分：

![](https://pic.downk.cc/item/5e900b56504f4bcb04595891.jpg)

通过不断改变一维空间中的数据分布情况，使得其相似度得分矩阵接近于二维空间，便可以实现降维。

# 2. 技术细节

### 相似度衡量
在高维空间衡量相似度使用正态分布，计算如下：

$$ p_{ij} = \frac{exp( -\mid\mid x_i -x_j \mid\mid^2 /2σ^2 )}{\sum_{k≠l}^{} {exp( -\mid\mid x_k -x_l \mid\mid^2 /2σ^2 )}} $$

低维空间的概率分布使用具有heavy-tailed特点的t分布，相似度衡量计算如下：

$$ q_{ij} = \frac{ (1 + \mid\mid y_i-y_j \mid\mid^2)^{-1} }{\sum_{k≠l}^{} {(1 + \mid\mid y_k-y_l \mid\mid^2 )^{-1} }} $$

t分布相对于正态分布，对离群点outlier不敏感：

![](http://www.datakit.cn/images/statistics/norm_t_dict.png)

### 方差
概率分布方差$σ^2$的选择决定了其余数据点对该数据点的“有效性”。

t-SNE使用[**困惑度(perplexity)**](https://en.wikipedia.org/wiki/Perplexity)选择方差。

概率分布$P_i$的困惑度定义为：

$$ perp(P_i) = 2^{H(P_i)} $$

其中$H(P_i)$表示$P_i$的熵。通过预先给定困惑度的值（5-50），可以选择合适的方差$σ^2$。

较低的困惑度意味着在匹配原分布并拟合每一个数据点到目标分布时只考虑最近的几个最近邻点，而较高的困惑度意味着拥有较大的全局观。

### 优化
优化两个概率分布$p_{ij}$和$q_{ij}$的KL散度：

$$ KL(P \mid\mid Q) = \sum_{i}^{} {\sum_{j}^{} {p_{ij}log\frac{p_{ij}}{q_{ij}}}} $$

# 3. 算法分析
t-SNE算法的特点
1. t-SNE倾向于保留数据中的局部特征，对于本征维数(intrinsic dimensionality)本身就很高的数据集，是不可能完整的映射到2-3维的空间；
2. 主要用于可视化，没有显式的预估部分，很难用于其他目的（比如预处理）。

# 4. 代码实现
使用PCA和t-SNE对Iris数据集降维可视化：

①导入相关的库：
```
from sklearn.datasets import load_iris,load_digits
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
```

②使用PCA和t-SNE降维：
```
digits = load_digits()
X_tsne = TSNE(n_components=2,random_state=33).fit_transform(digits.data)
X_pca = PCA(n_components=2).fit_transform(digits.data)
```

③可视化：
```
plt.figure(figsize=(10, 5))
plt.subplot(121)
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=digits.target,label="t-SNE")
plt.legend()
plt.subplot(122)
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=digits.target,label="PCA")
plt.legend()
plt.show()
```
![](https://pic3.zhimg.com/80/v2-6b2ea2764ac466409899e61db3acce96_720w.png)

# 5. 参考文献

paper：[Visualizing Data using t-SNE](http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)