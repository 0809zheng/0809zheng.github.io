---
layout: post
title: '集成学习中的组合(Blending)策略'
date: 2020-03-16
author: 郑之杰
cover: 'https://pic.downk.cc/item/5eda2944c2a9a83be55ec478.jpg'
tags: 机器学习
---

> An Ensemble Learning Method：Blending.

**Blending**是一种集成学习的方法，主要关注当已获得多个训练子模型时如何将其集成起来。若一共训练了$T$个子模型$$\{g_t,t=1,2,...,T\}$$；
**Blending**根据这些训练好的模型，采用“投票”机制获得最终的模型$\overline{g}$。

使用**Blending**能够带来以下好处：
1. 通过集成多个子模型能够提高泛化性能，减小过拟合的风险；
2. 通过集成多个子模型能够降低陷入较差局部极小值的风险；
3. 通过集成多个子模型能够扩大假设空间，学习到更好的近似。

![](https://pic.imgdb.cn/item/611debb14907e2d39c415fbd.jpg)

对于分类任务，**Blending**也被称作**投票法(voting)**；对于回归任务，**Blending**也被称作**平均法(averaging)**。

对训练模型的简单投票/平均方法被称作**Uniform Blending**，而对模型的加权投票/平均方法被称作**Linear Blending**。一般而言，在子模型性能相差较大时选用**Linear Blending**，在子模型性能相近时使用**Uniform Blending**。

# 1. Uniform Blending

![](https://pic.downk.cc/item/5efadce414195aa5947fe778.jpg)

## (1) 分类问题: 多数投票(majority voting)
对于输出为$±1$的二分类问题，最终的判别函数为：

$$ \overline{g}(x) = \text{sign}(\sum_{t=1}^{T} {g_t(x)}) $$

对于多分类问题，最终的判别函数为：

$$ \overline{g}(x) = \mathop{\arg \max}_{1≤k≤K}(\sum_{t=1}^{T} {[g_t(x)=k]}) $$

上述也被称为**相对多数投票法(plurality voting)**，即选择得票最多的类别；若同时有多个类别获得最高票，则从中随机地选择一个类别。

还有一种**绝对多数投票法(majority voting)**，即只有某个类别的票数过半，才预测为该类别；否则拒绝预测：

$$ \overline{g}(x) = \begin{cases} k, \quad \sum_{t=1}^{T} [g_t(x)=k] > 0.5 \sum_{k=1}^{K} \sum_{t=1}^{T}[g_t(x)=k] \\ \text{reject,} \quad \text{otherwise} \end{cases} $$

绝对多数投票法提供了一种拒绝预测的选项，在可靠性要求较高的学习任务重常用到。

## (2) 回归问题: 简单平均(simple averaging)
对于回归问题，模型采用所有训练模型的平均：

$$ \overline{g}(x) = \frac{1}{T} \sum_{t=1}^{T} {g_t(x)} $$

下面证明平均训练模型$\overline{g}$与理论最优模型$f$的平方误差要小于所有训练模型$g_t$与理论最优模型$f$误差的平均：

$$ \begin{aligned} \frac{1}{T} \sum_{t=1}^{T} {(g_t-f)^2} &= \frac{1}{T} \sum_{t=1}^{T} {(g_t^2-2g_tf+f^2)} = \frac{1}{T} \sum_{t=1}^{T} {g_t^2}-\frac{1}{T} \sum_{t=1}^{T} {2g_tf} +f^2 \\& = \frac{1}{T} \sum_{t=1}^{T} {g_t^2}-2\overline{g}f +f^2 = \frac{1}{T} \sum_{t=1}^{T} {g_t^2}-\overline{g}^2+\overline{g}^2-2\overline{g}f +f^2 \\ &= \frac{1}{T} \sum_{t=1}^{T} {g_t^2}-2\overline{g}^2+\overline{g}^2 +(\overline{g}-f)^2\\& = \frac{1}{T} \sum_{t=1}^{T} {g_t^2} - \frac{1}{T} \sum_{t=1}^{T} {2g_t\overline{g}}+\frac{1}{T} \sum_{t=1}^{T} {\overline{g}^2} + (\overline{g}-f)^2 \\ &= \frac{1}{T} \sum_{t=1}^{T} {(g_t-\overline{g})^2} + (\overline{g}-f)^2 > (\overline{g}-f)^2 \end{aligned} $$

定义：
- **偏差bias**：平均训练模型$\overline{g}$与理论最优模型$f$的平方误差$(\overline{g}-f)^2$
- **方差variance**：所有训练模型$g_t$与平均训练模型$\overline{g}$的平方误差的平均$\frac{1}{T} \sum_{t=1}^{T} {(g_t-\overline{g})^2}$

**Blending**旨在通过平均的方法减小算法的**方差**，从而提高算法的性能。


# 2. Linear Blending
**Linear Blending**是指对每一个训练模型$$\{g_t,t=1,2,...,T\}$$赋予权重$$\{α_t,t=1,2,...,T\}$$，然后采用加权的方法组合子模型。

通常要求$\sum_{t=1}^{T}α_t=1$。由于必须使用非负权重才能确保集成模型的性能优于单一最佳子模型，因此引入非负约束$α_t≥0$。

在求解最佳参数$α_t$时，由于训练集已经用来训练模型，因此选择一个验证集$\{(x^1,y^1),...,(x^N,y^N)\}$，使得在验证集上误差最小：

$$ \mathop{\arg \min}_{α_t}  \frac{1}{N} \sum_{n=1}^{N} {\text{error}(y^i,\overline{g}(x^i))} $$

### (1) 分类问题: 加权平均(weighted averaging)
对于回归问题，模型采用所有训练模型的加权平均：

$$ \overline{g}(x) = \sum_{t=1}^{T} {α_tg_t(x)} $$

### (2) 回归问题: 加权投票(weighted voting)
对于输出为$±1$的二分类问题，最终的判别函数为：

$$ \overline{g}(x) = \text{sign}(\sum_{t=1}^{T} {α_tg_t(x)}) $$

对于多分类问题，最终的判别函数为：

$$ \overline{g}(x) = \mathop{\arg \max}_{1≤k≤K}(\sum_{t=1}^{T} {α_t[g_t(x)=k]}) $$


# 3. Stacking（Any Blending）
**Stacking**也称作**Any Blending**，是指用引入一个新的子模型对训练得到的子模型$$\{g_t,t=1,2,...,T\}$$进行组合。新的子模型可以是任何函数形式，**Uniform Blending**和**Linear Blending**都可以看做其特殊形式。

- 优点：模型复杂度提高，更容易获得更好的预测模型；
- 缺点：复杂模型也容易带来过拟合的危险。

![](https://pic.downk.cc/item/5efadd1d14195aa5947ff70a.jpg)

**Stacking**可以看作是一个**三层的神经网络**模型(输入层+隐藏层+输出层)。首先由一系列子模型分别得到任务的输出，再将这些子模型的输出看作**特征**，使用一个新的子模型根据特征获得最终的预测结果。在**Stacking**中，将模型的输出看作新的“特征”，这是具有启发性的。**Stacking**和神经网络的主要区别如下：
1. **Stacking**算法是逐步训练的，类似于贪心算法，首先训练第一层子模型，再训练第二层子模型，这样得到的结果可能只是局部最优解；而神经网络是端到端训练的，通过一个损失函数获得模型整体的最优解。
2. 神经网络可以增加更多的层数(深度学习)，可以看作是更深层次的模型集成，往往会有更好的效果(过拟合风险也更大)。


