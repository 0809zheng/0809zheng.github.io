---
layout: post
title: '集成学习中的多样性度量(Diversity Measure)'
date: 2021-07-21
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/611deab74907e2d39c3e9f9c.jpg'
tags: 机器学习
---

> Diversity Measure in Ensemble Learning.

**集成学习(Ensemble Learning)**是指构建若干子模型，并通过某种策略结合起来获得最终的模型。在集成学习中，通常希望构建的子模型具有一定的**准确率**(至少不差于**弱学习器**,即泛化性能略优于随机猜测的学习器)，又具有一定的**多样性**(即不同子模型之间具有一定的差异)。

# 1. 多样性度量
**多样性度量(diversity measure)**是指衡量集成模型中子模型的多样性。通常是衡量子模型之间的两两相似/不相似性。
以输出为$±1$的二分类任务为例，子模型$g_i$与$g_j$的预测结果**列联表(contingency table)**为：

$$
\begin{array}{l|cc}
     & g_i=+1 & g_i=-1 \\
    \hline
    g_j=+1 & a & c \\
    g_j=-1 & b & d \\
\end{array}
$$

其中$a+b+c+d=N$为样本总数。基于该列联表给出一些常见的多样性度量：

### (1) 不合度量 disagreement measure

$$ \text{dis}_{ij} = \frac{b+c}{N} $$

$\text{dis}_{ij}$的值域为$[0,1]$，其值越大则多样性越大。

### (2) 相关系数 correlation coefficient

$$ \rho_{ij} = \frac{ad-bc}{\sqrt{(a+b)(a+c)(c+d)(b+d)}} $$

$\rho_{ij}$的值域为$[-1,1]$。若$g_i$与$g_j$无关则值为$0$；若$g_i$与$g_j$正相关则其值为正，否则为负。

### (3) Q-统计量 Q-statistic

$$ Q_{ij} = \frac{ad-bc}{ad+bc} $$

$Q_{ij}$与$$\rho_{ij}$$的符号相同，且$\|Q_{ij}\|≥ \|\rho_{ij}\|$。

### (4) $\kappa$-统计量 $\kappa$-statistic

$$ \kappa = \frac{p_1-p_2}{1-p_2} $$

其中$p_1$是两个子模型取得一致的概率，$p_2$是两个子模型偶然达成一致的概率，估计为：

$$ p_1 = \frac{a+d}{N} $$

$$ p_2 = \frac{(a+b)(a+c)+(c+d)(b+d)}{N^2} $$

若$g_i$与$g_j$完全一致，则$\kappa=1$；若$g_i$与$g_j$仅是偶然达成一致，则$\kappa=0$；$\kappa$通常取非负值，只有在$g_i$与$g_j$达成一致的概率甚至低于偶然达成一致的概率时取负值。$\kappa$**-误差图**是指将每一对(**pairwise**)子模型作为图上的一个点，横坐标是这对子模型的$\kappa$值，纵坐标是它们的平均误差。数据分布越靠上，每个子模型的准确率越低；数据分布越靠右，子模型的多样性越小。

![](https://pic.imgdb.cn/item/611deab74907e2d39c3e9f9c.jpg)

# 2. 多样性增强
集成学习中需要多样性较大的子模型，增强子模型的多样性的方法一般是在学习过程中引入**随机性**，如：
- **数据样本**扰动：从初始数据集中采样不同的子数据集，进而训练出不同的子模型；如**bagging, boosting**。该方法适用于对数据样本的扰动敏感的子模型(如决策树,神经网络)，训练样本的轻微扰动会导致子模型的显著变化。不适合对于数据样本的扰动不敏感的子模型(如线性模型,支持向量机,朴素贝叶斯,k近邻)。
- **输入特征**扰动：从初始的高维特征空间投影到低维特征空间，构成不同的子空间，进而训练出不同的子模型；如随机森林。
- **输出表示**扰动：改变输出的表示以增强多样性。
- **算法参数**扰动：改变模型的超参数以增强多样性。
