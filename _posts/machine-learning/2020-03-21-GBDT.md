---
layout: post
title: 'GBDT：梯度提升决策树'
date: 2020-03-21
author: 郑之杰
cover: 'https://pic.downk.cc/item/5efb035714195aa5948c4ccf.jpg'
tags: 机器学习
---

> Gradient Boosted Decision Tree.

**梯度提升决策树（Gradient Boosted Decision Tree，GBDT）**是一种把[**boosting**](https://0809zheng.github.io/2020/03/18/boosting.html)和[**决策树**](https://0809zheng.github.io/2020/03/19/decision-tree.html)结合起来的方法。

根据要解决的任务不同，又可以细分为：
- 分类：**Adaptive Boosted Decision Tree**
- 回归：**Gradient Boosted Decision Tree**

# 1. Adaptive Boosted Decision Tree
通常的**boosting**是对样本赋予不同的权重，训练得到不同的模型。

决策树没有显式地定义损失函数，直接对样本赋予权重的方法实现起来是困难的，因此采用类似**bootstrap**的方法，预先按照样本的权重比例对样本集进行抽样，每次得到一个新的样本集，其中每个样本出现的概率和它的权重是差不多的。

在抽样得到的样本集上训练得到一个子模型$g_t$后，需要确定该子模型在最终模型中所占的权重。

当$g_t$分类错误率$ε_t=0$，则表示该模型在该数据集上完全分类正确，对应权重$α_t=+∞$；而决策树不进行剪枝的话，很容易过拟合，实现$ε_t=0$，从而使权重无限大。因此在训练子模型的时候需要对决策树进行剪枝或限制最大深度。

因此**Adaptive Boosted Decision Tree**的主要实现包括：
- 使用**AdaBoost**集成决策树；
- 训练子树时采用按权重抽样得到数据集的方法；
- 对训练的子树进行剪枝。

# 2. Gradient Boosted Decision Tree
**Gradient Boost**并不是对每个训练样本赋权重值，而是通过对损失函数求梯度进行模型更新的。

设通过$T-1$轮训练得到模型：

$$ f_{T-1} = \sum_{t=1}^{T-1} {α_tg_t} $$

第$T$轮训练的决策树模型记为$g_T$，则通过$T$轮训练得到模型：

$$ f_{T} = f_{T-1} + α_Tg_T $$

损失函数采用平方误差：

$$ loss = \frac{1}{N}\sum_{n=1}^{N} {(y_n-f)^2} $$

模型按照梯度下降方法更新：

$$ f_{T} = f_{T-1} - η\frac{\partial loss}{\partial f \mid f = f_{T-1}} = f_{T-1} + η\frac{1}{N} \sum_{n=1}^{N} {2(y_n-f)} $$

因此$g_T$与$\frac{1}{N} \sum_{n=1}^{N} {2(y_n-f)}$在函数空间中应具有同样的方向，即在**GBDT**中，每一次训练的**CART决策树**拟合当前预测值与真实值之间的**残差（residual）**。
