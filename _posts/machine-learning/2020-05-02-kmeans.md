---
layout: post
title: 'K-Means聚类'
date: 2020-05-02
author: 郑之杰
cover: 'https://pic.downk.cc/item/5eb37405c2a9a83be5dcba4a.jpg'
tags: 机器学习
---

> K-Means Clustering.

**本文目录**：
1. K-means
2. K-medoids

# 1. K-means
1. 随机选择$k$个初始聚类中心；
![](https://pic.downk.cc/item/5eb22057c2a9a83be5a9fd0b.jpg)
2. 把每个样本点划分到与其最近的聚类中心所属的类别；
![](https://pic.downk.cc/item/5eb22082c2a9a83be5aa19de.jpg)
3. 更新聚类中心为每一个聚类的均值：
![](https://pic.downk.cc/item/5eb220c7c2a9a83be5aa3ff0.jpg)
4. 重复步骤$2$和$3$，直至数据的划分不再变化。

**K-Means**算法的缺点：
- 需要多次遍历样本集合，计算复杂度高；
- 只能找到类球形的类，不能发现任意的类；
![](https://pic.downk.cc/item/5eb22193c2a9a83be5aabc23.jpg)
- 初始聚类中心对结果影响较大；
- 对噪声敏感。

# 2. K-medoids
**K-Means**与**K-Medoids**算法过程类似，区别在于：
- **K-Means**算法用聚类的均值作为新的聚类中心；
- **K-Medoids**算法用类内最靠近中心的数据点作为新的聚类中心。

**K-Medoids**算法对**outlier**不敏感。