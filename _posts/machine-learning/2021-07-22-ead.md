---
layout: post
title: '集成学习中的误差-分歧分解(Error-Ambiguity Decomposition)'
date: 2021-07-22
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60f508195132923bf8403c92.jpg'
tags: 机器学习
---

> Error-Ambiguity Decomposition in Ensemble Learning.

**集成学习(Ensemble Learning)**是指构建若干子模型，并通过某种策略结合起来获得最终的模型。在集成学习中，通常希望构建的子模型具有一定的**准确率**(至少不差于**弱学习器**,即泛化性能略优于随机猜测的学习器)，又具有一定的**多样性**(即不同子模型之间具有一定的差异)。

记理论最优模型为$f$，训练得到的$T$个子模型为$$\{g_t,t=1,2,...,T\}$$，集成模型$\overline{g}$采用所有训练模型的平均(以回归问题为例，分类问题结论相似)：

$$ \overline{g}(x) = \frac{1}{T} \sum_{t=1}^{T} {g_t(x)} $$

记子模型的**平均泛化误差**为$\overline{E}$，计算为所有子模型$g_t$与理论最优模型$f$误差的平方平均：

$$ \overline{E} = \frac{1}{T} \sum_{t=1}^{T} {(g_t-f)^2} $$

对$\overline{E}$进行如下分解：

$$ \begin{aligned} \frac{1}{T} \sum_{t=1}^{T} {(g_t-f)^2} &= \frac{1}{T} \sum_{t=1}^{T} {(g_t^2-2g_tf+f^2)} = \frac{1}{T} \sum_{t=1}^{T} {g_t^2}-\frac{1}{T} \sum_{t=1}^{T} {2g_tf} +f^2 \\ &= \frac{1}{T} \sum_{t=1}^{T} {g_t^2}-2\overline{g}f +f^2 = \frac{1}{T} \sum_{t=1}^{T} {g_t^2}-\overline{g}^2+\overline{g}^2-2\overline{g}f +f^2 \\ &= \frac{1}{T} \sum_{t=1}^{T} {g_t^2}-2\overline{g}^2+\overline{g}^2 +(\overline{g}-f)^2 \\&= \frac{1}{T} \sum_{t=1}^{T} {g_t^2} - \frac{1}{T} \sum_{t=1}^{T} {2g_t\overline{g}}+\frac{1}{T} \sum_{t=1}^{T} {\overline{g}^2} + (\overline{g}-f)^2 \\ &= \frac{1}{T} \sum_{t=1}^{T} {(g_t-\overline{g})^2} + (\overline{g}-f)^2 \end{aligned} $$

上式第一项被称为子模型的**分歧(ambiguity)**，记作$\overline{A}=\frac{1}{T} \sum_{t=1}^{T} {(g_t-\overline{g})^2}$，计算为所有子模型$g_t$与集成模型$\overline{g}$误差的平方平均，表示子模型在样本集上的不一致性，即体现子模型的多样性。

上式第二项被称为集成模型的**泛化误差**，记为$E=(\overline{g}-f)^2$，计算为集成模型$\overline{g}$与理论最优模型$f$误差的平方，用于衡量集成模型的好坏。

对分解式整理可得：

$$ E= \overline{E}-\overline{A} $$

上式被称作**误差-分歧分解(Error-Ambiguity Decomposition)**，表示集成学习中集成模型的**泛化误差** $E$是由子模型的**平均泛化误差** $\overline{E}$和子模型的**分歧** $\overline{A}$共同决定的。子模型**准确率**越高(即$\overline{E}$越小)、子模型**多样性**越大(即$\overline{A}$越大)，则集成越好(即$E$越小)。

