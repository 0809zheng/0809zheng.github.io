---
layout: post
title: '集成学习中的多样性分析'
date: 2021-07-22
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60f508195132923bf8403c92.jpg'
tags: 机器学习
---

> Diversity Analysis in Ensemble Learning.

**集成学习(Ensemble Learning)**是指构建若干子模型，并通过某种策略结合起来获得最终的模型。在集成学习中，通常希望构建的子模型具有一定的**准确率**(至少不差于**弱学习器**,即泛化性能略优于随机猜测的学习器)，又具有一定的**多样性**(即不同子模型之间具有一定的差异)。

# 1. 误差-分歧分解 Error-Ambiguity Decomposition
记理论最优模型为$f$，训练得到的$T$个子模型为$$\{g_t,t=1,2,...,T\}$$，集成模型$\overline{g}$采用所有训练模型的平均(以回归问题为例，分类问题结论相似)：

$$ \overline{g}(x) = \frac{1}{T} \sum_{t=1}^{T} {g_t(x)} $$

记子模型的**平均泛化误差**为$\overline{E}$，计算为所有子模型$g_t$与理论最优模型$f$误差的平方平均：

$$ \overline{E} = \frac{1}{T} \sum_{t=1}^{T} {(g_t-f)^2} $$

对$\overline{E}$进行如下分解：

$$ \frac{1}{T} \sum_{t=1}^{T} {(g_t-f)^2} = \frac{1}{T} \sum_{t=1}^{T} {(g_t^2-2g_tf+f^2)} = \frac{1}{T} \sum_{t=1}^{T} {g_t^2}-\frac{1}{T} \sum_{t=1}^{T} {2g_tf} +f^2 \\ = \frac{1}{T} \sum_{t=1}^{T} {g_t^2}-2\overline{g}f +f^2 = \frac{1}{T} \sum_{t=1}^{T} {g_t^2}-\overline{g}^2+\overline{g}^2-2\overline{g}f +f^2 \\ = \frac{1}{T} \sum_{t=1}^{T} {g_t^2}-2\overline{g}^2+\overline{g}^2 +(\overline{g}-f)^2 = \frac{1}{T} \sum_{t=1}^{T} {g_t^2} - \frac{1}{T} \sum_{t=1}^{T} {2g_t\overline{g}}+\frac{1}{T} \sum_{t=1}^{T} {\overline{g}^2} + (\overline{g}-f)^2 \\ = \frac{1}{T} \sum_{t=1}^{T} {(g_t-\overline{g})^2} + (\overline{g}-f)^2  $$

上式第一项被称为子模型的**分歧(ambiguity)**，记作$\overline{A}=\frac{1}{T} \sum_{t=1}^{T} {(g_t-\overline{g})^2}$，计算为所有子模型$g_t$与集成模型$\overline{g}$误差的平方平均，表示子模型在样本集上的不一致性，即体现子模型的多样性。

上式第二项被称为集成模型的**泛化误差**，记为$E=(\overline{g}-f)^2$，计算为集成模型$\overline{g}$与理论最优模型$f$误差的平方，用于衡量集成模型的好坏。

对分解式整理可得：

$$ E= \overline{E}-\overline{A} $$

上式被称作**误差-分歧分解(Error-Ambiguity Decomposition)**，表示集成学习中集成模型的**泛化误差** $E$是由子模型的**平均泛化误差** $\overline{E}$和子模型的**分歧** $\overline{A}$共同决定的。子模型**准确率**越高(即$\overline{E}$越小)、子模型**多样性**越大(即$\overline{A}$越大)，则集成越好(即$E$越小)。

# 2. 多样性度量
**多样性度量(diversity measure)**是指衡量集成模型中子模型的多样性。通常是衡量子模型之间的两两相似/不相似性。
以输出为$±1$的二分类任务为例，子模型$g_i$与$g_j$的预测结果**列联表(contingency table)**为：

$$
\begin{array}{l|cc}
     & g_i=+1 & g_i=-1 \\
    \hline
    g_j=+1 & a & c \\
    g_j=-1 & b & d \\
\end{array}
$$

其中$a+b+c+d=N$为样本总数。基于该列联表给出一些常见的多样性度量：

### (1) 不合度量 disagreement measure

$$ \text{dis}_{ij} = \frac{b+c}{N} $$

$\text{dis}_{ij}$的值域为$[0,1]$，其值越大则多样性越大。

### (2) 相关系数 correlation coefficient

$$ \rho_{ij} = \frac{ad-bc}{\sqrt{(a+b)(a+c)(c+d)(b+d)}} $$

$\rho_{ij}$的值域为$[-1,1]$。若$g_i$与$g_j$无关则值为$0$；若$g_i$与$g_j$正相关则其值为正，否则为负。

### (3) Q-统计量 Q-statistic

$$ Q_{ij} = \frac{ad-bc}{ad+bc} $$

$Q_{ij}$与$$\rho_{ij}$$的符号相同，且$\|Q_{ij}\|≥ \|\rho_{ij}\|$。

### (4) $\kappa$-统计量 $\kappa$-statistic

$$ \kappa = \frac{p_1-p_2}{1-p_2} $$

其中$p_1$是两个子模型取得一致的概率，$p_2$是两个子模型偶然达成一致的概率，估计为：

$$ p_1 = \frac{a+d}{N} $$

$$ p_2 = \frac{(a+b)(a+c)+(c+d)(b+d)}{N^2} $$

若$g_i$与$g_j$完全一致，则$\kappa=1$；若$g_i$与$g_j$仅是偶然达成一致，则$\kappa=0$；$\kappa$通常取非负值，只有在$g_i$与$g_j$达成一致的概率甚至低于偶然达成一致的概率时取负值。$\kappa$**-误差图**是指将每一对(**pairwise**)子模型作为图上的一个点，横坐标是这对子模型的$\kappa$值，纵坐标是它们的平均误差。数据分布越靠上，每个子模型的准确率越低；数据分布越靠右，子模型的多样性越小。

![](https://pic.imgdb.cn/item/611deab74907e2d39c3e9f9c.jpg)

# 3. 多样性增强
集成学习中需要多样性较大的子模型，增强子模型的多样性的方法一般是在学习过程中引入**随机性**，如：
- **数据样本**扰动：从初始数据集中采样不同的子数据集，进而训练出不同的子模型；如**bagging, boosting**。该方法适用于对数据样本的扰动敏感的子模型(如决策树,神经网络)，训练样本的轻微扰动会导致子模型的显著变化。不适合对于数据样本的扰动不敏感的子模型(如线性模型,支持向量机,朴素贝叶斯,k近邻)。
- **输入特征**扰动：从初始的高维特征空间投影到低维特征空间，构成不同的子空间，进而训练出不同的子模型；如随机森林。
- **输出表示**扰动：改变输出的表示以增强多样性。
- **算法参数**扰动：改变模型的超参数以增强多样性。




