---
layout: post
title: 'Boosting'
date: 2020-03-18
author: 郑之杰
cover: 'https://pic.downk.cc/item/5efb009814195aa5948b6b2e.jpg'
tags: 机器学习
---

> An Ensemble Learning Method：Boosting.

**Boosting**是一种集成学习的方法，主要关注在同一个数据集上训练不同的模型，这些模型的训练并不是独立的，而是在每次训练一个模型时，会赋予上一次训练模型中误差较大的样本较大的权重，赋予上一次训练模型中误差较小的样本较小的权重。

# 1. Boosting
**Boosting**的思想是，每一次训练模型时，对$N$个样本赋予不同的**权重**$w_1,w_2,...,w_N$;

计算损失函数：

$$ L = \frac{1}{N} \sum_{n=1}^{N} {w_n error(y_n,g(x_n))} $$

若需要训练$T$个模型$g_t,t=1,2,...,T$；设置初始权重$w_n^{(1)}=\frac{1}{N},n=1,2,...,N$;

设第$t$次训练设置的权重为$w_1^{(t)},w_2^{(t)},...,w_N^{(t)}$，得到模型$g_t$；

对于第$t+1$次训练，希望网络能够训练得到与$g_t$互补的模型，即假设$g_t$在重新设置权重的样本集上只能得到随机的效果（以分类为例）：

$$ \frac{\sum_{n=1}^{N} {w_n^{(t+1)}[y_n ≠ g_t(x_n)]}}{\sum_{n=1}^{N} {w_n^{(t+1)}}} = \frac{1}{2} $$

若记$ε_t$为$g_t$在其第$t$次赋值的样本集上训练后的错误率，即：

$$ ε_t = \frac{\sum_{n=1}^{N} {w_n^{(t)}[y_n ≠ g_t(x_n)]}}{\sum_{n=1}^{N} {w_n^{(t)}}} $$

**Boosting**要求每个分类器虽然是弱分类器，但仍具有超过统计随机值的表现水平，即通常$ε_t<0.5$。

则权重更新可以表示如下，增大预测错误样本的权重，减小预测正确样本的权重：

$$ w_n^{(t+1)} = \begin{cases} w_n^{(t)} × (1-ε_t) , & y_n ≠ g_t(x_n) \\ w_n^{(t)} × ε_t , & y_n = g_t(x_n) \end{cases} $$

经过上述过程训练，得到一系列训练模型$g_t,t=1,2,...,T$，便可通过**Blending**的方法集成模型。

# 2. AdaBoost
**AdaBoost(Adaptive Boosting)**是在**Boosting**通过为样本权重赋值训练模型的基础上进一步对模型集成的方法，主要包括：
- 单一模型的训练算法
- 样本权重的赋值方法
- 不同样本的组合方法

引入一个参数$d_t$，**AdaBoost**的权重更新方法如下：

$$ w_n^{(t+1)} = \begin{cases} w_n^{(t)} × d_t , & y_n ≠ g_t(x_n) \\ \frac{w_n^{(t)}}{d_t}, & y_n = g_t(x_n) \end{cases} $$

通常参数$d_t>1$，即增大预测错误样本的权重，减小预测正确样本的权重。

同样希望上一个训练的模型在新权重样本上的表现是随机的：

$$ \frac{\sum_{n=1}^{N} {w_n^{(t+1)}[y_n ≠ g_t(x_n)]}}{\sum_{n=1}^{N} {w_n^{(t+1)}}} = \frac{1}{2} $$

将更新公式代入得：

$$ \frac{\sum_{n=1}^{N} {w_n^{(t)} × d_t[y_n ≠ g_t(x_n)]}}{\sum_{n=1}^{N} {w_n^{(t)} × d_t[y_n ≠ g_t(x_n)]} + \sum_{n=1}^{N} {\frac{w_n^{(t)}}{d_t}[y_n = g_t(x_n)]}} = \frac{1}{2} $$

将$d_t$提出求和项：

$$ \frac{d_t\sum_{n=1}^{N} {w_n^{(t)}[y_n ≠ g_t(x_n)]}}{d_t\sum_{n=1}^{N} {w_n^{(t)} [y_n ≠ g_t(x_n)]} + \frac{1}{d_t}\sum_{n=1}^{N} {w_n^{(t)}[y_n = g_t(x_n)]}} = \frac{1}{2} $$

分子分母同除以$\sum_{n=1}^{N} {w_n^{(t)}}$，并注意到$ ε_t = \frac{\sum_{n=1}^{N} {w_n^{(t)}[y_n ≠ g_t(x_n)]}}{\sum_{n=1}^{N} {w_n^{(t)}}} $，得：

$$ \frac{d_tε_t}{d_tε_t + \frac{1}{d_t}(1-ε_t)} = \frac{1}{2} $$

解得：

$$d_t = \sqrt{\frac{(1-ε_t)}{ε_t}}$$

记每一个模型$g_t$在最终模型中所占的权重为$α_t$，则计算该权重的公式为：

$$ α_t = ln(d_t) = ln(\sqrt{\frac{(1-ε_t)}{ε_t}}) $$

分析：
- 当$g_t$分类错误率$ε_t=0.5$，则该模型与随机分类无异，对应权重$α_t=0$;
- 当$g_t$分类错误率$ε_t=0$，则该模型完全分类正确，对应权重$α_t=+∞$；
- 分类错误率$ε_t$越小，模型$g_t$投票时所占比重越大。

训练得到一系列训练模型$g_t,t=1,2,...,T$后，最终的模型为：

$$ G = sign(\sum_{t=1}^{T} {α_tg_t}) $$

### VC维
定义某假设函数对于观测到的样本的误差为$E_{in}$，该误差可以计算得到；定义该假设函数对于总样本的误差为$E_{out}$，该误差不可计算。

**AdaBoost**的**VC维**表示为：

$$ E_{out} ≤ E_{in} + O(\sqrt{O(d_{VC}(H)·TlogT)·\frac{logN}{N}}) $$

**AdaBoost**具有良好的性质：
- 若基本模型仅比随机结果好一些($ε_t<\frac{1}{2}$)，则经过$T=O(logN)$次迭代后$E_{in}≈0$；
- 若样本数$N$很大，则最终得到的模型满足$E_{out}≈E_{in}$。

### 指数损失 Exponential Error
对于分类任务，当标签是$±1$时，权重更新可以表示为：

$$ w_n^{(t+1)} = \begin{cases} w_n^{(t)} × d_t , & y_n ≠ g_t(x_n) \\ \frac{w_n^{(t)}}{d_t}, & y_n = g_t(x_n) \end{cases} \\ = w_n^{(t)} × d_t^{-y_ng_t(x_n)} = w_n^{(t)} × exp(-y_nα_tg_t(x_n)) $$

初始权重$w_n^{(1)}=\frac{1}{N}$，则权重更新为：

$$ w_n^{(T+1)} = w_n^{(T)} × exp(-y_nα_Tg_T(x_n)) = \frac{1}{N} \prod_{t=1}^{T} {exp(-y_nα_tg_t(x_n))} = \frac{1}{N} exp(-y_n \sum_{t=1}^{T} {α_tg_t(x_n)}) $$

对于单个样本$(x_n,y_n)$的权重$w_n$，当算法对该样本分类正确时，$y_n$应与$g(x_n)$同号，且$g(x_n)$越大越好（类似于间隔的概念），即$w_n$是逐渐**减小**的。

对于所有样本，可以列出一个最优化问题，最小化**样本权重**：

$$ min \quad \sum_{n=1}^{N} {w_n^{(T+1)}} = \frac{1}{N} \sum_{n=1}^{N} {exp(-y_n \sum_{t=1}^{T} {α_tg_t(x_n)})} $$

上式称为**指数损失（exponential error）**，是$0/1$损失的一个上界：

![](https://pic.downk.cc/item/5edddb6ec2a9a83be54ae6dc.jpg)

### 从优化角度理解AdaBoost
**AdaBoost**可以被写作一个优化问题，即求第$t$个模型$g_t$及其权重$α_t$时，最小化目标函数：

$$ min_{α_t,g_t} \quad \frac{1}{N} \sum_{n=1}^{N} {exp(-y_n (\sum_{τ=1}^{t-1} {α_τg_τ(x_n)}+α_tg_t(x_n)))} $$

化简为：

$$ min_{α_t,g_t} \quad \sum_{n=1}^{N} {w_n^{(t)}exp(-y_nα_tg_t(x_n))} $$

把上式拆分成$y_n=g_t(x_n)$和$y_n≠g_t(x_n)$两种情况：

$$ min_{α_t,g_t} \quad \sum_{n=1}^{N} {[y_n=g_t(x_n)]w_n^{(t)}exp(-α_t)+[y_n≠g_t(x_n)]w_n^{(t)}exp(α_t)} \\ = \sum_{n=1}^{N} {w_n^{(t)}} \sum_{n=1}^{N} {(1-ε_t)exp(-α_t)+ε_texp(α_t)} $$

上式对$α_t$求导，令其为0，可以得到：

$$ -(1-ε_t)exp(-α_t)+ε_texp(α_t)=0 $$

解得：

$$ α_t = ln(\sqrt{\frac{(1-ε_t)}{ε_t}}) $$

这与之前的定义是吻合的。

# 3. Gradient Boosting
**AdaBoost**方法通过对样本赋予不同的权重，训练不同的子模型$g_t$，通过这些子模型加权投票得到最终的结果：

$$ G = sign(\sum_{t=1}^{T} {α_tg_t}) $$

**梯度提升（Gradient Boosting）**则是每次寻找一个新的模型函数$g_t$，来减小最终的损失。

假设经过了$T-1$次训练，得到的未经过符号函数的模型为：

$$ f_{T-1} = \sum_{t=1}^{T-1} {α_tg_t} $$

在第$T$次训练中，需要确定模型$g_T$及其权重$α_T$，使得：

$$ f_{T} = f_{T-1} + α_Tg_T $$

损失函数使用指数损失：

$$ loss = \frac{1}{N} \sum_{n=1}^{N} {exp(-y_n \sum_{t}^{} {α_tg_t(x_n)})} = \frac{1}{N} \sum_{n=1}^{N} {exp(-y_n f)} $$

采用梯度下降法更新参数，注意是对函数求梯度：

$$ f_{T} = f_{T-1} - η\frac{\partial loss}{\partial f \mid f = f_{T-1}} = f_{T-1} + η\frac{1}{N} \sum_{n=1}^{N} {exp(-y_n f_{T-1})}y_n $$

注意到$g_T$与$\frac{1}{N} \sum_{n=1}^{N} {exp(-y_n f_{T-1})}y_n$在函数空间中应具有同样的方向，而$α_T$可以看作学习率。

采用不同的损失函数，会有不同的函数更新方式。
