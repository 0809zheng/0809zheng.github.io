---
layout: post
title: '集成学习中的提升(Boosting)方法'
date: 2020-03-18
author: 郑之杰
cover: 'https://pic.downk.cc/item/5efb009814195aa5948b6b2e.jpg'
tags: 机器学习
---

> An Ensemble Learning Method：Boosting.


# 1. Boosting的基本思想
**Boosting**是一种**集成学习**的方法，主要关注在同一个数据集上训练相同类型的子模型，这些子模型被称作**基学习器(base learner)**。这些基学习器的训练并不是独立的，而是首先从初始数据集中训练一个基学习器，之后在每次训练一个新的基学习器时，会赋予对于上一个基学习器误差较大的样本较大的权重，赋予误差较小的样本较小的权重；如此反复进行，直至训练得到$T$个基学习器，最终对其进行加权组合。

上述**重加权(re-weighting)**的方法使得不同的基学习器能对特定的数据分布进行学习。若需要训练$T$个基学习器$$\{g_t,t=1,2,...,T\}$$，设置训练集中的$N$个样本的初始权重$$\{w_n^{(1)}=\frac{1}{N},n=1,2,...,N\}$$；在第$t$轮训练过程中，根据样本分布为每个训练样本重新赋予权重$w_1^{(t)},w_2^{(t)},...,w_N^{(t)}$。记$ε_t$为基学习器$g_t$在第$t$次赋值的训练集上训练后的**错误率**，即：

$$ ε_t = \frac{\sum_{n=1}^{N} {w_n^{(t)}[y_n ≠ g_t(x_n)]}}{\sum_{n=1}^{N} {w_n^{(t)}}} $$

则一种简单的样本权重更新方式表示如下，即增大错误预测样本的权重，减小正确预测样本的权重：

$$ w_n^{(t+1)} = \begin{cases} w_n^{(t)} × (1-ε_t) , & y_n ≠ g_t(x_n) \\ w_n^{(t)} × ε_t , & y_n = g_t(x_n) \end{cases} $$

经过上述过程训练，得到$T$个训练好的基学习器$$\{g_t,t=1,2,...,T\}$$，之后可以通过[**Blending**](https://0809zheng.github.io/2020/03/16/blending.html)方法集成模型，从而得到最终的结果。

**Boosting**要求每个基学习器虽然是**弱学习器**，但仍具有超过统计随机值的表现水平(即比随机猜测好)，即通常$ε_t<0.5$。再训练的每一轮需要检测当前生成的基学习器是否满足该基本条件，一旦条件不满足，则当前基学习器即被抛弃，且学习过程停止。这样会导致最终获得的基学习器个数远小于$T$个，使得集成学习性能不佳。

对于无法接受带有权重的样本的基学习器，可以通过**重采样(re-sampling)**方法实现**Boosting**。即在每一轮训练中，根据样本分布对训练集进行重新采样，再根据重采样的样本集训练基学习器。这种方法可以通过**重启动**避免由于不满足弱学习器条件而停止的问题，即抛弃当前不满足条件的基学习器后，可根据当前分布对训练样本重新采样，再根据新的采样结果训练学习器，从而保证学习过程可以持续到$T$轮结束。

**Boosting**主要关注降低**偏差**，因此能基于泛化性能相当弱的弱学习器构建出很强的集成学习器。

# 2. AdaBoost
**AdaBoost(Adaptive Boosting)**是在**Boosting**(为样本权重重新赋值)的基础上进一步对模型集成的方法，主要包括：
- 单一模型的训练算法：**指数损失函数**
- 样本权重的赋值方法：$d_t = \sqrt{\frac{(1-ε_t)}{ε_t}}$
- 不同样本的组合方法：**加性模型**

若通过$T$轮训练获得$T$个基学习器$$\{g_t,t=1,2,...,T\}$$，则**AdaBoost**是一种**加性模型(additive model)**，即基学习器的线性组合：

$$ G(x) = \sum_{t=1}^{T} {α_tg_t(x)} $$

基学习器的获得是通过迭代地优化**指数损失函数(exponential loss function)**实现的：

$$ \mathcal{L}_{\text{exp}} = \text{exp}(-yG(x)) $$

值得一提的是，标准的**AdaBoost**只适用于二分类任务。下面以输出标签为$±1$的二分类为例，介绍**AdaBoost**算法。

## (1) AdaBoost算法的流程
假设需要训练$T$个基学习器$$\{g_t,t=1,2,...,T\}$$，训练开始时设置训练集中的$N$个样本的初始**样本权重**$$\{w_n^{(1)}=\frac{1}{N},n=1,2,...,N\}$$。

引入一个参数$d_t$，则**AdaBoost**的第$t+1$轮训练中，训练集第$n$个样本权重的更新方法如下：

$$ w_n^{(t+1)} = \begin{cases} \frac{1}{Z_t}w_n^{(t)} × d_t , & y_n ≠ g_t(x_n) \\ \frac{1}{Z_t}\frac{w_n^{(t)}}{d_t}, & y_n = g_t(x_n) \end{cases} $$

其中$Z_t$表示第$t$轮训练中所有样本权重的**加权和**，用于对训练样本集的权重进行归一化，计算为：

$$ Z_t=\sum_{n=1}^{N} \{ w_n^{(t)} × d_t [y_n ≠ g_t(x_n)]+ \frac{w_n^{(t)}}{d_t}[y_n = g_t(x_n)] \} =\sum_{n=1}^{N} w_n^{(t)} d_t^{-y_ng_t(x_n)} $$

注意到所有样本权重的**加权和**$Z_t$并不等于样本权重之和！事实上通过引入$Z_t$使得每轮训练中的样本权重值和都满足$\sum_{n=1}^{N} w_n^{(t)}=1$。

通常参数$d_t>1$，即增大错误预测样本的权重，减小正确预测样本的权重。
对于第$t+1$次训练，希望新的基学习器能够训练得到与$g_t$**互补**的模型，即假设$g_t$在重新设置权重的训练集上只能得到**随机**的效果（以分类为例）：

$$ \frac{\sum_{n=1}^{N} {w_n^{(t+1)}[y_n ≠ g_t(x_n)]}}{\sum_{n=1}^{N} {w_n^{(t+1)}}} = \frac{1}{2} $$

将训练样本权重的更新公式代入得：

$$ \frac{\sum_{n=1}^{N} {\frac{1}{Z_t} w_n^{(t)} × d_t[y_n ≠ g_t(x_n)]}}{\sum_{n=1}^{N} {\frac{1}{Z_t}w_n^{(t)} × d_t[y_n ≠ g_t(x_n)]} + \frac{1}{Z_t}\sum_{n=1}^{N} {\frac{w_n^{(t)}}{d_t}[y_n = g_t(x_n)]}} = \frac{1}{2} $$

将$d_t$提出求和项：

$$ \frac{d_t\sum_{n=1}^{N} {w_n^{(t)}[y_n ≠ g_t(x_n)]}}{d_t\sum_{n=1}^{N} {w_n^{(t)} [y_n ≠ g_t(x_n)]} + \frac{1}{d_t}\sum_{n=1}^{N} {w_n^{(t)}[y_n = g_t(x_n)]}} = \frac{1}{2} $$

注意到**错误率**的定义$ε_t = \frac{\sum_{n=1}^{N} {w_n^{(t)}[y_n ≠ g_t(x_n)]}}{\sum_{n=1}^{N} {w_n^{(t)}}} = \sum_{n=1}^{N} {w_n^{(t)}[y_n ≠ g_t(x_n)]}$，得：

$$ ε_t = \frac{\sum_{n=1}^{N} {w_n^{(t)}[y_n ≠ g_t(x_n)]}}{\sum_{n=1}^{N} {w_n^{(t)}}} $$

$$ \frac{d_tε_t}{d_tε_t + \frac{1}{d_t}(1-ε_t)} = \frac{1}{2} $$

解得：

$$d_t = \sqrt{\frac{(1-ε_t)}{ε_t}}$$

记每一个基学习器$g_t$在最终模型中所占的**模型权重**为$α_t$，则计算该权重的公式为：

$$ α_t = ln(d_t) = ln(\sqrt{\frac{(1-ε_t)}{ε_t}}) = \frac{1}{2} ln(\frac{(1-ε_t)}{ε_t}) $$

分析：
- 当$g_t$的分类错误率$ε_t=0.5$，则该模型与随机分类无异，对应权重$α_t=0$;
- 当$g_t$的分类错误率$ε_t=0$，则该模型完全分类正确，对应权重$α_t=+∞$；
- 分类错误率$ε_t$越小，投票时模型$g_t$所占比重越大。

训练得到一系列基学习器$$\{g_t,t=1,2,...,T\}$$后，最终的集成模型为：

$$ G = \text{sign}(\sum_{t=1}^{T} {α_tg_t}) $$

## (2) AdaBoost的VC维
定义某假设函数对于观测样本的误差为$E_{in}$，该误差可以计算得到；定义该假设函数对于总样本的误差为$E_{out}$，该误差不可计算。
则**AdaBoost**的**VC维**表示为：

$$ E_{out} ≤ E_{in} + O(\sqrt{O(d_{VC}(H)·TlogT)·\frac{logN}{N}}) $$

**AdaBoost**具有良好的性质：
- 若基本模型仅比随机结果好一些($ε_t<\frac{1}{2}$)，则经过$T=O(logN)$次迭代后$E_{in}≈0$；
- 若样本数$N$很大，则最终得到的模型满足$E_{out}≈E_{in}$。

## (3) 指数损失 Exponential Loss
根据定义的模型权重$$\{α_t,t=1,2,...,T\},α_t = ln(d_t)$$
对于标签是$±1$的二分类任务，样本集的权重更新可以表示为：

$$ w_n^{(t+1)} = \begin{cases} \frac{1}{Z_t}w_n^{(t)} × d_t , & y_n ≠ g_t(x_n) \\ \frac{1}{Z_t}\frac{w_n^{(t)}}{d_t}, & y_n = g_t(x_n) \end{cases} \\ = \frac{1}{Z_t} w_n^{(t)} d_t^{-y_ng_t(x_n)} = \frac{1}{Z_t} w_n^{(t)} \text{exp}(-y_nα_tg_t(x_n)) $$

其中所有样本权重的**加权和**$Z_t$也可由模型权重$$α_t$$表示为：

$$ Z_t=\sum_{n=1}^{N} w_n^{(t)} d_t^{-y_ng_t(x_n)} = \sum_{n=1}^{N} w_n^{(t)} \text{exp}(-y_nα_tg_t(x_n)) $$

若初始样本权重$w_n^{(1)}=\frac{1}{N}$，则样本权重更新为：

$$ w_n^{(T+1)} = \frac{1}{Z_T} w_n^{(T)} \text{exp}(-y_nα_Tg_T(x_n)) \\= \frac{1}{Z_T}  \frac{1}{Z_{T-1}} w_n^{(T-1)} \text{exp}(-y_nα_{T-1}g_{T-1}(x_n)) \text{exp}(-y_nα_Tg_T(x_n)) \\...\\ = \frac{1}{\prod_{t=1}^{T}Z_t} \frac{1}{N} \prod_{t=1}^{T} {\text{exp}(-y_nα_tg_t(x_n))} = \frac{1}{\prod_{t=1}^{T}Z_t} \frac{1}{N} \text{exp}(-y_n \sum_{t=1}^{T} {α_tg_t(x_n)}) $$

对于单个样本$(x_n,y_n)$的样本权重$w_n$，当算法对该样本分类正确时，$y_n$应与$g(x_n)$同号，且$g(x_n)$越大越好（类似于间隔的概念），即$w_n$是逐渐**减小**的。

对于所有样本，可以列出一个最优化问题，即最小化所有**样本权重**：

$$ \mathop{\min}  \sum_{n=1}^{N} {w_n^{(T+1)}} \\ = \mathop{\min} \sum_{n=1}^{N} \frac{1}{\prod_{t=1}^{T}Z_t} \frac{1}{N} \text{exp}(-y_n \sum_{t=1}^{T} {α_tg_t(x_n)})  \\ ∝ \mathop{\min} \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n \sum_{t=1}^{T} {α_tg_t(x_n)})} $$

上式称为**指数损失(exponential loss)**。故**AdaBoost**的优化问题也可以看作通过**加性模型**(即每次训练一个新的基学习器$g_t$)迭代地优化**指数损失函数**。值得一提的是，上式省略了所有样本权重的加权和$Z_t$，后面会看到这也导致了**AdaBoost**算法的训练误差界。

### ⚪ 指数损失函数与0/1损失
**指数损失函数**是$0/1$损失的一个上界：

$$ \frac{1}{N} \sum_{n=1}^{N} \Bbb{I}(y_n ≠ G(x_n)) ≤ \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n G(x_n))} $$

![](https://pic.downk.cc/item/5edddb6ec2a9a83be54ae6dc.jpg)

下面证明在分类任务中指数损失是$0/1$损失的一个**一致(consistent)**替代损失函数。不妨记**AdaBoost**获得的集成模型为$G(x) = \sum_{t=1}^{T} {α_tg_t(x)}$，则指数损失表示为$\mathcal{L}_{\text{exp}} = \text{exp}(-yG(x)) = \text{exp}(-G(x))P(y=1\|x) + \text{exp}(G(x))P(y=-1\|x)$，令其对$G(x)$的偏导为$0$：

$$ \frac{\partial \mathcal{L}_{\text{exp}}}{\partial G(x)} = -\text{exp}(-G(x))P(y=1|x) + \text{exp}(G(x))P(y=-1|x) = 0 $$

解得：

$$ G(x) = \frac{1}{2} \text{ln}\frac{P(y=1|x)}{P(y=-1|x)} $$

则**AdaBoost**的输出可以表示为：

$$ \text{sign}(G(x)) = \text{sign}(\frac{1}{2} \text{ln}\frac{P(y=1|x)}{P(y=-1|x)}) \\ = \begin{cases} 1, \quad P(y=1|x)>P(y=-1|x) \\ -1, \quad P(y=1|x)<P(y=-1|x) \end{cases} \\ = \mathop{\arg \max}_{y \in \{-1,1\} } P(y|x) $$

上式表示若指数损失最小化，则模型达到贝叶斯最优错误率，这与$0/1$损失是一致的。而指数损失是连续可微函数，因此**AdaBoost**选用指数损失函数作为$0/1$损失函数的替代。

### ⚪ AdaBoost算法的训练误差界
下面求指数损失函数的误差界：

$$ \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n G(x_n))} = \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n \sum_{t=1}^{T} {α_tg_t(x_n)})} \\ = \frac{1}{N} \sum_{n=1}^{N} \prod_{t=1}^{T} {\text{exp}(-y_n  {α_tg_t(x_n)})} \\ (\text{由 } w_n^{(t+1)} = \frac{1}{Z_t} w_n^{(t)} × \text{exp}(-y_nα_tg_t(x_n))) \\ = \frac{1}{N} \sum_{n=1}^{N} \prod_{t=1}^{T}  \frac{w_n^{(t+1)}Z_t}{w_n^{(t)}} = \frac{1}{N} \sum_{n=1}^{N}(\frac{w_n^{(2)}Z_1}{w_n^{(1)}}\frac{w_n^{(3)}Z_2}{w_n^{(2)}}\cdot\cdot\cdot \frac{w_n^{(T+1)}Z_T}{w_n^{(T)}}) \\ = \frac{1}{N} \sum_{n=1}^{N}\frac{w_n^{(T+1)}}{w_n^{(1)}} \prod_{t=1}^{T} Z_t = \frac{1}{N} \sum_{n=1}^{N}\frac{w_n^{(T+1)}}{1/N} \prod_{t=1}^{T} Z_t = \sum_{n=1}^{N}w_n^{(T+1)} \prod_{t=1}^{T} Z_t = \prod_{t=1}^{T} Z_t $$

因此指数损失函数的误差界与所有样本权重的加权和$Z_t$相关；该结论从指数损失函数的推导过程中也能看出(推导过程省略了$\prod_{t=1}^{T} Z_t$)。因此在每轮训练时，通过选择合适的$g_t$使得$Z_t$尽可能小，从而使训练误差下降最快。

### ⚪ 二分类问题的AdaBoost算法的训练误差界
上面求得了指数损失函数的误差界$\prod_{t=1}^{T} Z_t$，对于二分类问题,注意到**错误率**的定义$ε_t = \frac{\sum_{n=1}^{N} {w_n^{(t)}[y_n ≠ g_t(x_n)]}}{\sum_{n=1}^{N} {w_n^{(t)}}} = \sum_{n=1}^{N} {w_n^{(t)}[y_n ≠ g_t(x_n)]}$，则有：

$$ \prod_{t=1}^{T} Z_t = \prod_{t=1}^{T} \sum_{n=1}^{N} w_n^{(t)} \text{exp}(-y_nα_tg_t(x_n)) \\ = \prod_{t=1}^{T} \sum_{n=1}^{N} ( w_n^{(t)} [y_n ≠ g_t(x_n)] \text{exp}(α_t) + w_n^{(t)} [y_n = g_t(x_n)] \text{exp}(-α_t) ) \\ = \prod_{t=1}^{T} (ε_t\text{exp}(α_t)+(1-ε_t)\text{exp}(-α_t)) \\ = \prod_{t=1}^{T}(ε_t\sqrt{\frac{1-ε_t}{ε_t}}+\sqrt{(1-ε_t)\frac{ε_t}{1-ε_t}}) = \prod_{t=1}^{T} 2\sqrt{ε_t(1-ε_t)} \\ (\text{由 }\gamma_t = \frac{1}{2}-ε_t) \\ = \prod_{t=1}^{T} \sqrt{1-4\gamma_t^2} ≤ \prod_{t=1}^{T} \text{exp}(-2\gamma_t^2) =  \text{exp}(-2\sum_{t=1}^{T}\gamma_t^2) $$

根据上式可得对于二分类问题，**AdaBoost**算法的训练误差以指数速率下降。

## (4) 从优化角度理解AdaBoost：如何选择$α_t$
**AdaBoost**可以被写作一个优化问题，即求第$t$个基学习器$g_t$及其权重$α_t$时，最小化**指数损失**函数：

$$ \mathop{\min}_{α_t,g_t} \mathcal{L}_{\text{exp}} = \mathop{\min}_{α_t,g_t} \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n \sum_{τ=1}^{t} {α_τg_τ(x_n)})} \\ = \mathop{\min}_{α_t,g_t}  \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n (\sum_{τ=1}^{t-1} {α_τg_τ(x_n)}+α_tg_t(x_n)))} $$

根据样本集的权重更新规律：

$$ w_n^{(T+1)}  = \frac{1}{N} \text{exp}(-y_n \sum_{t=1}^{T} {α_tg_t(x_n)}) $$

指数损失目标函数化简为：

$$ \mathop{\min}_{α_t,g_t}  \sum_{n=1}^{N} {w_n^{(t)}\text{exp}(-y_nα_tg_t(x_n))} $$

把上式拆分成$y_n=g_t(x_n)$和$y_n≠g_t(x_n)$两种情况：

$$ \mathop{\min}_{α_t,g_t}  \sum_{n=1}^{N} [y_n=g_t(x_n)]w_n^{(t)}\text{exp}(-α_t)+\sum_{n=1}^{N} [y_n≠g_t(x_n)]w_n^{(t)}\text{exp}(α_t) \\ =  \mathop{\min}_{α_t,g_t}  \sum_{n=1}^{N} {w_n^{(t)}} \{ \frac{\sum_{n=1}^{N} {[y_n=g_t(x_n)]w_n^{(t)}}}{\sum_{n=1}^{N} {w_n^{(t)}}} \text{exp}(-α_t)+\frac{\sum_{n=1}^{N} {[y_n≠g_t(x_n)]w_n^{(t)}}}{\sum_{n=1}^{N} {w_n^{(t)}}} \text{exp}(α_t)  \} \\ = \mathop{\min}_{α_t,g_t} \sum_{n=1}^{N} {w_n^{(t)}} [ (1-ε_t)\text{exp}(-α_t)+ε_t \text{exp}(α_t)] $$

上式对$α_t$求导，令其为$0$，可以得到：

$$ -(1-ε_t)\text{exp}(-α_t)+ε_t \text{exp}(α_t)=0 $$

解得：

$$ α_t = \frac{1}{2} ln(\sqrt{\frac{(1-ε_t)}{ε_t}}) $$

这与之前的定义是吻合的。

# 3. Gradient Boosting
之前介绍的**AdaBoost**方法通过对样本赋予不同的权重，训练不同的基学习器$g_t$，通过这些基学习器加权投票得到最终的结果：

$$ G = \text{sign}(\sum_{t=1}^{T} {α_tg_t}) $$

**梯度提升（Gradient Boosting）**则是每次寻找一个新的模型函数$g_t$，来减小最终的损失。**AdaBoost**可以看作一种解析方法，即每一次迭代都是通过确定的公式计算得到的；梯度提升可以看作其对应的数值方法，即每一次迭代是通过梯度上升方法近似得到的。

假设经过了$T-1$次训练，得到的未经过符号函数的模型为：

$$ f_{T-1} = \sum_{t=1}^{T-1} {α_tg_t} $$

在第$T$次训练中，需要确定模型$g_T$及其权重$α_T$，使得：

$$ f_{T} = f_{T-1} + α_Tg_T $$

损失函数使用指数损失：

$$ \text{loss} = \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n \sum_{t}^{} {α_tg_t(x_n)})} = \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n f)} $$

采用梯度下降法更新参数，注意是对函数求梯度：

$$ f_{T} = f_{T-1} - η\frac{\partial \text{loss}}{\partial f | f = f_{T-1}} = f_{T-1} + η\frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n f_{T-1})}y_n $$

注意到$g_T$与$\frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n f_{T-1})}y_n$在函数空间中应具有同样的方向，而$α_T$可以看作学习率。

采用不同的损失函数，会有不同的函数更新方式。
