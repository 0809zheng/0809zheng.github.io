---
layout: post
title: '集成学习中的提升(Boosting)方法'
date: 2020-03-18
author: 郑之杰
cover: 'https://pic.downk.cc/item/5efb009814195aa5948b6b2e.jpg'
tags: 机器学习
---

> An Ensemble Learning Method：Boosting.


# 1. Boosting的基本思想
**Boosting**是一种**集成学习**的方法，主要关注在同一个数据集上按顺序训练一系列相同类型的子模型，这些子模型被称作**基学习器(base learner)**，不同的基学习器能对不同的数据分布进行学习。**Boosting**主要关注降低**偏差**，因此能基于泛化性能相当弱的弱学习器构建出很强的集成学习器。

**Boosting**的基学习器并不是独立训练的，而是首先从初始数据集中训练一个基学习器，之后在每次训练一个新的基学习器时，会利用已经获得的基学习器的信息；如此反复进行，直至训练得到$T$个基学习器，最终对其进行加权组合。常见的**Boosting**策略有：
- **重加权(re-weighting)**：对每个训练样本赋予权重，如**AdaBoost**
- **重采样(re-sampling)**：根据权重对数据集采样
- **梯度(gradient)**方法：利用损失函数的负梯度信息，如**Gradient Boosting**

以重加权法为例，**Boosting**的基本流程如下。若需要训练$T$个基学习器$$\{g_t,t=1,2,...,T\}$$，设置训练集中的$N$个样本的初始权重$$\{w_n^{(1)}=\frac{1}{N},n=1,2,...,N\}$$；在第$t$轮训练过程中，根据样本分布为每个训练样本重新赋予权重$w_1^{(t)},w_2^{(t)},...,w_N^{(t)}$。记$ε_t$为基学习器$g_t$在第$t$次赋值的训练集上训练后的**错误率**，即：

$$ ε_t = \frac{\sum_{y_n ≠ g_t(x_n)} {w_n^{(t)}}}{\sum_{n=1}^{N} {w_n^{(t)}}} $$

则一种简单的样本权重更新方式表示如下，即增大错误预测样本的权重，减小正确预测样本的权重：

$$ w_n^{(t+1)} = \begin{cases} w_n^{(t)} × (1-ε_t) , & y_n ≠ g_t(x_n) \\ w_n^{(t)} × ε_t , & y_n = g_t(x_n) \end{cases} $$

经过上述过程训练，得到$T$个训练好的基学习器$$\{g_t,t=1,2,...,T\}$$，之后可以通过[**Blending**](https://0809zheng.github.io/2020/03/16/blending.html)方法集成模型，从而得到最终的结果。

重加权法要求每个基学习器虽然是**弱学习器**，但仍具有超过统计随机值的表现水平(即比随机猜测好)，即通常$ε_t<0.5$。再训练的每一轮需要检测当前生成的基学习器是否满足该基本条件，一旦条件不满足，则当前基学习器即被抛弃，且学习过程停止。这样会导致最终获得的基学习器个数远小于$T$个，使得集成学习性能不佳。

对于无法接受带有权重的样本的基学习器（比如决策树），可以通过**重采样(re-sampling)**方法实现**Boosting**。即在每一轮训练中，根据样本分布对训练集进行重新采样，再根据重采样的样本集训练基学习器。这种方法可以通过**重启动**避免由于不满足弱学习器条件而停止的问题，即抛弃当前不满足条件的基学习器后，可根据当前分布对训练样本重新采样，再根据新的采样结果训练学习器，从而保证学习过程可以持续到$T$轮结束。



# 2. 自适应梯度提升 AdaBoost
**AdaBoost(Adaptive Boosting)**是为**二分类**任务设计的**Boosting**重加权方法，主要包括：
- 单一模型的训练算法：**指数损失函数**
- 样本权重的赋值方法：$d_t = \sqrt{\frac{(1-ε_t)}{ε_t}}$
- 不同样本的组合方法：**加性模型**

若通过$T$轮训练获得$T$个基学习器$$\{g_t,t=1,2,...,T\}$$，则**AdaBoost**是一种**加性模型(additive model)**，即基学习器的线性组合：

$$ G(x) = \sum_{t=1}^{T} {α_tg_t(x)} $$

基学习器的获得是通过迭代地优化**指数损失函数(exponential loss function)**实现的：

$$ \mathcal{L}_{\text{exp}} = \text{exp}(-yG(x)) $$

下面以输出标签为$±1$的二分类为例，介绍**AdaBoost**算法。

## (1) AdaBoost算法的流程
假设需要训练$T$个基学习器$$\{g_t,t=1,2,...,T\}$$，训练开始时设置训练集中的$N$个样本的初始**样本权重**$$\{w_n^{(1)}=\frac{1}{N},n=1,2,...,N\}$$。

引入一个参数$d_t$，则**AdaBoost**的第$t+1$轮训练中，训练集第$n$个样本权重的更新方法如下：

$$ w_n^{(t+1)} = \begin{cases} \frac{1}{Z_t}w_n^{(t)} × d_t , & y_n ≠ g_t(x_n) \\ \frac{1}{Z_t}\frac{w_n^{(t)}}{d_t}, & y_n = g_t(x_n) \end{cases} $$

其中$Z_t$表示第$t$轮训练中所有样本权重的**加权和**，用于对训练样本集的权重进行归一化，计算为：

$$ \begin{aligned} Z_t&=\sum_{y_n ≠ g_t(x_n)}  w_n^{(t)} × d_t +\sum_{y_n = g_t(x_n)} \frac{w_n^{(t)}}{d_t} \\&=\sum_{n=1}^{N} w_n^{(t)} d_t^{-y_ng_t(x_n)} \end{aligned} $$

注意到所有样本权重的**加权和**$Z_t$并不等于样本权重之和！事实上通过引入$Z_t$使得每轮训练中的样本权重值和都满足$\sum_{n=1}^{N} w_n^{(t)}=1$。

通常参数$d_t>1$，即增大错误预测样本的权重，减小正确预测样本的权重。
对于第$t+1$次训练，希望新的基学习器能够训练得到与$g_t$**互补**的模型，即假设$g_t$在重新设置权重的训练集上只能得到**随机**的效果：

$$ \frac{\sum_{y_n ≠ g_t(x_n)} {w_n^{(t+1)}}}{\sum_{n=1}^{N} {w_n^{(t+1)}}} = \frac{1}{2} $$

将训练样本权重的更新公式代入得：

$$ \frac{\sum_{y_n ≠ g_t(x_n)} {\frac{1}{Z_t} w_n^{(t)} × d_t}}{\sum_{y_n ≠ g_t(x_n)} {\frac{1}{Z_t}w_n^{(t)} × d_t} + \sum_{y_n = g_t(x_n)}\frac{1}{Z_t}{\frac{w_n^{(t)}}{d_t}}} = \frac{1}{2} $$

将$d_t$提出求和项，并约分$1/Z_t$项：

$$ \frac{d_t\sum_{y_n ≠ g_t(x_n)} { w_n^{(t)} }}{d_t\sum_{y_n ≠ g_t(x_n)} {w_n^{(t)}  } + \frac{1}{d_t}\sum_{y_n = g_t(x_n)}w_n^{(t)}} = \frac{1}{2} $$

注意到**错误率** $ε_t \propto \sum_{y_n ≠ g_t(x_n)} {w_n^{(t)}}$，得：

$$ \frac{d_tε_t}{d_tε_t + \frac{1}{d_t}(1-ε_t)} = \frac{1}{2} $$

解得：

$$d_t = \sqrt{\frac{(1-ε_t)}{ε_t}}$$

记每一个基学习器$g_t$在最终模型中所占的**模型权重**为$α_t$，则计算该权重的公式为：

$$ α_t = \ln(d_t) = \ln(\sqrt{\frac{(1-ε_t)}{ε_t}}) = \frac{1}{2} \ln(\frac{1-ε_t}{ε_t}) $$

分析：
- 当$g_t$的分类错误率$ε_t=0.5$，则该模型与随机分类无异，对应权重$α_t=0$;
- 当$g_t$的分类错误率$ε_t=0$，则该模型完全分类正确，对应权重$α_t=+∞$；
- 分类错误率$ε_t$越小，投票时模型$g_t$所占比重越大。

训练得到一系列基学习器$$\{g_t,t=1,2,...,T\}$$后，最终的集成模型为：

$$ G = \text{sign}(\sum_{t=1}^{T} {α_tg_t}) $$

## (2) AdaBoost的VC维
定义某假设函数对于观测样本的误差为$E_{in}$，该误差可以计算得到；定义该假设函数对于总样本的误差为$E_{out}$，该误差不可计算。
则**AdaBoost**的**VC维**表示为：

$$ E_{out} ≤ E_{in} + O(\sqrt{O(d_{VC}(H)·T\log T)·\frac{\log N}{N}}) $$

**AdaBoost**具有良好的性质：
- 若基本模型仅比随机结果好一些($ε_t<\frac{1}{2}$)，则经过$T=O(\log N)$次迭代后$E_{in}≈0$；
- 若样本数$N$很大，则最终得到的模型满足$E_{out}≈E_{in}$。

## (3) 指数损失 Exponential Loss
根据定义的模型权重$$\{α_t,t=1,2,...,T\},α_t = \ln(d_t)$$
对于标签是$±1$的二分类任务，样本集的权重更新可以表示为：

$$ \begin{aligned} w_n^{(t+1)} &= \begin{cases} \frac{1}{Z_t}w_n^{(t)} × d_t , & y_n ≠ g_t(x_n) \\ \frac{1}{Z_t}\frac{w_n^{(t)}}{d_t}, & y_n = g_t(x_n) \end{cases} \\ &= \frac{1}{Z_t} w_n^{(t)} d_t^{-y_ng_t(x_n)} \\ &= \frac{1}{Z_t} w_n^{(t)} \text{exp}(-y_nα_tg_t(x_n)) \end{aligned} $$

其中所有样本权重的**加权和**$Z_t$也可由模型权重$$α_t$$表示为：

$$ Z_t=\sum_{n=1}^{N} w_n^{(t)} d_t^{-y_ng_t(x_n)} = \sum_{n=1}^{N} w_n^{(t)} \text{exp}(-y_nα_tg_t(x_n)) $$

若初始样本权重$w_n^{(1)}=\frac{1}{N}$，则样本权重更新为：

$$ \begin{aligned} w_n^{(T+1)} &= \frac{1}{Z_T} w_n^{(T)} \text{exp}(-y_nα_Tg_T(x_n)) \\&= \frac{1}{Z_T}  \frac{1}{Z_{T-1}} w_n^{(T-1)} \text{exp}(-y_nα_{T-1}g_{T-1}(x_n)) \text{exp}(-y_nα_Tg_T(x_n)) \\&...\\ &= \frac{1}{\prod_{t=1}^{T}Z_t} \frac{1}{N} \prod_{t=1}^{T} {\text{exp}(-y_nα_tg_t(x_n))} \\&= \frac{1}{\prod_{t=1}^{T}Z_t} \frac{1}{N} \text{exp}(-y_n \sum_{t=1}^{T} {α_tg_t(x_n)}) \end{aligned} $$

对于单个样本$(x_n,y_n)$的样本权重$w_n$，当算法对该样本分类正确时，$y_n$应与$g(x_n)$同号，且$g(x_n)$越大越好（类似于间隔的概念），即$w_n$是逐渐**减小**的。

对于所有样本，可以列出一个最优化问题，即最小化所有**样本权重**：

$$ \begin{aligned} \mathop{\min}  \sum_{n=1}^{N} {w_n^{(T+1)}} & = \mathop{\min} \sum_{n=1}^{N} \frac{1}{\prod_{t=1}^{T}Z_t} \frac{1}{N} \text{exp}(-y_n \sum_{t=1}^{T} {α_tg_t(x_n)})  \\ &\propto \mathop{\min} \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n \sum_{t=1}^{T} {α_tg_t(x_n)})} \end{aligned} $$

上式称为**指数损失(exponential loss)**。故**AdaBoost**的优化问题也可以看作通过**加性模型**(即每次训练一个新的基学习器$g_t$)迭代地优化**指数损失函数**。值得一提的是，上式省略了所有样本权重的加权和$Z_t$，后面会看到这也导致了**AdaBoost**算法的训练误差界。

### ⚪ 指数损失函数与0/1损失
**指数损失函数**是$0/1$损失的一个上界：

$$ \frac{1}{N} \sum_{n=1}^{N} \Bbb{I}(y_n ≠ G(x_n)) ≤ \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n G(x_n))} $$

![](https://pic.imgdb.cn/item/63a7fe4708b6830163ec7a22.jpg)

下面证明在分类任务中指数损失是$0/1$损失的一个**一致(consistent)**替代损失函数。不妨记**AdaBoost**获得的集成模型为$G(x) = \sum_{t=1}^{T} {α_tg_t(x)}$，则指数损失表示为：

$$ \mathcal{L}_{\text{exp}} = \text{exp}(-yG(x)) = \text{exp}(-G(x))P(y=1|x) + \text{exp}(G(x))P(y=-1|x) $$

令其对$G(x)$的偏导为$0$：

$$ \frac{\partial \mathcal{L}_{\text{exp}}}{\partial G(x)} = -\text{exp}(-G(x))P(y=1|x) + \text{exp}(G(x))P(y=-1|x) = 0 $$

解得：

$$ G(x) = \frac{1}{2} \text{ln}\frac{P(y=1|x)}{P(y=-1|x)} $$

则**AdaBoost**的输出可以表示为：

$$ \begin{aligned} \text{sign}(G(x)) &= \text{sign}(\frac{1}{2} \text{ln}\frac{P(y=1|x)}{P(y=-1|x)}) \\ &= \begin{cases} 1, & P(y=1|x)>P(y=-1|x) \\ -1, & P(y=1|x)<P(y=-1|x) \end{cases} \\ &= \mathop{\arg \max}_{y \in \{-1,1\} } P(y|x) \end{aligned} $$

上式表示若指数损失最小化，则模型达到贝叶斯最优错误率，这与$0/1$损失是一致的。而指数损失是连续可微函数，因此**AdaBoost**选用指数损失函数作为$0/1$损失函数的替代。

### ⚪ AdaBoost算法的训练误差界
下面求指数损失函数的误差界：

$$ \begin{aligned} \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n G(x_n))} &= \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n \sum_{t=1}^{T} {α_tg_t(x_n)})} \\ &= \frac{1}{N} \sum_{n=1}^{N} \prod_{t=1}^{T} {\text{exp}(-y_n  {α_tg_t(x_n)})} \\ &(\text{由 } w_n^{(t+1)} = \frac{1}{Z_t} w_n^{(t)} × \text{exp}(-y_nα_tg_t(x_n))) \\ &= \frac{1}{N} \sum_{n=1}^{N} \prod_{t=1}^{T}  \frac{w_n^{(t+1)}Z_t}{w_n^{(t)}} \\ &= \frac{1}{N} \sum_{n=1}^{N}(\frac{w_n^{(2)}Z_1}{w_n^{(1)}}\frac{w_n^{(3)}Z_2}{w_n^{(2)}}\cdot\cdot\cdot \frac{w_n^{(T+1)}Z_T}{w_n^{(T)}}) \\ &= \frac{1}{N} \sum_{n=1}^{N}\frac{w_n^{(T+1)}}{w_n^{(1)}} \prod_{t=1}^{T} Z_t \\ &= \frac{1}{N} \sum_{n=1}^{N}\frac{w_n^{(T+1)}}{1/N} \prod_{t=1}^{T} Z_t = \sum_{n=1}^{N}w_n^{(T+1)} \prod_{t=1}^{T} Z_t \\ &= \prod_{t=1}^{T} Z_t \end{aligned} $$

因此指数损失函数的误差界与所有样本权重的加权和$Z_t$相关；该结论从指数损失函数的推导过程中也能看出(推导过程省略了$\prod_{t=1}^{T} Z_t$)。因此在每轮训练时，通过选择合适的$g_t$使得$Z_t$尽可能小，从而使训练误差下降最快。

### ⚪ 二分类问题的AdaBoost算法的训练误差界
上面求得了指数损失函数的误差界$\prod_{t=1}^{T} Z_t$，对于二分类问题,注意到**错误率**的定义$ε_t \propto \sum_{y_n ≠ g_t(x_n)} {w_n^{(t)}}$，则有：

$$ \begin{aligned} \prod_{t=1}^{T} Z_t &= \prod_{t=1}^{T} \sum_{n=1}^{N} w_n^{(t)} \text{exp}(-y_nα_tg_t(x_n)) \\ &= \prod_{t=1}^{T} (\sum_{y_n ≠ g_t(x_n)} w_n^{(t)}  \text{exp}(α_t) + \sum_{y_n = g_t(x_n)}w_n^{(t)}  \text{exp}(-α_t) ) \\ &= \prod_{t=1}^{T} (ε_t\text{exp}(α_t)+(1-ε_t)\text{exp}(-α_t)) \\ &= \prod_{t=1}^{T}(ε_t\sqrt{\frac{1-ε_t}{ε_t}}+\sqrt{(1-ε_t)\frac{ε_t}{1-ε_t}}) = \prod_{t=1}^{T} 2\sqrt{ε_t(1-ε_t)} \\ &(\text{由 }\gamma_t = \frac{1}{2}-ε_t) \\ &= \prod_{t=1}^{T} \sqrt{1-4\gamma_t^2} \\ & \leq \prod_{t=1}^{T} \text{exp}(-2\gamma_t^2) =  \text{exp}(-2\sum_{t=1}^{T}\gamma_t^2) \end{aligned} $$

根据上式可得对于二分类问题，**AdaBoost**算法的训练误差以指数速率下降。

## (4) 从优化角度理解AdaBoost：如何选择$α_t$
**AdaBoost**可以被写作一个优化问题，即求第$t$个基学习器$g_t$及其权重$α_t$时，最小化**指数损失**函数：

$$ \begin{aligned} \mathop{\min}_{α_t,g_t} \mathcal{L}_{\text{exp}} &= \mathop{\min}_{α_t,g_t} \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n \sum_{τ=1}^{t} {α_τg_τ(x_n)})} \\ &= \mathop{\min}_{α_t,g_t}  \frac{1}{N} \sum_{n=1}^{N} {\text{exp}(-y_n (\sum_{τ=1}^{t-1} {α_τg_τ(x_n)}+α_tg_t(x_n)))} \end{aligned} $$

根据样本集的权重更新规律：

$$ w_n^{(T+1)}  = \frac{1}{N} \text{exp}(-y_n \sum_{t=1}^{T} {α_tg_t(x_n)}) $$

指数损失目标函数化简为：

$$ \mathop{\min}_{α_t,g_t}  \sum_{n=1}^{N} {w_n^{(t)}\text{exp}(-y_nα_tg_t(x_n))} $$

把上式拆分成$y_n=g_t(x_n)$和$y_n≠g_t(x_n)$两种情况：

$$ \begin{aligned} & \mathop{\min}_{α_t,g_t} \sum_{y_n=g_t(x_n)} w_n^{(t)}\text{exp}(-α_t)+\sum_{y_n\neq g_t(x_n)} w_n^{(t)}\text{exp}(α_t) \\ &=  \mathop{\min}_{α_t,g_t}  \sum_{n=1}^{N} {w_n^{(t)}} \{ \frac{\sum_{y_n=g_t(x_n)} {w_n^{(t)}}}{\sum_{n=1}^{N} {w_n^{(t)}}} \text{exp}(-α_t)+\frac{\sum_{y_n≠g_t(x_n)} {w_n^{(t)}}}{\sum_{n=1}^{N} {w_n^{(t)}}} \text{exp}(α_t)  \} \\ &= \mathop{\min}_{α_t,g_t} \sum_{n=1}^{N} {w_n^{(t)}} [ (1-ε_t)\text{exp}(-α_t)+ε_t \text{exp}(α_t)] \end{aligned} $$

上式对$α_t$求导，令其为$0$，可以得到：

$$ -(1-ε_t)\text{exp}(-α_t)+ε_t \text{exp}(α_t)=0 $$

解得：

$$ α_t = \frac{1}{2} \ln(\sqrt{\frac{(1-ε_t)}{ε_t}}) $$

这与之前的定义是吻合的。

# 3. 梯度提升 Gradient Boosting

**Boosting**方法按顺序训练一系列基学习器$g_t$，通过加权这些基学习器得到最终的结果：

$$ G(x) = \sum_{t=1}^{T} {α_tg_t(x)} $$

对于二分类任务，**AdaBoost**把损失函数设置为指数损失函数，从而给出了每次寻找一个新的模型函数$g_t$的解析方法，即每一次迭代都是通过确定的公式计算得到的。然而对于一般损失函数而言，每一步优化过程并不容易。

**Freidman**提出了**梯度提升 (Gradient Boosting)**方法，在每一轮迭代中，首先计算出当前模型在所有样本上的**负梯度**，然后以该值为目标训练一个新的弱分类器进行拟合并计算出该弱分类器的权重，最终实现对模型的更新。

在梯度提升过程中，假设经过了$T-1$次训练得到的模型为：

$$ f_{T-1}(x) = \sum_{t=1}^{T-1} {α_tg_t(x)} $$

在第$T$次训练中，需要确定模型$g_T$及其权重$α_T$，使得：

$$ f_{T}(x) = f_{T-1}(x) + α_Tg_T(x) $$

对于任意损失函数$L(y,f(x))$，对其在$f(x)=f_{T-1}(x)$处进行一阶泰勒展开：

$$ L(y,f(x)) ≈ L(y,f_{T-1}(x)) + [\frac{\partial  L(y,f(x))}{\partial f(x)}]_{f(x)=f_{T-1}(x)} (f(x)-f_{T-1}(x)) $$

令$f(x)=f_{T}(x)$：

$$ L(y,f_{T}(x)) ≈ L(y,f_{T-1}(x)) + [\frac{\partial  L(y,f(x))}{\partial f(x)}]_{f(x)=f_{T-1}(x)} (f_{T}(x)-f_{T-1}(x)) $$

因此若希望损失函数减小，则$f_{T}(x)-f_{T-1}(x)$应与$$[\frac{\partial  L(y,f(x))}{\partial f(x)}]_{f(x)=f_{T-1}(x)}$$异号，记为：

$$ f_{T}(x)-f_{T-1}(x) = -\alpha_T [\frac{\partial  L(y,f(x))}{\partial f(x)}]_{f(x)=f_{T-1}(x)} $$

上式可以看作在函数空间中的梯度下降算法，$α_T$可以看作学习率：

$$ f_{T}(x) = f_{T-1}(x)-\alpha_T [\frac{\partial  L(y,f(x))}{\partial f(x)}]_{f(x)=f_{T-1}(x)} $$

因此第$T$次训练中的模型$g_T$可以拟合当前损失函数的负梯度：

$$ g_T(x) = -[\frac{\partial  L(y,f(x))}{\partial f(x)}]_{f(x)=f_{T-1}(x)} $$
