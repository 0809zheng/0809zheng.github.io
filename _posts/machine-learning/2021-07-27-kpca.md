---
layout: post
title: '核主成分分析'
date: 2021-07-27
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/610108ee5132923bf87767d5.jpg'
tags: 机器学习
---

> Kernelized Principal Component Analysis.

**核主成分分析**(**kernelized principal component analysis,KPCA**)是指将[核方法](https://0809zheng.github.io/2021/07/23/kernel.html)引入[主成分分析](https://0809zheng.github.io/2020/04/11/PCA.html)，从而把线性降维方法转变成非线性降维方法。

**KPCA**的基本流程是对于维度为$d$的原始样本空间中的样本点$x \in \Bbb{R}^d$，引入映射$\phi$将其投影到维度为$d'$的高维特征空间$z=\phi(x) \in \Bbb{R}^{d'}$，再在特征空间中实施**PCA**方法，得到降维结果$y=W^Tz \in \Bbb{R}^{k}$。

![](https://pic.imgdb.cn/item/610108ee5132923bf87767d5.jpg)

其中线性变换矩阵$W \in \Bbb{R}^{d' \times k}$将高维样本投影到维度为$k$的低维空间中，根据**表示定理**其第$j$维的投影向量$w_j \in \Bbb{R}^{d'}$可以表示成：

$$ w_j = \sum_{i=1}^{N}\alpha_i^j\phi(x_i) = Z\alpha^j $$

其中$\alpha^j=(\alpha_1^j,\alpha_2^j,...,\alpha_N^j) \in \Bbb{R}^N$。

根据主成分分析的结果，投影向量$w_j$应满足：

$$ (\sum_{i=1}^{N}\phi(x_i)\phi(x_i)^T)w_j = λ_jw_j $$

由于$\sum_{i=1}^{N}\phi(x_i)\phi(x_i)^T=\sum_{i=1}^{N}z_iz_i^T=ZZ^T$及$w_j =Z\alpha^j$，上式可以表示为：

$$ ZZ^TZ\alpha^j = λ_jZ\alpha^j $$

一般情况下，我们不清楚映射$\phi$的具体形式，因此引入核函数：

$$ \kappa(x_i,x_j)=\phi(x_i)^T\phi(x_j), \quad K=Z^TZ $$

则原式进一步表示为：

$$ ZK\alpha^j = Zλ_j\alpha^j $$

若下式满足，则上式自动满足：

$$ K\alpha^j = λ_j\alpha^j $$

即$\alpha^j$是$K$的特征向量。取$K$最大的$k$个特征向量$\{\alpha^1,\alpha^2,...,\alpha^k\}$，则原始样本$x$投影后的第$j$维坐标$y_j$为：

$$ y_j = w_j^T\phi(x)=\sum_{i=1}^{N}\alpha_i^j\phi(x_i)^T\phi(x) = \sum_{i=1}^{N}\alpha_i^j\kappa(x_i,x) $$



