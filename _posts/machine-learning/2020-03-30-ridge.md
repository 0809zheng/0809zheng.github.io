---
layout: post
title: 'Ridge/LASSO Regression：岭回归与LASSO回归'
date: 2020-03-30
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60e7fa3f5132923bf811b50d.jpg'
tags: 机器学习
---

> Ridge Regression and LASSO Regression.

岭回归与**LASSO**回归是将正则化思想引入线性回归模型后得到的方法。
- 岭回归对模型的参数加入**L2正则化**，能够有效的防止模型**过拟合**，解决非满秩下**求逆**困难的问题
- **LASSO**回归全称为**Least Absolute Shrinkage and Selection Operator(最小绝对收缩选择算子,套索算法)**，对模型的参数加入**L1正则化**，能够**稀疏**矩阵，进行庞大特征数量下的**特征选择**

# 1. Ridge Regression
**岭回归（Ridge Regression）**是引入了**L2正则化**的线性回归，求解问题如下：

$$ L(w) = \sum_{i=1}^{N} {(w^Tx^{(i)}-y^{(i)})^2} + λw^2 = (Xw-y)^T(Xw-y)+ λw^Tw $$

令梯度为零可以得到：

$$ \nabla_wL(w) = 2X^TXw - 2Xy + 2λw = 0 $$

该问题的解析解为：

$$ w = (X^TX+λI)^{-1}Xy $$

注意到正则化系数$λ$通常大于零，故矩阵$X^TX+λI$一定**可逆**。

### 讨论：岭回归等价于噪声和先验服从高斯分布的最大后验估计
引入高斯噪声$ε$~$N(0,σ^2)$，对线性回归建模：

$$ y = w^Tx + ε $$

贝叶斯角度认为参数$w$不再是常数，而是随机变量，假设其先验概率为$N(0,σ_0^2)$,

由贝叶斯定理，参数$w$的后验概率：

$$ P(w | y) = \frac{P(y | w)P(w)}{P(y)} $$

由最大后验估计：

$$ \hat{w} = \mathop{\arg \max}_{w}logP(w | y) = \mathop{\arg \max}_{w}logP(y | w)P(w) \\ = \mathop{\arg \max}_{w} log(\prod_{i=1}^{N} {\frac{1}{\sqrt{2\pi}σ}exp(-\frac{(y^{(i)}-w^Tx^{(i)})^2}{2σ^2})\frac{1}{\sqrt{2\pi}σ_0}exp(-\frac{w^Tw}{2σ_0^2})}) \\ = \mathop{\arg \max}_{w} \sum_{i=1}^{N} {log(\frac{1}{\sqrt{2\pi}σ}exp(-\frac{(y^{(i)}-w^Tx^{(i)})^2}{2σ^2})\frac{1}{\sqrt{2\pi}σ_0}exp(-\frac{w^Tw}{2σ_0^2}))} \\ = \mathop{\arg \max}_{w} \sum_{i=1}^{N} {-\frac{(y^{(i)}-w^Tx^{(i)})^2}{2σ^2}-\frac{w^Tw}{2σ_0^2}} \\ = \mathop{\arg \min}_{w} \sum_{i=1}^{N} {(y^{(i)}-w^Tx^{(i)})^2+\frac{σ^2}{σ_0^2}w^Tw} $$

该问题等价于引入**L2**正则化的最小二乘法（岭回归）。

# 2. Kernel Ridge Regression
将[核方法](https://0809zheng.github.io/2021/07/23/kernel.html)引入岭回归，即可得到**核岭回归(Kernelized Ridge Regression)**。

岭回归的目标函数：

$$ L(w) = \frac{1}{N}\sum_{i=1}^{N} {(w^Tx^{(i)}-y^{(i)})^2} + \frac{λ}{N}w^Tw $$

根据表示定理，上述优化问题的最优解$w$可以表示为**所有样本的线性组合**：

$$ w^* = \sum_{n=1}^{N} {β_nx^{(n)}} $$

代入目标函数：

$$ L(β) = \frac{1}{N} \sum_{i=1}^{N} {(\sum_{n=1}^{N} {β_n{x^{(n)}}^Tx^{(i)}}-y^{(i)})^2} + \frac{λ}{N}\sum_{i=1}^{N} {\sum_{j=1}^{N} {β_iβ_j{x^{(i)}}^Tx^{(j)}}} $$

引入核函数来代替样本的特征转换和内积运算，记$$K(x,x')={φ(x)}^Tφ(x')$$，则核岭回归的损失函数为：

$$ L(β) = \frac{1}{N} \sum_{i=1}^{N} {(\sum_{n=1}^{N} {β_nK(x^{(n)},x^{(i)})}-y^{(i)})^2} + \frac{λ}{N}\sum_{i=1}^{N} {\sum_{j=1}^{N} {β_iβ_jK(x^{(i)},x^{(j)})}} $$

上式写作矩阵形式：

$$ L(β) = \frac{1}{N} (y-Kβ)^T(y-Kβ) + \frac{λ}{N}β^TKβ $$

对$β$求梯度，令其为零，(注意到$K$是对称矩阵)可得：

$$ ▽_βL(β) = \frac{1}{N}(2K^TKβ-2K^Ty) + \frac{λ}{N}2Kβ = \frac{2K^T}{N}(Kβ-y+λβ) = 0 $$

可解得引入核岭回归的解析解：

$$ β = (K+λI)^{-1}y $$

对矩阵$K+λI$的讨论：
- 由于$λ>0$，$K$是半正定矩阵，其逆矩阵一定存在。
- 该矩阵是一个**稠密(dense)**矩阵，大部分元素不是$0$，求逆过程计算量较大。

求得$β$后，可以得到回归函数：

$$ y = \sum_{n=1}^{N} {β_nφ(x^{(n)})^Tφ(x)} = \sum_{n=1}^{N} {β_nK(x^{(n)},x)} $$

# 3. LASSO回归 LASSO Regression
**LASSO回归**是引入了**L1正则化**的线性回归，求解问题如下：

$$ L(w) = \sum_{i=1}^{N} {(w^Tx^{(i)}-y^{(i)})^2} + λ\sum_{d}^{} {| w_d |} $$

**LASSO回归**等价于噪声服从高斯分布、参数服从拉普拉斯分布的最大后验估计。



