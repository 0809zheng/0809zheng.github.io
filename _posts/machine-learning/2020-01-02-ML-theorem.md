---
layout: post
title: '机器学习的一些定理'
date: 2020-01-02
author: 郑之杰
cover: ''
tags: 机器学习
---

> Some Machine Learning Theorems.

本文目录：
1. 没有免费午餐定理
2. 霍夫丁不等式
3. VC维
4. 模型复杂度
5. 概率近似正确
6. 奥卡姆剃刀
7. 抽样偏差
8. 数据窥探
9. 维度灾难


# 1. 没有免费午餐定理
**没有免费午餐（No Free Lunch）定理**表明没有一个学习算法可以在任何领域总是产生最准确的学习器。

不管采用何种学习算法，至少存在一个目标函数，能够使得随机猜测算法是更好的算法。

平常所说的一个学习算法比另一个算法更“优越”，效果更好，只是针对特定的问题、特定的先验信息、数据的分布、训练样本的数目、代价或奖励函数等。

# 2. 霍夫丁不等式
假设给定的样本集是从总样本集中随机抽样得到的有限集合，下面从分类的角度出发推导霍夫丁不等式。

记总样本集服从分布$P$，存在一个能将总样本集中的所有样本完全正确分类的函数$f$；

记观测到的样本集是$${x_1,x_2,...,x_N}$$，模型的假设空间为$H$，从中选择一个假设函数$h$；

定义该假设函数对于观测到的样本的误差，该误差可以计算得到：
- **in-sample error**：$$E_{in}(h)=\sum_{n=1}^{N} {[h(x_n)≠y_n]}$$

定义该假设函数对于总样本的误差，该误差不可计算：
- **out-of-sample error**：$$E_{out}(h)=ε_{x \text{~} P}[h(x)≠f(x)]$$

### 单个假设

**霍夫丁不等式(Hoeffding’s inequality)**描述了这两个误差的关系：

$$ P[\mid E_{in}(h)-E_{out}(h) \mid > ε] ≤ 2exp(-2ε^2N) $$

### 有限假设空间
若模型的假设空间$H$中存在有限的$M$个假设函数$h$，则对应的霍夫丁不等式：

$$ P[\exists h \in H \quad s.t. \quad \mid E_{in}(h)-E_{out}(h) \mid > ε] ≤ \sum_{i=1}^{M} {P[\mid E_{in}(h_i)-E_{out}(h_i) \mid > ε]} ≤ 2Mexp(-2ε^2N) $$

### 无限假设空间
若模型的假设空间$H$存在无穷多个假设函数$h$，事实上许多假设函数对于有限个样本点来说是等价的，比如只对一个样本点进行二分类，尽管可能有无穷多个假设函数，但最终的结果只有两种。

引入**成长函数（growth function）**$m_H$，则对应的霍夫丁不等式：

$$ P[\exists h \in H \quad s.t. \quad \mid E_{in}(h)-E_{out}(h) \mid > ε] ≤ 4m_H(2N)exp(-\frac{1}{8}ε^2N) $$

引入**break point** $k$，是指对于$k$个样本，该假设空间内的所有假设都不能得到所有分类的结果（$2^k$种）。

- 比如二维空间的感知机，对于两个样本点可以得到所有的$4$种情况，但是对于三个样本点不能实现所有的$8$种情况（考虑三个样本点共线时，如下图所示），则对于二维空间的感知机，$k=3$。
- 当$k$是一个模型的**break point**时，所有大于$k$的正整数也是该模型的**break point**。

![](https://pic.downk.cc/item/5ece267ac2a9a83be587c7ac.jpg)

若假设空间$H$中存在**break point** $k$，则成长函数$m_H$存在一个上界$B(N,k)$:

$$ m_H ≤ B(N,k) ≤ \sum_{i=0}^{k-1} {C_N^i} ≤ N^{k-1} $$

上式右端是$N$的多项式函数，即成长函数$m_H$的增长不会超过多项式级别。

# 3. VC维
**VC维（Vapnik-Chervonenkis Dimension）**的定义是模型最小的**break point**减一：

$$ d_{VC} = min(k)-1 $$

$VC$维表示当给定不超过该数量的样本点时，任何一种可能的标记情况都可以找到假设空间中的一个假设进行正确的分类。

当引入$VC$维，霍夫丁不等式可以写作：

$$ P[\exists h \in H \quad s.t. \quad \mid E_{in}(h)-E_{out}(h) \mid > ε] ≤ 4(2N)^{d_{VC}}exp(-\frac{1}{8}ε^2N) $$

上式右端也称为**Vapnik-Chervonenkis(VC) bound**。

$VC$ $Bound$是比较宽松的，而如何收紧它却不是那么容易。但$VC$ $Bound$基本上对所有模型的宽松程度是基本一致的，不同模型之间还是可以横向比较。

$VC$维表示模型的能力或自由度，通常可以用模型中可自由调节的参数个数近似。

常用模型的$VC$维:
- $d$维的感知机：$d_{VC}=d+1$
- 在$d$维空间，当数据点分布在半径为$R$的超球体内时，间距为$ρ$的支持向量机的$VC$维满足：$d_{VC}(A_ρ) ≤ min(\frac{R^2}{ρ^2},d)+1$
- 神经元数量为$V$、权重数量为$D$的神经网络：$d_{VC}=VD$

# 4. 模型复杂度
根据引入$VC$维的霍夫丁不等式：

$$ P[\exists h \in H \quad s.t. \quad \mid E_{in}(h)-E_{out}(h) \mid > ε] ≤ 4(2N)^{d_{VC}}exp(-\frac{1}{8}ε^2N) $$

若记上式右端的概率值为$δ$，则误差$E_{in}$和$E_{out}$超过$ε$的概率不超过$δ$，或者说以大于$1-δ$的概率认为：

$$ \mid E_{in}-E_{out} \mid ≤ ε = \sqrt{\frac{8}{N}ln(\frac{4(2N)^{d_{VC}}}{δ})} $$

上式右端表示模型误差的上界，称为**模型复杂度（model complexity）**，记为$Ω(N,H,δ)$。

- 样本总数$N$越大，模型复杂度越小；
- 模型的假设空间$H$越大（$d_{VC}$越大），模型复杂度越大；
- 所允许超过误差的概率$δ$越大，模型复杂度越小。

下面给出误差$E_{in}$、$E_{out}$和模型复杂度$Ω$随$VC$维的变化：
- $VC$维越大，模型越复杂，$E_{in}$越小，模型复杂度$Ω$越大；
- $VC$维越小，模型越简单，$E_{in}$越大，模型复杂度$Ω$越小；
- 随着$VC$维增大，$E_{out}$先减小后增大，最好的$d_{VC}^*$在中间。

![](https://pic.downk.cc/item/5ece3333c2a9a83be5979cf5.jpg)

- 当$VC$维太小时，$E_{in}$、$E_{out}$都很大，模型对训练样本的拟合度太差，称为**欠拟合（underfitting）**；
- 当$VC$维太大时，$E_{in}$很小、$E_{out}$很大，对训练样本拟合过分好，称为**过拟合（overfitting）**；

**样本复杂度（Sample Complexity）**定义了选定$d_{VC}$，样本数据$N$选择多少合适。

理论上$N≈10000d_{VC}$，由于$VC$ $Bound$是比较宽松的，在实践中取$N≈10d_{VC}$。

# 5. 概率近似正确
由霍夫丁不等式可以观察到：

$$ P[\exists h \in H \quad s.t. \quad \mid E_{in}(h)-E_{out}(h) \mid > ε] ≤ 4m_H(2N)exp(-\frac{1}{8}ε^2N) $$

当样本数量$N$很大时，上式的概率上界接近0，误差$E_{in}$和$E_{out}$相差不会很大，可近似认为$E_{in}=E_{out}$，这个结论叫做**概率近似正确（probably approximately correct，PAC）**。

则可以训练一个算法，从模型的假设空间$H$中选择一个具体的模型使$E_{in}≈0$，则可以认为$E_{out}≈0$。

# 6. 奥卡姆剃刀
**奥卡姆剃刀（Occam's Razor）**
- 原理：“如无必要，勿增实体”（Entities must not be multiplied unnecessarily）
- 应用于机器学习：能够拟合数据的模型中最简单的是最好的。

模型“简单”是指模型复杂度低。对于一个模型具有更少的假设空间，对于一个假设具有更少的参数量。

# 7. 抽样偏差
**抽样偏差（sampling bias）**是指：如果数据抽样时存在偏差，模型学习的结果也会产生偏差。

尽可能保持训练环境和测试环境接近，即训练数据的分布和测试数据的分布一致。

# 8. 数据窥探
要防止**数据窥探（data snooping）**，即测试数据不能以任何形式泄漏到训练过程中。

在实际应用中很难做到完全不窥探数据。比如某个研究领域使用某基准benchmarks，前人已发表论文中提到的方法在解决这个问题时，已经间接泄露了一些数据信息；当你在前人的基础上开发出新的模型，实际上已经应用了泄漏的数据，增加了过拟合的风险。

一种常用的防止数据窥探的方法是不人为的引入先验知识。比如解决某一问题时，先不观察具体的数据集，而是先提出一些可能的解决方法和应用技巧，再将其应用到数据上；而不是通过数据来选择模型。

# 9. 维度灾难
**维度灾难（the curse of dimensionality）**指出，数据在高维空间中的分布具有稀疏性，从而产生有悖于低维空间（如三维空间）常识的现象。
- **例1.** 考虑$k$维空间中边长为$1$的超立方体内接超球体，其半径为$\frac{1}{2}$，其体积（测度）为$\text{const} \cdot (\frac{1}{2})^k $，当$k→∞$时，超球体体积为$0$，即高维空间中边长为$1$的超立方体内的数据集中分布在立方体**角落**。
- **例2.** 考虑$k$维空间中半径为$r$和$r+ε$的超球体，其中$ε$是一个很小的数。两个超球体的体积分别为$\text{const} \cdot (r)^k $和$\text{const} \cdot (r+ε)^k $，其体积之比为$\frac{(r)^k}{(r+ε)^k}=\frac{1}{(1+\frac{ε}{r})^k}$，当$k→∞$时体积之比趋近于$0$。即高维空间中的超球体内数据集中分布在**表面**上。
