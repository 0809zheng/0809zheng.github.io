---
layout: post
title: 'Big Transfer (BiT): General Visual Representation Learning'
date: 2020-07-04
author: 郑之杰
cover: 'https://pic.downk.cc/item/5f0035fa14195aa5947ee320.jpg'
tags: 论文阅读
---

> BiT：用于迁移学习的预训练卷积神经网络模型.

- TAPAS: Big Transfer (BiT): General Visual Representation Learning
- arXiv：[link](https://arxiv.org/abs/1912.11370)

# 模型介绍
作者提出了一个预训练的卷积神经网络模型：**BiT**，可用于迁移学习解决计算机视觉任务。

下图是模型在不同图像分类数据集上的表现，注意到即使限制每一类训练图像的数量（1到100），模型也具有不错的表现。

![](https://pic.downk.cc/item/5f00482e14195aa594869064.jpg)

按照训练集的大小，作者训练了三种模型，其中前两种预训练参数已开源：
- **BiT-S**：训练在**ILSVRC-2012**数据集上，共**1.3 million**张；
- **BiT-M**：训练在**ImageNet-21k**数据集上，共**1.4 million**张；
- **BiT-L**：训练在**JFT-300M**数据集上，共**300 million**张

受限于训练样本，模型在自然图像上的表现最好，在结构化任务（如目标检测）中与其余模型相差并不是很大：

![](https://pic.downk.cc/item/5f0049dc14195aa594874260.jpg)

# 预训练  Upstream Pre-Training
作者提出了两个训练重点：
1. 模型复杂度和训练集大小
2. 组归一化和权重标准化

### （1）scale

![](https://pic.downk.cc/item/5f004aca14195aa59487a838.jpg)

通过实验发现：
- 增加模型复杂度和训练集大小都能够提高分类准确率；
- 在大数据集上，模型复杂度的提升对结果的影响更大；
- 二者只提高其一可能会损害模型的表现。

### （2） Group Normalization(GN) and Weight Standardization(WS)
作者在模型中使用了：
- Group Normalization (GN)
- Weight Standardization (WS)

# 迁移 Transfer to Downstream Tasks
作者提出了一种迁移模型时超参数的选择方法：**BiT-HyperRule**。（详见论文3.3节）


