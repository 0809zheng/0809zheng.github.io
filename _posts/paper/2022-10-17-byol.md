---
layout: post
title: 'Bootstrap your own latent: A new approach to self-supervised Learning'
date: 2022-10-17
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63d8bc2fface21e9ef35012b.jpg'
tags: 论文阅读
---

> BYOL：通过在隐空间应用自举法实现自监督学习.

- paper：[Bootstrap your own latent: A new approach to self-supervised Learning](https://arxiv.org/abs/2006.07733)

**Bootstrap your own latent (BYOL)**是一种不使用负样本的对比学习方法，通过在特征表示空间应用**bootstrapping**方法构造损失函数。

**BYOL**使用两个神经网络，分别是参数$\theta$的在线网络**(online network)**和参数$\xi$的目标网络**(target network)**。在线网络通过对比损失更新参数，目标网络的参数$\xi$是在线网络参数$\theta$的**polyak**平均：$$\xi \leftarrow \tau \xi + (1-\tau)\theta$$。

给定图像$x$，**BYOL**损失构造如下：
1. 使用同一类数据增强构造图像$x$的两个增强版本$v=t(x),v'=t'(x)$；
2. 通过编码网络$f$构造特征表示$y_{\theta}=f_{\theta}(v),y_{\xi}'=f_{\xi}(v')$；
3. 通过映射头$g$构造隐变量$$z_{\theta}=g_{\theta}(y_{\theta}),z'_{\xi}=g_{\xi}(y'_{\xi})$$；
4. 通过预测网络$q$构造在线网络的输出$q_{\theta}(z_{\theta})$；
5. 对特征$q_{\theta}(z_{\theta})$和$z_{\xi}'$进行**L2**归一化；
6. 构造$q_{\theta}(z_{\theta})$和$z_{\xi}'$的均方误差损失$$\mathcal{L}_{\theta}^{\text{BYOL}}$$；
7. 交换$v$和$v'$重复上述过程，构造$$q_{\theta}(z'_{\theta})$$和$z_{\xi}$的均方误差损失$$\tilde{\mathcal{L}}_{\theta}^{\text{BYOL}}$$；
8. 总损失为$$\mathcal{L}_{\theta}^{\text{BYOL}}+\tilde{\mathcal{L}}_{\theta}^{\text{BYOL}}$$，更新参数$\theta$。

$$ \begin{aligned} \mathcal{L}_{\text{BYOL}} &= ||q_{\theta}(z_{\theta})-z'_{\xi}||_2^2 + ||q_{\theta}(z_{\theta}')-z_{\xi}||_2^2 \\ & \propto -2(\frac{<q_{\theta}(z_{\theta}),z'_{\xi}>}{||q_{\theta}(z_{\theta})||_2 \cdot ||z'_{\xi}||_2}+\frac{<q_{\theta}(z'_{\theta}),z_{\xi}>}{||q_{\theta}(z'_{\theta})||_2 \cdot ||z_{\xi}||_2}) \end{aligned}  $$

![](https://pic.imgdb.cn/item/63d8c180face21e9ef442d3e.jpg)

![](https://pic.imgdb.cn/item/63d8c3f2face21e9ef4ab92a.jpg)

作者分别报告了使用**BYOL**进行**ImageNet**图像分类任务时，微调输出线性层和微调整个网络对应的性能表现：

![](https://pic.imgdb.cn/item/63d8cb7eface21e9ef5f60f2.jpg)

![](https://pic.imgdb.cn/item/63d8cb8cface21e9ef5f87cc.jpg)

作者对批量大小和数据增强策略进行消融实验，由于**BYOL**不依赖于负样本，因此对这些超参数不敏感：

![](https://pic.imgdb.cn/item/63d8cbd1face21e9ef605bc4.jpg)

**BYOL**中编码网络选用**ResNet50**网络，映射头和预测网络选用**MLP**结构。在所有**MLP**的第一个线性层之后使用了**BatchNorm**。

![](https://pic.imgdb.cn/item/63d8cc81face21e9ef6280b7.jpg)

有[博客](https://generallyintelligent.com/blog/2020-08-24-understanding-self-supervised-contrastive-learning/)指出去除这些位置的**BatchNorm**后模型表现与随机参数无异，该博客作者给出的解释是**BatchNorm**同时考虑了一批样本之间的信息，无论样本特征多么相似都会被重新调整为$N(0,1)$分布，相当于任意样本都会与一个平均样本进行对比，从而隐式地构造了对比学习。

![](https://pic.imgdb.cn/item/63d8d30eface21e9ef746a05.jpg)

本文作者在[BYOL works even without batch statistics](https://arxiv.org/abs/2010.10241)中进行了进一步实验，发现在部分使用**BN**的场合，以及使用负样本的方法中去掉**BN**也会导致性能的严重下降。作者仍然坚持**BYOL**完全不依赖于负样本，因此**BN**隐式地使用平均样本不是**BYOL**成功的关键。

![](https://pic.imgdb.cn/item/63d8d3b6face21e9ef7607f8.jpg)

最后两组研究人员达成的一致共识是**BatchNorm**和负样本都能够防止模型崩溃(**model collapse**)，使得模型训练容易。作者进一步尝试了**GroupNorm**和权重标准化方法的组合，该方法没有引入批量统计信息，也使得**BYOL**具有较好的性能；即一个较好的参数初始化也能避免模型崩溃。

![](https://pic.imgdb.cn/item/63d8d4adface21e9ef786e21.jpg)