---
layout: post
title: 'Deep Metric Learning via Lifted Structured Feature Embedding'
date: 2022-11-03
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63c1230abe43e0d30e00d85f.jpg'
tags: 论文阅读
---

> 基于提升结构化特征嵌入的深度度量学习.

- paper：[Deep Metric Learning via Lifted Structured Feature Embedding](https://arxiv.org/abs/1511.06452)

**提升结构化损失(Lifted Structured Loss)**是为深度度量学习设计的损失函数，旨在最小化相似样本之间的距离，最大化不相似样本之间的距离。

在考虑样本之间的关系时，[Contrastive Loss](http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf)考虑了一个样本对$(x_i,x_j)$的相似关系，[<font color=blue>Triplet Loss</font>](https://0809zheng.github.io/2022/11/02/triplet.html)考虑了样本$x$及其正样本$x^+$和负样本$x^-$之间的三元组关系，而本文提出的**Lifted Structured Loss**同时考虑了一批样本内的所有样本对之间的关系。

![](https://pic.imgdb.cn/item/63c5109dbe43e0d30eb9c770.jpg)

对于一批样本中的所有正样本对$$(i,j) \in \mathcal{P}$$，**Lifted Structured Loss**构造为：

$$ \mathcal{L}_{\text{struct}} = \frac{1}{2| \mathcal{P}|} \sum_{(i,j) \in \mathcal{P}} \max(0,\mathcal{L}_{\text{struct}}^{(i,j)})^2 $$

其中样本对$(i,j)$之间的损失定义为：

$$ \mathcal{L}_{\text{struct}}^{(i,j)} = D_{ij} + \max(\mathop{\max}_{(i,k) \in \mathcal{N}} \epsilon-D_{ik},\mathop{\max}_{(j,l) \in \mathcal{N}} \epsilon-D_{jl}) $$

上式表示对于负样本应用了难例挖掘(**mining hard negatives**)，即在保证相似样本距离最小的同时，尽可能增大不相似样本中相似程度最高的样本特征距离。

在实践中上述损失函数不是平滑的，可能会收敛到较差的局部最优。因此对其进行放松：

$$ \mathcal{L}_{\text{struct}}^{(i,j)} = D_{ij} + \log(\sum_{(i,k) \in \mathcal{N}} \exp(\epsilon-D_{ik}),\sum_{(j,l) \in \mathcal{N}} \exp(\epsilon-D_{jl}))  $$

作者指出，通过随机对一些正样本对中较困难的负样本进行插值，能够提高负样本的质量。