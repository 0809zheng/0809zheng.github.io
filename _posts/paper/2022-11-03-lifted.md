---
layout: post
title: 'Deep Metric Learning via Lifted Structured Feature Embedding'
date: 2022-11-03
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63ca817dbe43e0d30eee333c.jpg'
tags: 论文阅读
---

> 基于提升结构化特征嵌入的深度度量学习.

- paper：[Deep Metric Learning via Lifted Structured Feature Embedding](https://arxiv.org/abs/1511.06452)

**提升结构化损失(Lifted Structured Loss)**是为深度度量学习设计的损失函数，旨在最小化相似样本之间的距离，最大化不相似样本之间的距离。

在考虑样本之间的关系时，[Contrastive Loss](http://yann.lecun.com/exdb/publis/pdf/chopra-05.pdf)考虑了一个样本对$(x_i,x_j)$的相似关系，[<font color=blue>Triplet Loss</font>](https://0809zheng.github.io/2022/11/02/triplet.html)考虑了样本$x$及其正样本$x^+$和负样本$x^-$之间的三元组关系，而本文提出的**Lifted Structured Loss**同时考虑了一批样本内的所有样本对之间的关系。

![](https://pic.imgdb.cn/item/63c5109dbe43e0d30eb9c770.jpg)

对于一批样本中的所有正样本对$$(i,j) \in \mathcal{P}$$，**Lifted Structured Loss**构造为：

$$ \mathcal{L}_{\text{struct}} = \frac{1}{2| \mathcal{P}|} \sum_{(i,j) \in \mathcal{P}} \max(0,\mathcal{L}_{\text{struct}}^{(i,j)})^2 $$

其中正样本对$(i,j)$之间的损失定义为：

$$ \mathcal{L}_{\text{struct}}^{(i,j)} = D_{ij} + \max(\mathop{\max}_{(i,k) \in \mathcal{N}} \epsilon-D_{ik},\mathop{\max}_{(j,l) \in \mathcal{N}} \epsilon-D_{jl}) $$

上式表示对于负样本应用了难例挖掘(**mining hard negatives**)，对于每一个正样本对$(i,j)$，模型分别挖掘其左变量$i$和右变量$j$对应的最困难的负样本，独立地找到距离左变量最近的负样本$k$和距离右边量最近的负样本$l$，通过比较$D_{ik}$和$D_{jl}$找出其中较小的距离对应的负样本$n \in (k,l)$。最后计算三元组$(i,j,n)$的**triplet loss**函数。

![](https://pic.imgdb.cn/item/63cb4a38be43e0d30e035553.jpg)


在实践中上述损失函数不是平滑的，嵌套的**max**函数在实际中容易导致网络收敛到较差的局部最优。因此可以改写为一个光滑的上界函数对其进行放松：

$$ \mathcal{L}_{\text{struct}}^{(i,j)} = D_{ij} + \log(\sum_{(i,k) \in \mathcal{N}} \exp(\epsilon-D_{ik}),\sum_{(j,l) \in \mathcal{N}} \exp(\epsilon-D_{jl}))  $$

作者指出，通过随机对一些正样本对中较困难的负样本进行插值，能够提高负样本的质量。