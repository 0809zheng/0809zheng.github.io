---
layout: post
title: 'Masked Autoregressive Flow for Density Estimation'
date: 2022-05-08
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/629c7ce50947543129da47cd.jpg'
tags: 论文阅读
---

> MAF：使用掩码自回归流进行密度估计.

- paper：[Masked Autoregressive Flow for Density Estimation](https://arxiv.org/abs/1705.07057)

# 1. 标准化流 normalizing flow

**normalization flow**是指用一系列可逆映射$x^k=f_k(x^{k-1}):\Bbb{R}^d\to \Bbb{R}^d$将一个分布类型较为简单的随机变量$x^0~q(x)$转换成分布比较复杂的随机变量$x^K=f(x^0)$：

$$ x^0 \leftrightarrow x^1 \leftrightarrow x^2 \leftrightarrow \cdots  \leftrightarrow  x^{K-1}  \leftrightarrow  x^{K} $$

根据[概率密度的变量替换公式](https://0809zheng.github.io/2022/04/30/variable.html)和链式法则，新变量变量$x^K$的分布为：

$$ q(x^K) = q(x^0)|\det \prod_{k=1}^{K} \frac{d f_k}{d x^{k-1}}|^{-1} $$

其对数似然函数为：

$$ \log q(x^K) = \log  q(x^0) - \sum_{k=1}^{K}|\det  \frac{d f_k}{d x^{k-1}}| $$

此时不需要显式地计算分布$q(x^K)$的概率密度函数，而是通过初始分布$q(x^0)$的概率密度以及映射过程产生的**Jacobian**行列式计算即可。

# 2. 自回归流 autoregressive flow

一般函数的**Jacobian**行列式计算复杂。如果**Jacobian**矩阵$\frac{d f_k}{d x^{k-1}}$为三角矩阵，则其行列式比较容易计算，等于各对角元素的乘积。

**自回归流**通过假设变量$z$的第$i$个维度的生成只依赖于前面的维度$x_{1:i-1}$，从而构造三角形式的**Jacobian**矩阵。具体地，把随机变量建模为自回归模型：

$$ p(x) = \prod_i p(x_i|x_{1:i-1}) $$

为简化运算，可以把条件概率建模为高斯分布：

$$ p(x_i|x_{1:i-1}) = \mathcal{N}(x_i|\mu_i,(\exp \alpha_i)^2) \\ \mu_i = g_{\mu_i}(x_{1:i-1}),\alpha_i = g_{\alpha_i}(x_{1:i-1}) $$

其中$\alpha_i$的建模是为了可以回归任意值，不必引入正数限制。

# 3. 掩码自回归流 Masked Autoregressive Flow
**掩码自回归流**模型通过堆叠多层自回归模型构造标准化流。若记每一层的输入变量为$u=[u_1,u_2,\cdots u_D]$，输出变量为$x=[x_1,x_2,\cdots x_D]$，则双射函数$x=f(u)$为：

$$ x_i = u_i \cdot \exp(\alpha_i) + \mu_i \\ \mu_i = g_{\mu_i}(x_{1:i-1}),\alpha_i = g_{\alpha_i}(x_{1:i-1}) $$

该函数包括尺度变换和平移变换，这些变换的因子依赖输出变量前面的维度，可以用神经网络拟合。

![](https://pic.imgdb.cn/item/629c90e90947543129f4d179.jpg)

双射函数$x=f(u)$的反函数$u=f^{-1}(x)$为：

$$  u_i  = (x_i- \mu_i)\cdot \exp(-\alpha_i) $$

![](https://pic.imgdb.cn/item/629c91ed0947543129f61288.jpg)

该模型的训练过程$u=f^{-1}(x)$比较快，当给定真实数据$x$时，基础分布$u$的每一个维度计算是独立的，可以并行计算。

该模型的数据生成过程$x=f(u)$比较慢，因为只有计算出$x_{1:i-1}$才能计算$x_i$，该过程无法并行进行。
