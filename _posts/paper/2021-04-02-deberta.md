---
layout: post
title: 'DeBERTa: Decoding-enhanced BERT with Disentangled Attention'
date: 2021-04-02
author: 郑之杰
cover: 'https://img.imgdb.cn/item/606bb7638322e6675c3f2408.jpg'
tags: 论文阅读
---

> DeBERTa：使用分解注意力机制和增强型掩膜解码器改进预训练语言模型.

- paper：DeBERTa: Decoding-enhanced BERT with Disentangled Attention
- arXiv：[link](https://arxiv.org/abs/2006.03654)
- code：[github](https://github.com/microsoft/DeBERTa)

作者提出了两种新技术：**分解注意力(disentangled attention)**和**增强型掩膜解码器(enhanced mask decoder)**，用来改进预训练语言模型的性能。前者将每个单词通过词嵌入分别编码为内容向量和位置向量，根据其内容和相对位置计算单词之间的注意力权重；后者在解码层中引入绝对位置来预测被遮挡的单词。


# Disentangled Attention

预训练语言模型通常用自注意力运算提取输入序列$H$的特征，计算如下：

![](https://img.imgdb.cn/item/606bc32b8322e6675c4865e6.jpg)

上述自注意力是根据上下文的**内容(content)**计算的，作者认为在计算注意力时还应该考虑其相对位置，比如**deep learning**这两个单词相邻时的依赖关系比分开时更强。因此作者引入了相对位置编码，该编码是对相对距离进行的词嵌入，**token** $ i $和**token** $ j $的相对距离计算如下，其最大距离被限定为$k$：

![](https://img.imgdb.cn/item/606bc4a78322e6675c49896b.jpg)

值得一提的是，相对位置的词嵌入$P$是在所有层共享的。在计算某一层的自注意力时，除了通常的**content**计算，还要考虑**content**和**position**之间的计算。

![](https://img.imgdb.cn/item/606bc5d68322e6675c4ac2b3.jpg)

# Enhanced Mask Decoder

作者认为，只考虑单词的相对位置和上下文是不够的。如句子**a new store opened beside the new mall**，在训练时遮挡住单词**store**和**mall**，很难对其进行预测。作者在模型最后一层(**softmax**函数前)引入了绝对位置，作为补充信息。


消融实验证明了这些技术的有效性：

![](https://img.imgdb.cn/item/606bc7208322e6675c4bedfc.jpg)