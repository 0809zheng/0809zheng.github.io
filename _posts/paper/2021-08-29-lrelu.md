---
layout: post
title: 'Rectifier Nonlinearities Improve Neural Network Acoustic Models'
date: 2021-08-29
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6128a89344eaada7392b17f3.jpg'
tags: 论文阅读
---

> LeakyReLU：使用修正的非线性提高神经网络声学模型.

- paper：Rectifier Nonlinearities Improve Neural Network Acoustic Models
- ICML2013：[link](http://robotics.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)


![](https://pic.imgdb.cn/item/6128aa4b44eaada7392f4ce2.jpg)

传统的激活函数常选取**S**型函数，如**tanh**函数。该函数当输入较大时输出会快速饱和，梯度迅速变小，导致梯度消失问题，从而使训练收敛更慢。**ReLU**激活函数避免了输入为正时的梯度消失，且输入为负时输出为$0$，能够使输出特征比较稀疏。但是输出为$0$也意味着梯度为$0$，反向传播时无法传播梯度，从而阻碍参数更新，减缓训练速度。

作者使用了**Leaky ReLU**避免了上述**dead ReLU**问题。在输入小于$0$时也提供较小的梯度。通过牺牲特征的稀疏性，获得更稳健的训练过程。

$$ \text{LeakyReLU}(x) = \begin{cases} x, & x\geq 0\\ 0.01x, & x<0 \end{cases} $$

作者在语音识别任务上进行了实验，实验结果见下表。**ReLU**和**LReLU**对应的网络性能高于**tanh**的网络。

![](https://pic.imgdb.cn/item/6128aa7844eaada7392fb90a.jpg)

作者进一步分析了**ReLU**系列激活函数带来性能提升的原因，统计了训练模型中隐藏层的每一个神经元的激活情况。具体地，统计了$10000$个输入样本在不同激活函数下最后一个隐藏层的神经元输出情况，并计算了每一个隐藏层神经元的激活概率（未饱和的概率）。对于**ReLU**和**LReLU**，饱和是指$h(x) < 0$；对于**tanh**，饱和是指$h(x)\leq -0.95$或$h(x)\geq 0.95$；特别地，还统计了**tanh**的负饱和情况，即$h(x)\leq -0.95$。结果如下：

![](https://pic.imgdb.cn/item/6128aa8644eaada7392fd515.jpg)

统计表明，使用**ReLU**和**LReLU**的网络，激活函数比使用**tanh**的网络更加**稀疏**(**sparse**)，即同时激活的神经元个数更少，而隐藏层神经元激活的稀疏性有助于保持对输入的不变性。

除了稀疏性，作者还讨论了隐藏层神经元激活的**弥散**(**disperse**)性。如对于四个神经元，若其激活概率为$[1,0,0,0]$，则激活具有稀疏性(只有一个神经元会被也必定被激活)，平均稀疏性为$0.25$，但此时失去了不确定性，包含较小的信息，这样的神经元激活具有稀疏性但不具有弥散性。若四个神经元的激活率为$[0.25,0.25,0.25,0.25]$，此时平均稀疏性仍为$0.25$，但分布具有弥散性，包含更多的不确定性和信息。可通过上图中分布曲线的斜率定性地判断分布的弥散性，曲线越平稳，表示分布的稀疏性越好。

作者还通过计算隐藏层中神经元激活概率的标准差来定量地比较隐藏层激活分布的弥散性。标准差越小，表明每个神经元激活的概率越接近，弥散性越好。计算得到**ReLU**和**LReLU**对应的标准差为$0.04$，而**tanh**的标准差为$0.14$。因此**ReLU**和**LReLU**不仅能够得到稀疏的隐藏层神经元激活，还能让信息更均匀地分布在各个神经元中。