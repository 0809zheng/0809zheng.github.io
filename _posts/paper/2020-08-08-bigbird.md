---
layout: post
title: 'Big Bird: Transformers for Longer Sequences'
date: 2020-08-08
author: 郑之杰
cover: 'https://pic.downk.cc/item/5f2e375114195aa594475f52.jpg'
tags: 论文阅读
---

> Big Bird：一种应用于长序列的Transformer模型.

- paper：Big Bird: Transformers for Longer Sequences
- arXiv：[link](https://arxiv.org/abs/2007.14062)

基于**Transformer**的模型使用注意力机制处理文本数据。若输入序列具有$n$个**tokens**，则每一层**full attention**的计算复杂度为$O(n^2)$（每一个**token**都需要和所有**tokens**交互），将随着序列长度成平方增长。

**Big Bird**没有使用**full attention**，而是结合了三种**attention**机制，将单层的计算复杂度减少为$O(n)$。其所使用的注意力机制包括：

![](https://pic.downk.cc/item/5f2e340114195aa594463791.jpg)

1. **Random attention**：每一个**token**随机地和一些**tokens**交互，当限制最大交互数量时，计算复杂度为$O(n)$；
2. **Window attention**：每一个**token**和自身附近的若干个**tokens**交互，当限制最大交互数量时，计算复杂度为$O(n)$；
3. **Global attention**：选择一些全局非常重要的**tokens**（如第一个**token**代表$\[CLS\]$），这些**tokens**与所有**tokens**交互；其余**tokens**只与被选择的**tokens**交互，计算复杂度为$O(n)$。

尽管每一层**tokens**的交互是有限的，当层数足够多时，可以实现任意两个**tokens**之间的交互。相比于**full attention**，**Big Bird**用增加深度的方法减少了每一层的计算量，使得可以处理更长的文本序列。

**Big Bird**仍然具有**Universal Approximator**和**Turing Completeness**的性质。