---
layout: post
title: 'A ConvNet for the 2020s'
date: 2022-12-25
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63ac376b08b6830163178d26.jpg'
tags: 论文阅读
---

> ConvNeXt: 设计现代化卷积神经网络.

- paper：[A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)

在图像分类任务中，视觉**Transformer**逐渐取代了卷积神经网络成为最先进的分类模型。本文作者重新审视了卷积网络的设计空间，并测试了纯粹的“卷积神经网络”所能达到的极限。通过逐步将标准**ResNet**现代化，构造了完全由标准卷积模块构建的**ConvNeXt**，并取得了分类任务的最优表现。

![](https://pic.imgdb.cn/item/63ac379808b683016317d464.jpg)

作者改进**ResNet**的路线图如图所示。通过把标准**ResNet**逐步魔改为现代视觉**Transformer**，在此过程中发现了导致性能差异的几个关键组件。本文主要考虑两个模型改进路线，分别是**ResNet-50 / Swin-T**结构(大约$4.5 \times 10^9$**FLOPs**)和**ResNet-200 / Swin-B**结构(大约$15 \times 10^9$**FLOPs**)。由于网络复杂性与最终性能密切相关，在整个研究过程中**FLOPs**大体上得到控制，尽管某些中间步骤中**FLOPs**可能高于或低于参考模型。所有的模型都在**ImageNet-1K**上进行训练和评估。

![](https://pic.imgdb.cn/item/63ac400508b683016323c118.jpg)

## 1. 训练技巧 Training Techniques

除了网络架构的设计之外，训练过程同样影响这最终性能。视觉**Transformer**引入了不同的训练技巧，主要涉及优化策略和相关的超参数设置。因此首先使用视觉**Transformer**的训练过程来训练**baseline**模型。
- 训练轮数从$90$提升到$300$；
- 使用**AdamW**优化器；
- 使用数据增强：**Mixup, Cutmix, RandAugment, Random Erasing**
- 使用正则化方法：**Stochastic Depth, Label Smoothing**

通过上述训练，**ResNet50**的准确率从**76.1**提高到**78.8**，说明**Transformer**性能的提升主要还应归因于训练技巧。后续的魔改过程中对所有模型采用固定的参数设置，所有结果都是三个不同随机种子下训练结果的平均。

## 2. 宏观设计 Macro Design

**SwinTransformer**的宏观架构设计沿袭着卷积网络的多阶段设计特点，每个阶段有不同的特征映射分辨率。
- 改变**阶段计算比(stage compute ratio)**。卷积网络不同特征图尺寸阶段的数量设计很大程度上是基于经验的，并且与下游的任务兼容。**Swin-T**的阶段计算比例为1:1:3:1，因此将**ResNet-50**每个阶段的块数从$(3,4,6,3)$调整为$(3,3,9,3)$，这将模型的精度从$78.8\%$提高到$79.4\%$。
- 改变**stem结构**。**stem**结构是指卷积神经网络的输入卷积层，标准**ResNet**的输入卷积层包含一个步长为$2$的$7 \times 7$卷积层和一个最大池化层，实现$4$倍下采样。**Swin Transformer**把输入图像拆分成$4 \times 4$的图像块作为输入，因此将**ResNet**的**stem**结构替换为一个步长为$4$的$4 \times 4$非重叠卷积层。准确率从$79.4\%$提高到$79.5\%$。

## 3. ResNet to ResNeXt

本节尝试采用**ResNeXt**的思想，它比普通的**ResNet**具有更好的**FLOPs**/准确性权衡。**ResNeXt**对网络的**bottleneck**块中对$3 \times 3$卷积层使用了分组卷积，可以显著地降低**FLOPs**，并通过扩大网络宽度来弥补容量损失。

作者把**ResNet**中的卷积替换为深度可分离卷积，即操作分别基于每个通道和空间维度，只在其中一个维度上混合信息，这样可以实现与视觉**Transformer**类似的空间的分离和通道的混合。与此同时将网络宽度增加到与**Swin-T**相同的通道数量(从**64**个增加到**96**个)，这使得网络性能达到$80.5\%$。

![](https://pic.imgdb.cn/item/63ac40ac08b683016324a9fd.jpg)

## 4. 反向瓶颈 Inverted Bottleneck

**Transformer**块的一个重要设计是反向瓶颈，即**MLP**块的特征维度是输入特征维度的四倍。因此作者在卷积块中也使用了扩展比为$4$的反向瓶颈(**96**到**384**)，这一变化使整个网络**FLOPs**减少到**4.6G**，并略微提高性能(从$80.5\%$提高到$80.6\%$)。在**ResNet-200 / Swin-B**模式中，这一步带来了更多的收益($81.9\%$至$82.6\%$)，同时也减少了**FLOPs**。

![](https://pic.imgdb.cn/item/63ac40ca08b683016324d4e1.jpg)

## 5. 大卷积核尺寸 Large Kernel Size

**Vision Transformer**的自注意力机制使得每一层都具有全局的感受野，而卷积网络通常是堆叠小卷积核，这在现代**gpu**上具有高效的硬件实现。虽然**Swin-T**重新将局部窗口引入到自注意力模块中，但窗口的大小至少是$7 \times 7$，明显大于**ResNet**的$3 \times 3$大小。

为了使用较大的卷积核，首先需要向上移动深度卷积的位置，这个设计决策在**transformer**中也很明显，即自注意力块被放置在**MLP**层之前。这种设计使得复杂/低效的模块(大卷积核)具有更少的通道，而高效、密集的$1 \times 1$层将做繁重的工作。这个中间步骤将**FLOPs**降低到**4.1G**，导致性能暂时下降到$79.9\%$。

![](https://pic.imgdb.cn/item/63ac40e708b6830163250257.jpg)

把$3 \times 3$卷积替换为$7 \times 7$，网络的性能从$79.9\%$提高到$80.6\%$，但网络的**FLOPs**基本保持不变。卷积核大小在$7 \times 7$时性能达到饱和点。至此设计出与**Swin Transformer**类似的卷积模块。

![](https://pic.imgdb.cn/item/63ac435f08b68301632949ce.jpg)

## 6. 微观设计 Micro Design
- 激活函数：卷积网络一般采用**ReLU**，而**transformer**模型常常采用**GELU**。把激活函数从**ReLU**改成**GELU**后模型效果没有变化（$80.6\%$）。
- 减少激活函数：只保留卷积模块中间$1 \times 1$卷积层之后的**GELU**，使模型性能从$80.6\%$提升至$81.3\%$。
- 减少归一化层：**transformer**中只在自注意力和**MLP**的输入端采用**LayerNorm**，而**ResNet**每个卷积层之后都采用**BatchNorm**。只保留卷积模块中间$1 \times 1$卷积层前的**BatchNorm**，此时模型性能有$0.1\%$的提升。将**BatchNorm**替换成**LayerNorm**后，模型性能提升至$81.5\%$。
- 调整下采样：**ResNet**中的下采样一般放在每个阶段最开始的卷积块中，采用步长为$2$的$3 \times 3$卷积；为了近似**Swin Transform**的**Patch Merging layer**，作者把下采样放在两个阶段之间，通过一个步长为$2$的$2 \times 2$卷积实现。为了解决训练发散问题，在**stem**之后每个下采样层之前以及全局平均池化之后都增加一个**LayerNorm**。

最终模型的性能提升至$82.0\%$，超过**Swin-T**（$81.3\%$）。