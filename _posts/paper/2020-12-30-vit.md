---
layout: post
title: 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'
date: 2020-12-30
author: 郑之杰
cover: 'https://pic.downk.cc/item/5feadb743ffa7d37b343e4ee.jpg'
tags: 论文阅读
---

> ViT：使用图像块序列的Transformer进行图像分类.

- paper：An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
- arXiv：[link](https://arxiv.org/abs/2010.11929)
- code：[github](https://github.com/lucidrains/vit-pytorch)

# 模型介绍

![](https://pic.downk.cc/item/5febde703ffa7d37b3adbfad.jpg)

模型的整体结构如上图所示。作者尽可能遵守原始**Transformer**的结构设计，其目的是使得针对**Transformer**设计的优化结构可以直接套用。

将输入图像$x \in \Bbb{R}^{H \times W \times C}$划分成若干**patch** $x_p \in \Bbb{R}^{N \times (P^2 \cdot C)}$，其有效序列长度为$N = \frac{HW}{P^2}$。

将每个**patch**展平后通过线性映射转化为一个维度为$D$的嵌入向量(**patch embedding**)，并在输入的起始位置增加一个可学习的类别嵌入，该向量在输出时的状态可作为图像的特征表示。在预训练和微调阶段，分类器将其作为输入。

增加$1D$位置编码(**position embedding**)后输入**Transformer**的编码器（实验发现$2D$位置编码对结果提升不明显）。预训练时在网络后增加一个**MLP**线性分类器进行图像分类。

微调时使用更高分辨率的图像。保持每一个图像**patch**的尺寸不变，这将使输入序列长度增加。**Transformer**可以输入任意长度的序列，但预训练的位置编码将不再匹配。为此使用$2D$插值调整位置编码。这部分是人为引入的**归纳偏置(inductive bias)**。


# 实验分析
作者训练了三个不同大小的**ViT**模型，其参数量如下表所示：

![](https://pic.downk.cc/item/5febe2393ffa7d37b3b44bb6.jpg)

在中等规模的数据集（如**ImageNet**）上训练，准确率要比基于卷积神经网络的模型（如**ResNet**）低几个点。这是因为**Transformer**缺少卷积神经网络的**归纳偏置**，如**平移等变性和局部性(translation equivariance and locality)**，这使得它在训练数据不足的时候泛化能力不强。作者认为在大尺度（$14M$-$300M$）的数据集上训练可以解决这个问题。

实验结果显示，在**JFT-300M**数据集上预训练后，基于**Transformer**的分类模型迁移到小数据集任务中超越了基于卷积神经网络的模型：

![](https://pic.downk.cc/item/5febe24f3ffa7d37b3b47020.jpg)

作者可视化了部分线性嵌入的权重和位置编码，表明模型学习到特征提取和位置敏感的信息。作者分析不同层中注意力平均距离（类似于卷积网络中的感受野大小），发现在浅层模型同时关注近距离和远距离的特征，在深层模型主要关注远距离特征。而卷积神经网络在浅层主要关注近距离特征。

![](https://pic.downk.cc/item/5febe3c23ffa7d37b3b6f776.jpg)