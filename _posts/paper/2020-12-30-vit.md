---
layout: post
title: 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'
date: 2020-12-30
author: 郑之杰
cover: 'https://pic.downk.cc/item/5feadb743ffa7d37b343e4ee.jpg'
tags: 论文阅读
---

> ViT：使用图像块序列的Transformer进行图像分类.

- paper：An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
- arXiv：[link](https://arxiv.org/abs/2010.11929)
- code：[github](https://github.com/lucidrains/vit-pytorch)

# 模型介绍

![](https://pic.downk.cc/item/5febde703ffa7d37b3adbfad.jpg)

模型的整体结构如上图所示。将输入图像$x \in \Bbb{R}^{H \times W \times C}$划分成若干**patch** $x_p \in \Bbb{R}^{N \times (P^2 \cdot C)}$，其有效序列长度为$N = \frac{HW}{P^2}$。

将每个**patch**展平后通过线性映射转化为一个维度为$D$的嵌入向量(**patch embedding**)，并在输入的起始位置增加可学习的类别查询向量，该向量在输出时的状态可作为图像的特征表示。

增加$1D$位置编码(**position embedding**)后输入**Transformer**的编码器（实验发现$2D$位置编码对结果提升不明显）。预训练时在网络后增加一个**MLP**线性分类器进行图像分类。

微调时使用更高分辨率的图像。保持每一个图像**patch**的尺寸不变，这将使输入序列长度增加。**Transformer**可以输入任意长度的序列，但预训练的位置编码将不再匹配。为此使用$2D$插值调整位置编码。这部分是人为引入的**归纳偏置(inductive bias)**。

与此同时，**Transformer**缺少卷积神经网络的**归纳偏置**，如**平移等变性和局部定位(translation equivariance and locality)**，这使得它在训练数据不足的时候泛化能力不强。作者认为在大尺度的数据集上训练可以解决这个问题。

# 实验分析
作者训练了三个不同大小的**ViT**模型，其参数量如下表所示：

![](https://pic.downk.cc/item/5febe2393ffa7d37b3b44bb6.jpg)

实验结果显示，在**JFT-300M**数据集上预训练后，基于**Transformer**的分类模型超越了基于卷积神经网络的模型：

![](https://pic.downk.cc/item/5febe24f3ffa7d37b3b47020.jpg)

作者可视化了部分线性嵌入的权重和位置编码，表明模型学习到特征提取和位置敏感的信息。作者分析不同层中注意力平均距离，发现在浅层模型同时关注近距离和远距离的特征，在深层模型主要关注远距离特征。而卷积神经网络在浅层主要关注近距离特征。

![](https://pic.downk.cc/item/5febe3c23ffa7d37b3b6f776.jpg)