---
layout: post
title: 'ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation'
date: 2022-07-08
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/64a388b91ddac507cc5bc6de.jpg'
tags: 论文阅读
---

> ViTPose：用于人体姿态估计的简单视觉Transformer基线.

- paper：[ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation](https://arxiv.org/abs/2204.12484)

**Vison Transformer**在视觉识别任务中效果优秀，但还没有工作在姿态估计任务上验证这种结构的有效性。本文提出了用于姿态估计的**Transformer**网络**ViTPose**，使用**ViT**结构作为**Backbone**，结合一个轻量级的**Decoder**，在**MS COCO**关键点估计**bechmark**上达到**SOTA**。

# 1. ViTPose的结构

**ViTPose**的网络结构设计比较简单，整体采用[<font color=Blue>ViT</font>](https://0809zheng.github.io/2020/12/30/vit.html) **backbone + decoder**的形式。

![](https://pic.imgdb.cn/item/64a38a991ddac507cc5f767b.jpg)

**ViT Backbone**分为**patch embedding**和多个**transformer**模块。**patch embedding**将图像分为$d\times d$的**patch**块。而每个**transformer**层包含 **multi-head self-attention(MHSA)** 与 **feed-forward network (FFN)** 模块。多个**transformer**层堆叠，构成了**backbone**。

**backbone**根据计算量大小可以分别选用**Vit-B, ViT-L，ViT-H**以及**ViTAE-G**。

![](https://pic.imgdb.cn/item/64a38b991ddac507cc61b362.jpg)

在**decoder**的选取上，作者选择了两种结构进行了对比:
1. 经典**Decoder**结构，两个**Deconv(+BN+ReLU) + 1x1conv**，每个**deconv**上采样**2**倍，最终输出**feature map**大小为输入的$1/4$倍;
2. 简单**Decoder**结构，双线性差值上采样$4$倍，然后是**ReLU+3x3conv**。

方案**1**非线性更高，因此在**CNN**的结构中使用比较多。**ResNet**系列在方案**1**上的结果远高于方案**2**，说明**CNN**结构的学习能力需要强有力的**decoder**来进一步加强。由于**Transformer**强大的学习能力，方案**2**这样的的简单**decoder**也能达到很高的精度。

![](https://pic.imgdb.cn/item/64a38d051ddac507cc64e956.jpg)

实验采用姿态估计中**Top-Down**的方案，即先用一个检测器检测出单个人体框，然后用**ViTPose**对人体框进行姿态估计。第一步的检测器在**COCO**的**val**集上用的是**SimpleBaseline**，而在最后的**COCO test-dev**集上与**SOTA**方案的比较实验中，采用了**Bigdet**。**SOTA**结果是在**576x432**输入，采用**1B**参数量的**ViTAE-G**作为**backbone**，使用**MS COCO + AI Challenger**训练的情况下获得的。

![](https://pic.imgdb.cn/item/64a391b71ddac507cc6cd092.jpg)

# 2. ViTPose的特性

### ⚪ 预训练的灵活性

一般情况下**backbone**都需要**ImageNet**上预训练。本文提出了三种预训练方案： 
1. 采用**ImageNet**预训练分类任务，比较经典的方法，数据集总共**1M**图片
2. 采用**MS COCO**预训练**MAE**任务，将$75\%$的**patch**随机**mask**掉，然后让网络学习恢复这些**patch**，数据集共**150K**图片
3. 任务框架同方案**2**，不过数据集采用**MS COCO + AI Challenger**，共**500K**图片

由于**ViTPose**是单人检测模型，因此将**MS COCO**和**AI Challenger**中的单个人体**crop**出来，与**ImageNet**单个**object**的数据分布保持一致。然后在**3**个数据集上分别训练**1600**个**epoch**，再在**MS COCO**上**fine tune** **210**个**epoch**。

采用**VitPose-B**结构，在**MS COCO val set**上，三种预训练方案的结果如下。可以看到使用**MS COCO + AI Challenger**，在只有一半数据量的情况下，可以达到比**ImageNet**更好的效果。

![](https://pic.imgdb.cn/item/64a38f941ddac507cc6949b4.jpg)

### ⚪ 分辨率的灵活性

**ViTPose**可以通过使用更大的输出尺寸来训练，也可以通过减小**backbone**中的下采样来构造更大尺度的**feature map**，这两种操作都能提高精度，具体如下：
- 更大尺寸的输入：直接缩放原始图像，得到对应大小的输入 
- 更大尺寸的特征：降低采样倍数，修改**patch**层的**stride**参数

分辨率越大结果越高。

![](https://pic.imgdb.cn/item/64a38fee1ddac507cc69df61.jpg)

### ⚪ 注意力的灵活性

**Transformer**中的**Attention**的计算量是**Feature map**尺寸的平方，因此是很大的，而且显存占用也很大。因此作者用了**Shift Window**和**Pooling Window**两种方案来缓解这个问题。

![](https://pic.imgdb.cn/item/64a3904b1ddac507cc6a82d7.jpg)

### ⚪ 微调的灵活性

与**NLP**任务中一样，作者验证了只固定**MHSA**模块的参数，精度下降不多，而固定**FFN**的参数，则精度下降明显，因此作者认为**MHSA**更偏向与任务无关，而**FFN**则更具体任务关系更密切。

![](https://pic.imgdb.cn/item/64a390881ddac507cc6adbd4.jpg)

### ⚪ 多任务的灵活性

作者还尝试了采用同一个**backbone**，多个**decoder**，每个**decoder**对应一个数据集的任务，实验验证一次训练，多个数据集上的结果都能比较好，且比单个数据集精度有提升:

![](https://pic.imgdb.cn/item/64a390c31ddac507cc6b37aa.jpg)

### ⚪ 蒸馏

作者提出了一个基于**Transformer**的蒸馏方法，与常见的用损失来监督**Teacher**和**Student**网络的思路不太一样，具体如下:
1. 在大模型的**patch embedding**后的**visual token**后面增加一个知识**token**模块，并进行随机初始化 
2. 固定大模型的参数，只训练知识**token**模块 
3. 将训练好的知识**token**模块接到小模型的**visual token**后面，且固定知识**token**的参数，只训练小模型的其他参数

通过这样的流程，将所有的知识都融合到了知识**token**模块的参数里面，并且从大模型传递到小模型。

![](https://pic.imgdb.cn/item/64a391291ddac507cc6be18c.jpg)

