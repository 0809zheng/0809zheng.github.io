---
layout: post
title: 'Revealing the Dark Secrets of Masked Image Modeling'
date: 2022-11-27
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6559fb72c458853aef18b0eb.jpg'
tags: 论文阅读
---

> 揭露掩码图像建模方法的有效性.

- paper：[Revealing the Dark Secrets of Masked Image Modeling](https://arxiv.org/abs/2205.13543)

作为预训练的掩码图像建模（**MIM**）被证明对于许多视觉下游任务是有效的，但**MIM**如何工作仍然不清楚。为了研究这些问题，作者从可视化和实验两个角度将**MIM**与监督模型进行了比较，试图揭示这两个预训练任务之间的关键表征差异。

## 1. 可视化

本文从三个角度研究预训练模型的注意力图：
1. 平均注意力距离：以测量它是局部注意力还是全局注意力；
2. 注意力分布熵：以衡量它是聚焦注意力还是广播注意力；
3. 不同注意头的**KL**散度：以调查这些注意头是在注意不同的**token**还是类似的**token**。

### （1）平均注意力距离

图像具有很强的局部性：彼此相邻的像素往往高度相关，这促使在视觉感知架构中使用局部先验。因此研究**MIM**模型是否会给模型带来局部归纳偏置是很有价值的。通过计算每个层的每个注意力头部的平均注意力距离来实现这一点。

下图显示了不同层的不同注意力头部的平均注意力距离，包括监督模型（**DeiT**）、对比学习模型（**MoCo v3**）和以**ViT-B**为主干的**SimMIM**模型。监督模型与对比学习模型倾向于在较低层学习局部注意，但在较高层学习全局注意力。但对于**SimMIM**训练的模型，每一层都有不同倾向于聚集局部和全局像素的注意力头部，并且平均注意力距离与监督模型的较低层相似。随着层数的增加，平均注意力距离变得更小。**MIM**给训练的模型带来了局部归纳偏置，即模型倾向于在注意力头部聚集附近的像素。

![](https://pic.imgdb.cn/item/6559ffb8c458853aef2866ec.jpg)

**SimMIM**设计了一种新的度量**AvgDist**，它测量掩码像素到最近可见像素的平均欧几里德距离。高微调精度的模型大致分布在**AvgDist** $[10, 20]$的范围内，并且这些模型具有较小的平均注意力距离。也就是说，**MIM**中适度的预测距离将带来更大的局部强度，并带来更好的微调性能。

![](https://pic.imgdb.cn/item/655a0046c458853aef2a646b.jpg)

### （2）注意力分布熵

通过平均每个头部的注意力分布的熵来测量注意力图，以确定注意力头部是注意在几个**token**上还是广泛注意多个**token**。对于监督模型与对比模型，在较低层中的一些注意力头部具有非常集中的注意力，但在较高层中，大多数注意力头部的注意力都非常广泛。但是对于**MIM**模型，不同注意力头部的熵值在所有层中都是不同的，一些注意力头部更加集中，一些头部具有非常广泛的注意力。

![](https://pic.imgdb.cn/item/655a00ffc458853aef2d5dae.jpg)

### （3）注意力头的多样性

通过计算每个层中不同头部的注意力图之间的**Kullback–Leibler**（**KL**）散度，进一步探索不同头部是否注意不同/相似的**token**。**MIM**模型的所有层上，不同的注意力头倾向于聚合不同的**token**。但对于监督模型和对比学习模型，注意力头部的多样性随着层次的加深而变小，几乎从最后三个层次消失。

![](https://pic.imgdb.cn/item/655a016fc458853aef2ef2cb.jpg)

失去不同注意力头部的多样性可能会限制模型的容量。为了调查注意力头部的多样性损失是否有任何不利影响，从最后开始逐渐删除层，并在为**COCO val2017**姿态估计和**NYUv2**深度估计的下游任务微调模型时仅加载之前的层。当丢弃两到八层时，尽管模型变小，但监督预训练模型在**COCO val2017**姿态估计上的性能优于基线，并且在**NYUv2**深度估计上的表现与基线相当。这表明，在有监督的预训练模型中，注意力头部差异较小的最后几层确实会影响下游任务的执行。

![](https://pic.imgdb.cn/item/655a0201c458853aef311e89.jpg)


### （4）特征图的相似性

通过**CKA**相似性研究不同层的特征图之间的相似性。对于监督模型，不同层学习不同的表示结构，它们的**CKDA**相似性差异很大（$[0.5,1.0]$）。对于对比学习与**MIM**，不同层的表示结构几乎相同，它们的**CKA**相似性都非常大（$[0.9, 1.0]$）。

![](https://pic.imgdb.cn/item/655a034bc458853aef361e46.jpg)

## 2. 实验

本节通过比较**MIM**和监督预训练模型在三种类型的任务上的微调性能进行了大规模研究，即语义理解任务（例如不同领域中的图像分类）、几何和运动任务（例如姿态/深度估计和视频对象跟踪），以及同时执行两种类型的任务的组合任务（例如目标检测）。

### （1）语义理解任务

不同语义理解任务的结果如表所示。对于类别被**ImageNet**类别充分覆盖的分类数据集（例如**CIFAR-10/100**），作为预训练，监督模型可以获得比**MIM**模型更好的性能。然而对于其他数据集，如细粒度分类数据集（如**Food**、**Birdsnap**、**iNaturalist**），或具有不同输出类别的数据集（例如**CoG**），监督模型中的大多数表示能力难以转移；因此**MIM**模型显著优于监督的模型。

![](https://pic.imgdb.cn/item/655a0401c458853aef38f15e.jpg)

### （2）几何和运动任务

本节研究**MIM**模型如何执行几何和运动任务，这些任务需要定位对象的能力，并且较少依赖于语义信息。
- 对于姿态估计，使用**ImageNet-1K**预训练的**MIM**模型在很大程度上超过了监督模型。即使使用**ImageNet-22K**对监督的模型进行预训练，其性能仍比使用**ImageNet-1K**预训练的**MIM**模型差。
- 对于深度估计，与**ImageNet-22K**的监督预训练相比，**MIM**预训练确实将**SwinV2-B**的性能提高了**0.03 RMSE**。在监督预训练的情况下，较大的模型**SwinV2-L**对**NYUv2**数据集没有任何增益；而在**MIM**预训练的条件下，**SwinV2-L**比**SwinV2-B**的**RMSE**增益约为**0.02**。
- 对于视频对象跟踪，**MIM**模型也显示出比监督预训练模型更强的传输能力。在长期数据集**LaSOT**上，具有**MIM**预训练**SwinV2-B**骨干的**SwinTrack**获得了与**SOTA MixFormer-L**相当的结果。

![](https://pic.imgdb.cn/item/655a0e51c458853aef67a799.jpg)

### （3）目标检测任务

在目标检测**COCO**上**MIM**模型优于以**SwinV2-B**为骨干监督模型。绘制了对象分类**Lcls**和定位**Lbbox**的损失曲线，发现**MIM**模型有助于定位任务更快更好地收敛，而监督模型更有利于对象分类。即**MIM**模型在几何和运动任务上的表现更好，而在其类别被**ImageNet**（如**COCO**）充分覆盖的任务上，**MIM**模型的表现相当或稍差。

![](https://pic.imgdb.cn/item/655a0f20c458853aef6bbec4.jpg)