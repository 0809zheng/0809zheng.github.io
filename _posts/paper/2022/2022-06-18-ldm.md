---
layout: post
title: 'High-Resolution Image Synthesis with Latent Diffusion Models'
date: 2022-06-18
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/667235cad9c307b7e9c4c710.png'
tags: 论文阅读
---

> 通过隐扩散模型实现高分辨率图像合成.

- paper：[High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752)
- code：[Stable Diffusion](https://github.com/Stability-AI/stablediffusion)


扩散模型是一类隐变量模型，其隐变量通常具有较高的维度（与原始图像相同的维度），因此扩散模型的计算负担非常大。考虑一张尺寸为$512\times 512$的**RGB**图像，其具有$786432$维，因此直接在图像空间中训练扩散模型是昂贵的。

观察到图像的大多数像素提供的是感知细节，对图像进行像素压缩后仍然能保持图像的语义和概念细节，因此可以考虑在生成模型建模的过程中引入**感知压缩(perceptual compression)**和**语义压缩(semantic compression)**：首先通过自编码器压缩像素级的冗余，然后通过在隐空间中执行扩散过程学习语义概念。

![](https://pic.imgdb.cn/item/6672805dd9c307b7e93c9611.png)

**隐扩散模型（latent diffusion model）**没有直接在高维图像空间中操作，而是首先把图像压缩到隐空间，再在隐空间中构造扩散过程；压缩过程是通过[**VQ-VAE**](https://0809zheng.github.io/2020/11/10/vqvae.html)实现的。编码器$\mathcal{E}$把输入图像$x \in \mathbb{R}^{H\times W\times 3}$压缩为隐空间向量$z= \mathcal{E}(x)\in \mathbb{R}^{h\times w\times c}$，其空间尺寸下采样率$f=H/h=W/w=2^m$；解码器$\mathcal{D}$从隐空间向量中恢复原始图像$\tilde{x}=\mathcal{D}(z)$。

以**Stable Diffusion v1**为例，其使用隐扩散模型的隐空间尺寸为$4\times 64\times 64$，比图像空间（$512\times 512$）小$48$倍。

![](https://pic.imgdb.cn/item/667247fad9c307b7e9df5cdb.png)

隐扩散模型的采样过程如下：
1. 随机生成一个隐空间矩阵；![](https://pic.imgdb.cn/item/66724d25d9c307b7e9e819d0.png)
2. 噪声预测器根据输入条件（如文本）估计隐空间矩阵的噪声；![](https://pic.imgdb.cn/item/66724d25d9c307b7e9e819d7.png)
3. 从隐空间矩阵中减去预测噪声；![](https://pic.imgdb.cn/item/66724d25d9c307b7e9e819e9.png)
4. 在采样过程中重复步骤2和3；
5. 使用**VQ-VAE**的解码器把隐空间矩阵解码为图像。![](https://pic.imgdb.cn/item/66724d25d9c307b7e9e81a06.png)

隐扩散模型的主体结构（噪声预测器）采用条件**UNet**模型，通过交叉注意力机制（**cross-attention mechanism**）实现灵活地条件输入，将条件信息$y$通过特定编码器$\tau_\theta$后输入交叉注意力：

![](https://pic.imgdb.cn/item/667281bdd9c307b7e93ed7d6.png)

## ⚪ Stable Diffusion v1.5

**SDv1.5**模型的训练过程包括：
- 在[laion2B-en](https://huggingface.co/datasets/laion/laion2B-en)数据集上以$256\times 256$的分辨率训练237k步；
- 在[laion-high-resolution](https://huggingface.co/datasets/laion/laion-high-resolution)数据集上以$512\times 512$的分辨率训练194k步；
- 在[laion-aesthetics v2 5+](https://laion.ai/blog/laion-aesthetics/)数据集上以$512\times 512$的分辨率训练225k步（10\%的文本条件丢弃率）。

**SDv1.5**模型使用Open AI的**CLIP ViT-L/14**模型进行文本嵌入。

## ⚪ Stable Diffusion v2

**SDv2**模型的训练过程包括：
- 在[laion-5B](https://laion.ai/blog/laion-5b/)数据集上（过滤掉了露骨和色情内容）以$256\times 256$的分辨率训练550k步；
- 在上述数据集中超过$512$分辨率的图像上以$512\times 512$的分辨率训练850k步；
- 在上述数据集中通过渐进蒸馏训练150k步；
- 在上述数据集上以$768\times 768$的分辨率训练140k步.

**SDv2**模型使用**OpenCLIP**模型进行文本嵌入，相比于**CLIP**模型的优势：
- **OpenCLIP**模型比**CLIP**模型大五倍，更大的文本编码器能够改进图像质量；
- **CLIP**模型是开源的，但是其训练数据未开源；**OpenCLIP**模型完全在开源数据上进行训练，在学习和优化时更透明。
