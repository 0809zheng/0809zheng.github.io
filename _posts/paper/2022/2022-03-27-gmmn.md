---
layout: post
title: 'Generative Moment Matching Networks'
date: 2022-03-27
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63903dc9b1fccdcd36058d87.jpg'
tags: 论文阅读
---

> 生成式矩匹配网络.

- paper：[Generative Moment Matching Networks](https://arxiv.org/abs/1502.02761v1)


本文作者通过积分概率度量(**integral probability metrics, IPM**)中的最大平均差异(**maximum mean discrepancy, MMD**)构造了一种生成模型。具体地，设计了一种生成式矩匹配网络(**Generative Moment Matching Network, GMMN**)从随机噪声中采样生成数据，并通过匹配真实数据分布$P_{data}(x)$和生成数据分布$P_G(x)$之间所有阶次的统计量训练网络。

# 1. 积分概率度量与最大平均差异

[积分概率度量](https://0809zheng.github.io/2022/12/06/ipm.html)寻找满足某种限制条件的函数集合$$\mathcal{F}$$中的连续函数$f(\cdot)$，使得该函数能够提供足够多的关于矩的信息；然后寻找一个最优的$$f(x)\in \mathcal{F}$$使得两个概率分布$p(x)$和$q(x)$之间的差异最大，该最大差异即为两个分布之间的距离：

$$ d_{\mathcal{F}}(p(x),q(x)) = \mathop{\sup}_{f(x)\in \mathcal{F}} \Bbb{E}_{x \text{~} p(x)}[f(x)]-\Bbb{E}_{x \text{~} q(x)}[f(x)] $$

[最大平均差异](https://0809zheng.github.io/2022/12/06/ipm.html)没有直接定义函数空间$$\mathcal{F}$$，而是通过[核方法](https://0809zheng.github.io/2021/07/23/kernel.html)在内积空间中间接定义了连续函数$f(\cdot)$，使得函数$f(\cdot)$能够隐式地计算所有阶次的统计量。

**MMD**将函数$f(\cdot)$限制在再生希尔伯特空间(**reproducing kernel Hilbert space, RKHS**)的单位球内：$\|\|f\|\|_{\mathcal{H}} \leq 1$。**MMD**定义如下：

$$ \text{MMD}(f,p(x),q(x)) = \mathop{\sup}_{||f||_{\mathcal{H}} \leq 1} \Bbb{E}_{x \text{~} p(x)}[f(x)]-\Bbb{E}_{x \text{~} q(x)}[f(x)] $$

根据**Riesz**表示定理，希尔伯特空间中的函数$f(x)$可以表示为内积形式：

$$ f(x) = <f,\phi(x)>_{\mathcal{H}} $$

则**MMD**可以写作：

$$ \begin{aligned} \text{MMD}(f,p(x),q(x)) &= \mathop{\sup}_{||f||_{\mathcal{H}} \leq 1} <f,\Bbb{E}_{x \text{~} p(x)}[\phi(x)]>_{\mathcal{H}} -<f,\Bbb{E}_{x \text{~} q(x)}[\phi(x)]>_{\mathcal{H}} \\ &= \mathop{\sup}_{||f||_{\mathcal{H}} \leq 1} <f,\Bbb{E}_{x \text{~} p(x)}[\phi(x)]-\Bbb{E}_{x \text{~} q(x)}[\phi(x)]>_{\mathcal{H}}  \end{aligned} $$

**MMD**的平方可以写作：

$$ \begin{aligned} \text{MMD}^2(f,p(x),q(x)) &= (\mathop{\sup}_{||f||_{\mathcal{H}} \leq 1} <f,\Bbb{E}_{x \text{~} p(x)}[\phi(x)]-\Bbb{E}_{x \text{~} q(x)}[\phi(x)]>_{\mathcal{H}})^2 \\ &= ||\Bbb{E}_{x \text{~} p(x)}[\phi(x)]-\Bbb{E}_{x \text{~} q(x)}[\phi(x)]||_{\mathcal{H}}^2 \end{aligned} $$

其中上式应用了两个向量内积的最大值必定在两向量同方向上取得，即当$$f=\Bbb{E}_{x \text{~} p(x)}[\phi(x)]-\Bbb{E}_{x \text{~} q(x)}[\phi(x)]$$时取得最大值。

对期望的计算应用蒙特卡洛估计，可得：

$$ \begin{aligned} \text{MMD}^2(f,p(x),q(x))  &= ||\Bbb{E}_{x \text{~} p(x)}[\phi(x)]-\Bbb{E}_{x \text{~} q(x)}[\phi(x)]||_{\mathcal{H}}^2 \\  &≈ ||\frac{1}{m}\sum_{i=1}^m\phi(x_i)-\frac{1}{n}\sum_{j=1}^n\phi(x_j)||_{\mathcal{H}}^2 \end{aligned} $$

引入核函数 $\kappa(x,x')=<\phi(x),\phi(x')>_{\mathcal{H}}$，则**MMD**的平方近似计算为：

$$ \begin{aligned} \text{MMD}^2(f,p(x),q(x)) = & \Bbb{E}_{x,x' \text{~} p(x)} [\kappa(x,x')] + \Bbb{E}_{x,x' \text{~} q(x)} [\kappa(x,x')] \\ & -2\Bbb{E}_{x \text{~} p(x),x' \text{~} q(x)} [\kappa(x,x')] \\  ≈ &\frac{1}{m^2}\sum_{i=1}^m\sum_{i'=1}^m \kappa(x_i,x_{i'})+\frac{1}{n^2}\sum_{j=1}^n\sum_{j'=1}^n \kappa(x_j,x_{j'}) \\ & - \frac{2}{mn}\sum_{i=1}^m\sum_{j=1}^n \kappa(x_i,x_j) \end{aligned} $$

# 2. 生成式矩匹配网络

![](https://pic.imgdb.cn/item/63903e3db1fccdcd3606a70f.jpg)

**生成式矩匹配网络(GMMN)**既可以在输入数据空间中使用，也可以在自编码器的编码空间中使用。

预先设置核函数的形式，在真实数据分布$x_i$和生成数据分布$x_j$中随机采样后，可构造网络的**MMD**损失：

$$ \mathcal{L}_{MMD} = \frac{1}{m^2}\sum_{i=1}^m\sum_{i'=1}^m \kappa(x_i,x_{i'})+\frac{1}{n^2}\sum_{j=1}^n\sum_{j'=1}^n \kappa(x_j,x_{j'}) - \frac{2}{mn}\sum_{i=1}^m\sum_{j=1}^n \kappa(x_i,x_j) $$

若核函数选用高斯核$$\kappa(x,x') = e^{-\frac{\|x-x'\|^2}{2\sigma}}$$，则生成数据的梯度为：

$$ \frac{\partial \mathcal{L}_{MMD}}{\partial x_j} = \frac{1}{n^2}\sum_{j'=1}^n \frac{1}{\sigma} \kappa(x_j,x_{j'})(x_{j'}-x_j) - \frac{2}{mn}\sum_{i=1}^m \frac{1}{\sigma} \kappa(x_i,x_j)(x_j-x_i) $$

通过反向传播算法，可以更新**GMMN**网络的参数。