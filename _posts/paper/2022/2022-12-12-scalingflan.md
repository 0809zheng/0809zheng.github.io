---
layout: post
title: 'Scaling Instruction-Finetuned Language Models'
date: 2022-12-12
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67ff1b8b88c538a9b5d2534b.png'
tags: 论文阅读
---

> 扩展指令微调的语言模型.

- paper：[Scaling Instruction-Finetuned Language Models](https://arxiv.org/abs/2210.11416)

# 0. TL; DR

本文探讨了通过指令微调（**instruction finetuning**）提升语言模型性能的方法，重点关注模型规模、微调任务数量以及思维链（**chain-of-thought, CoT**）数据的使用。研究发现，指令微调能够显著提升语言模型在多种任务上的表现，尤其是在零样本（**zero-shot**）和少样本（**few-shot**）学习场景中。此外，指令微调还提高了模型的多语言能力和对有害内容的识别能力。

# 1. 背景介绍
随着语言模型规模的不断扩大，其在自然语言处理（**NLP**）任务中的表现逐渐接近人类水平。然而，这些模型在零样本学习任务中仍然面临挑战，因为它们需要在没有额外样本的情况下直接根据任务描述生成答案。为了提升模型在零样本和少样本任务中的性能，研究者们提出了指令微调的方法，即通过在多个任务上进行微调，使模型能够更好地理解和执行自然语言指令。此前的研究已经证明了指令微调在提升模型性能方面的有效性，但大多集中在较小规模的模型（如 **137B** 参数）和有限的任务数量上。本文则进一步扩展了指令微调的研究，探索了在更大规模模型（**540B** 参数）和更多任务（**1.8K** 任务）上的效果，并引入了思维链数据以提升模型的推理能力。

# 2. 方法介绍

指令微调的核心思想是通过自然语言指令来描述任务，并在多个任务上对模型进行微调，从而使模型能够更好地理解和执行这些指令。研究者们将 **1.8K** 个任务的数据集组合在一起，这些任务涵盖了自然语言推理、阅读理解、情感分析、翻译等多种类型。每个任务都被手动编写了多个指令模板，以增加任务描述的多样性。

![](https://pic1.imgdb.cn/item/67ff1ca988c538a9b5d25798.png)

思维链是一种推理方法，通过逐步展示推理过程来帮助模型更好地理解和执行任务。研究者们在指令微调中引入了思维链数据，这些数据包含人类标注的推理步骤。例如，对于一个数学问题，思维链数据会展示如何逐步计算出答案。通过在微调过程中加入思维链数据，模型能够学习到更复杂的推理过程，从而在推理任务中表现更好。

![](https://pic1.imgdb.cn/item/67ff1cdd88c538a9b5d25850.png)

为了验证指令微调的效果，研究者们构建了一个包含 **1.8K** 任务的微调数据集，这些任务来自多个不同的数据集组合。每个数据集都被手动编写了多个指令模板，以增加任务描述的多样性。此外，研究者们还对每个任务设置了最大样本数限制，以避免某些任务因样本过多而主导微调过程。

![](https://pic1.imgdb.cn/item/67ff1d1f88c538a9b5d258fd.png)

在微调过程中，研究者们使用了 **Adafactor** 优化器，并采用了打包（**packing**）技术来提高训练效率。打包技术允许将多个训练样本组合成一个序列，从而提高计算效率。微调的步数、学习率、批量大小等超参数根据模型规模进行了调整。例如，对于 **540B** 参数的模型，微调过程仅占预训练计算量的 **0.2%**，但性能提升显著。

![](https://pic1.imgdb.cn/item/67ff1d5788c538a9b5d2599d.png)

# 3. 实验分析

实验结果表明，指令微调在不同规模的模型上都能显著提升性能。随着模型规模的增大和微调任务数量的增加，模型在零样本和少样本任务上的表现进一步提升。此外，指令微调还提高了模型的多语言能力和对有害内容的识别能力。

![](https://pic1.imgdb.cn/item/67ff1da488c538a9b5d25aac.png)


引入思维链数据后，模型在推理任务上的表现显著提升。这表明思维链数据能够帮助模型学习更复杂的推理过程，从而在需要多步推理的任务中表现更好。

![](https://pic1.imgdb.cn/item/67ff1e3588c538a9b5d25cc3.png)

除了在标准 **NLP** 基准测试中的表现，指令微调还显著提升了模型在开放式生成任务中的性能。例如，在一个包含创造力、推理、规划等挑战性问题的评估集中，**Flan-PaLM 540B** 的生成结果在人类评估中优于 **PaLM 540B**，尤其是在需要思维链的任务中。这表明指令微调能够使模型更好地理解和生成自然语言指令，从而在零样本和少样本任务中表现更好。

![](https://pic1.imgdb.cn/item/67ff1ebc88c538a9b5d25e88.png)