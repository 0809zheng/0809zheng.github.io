---
layout: post
title: 'Learning to Encode Position for Transformer with Continuous Dynamical Model'
date: 2022-07-02
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/62c2af0a5be16ec74a3bbb1a.jpg'
tags: 论文阅读
---

> FLOATER：基于连续动力系统的递归位置编码.

- paper：[Learning to Encode Position for Transformer with Continuous Dynamical Model](https://arxiv.org/abs/2003.09229)

针对**Transformer**中的位置编码模块，作者提出了一种基于连续动力系统(**Continuous Dynamical System**)的位置编码方法**FLOATER (flow-based Transformer)**，该方法具有如下特点：
1. 显式引入了顺序性(**sequential**)的归纳偏差，能够泛化到任意长度的文本输入；
2. 是一种数据驱动(**data-driven**)的编码方式，额外增加的参数量与文本长度无关；
3. **sinusoidal**编码可以看作是**FLOATER**的一个特例，并可以进一步初始化**FLOATER**；
4. 可以在不增加参数量的前提下将位置信息直接注入到每一层的输入中。

# 1. Transformer中的位置编码

**Transformer**由于其置换等价性，对输入序列的位置不够敏感。对于一些对位置信息要求较高的任务，需要有效地编码位置信息。当前流行的位置编码包括如下三种解决方案：

![](https://pic.imgdb.cn/item/62c2b1a05be16ec74a3f458d.jpg)

### ⚪ 三角函数编码 Sinusoidal
- 优点：三角函数是平滑的；显式刻画了位置的顺序信息，可外推到长句上。
- 缺点：不包含可学习的参数，不够灵活。

### ⚪ 嵌入编码 Embedding
- 优点：通过可学习的参数从数据中自动学习位置信息。
- 缺点：需要额外增加参数量；需要预先指定输入文本的最大长度；不同位置的编码不够平滑，不能充分建模位置的相对顺序关系。

### ⚪ 相对位置编码 Relative
- 优点：直接建模词与词之间的相对位置关系；具有可学习的参数。
- 缺点：需要额外增加参数量，需要预先指定最大相对距离防止参数量过大；学习到的位置编码不够平滑。

### ⚪ 更好的位置编码

对比上述位置编码的特点，作者指出一个好的位置编码应满足如下的三个条件：
1. **inductive**：具有外推性，能处理比训练序列更长的序列数据
2. **data-driven**：位置编码能从数据中学习
3. **parameter efficient**：引入的额外参数量应该不受文本长度的影响

# 2. FLOATER

本文将位置编码建模为一个连续动力系统。一方面显式引入了顺序性的归纳偏置，能够自然外推到长句子上；另一方面位置编码是通过训练得到的，额外增加的参数量不会随着序列的长度而变化。

![](https://pic.imgdb.cn/item/62c2cb575be16ec74a6167e1.jpg)

### ⚪ Position Encoding with Dynamical Systems

位置编码可以表示为一个离散序列$$\{p_i \in \Bbb{R}^d : i=1,...,T\}$$，若将该序列连续化为$p(t)$，为了建立序列的自相关性，使用常微分方程(**Neural ODE**)构建一个连续动力系统：

$$ \frac{d  p(t)}{dt} = h(t,p(t);\theta_h) $$

或表示为积分形式：

$$ p(t) = p(s) + \int_{s}^{t} h(\tau,p(\tau);\theta_h) d\tau $$

其中$h(\tau,p(\tau);\theta_h)$是由$\theta_h$定义的神经网络。离散的位置编码可以通过对时间离散化$t_i=i\cdot \Delta t$后通过递归计算得到：

$$ p_N = p_{N-1} + \int_{(N-1)\Delta t}^{N \Delta t} h(\tau,p(\tau);\theta_h) d\tau $$

在训练时采用**adjoint**方法计算$\theta_h$的梯度：

$$ \frac{dL}{d \theta_h} = - \int_{t}^{s} a(\tau)^T \frac{\partial h(\tau,p(\tau);\theta_h)}{\partial \theta_h} d\tau $$

**adjoint**状态$a(\tau)$可以通过求解**adjoint**方程得到：

$$ \frac{da(\tau)}{d \tau} = - a(\tau)^T \frac{\partial h(\tau,p(\tau);\theta_h)}{\partial p(\tau)} $$

上述方程可以使用**Runge-Kutta**法或中点法来求解。

### ⚪ Parameter Sharing and Warm-start Training

普通的可学习编码可以在**Transformer**的每一层都注入位置信息，但是每增加一层的位置信息，用于位置编码的参数量就会翻倍。作者设计的连续动力系统能够在每一层共享参数($\theta_h^{(1)}=\theta_h^{(2)}=\cdots \theta_h^{(N)}$)的同时，使得每一层的位置编码是不一样的。这是因为每一层的初始状态不同，所以求解得到每一层的位置编码是不一样的：

$$ p^{(n)}(t) = p^{(n)}(s) + \int_{s}^{t} h^{(n)}(\tau,p^{(n)}(\tau);\theta_h^{(n)}) d\tau $$

注意到三角函数编码是**FLOATER**的一个特例：

$$ \begin{aligned} p_{i+1}[j]-p_i[j] &= \begin{cases} \sin((i+1)\cdot c^{\frac{j}{d}})-\sin(i\cdot c^{\frac{j}{d}}), & \text{if } j \text{ is even} \\ \cos((i+1)\cdot c^{\frac{j-1}{d}})-\cos(i\cdot c^{\frac{j-1}{d}}), & \text{if } j \text{ is odd} \end{cases}\\&  = \begin{cases} \int_{i}^{i+1}c^{-\frac{j}{d}} \cos(\tau \cdot c^{\frac{j}{d}}) d\tau ,  & \text{if } j \text{ is even} \\ \int_{i}^{i+1}-c^{-\frac{j-1}{d}} \sin(\tau \cdot c^{\frac{j-1}{d}}) d\tau , & \text{if } j \text{ is odd} \end{cases} \end{aligned} $$

因此可以使用三角函数编码作为**FLOATER**的参数初始化，然后在下游任务上微调模型。由于微分方程求解器无法利用GPU并行计算能力，常微分方程带来的额外时间开销是不容忽视的。使用三角函数编码来初始化**FLOATER**能够避免从头训练模型，减小时间开销。

# 3. 实验分析

实验结果表明，**FLOATER**的表现超过了三角函数编码和可学习编码，且对每一层都注入位置信息会带来性能提升。

![](https://pic.imgdb.cn/item/62c2d9bc5be16ec74a738282.jpg)

作者从**WMT14 En-De**数据集中取出长度小于$80$的短句(占数据集的$98.6\%$)训练模型，并在其余长句上测试模型。测试结果表明**FLOATER**在长句翻译的表现上显著优于其他位置编码方式。

![](https://pic.imgdb.cn/item/62c2dab55be16ec74a74c818.jpg)

本文通过连续动态系统构造的位置编码具有递归形式，因此也可以用循环神经网络建模。作者对比了几种不同的循环网络：

![](https://pic.imgdb.cn/item/62c2db595be16ec74a758bd5.jpg)

作者展示了不同位置编码方式得到的编码矩阵。可学习编码底部几乎是常数，这是由于训练集中长文本太少，位置编码得不到充分的训练。而**FLOATER**的编码看起来更光滑。**RNN**存在梯度消失问题，因此低维度的位置特征没有得到充分学习。

![](https://pic.imgdb.cn/item/62c2dbda5be16ec74a76368e.jpg)