---
layout: post
title: 'Self-Attention with Relative Position Representations'
date: 2022-07-03
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/62c4e17d5be16ec74aa11ab5.jpg'
tags: 论文阅读
---

> 自注意力机制中的相对位置编码.

- paper：[Self-Attention with Relative Position Representations](https://arxiv.org/abs/1803.02155)

# 1. 绝对位置编码

在自注意力机制中通过仿射变换生成查询向量、键向量和值向量：

$$ \begin{aligned}q_i & = x_i W^Q \\ k_i &= x_i W^K \\ v_i &= x_iW^V  \end{aligned} $$

然后通过**softmax**归一化和点积运算计算输出：

$$ \begin{aligned} e_{ij} &= \frac{q_ik_j^T}{\sqrt{d_z}} \\ \alpha_{ij} &= \frac{\exp(e_{ij})}{\sum_{k=1}^{n}\exp(e_{ik})} \\ z_i &= \sum_{j=1}^{n} \alpha_{ij}v_j \end{aligned} $$

上述运算在结构上没有显式地建模位置信息。所以需要在输入中添加入绝对位置的表示：

$$ \begin{aligned}q_i & = (x_i+p_i) W^Q \\ k_i &= (x_i+p_i) W^K \\ v_i &= (x_i+p_i) W^V  \end{aligned} $$

此时输出计算为：

$$ \begin{aligned} e_{ij} &= \frac{(x_iW^Q+p_iW^Q)  (x_jW^K+p_jW^K)^T}{\sqrt{d_z}} \\ \alpha_{ij} &= \frac{\exp(e_{ij})}{\sum_{k=1}^{n}\exp(e_{ik})} \\ z_i &= \sum_{j=1}^{n} \alpha_{ij}(x_jW^V+p_jW^V)  \end{aligned} $$

# 2. 相对位置编码

作者把输入**token**之间的位置关系建模为一个有向图，其中输入$x_i$和$x_j$之间的边由向量$$a_{ij}^V,a_{ij}^K \in \Bbb{R}^{d_z}$$表示。

![](https://pic.imgdb.cn/item/62c4e6a35be16ec74aa5f9a2.jpg)

此时对于输入$x_i$，其自注意力机制中关于$x_j$的位置编码项$p_jW^V,p_jW^K$被替换为$a_{ij}^V,a_{ij}^K$，并且去掉了$x_i$的位置编码项$p_iW^Q$：

$$ \begin{aligned} e_{ij} &= \frac{x_i W^Q (x_j W^K+a_{ij}^K)^T}{\sqrt{d_z}} \\ \alpha_{ij} &= \frac{\exp(e_{ij})}{\sum_{k=1}^{n}\exp(e_{ik})} \\ z_i &= \sum_{j=1}^{n} \alpha_{ij}(x_jW^V+a_{ij}^V)\end{aligned} $$

$a_{ij}^V,a_{ij}^K$被预先设置为一系列可学习的位置嵌入，且只考虑相对位置最大值为$L$的情况。作者认为精确的相对位置信息在一定距离之外是无用的，且通过裁剪最大距离也使模型能够推广到任意序列长度。则相对位置编码共包括$4L+2$个长度为$d_z$的向量：

$$ \begin{aligned} a_{ij}^K &= w^K_{\text{clip}(j-i,-L,L)} \\ a_{ij}^V &= w^V_{\text{clip}(j-i,-L,L)} \\ w^K &= (w_{-L}^K,\cdots w_{L}^K) \\ w^V &= (w_{-L}^V,\cdots w_{L}^V) \end{aligned} $$

实验结果表明相对位置编码能够提高机器翻译的性能：

![](https://pic.imgdb.cn/item/62c4ee625be16ec74aadbee3.jpg)