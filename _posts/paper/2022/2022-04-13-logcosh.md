---
layout: post
title: 'Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder'
date: 2022-04-13
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/628c82e309475431298117da.jpg'
tags: 论文阅读
---

> 使用对数双曲余弦损失改进变分自编码器.

- paper：[Log Hyperbolic Cosine Loss Improves Variational Auto-Encoder](https://openreview.net/forum?id=rkglvsC9Ym)

在变分自编码器**VAE**中，解码样本和原输入之间的重构损失函数默认选择**L2**损失，本文作者建议将其替换为对数双曲余弦(**log cosh**)损失，实验结果表明其能够显著改善**VAE**的重构质量。

# 1. 对数双曲余弦损失 Log Hyperbolic Cosine Loss

对数双曲余弦(**log cosh**)函数的表达式如下：

$$ f(t;a) = \frac{1}{a} \log( \cosh(at)) = \frac{1}{a} \log(\frac{e^{at}+e^{-at}}{2}) $$

对于较大的$\| t \|$，该函数接近**L1**函数，对于较小的$\| t \|$，该函数接近**L2**函数，从而结合了**L2**函数的平滑特点以及**L1**函数的鲁棒性和图像清晰度优势。

![](https://pic.imgdb.cn/item/628c82b6094754312980de25.jpg)

此外，该函数的导数是简单的**tanh**函数，容易训练且实现简单：

$$ \nabla_t f(t;a)= \nabla_t \frac{1}{a} \log(\frac{e^{at}+e^{-at}}{2}) = \frac{1}{a}\frac{2}{e^{at}+e^{-at}} \nabla_t \frac{e^{at}+e^{-at}}{2} \\= \frac{1}{a}\frac{2}{e^{at}+e^{-at}} \frac{ae^{at}-ae^{-at}}{2} = \frac{e^{at}-e^{-at}}{e^{at}+e^{-at}} = \tanh (at) $$

# 2. LogCosh VAE的Pytorch

**LogCosh VAE**的完整**pytorch**实现可参考[PyTorch-VAE](https://github.com/AntixK/PyTorch-VAE/blob/master/models/mssim_vae.py)，与标准**VAE**的主要区别在于构造重构损失时使用**log cosh**替代均方误差：

```python
t = recons - input
recons_loss = self.alpha * t + \
              torch.log(1. + torch.exp(- 2 * self.alpha * t)) - \
              torch.log(torch.tensor(2.0))
recons_loss = (1. / self.alpha) * recons_loss.mean()

kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)

loss = recons_loss + self.beta * kld_loss
```

