---
layout: post
title: 'In Defense of the Triplet Loss for Person Re-Identification'
date: 2022-11-16
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63ccfa27be43e0d30e95f3c0.jpg'
tags: 论文阅读
---

> 为人体重识别任务中的三元组损失辩护.

- paper：[In Defense of the Triplet Loss for Person Re-Identification](https://arxiv.org/abs/1703.07737)


[<font color=blue>Triplet Loss</font>](https://0809zheng.github.io/2022/11/02/triplet.html)是最常用的度量学习损失，它为每一个样本$x$选择一个正样本$x^+$和一个负样本$x^-$，使得正样本对之间的距离比负样本对之间的距离小于**margin**值$\epsilon$。

$$ \max(0, D[f_{\theta}(x),f_{\theta}(x^+)] -D[f_{\theta}(x),f_{\theta}(x^-)] + \epsilon) $$

**Triplet loss**的缺点在于随机从训练集中挑选三个样本，可能会采样到简单的样本组合，即非常相似的正样本对和差异很大的负样本对，让网络一直学习简单的样本会限制网络的泛化能力。作者提出了一种在线**batch hard sample mining**的改进版**Triplet loss**。

改进的**Batch Hard Triplet Loss**对每一个样本$x$选择$K$个正样本$x^+_k$和$K$个负样本$x^-_k$，并选用最难的正样本对和负样本对构造损失：

$$ \max(0, \mathop{\max}_k D[f_{\theta}(x),f_{\theta}(x^+_k)] - \mathop{\min}_k D[f_{\theta}(x),f_{\theta}(x^-_k)] + \epsilon) $$

由于正负样本对是在训练时自动构建的，因此是一种在线难例挖掘(**online hard-mining**)机制。实验结果表明这种机制与**soft margin**一起使用时效果最好。

![](https://pic.imgdb.cn/item/63cd0351be43e0d30ea66b32.jpg)

此外作者还讨论了是否有必要采用预训练模型。结果表明利用预训练模型确实可以获得更好一点的效果，但是从头开始训练的网络也不会太差；
特别的，预训练模型往往体积较大模式固定，不如自己设计网络来的灵活。同时，预训练模型往往有其自己的固定输入，如果修改其输入很可能会得到相反的效果。

![](https://pic.imgdb.cn/item/63cd03aabe43e0d30ea6e272.jpg)