---
layout: post
title: 'Large Scale GAN Training for High Fidelity Natural Image Synthesis'
date: 2022-05-27
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63a01e5eb1fccdcd36631905.jpg'
tags: 论文阅读
---

> BigGAN：用于高保真度自然图像合成的大规模GAN.

- paper：[Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096)

**BigGAN**整体结构与[<font color=Blue>SAGAN</font>](https://0809zheng.github.io/2022/05/25/sagan.html)相同，在网络中引入了[自注意力机制](https://0809zheng.github.io/2020/11/21/SAinCNN.html)，有助于对图像区域中长距离、多层次的依赖关系进行建模。此外，生成器和判别器均应用了[谱归一化](https://0809zheng.github.io/2022/02/08/sngan.html)，使得网络满足**Lipschitz**连续性；训练时遵循[TTUR](https://0809zheng.github.io/2022/03/24/ttur.html)原则，判别器$D$和生成器$G$的学习率是不相等的。

在此基础上，**BigGAN**采用了更大的数据批量、截断策略和模型稳定性的控制。

## 1. 更大的数据批量

作者发现简单地将**batch size**增大就可以实现性能上较好的提升，文章做了实验验证，把**batch size**调整到**SAGAN**的$8$倍($2048$)：

![](https://pic.imgdb.cn/item/63a02dcfb1fccdcd3681c554.jpg)

增大**batch size**使得每批次数据能覆盖更多模式的结果，为生成和判别网络提供更好的梯度，还会消耗更少的时间训练出更好性能的模型；但也会使得模型的训练稳定性下降。增大**batch size**时，网络每层的通道数也相应的增加，当通道增加一倍时，模型中的参数数量也大约增大一倍。

**BigGAN**在**ImageNet**数据集上进行训练，因此在网络中引入条件标签$c$实现条件生成。作者采用共享嵌入的方式，把条件标签投影到每个**BatchNorm**层的仿射参数，并通过条件**BN**实现条件生成过程。在先验分布$z$的嵌入过程中，作者也采用不同层次的嵌入，把先验$z$拆分到每个模块，并与条件嵌入连接，共同作为条件**BN**的输入参数。

![](https://pic.imgdb.cn/item/63a02f4ab1fccdcd368500fa.jpg)

# 2. 截断技巧 truncation trick

在生成图像时，噪声先验$z$一般选择标准正态分布$N(0,I)$或者均匀分布$U[−1,1]$。为了提高生成图像的质量，作者提出了一种**truncation trick**，用于平衡模型生成具体图像或生成不同类型的图像的能力。

截断技巧是指从先验分布$z$采样时，通过设置阈值的方式来截断采样，其中超出范围的采样值被丢弃并重新采样。这个阈值可以根据生成质量指标决定。

通过对阈值的设定，随着阈值的下降（噪声$z$的取值范围越窄）生成的质量会越来越好，但是由于阈值的下降，采样的范围变窄，就会造成生成结果单一化，多样性不足。

![](https://pic.downk.cc/item/5ed868d3c2a9a83be5ba48c5.jpg)

一些较大的模型不适合直接截断，在输入截断噪声时会生成图像中会产生伪影。为了抵消这种影响，可以通过让生成器的权重更加平滑来适应截断的输入噪声，作者引入了生成器权重$W$的正交正则化:

$$ R_{\beta}(W) = \beta ||W^TW\odot (1-I)||_F^2 $$

# 3. 模型稳定性的控制

作者在训练期间监测一系列权重、梯度和损失的统计数据，以寻找可能预示训练崩溃开始的指标。实验发现每个权重矩阵的前三个奇异值$σ_0,σ_1,σ_2$是最有用的，它们可以使用**Alrnoldi**迭代方法进行有效计算。

下图报告了第一个奇异值$σ_0$在应用谱归一化之前在不同网络层中的数值大小。在一些层中，奇异值$σ_0$的数值会快速增大，导致训练崩溃。

![](https://pic.imgdb.cn/item/63a041bfb1fccdcd36a85c81.jpg)

### ⚪ 生成器的稳定性

为了缓解生成器的训练崩溃，可以适当调整奇异值$σ_0$的数值来修正权重$W$：

$$ W = W - \max(0, \sigma_0-\sigma_{clamp})\nu_0\mu_0^T $$

上述操作是为了将权重的第一个奇异值$σ_0$控制住，防止其数值突然性的爆炸。实验观察到即使在某些情况下可以一定程度地改善网络性能，但没有任何组合可以防止训练崩溃（崩溃无法避免）。

### ⚪ 判别器的稳定性

判别器的奇异值$σ_0$变化是有噪声的，但整个过程是平稳增长。为了去除噪声，可以对判别器的权重应用梯度惩罚：

$$ R = \frac{\gamma}{2} E_{p(x)} [||\nabla_x D(x)||_F^2] $$

结果表明对判别器的惩罚足够高，可以实现训练稳定性，但是性能会有显著地下降。