---
layout: post
title: 'Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB'
date: 2022-04-06
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/624d4f6b239250f7c59458ad.jpg'
tags: 论文阅读
---

> 使用遮挡鲁棒姿态图从单目相机中重构三维姿态.

- paper：[Single-Shot Multi-Person 3D Pose Estimation From Monocular RGB](https://arxiv.org/abs/1712.03453)
- code：[github](https://github.com/Daniil-Osokin/lightweight-human-pose-estimation-3d-demo.pytorch)

本文提出了一种从单目相机中恢复3D多人姿态的方法。该方法使用了对遮挡鲁棒的姿态图(**occlusion-robust pose-map, ORPM**)，从而在目标强烈遮挡的情况下也能进行全身的姿态估计。**ORPM**将场景中所有人的3D关节位置编码为固定数量的映射图，并使用身体部位关联(**body part association**)推断任意数量的人体目标。

## 1. 多目标数据集

由于目前没有公开并广泛认可的大规模多人**3D**姿态估计数据集，作者通过已有单人**3D**姿态估计数据集**MPI-INF-3DHP**合成了**MuCo-3DHP**数据集，这是一个包含复杂的多人交互和遮挡的真实图像数据集，数据集中还使用了数据增强。

为了保证测试结果的有效性，从实际场景中拍摄并采集了小规模的**3D**测试集**MuPoTS-3D**，使用**3DPCK**指标衡量模型性能，即如果一个关节点位于以真实关节为中心的**15cm**的球内，则认为关节的预测是正确的。两个数据集展示如下：

![](https://pic.imgdb.cn/item/624d5567239250f7c5a1d2db.jpg)

## 2. 遮挡鲁棒的姿态图

该方法基于**位置图**(**location-map**)。位置图是一种关节特定的特征通道，在每个关节的2D像素坐标处存储关节的3D坐标信息。因此对于每个关节，网络需要估计一个2D的像素位置热图以及三个位置图。对于大小为$H \times W$的图像，使用大小为$H/k \times W/k$的$3n$个位置图来存储所有$n$个关节的3D位置，其中$k$是下采样因子。通过位置图，关节的3D位置可以直接从关节的2D像素位置处直接读出来。

![](https://pic.imgdb.cn/item/624d5850239250f7c5a8419e.jpg)

尽管这种简单的位置图可以实现完整的**3D**姿态估计，但仍有两个缺点。 首先位置图假定假设目标的所有关节点都是可见的，无法处理有部分遮挡的情。 其次如果扩展到多人场合，则位置图的输出与人数呈倍数关系，导致计算量很大，且无法固定网络的输出数量。

作者提出了**遮挡鲁棒的姿态图**(**occlusion-robust pose-map, ORPM**)，**ORPM**不仅能解决遮挡问题，同时还固定了输出数量。**ORPM**仍然为每个关节点构造了大小为$H/k \times W/k$的$3$个位置图，为了解决遮挡产生的冲突，在位置图中引入冗余(**redundancy**)策略，具体包括：（1）允许从躯干位置的关节点（颈部或骨盆）出发读出完整的基础姿态；（2）在**2D**检测可用的情况下，通过读取头部和单个肢体的姿态，进一步优化基础姿态；（3）完整的肢体姿态可以在该肢体的任何**2D**关节位置读取。 

为了从**ORPM**中读取每个目标的**3D**关节点信息，作者采用了一种**read-out pixel location**关节点读出过程。首先在**ORPM**中选择人体关节点的一个基础点，然后根据肢体节点微调关节点。

![](https://pic.imgdb.cn/item/627f7c210947543129abe3ff.jpg)

```python
num_kpt_panoptic = 19 # 预定义关节点数
keypoint_treshold = 0.1 # 置信度阈值
    poses_3d = np.ones((len(poses_2d), num_kpt_panoptic * 4), dtype=np.float32) * -1 # 三维坐标及关节点的置信度
    for pose_id in range(poses_3d.shape[0]):
        if poses_2d[pose_id, 2] <= keypoint_treshold:
            continue
        pose_3d = poses_3d[pose_id]
        neck_2d = poses_2d[pose_id, 0:2].astype(np.int32)
        # 选定neck节点作为基础点
        for kpt_id in range(num_kpt_panoptic):
            map_3d = ORPM[kpt_id * 3:(kpt_id + 1) * 3]  
            pose_3d[kpt_id * 4] = map_3d[0, neck_2d[1], neck_2d[0]]
            pose_3d[kpt_id * 4 + 1] = map_3d[1, neck_2d[1], neck_2d[0]]
            pose_3d[kpt_id * 4 + 2] = map_3d[2, neck_2d[1], neck_2d[0]]
            pose_3d[kpt_id * 4 + 3] = poses_2d[pose_id, kpt_id * 3 + 2]
           
        # 根据肢体关节点微调所有关节点
        for limb in limbs:
            for kpt_id_from in limb:
                if poses_2d[pose_id, kpt_id_from * 3 + 2] <= keypoint_treshold:
                    continue
                for kpt_id_where in limb:
                    kpt_from_2d = poses_2d[pose_id, kpt_id_from * 3:kpt_id_from * 3 + 2].astype(np.int32)
                    map_3d = features[kpt_id_where * 3:(kpt_id_where + 1) * 3]
                    pose_3d[kpt_id_where * 4] = map_3d[0, kpt_from_2d[1], kpt_from_2d[0]]
                    pose_3d[kpt_id_where * 4 + 1] = map_3d[1, kpt_from_2d[1], kpt_from_2d[0]]
                    pose_3d[kpt_id_where * 4 + 2] = map_3d[2, kpt_from_2d[1], kpt_from_2d[0]]
                break
```

## 3. 网络结构

网络结构如图所示。在训练阶段时首先使用**COCO**数据集预训练网络，从而生成二维关节点热图和部位亲和场，并作为三维网络的输入。微调阶段使用**MuCo-3DHP**数据集。推理阶段输入图片获得三维姿态的**ORPM**。

![](https://pic.imgdb.cn/item/627f7f1e0947543129b63bac.jpg)