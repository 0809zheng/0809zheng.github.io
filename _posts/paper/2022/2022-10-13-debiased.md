---
layout: post
title: 'Debiased Contrastive Learning'
date: 2022-10-13
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63d49b11face21e9ef407ae7.jpg'
tags: 论文阅读
---

> 偏差修正的对比学习.

- paper：[Debiased Contrastive Learning](https://arxiv.org/abs/2007.00224)

对比学习中的**采样偏差(sampling bias)**是指由于样本的真实标签是未知的，因此负样本可能采样到**假阴性(false negative)**样本。采样偏差可能导致性能的显著降低。

![](https://pic.imgdb.cn/item/63d4d79cface21e9efcb6f1f.jpg)

记观测到**anchor**样本类别的概率为$\eta^+$，观测到其他类别的概率为$\eta^-=1-\eta^+$；对于样本$x$，观测到一个正样本$x'$的概率为$p_x^+(x')$，观测到一个负样本$x'$的概率为$p_x^-(x')$。对于采样样本$x'$，其实际采样分布为：

$$ p(x') = \eta^+ p_x^+(x') + \eta^- p_x^-(x') $$

因此观测到负样本的概率为$p_x^-(x')$被修正为：

$$ p_x^-(x') = (p(x')- \eta^+ p_x^+(x')) /\eta^- $$

标准的对比学习损失写作：

$$ \mathcal{L}_{contrastive} = \Bbb{E}_{(x,x^+)\text{~}p_{pos},\{x_i^-\}_{i=1}^M\text{~}p_{data}} [-\log \frac{\exp(f(x)^Tf(x^+)/\tau)}{\exp(f(x)^Tf(x^+)/\tau)+\sum_{i=1}^{M}\exp(f(x)^Tf(x^-_i)/\tau)}] $$

可以通过修正项对对比学习损失中分母中$$\exp(f(x)^Tf(x^-_i)/\tau)$$的进行偏差修正。给定从原分布$p$中采样的样本$$\{u_i\}_{i=1}^N$$和从正样本分布$p_x^+$中采样的样本$$\{v_i\}_{i=1}^M$$，则修正为：

$$ g(x,\{u_i\}_{i=1}^N,\{v_i\}_{i=1}^M) = \max(\frac{1}{\eta^-}(\frac{1}{N}\sum_{i=1}^N \exp(f(x)^Tf(u_i))-\frac{\eta^+}{M}\sum_{i=1}^M \exp(f(x)^Tf(v_i))),\exp(-1/\tau)) $$

其中$$\exp(-1/\tau)$$是原式$$\exp(f(x)^Tf(x^-_i)/\tau)$$的理论下界。最终对比损失修正如下：

$$ \mathcal{L}_{unbiased} = \Bbb{E}_{x,\{u_i\}_{i=1}^N\text{~}p;x^+,\{v_i\}_{i=1}^M\text{~}p_x^+} [-\log \frac{\exp(f(x)^Tf(x^+))}{\exp(f(x)^Tf(x^+))+Ng(x,\{u_i\}_{i=1}^N,\{v_i\}_{i=1}^M)}] $$

$M$越大则偏差修正的程度越大；当$M=1$时可取$v=x^+$，上式可以被简化为：

$$ \mathcal{L}_{unbiased} = \Bbb{E}_{x,\{u_i\}_{i=1}^N\text{~}p;x^+\text{~}p_x^+} [-\log \frac{\exp(f(x)^Tf(x^+))}{\exp(f(x)^Tf(x^+))+\max(\frac{1}{\eta^-}(\sum_{i=1}^N \exp(f(x)^Tf(u_i))-N\eta^+ \exp(f(x)^Tf(x^+))),N\exp(-1/\tau))}] $$

![](https://pic.imgdb.cn/item/63d4e5ccface21e9efeed3bc.jpg)

作者展示了在不同的偏差修正程度下，学习到特征表示的分布情况：

![](https://pic.imgdb.cn/item/63d4e642face21e9efeff116.jpg)