---
layout: post
title: 'GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models'
date: 2022-06-25
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6672a0e4d9c307b7e9795aa2.png'
tags: 论文阅读
---

> GLIDE：通过文本引导的扩散模型实现真实图像生成与编辑.

- paper：[GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models](https://arxiv.org/abs/2112.10741)

受引导扩散模型生成真实图像的能力和文本到图像模型处理任意形式提示的能力的启发，本文将引导扩散应用于文本条件图像生成问题。首先训练一个35亿参数的扩散模型，该模型使用文本编码器以自然语言描述为条件；然后比较两种将扩散模型引导至文本提示的技术：CLIP引导和无分类器引导；通过评估发现无分类器引导可以产生更高质量的图像。

### ⚪ CLIP 引导

**CLIP**引导的条件扩散模型的采样过程为：

$$
\mathbf{x}_{t-1} = \boldsymbol{\mu}_\theta\left(\mathbf{x}_t, t\right)+\sigma_t^2 \nabla_{\mathbf{x}_t} (f(x_t)\cdot g(c))+ \sigma_t \mathbf{z},\mathbf{z} \sim \mathcal{N}\left(\mathbf{0}, \mathbf{I}\right)
$$

其中$f(x),g(c)$是**CLIP**模型的图像编码器和文本编码器。两者的点积衡量图像与文本的匹配程度。

### ⚪ 无分类器引导

为了使用通用文本提示实现无分类器引导，在训练期间随机用空序列Ø替换文本标题。然后使用修改的噪声预测进行引导：

$$
\begin{aligned}
\overline{\boldsymbol{\epsilon}}_\theta\left(\mathbf{x}_t \mid c\right) &=s\boldsymbol{\epsilon}_\theta\left(\mathbf{x}_t\mid c\right) +(1-s) \boldsymbol{\epsilon}_\theta\left(\mathbf{x}_t\mid \phi \right)
\end{aligned}
$$

### ⚪ 实验分析

实验训练了一个 64x64 分辨率的 35 亿参数文本条件扩散模型，以及一个 15 亿参数的文本条件上采样扩散模型（将分辨率提高到 256x256），还训练了一个带噪声的 64x64 ViT-L CLIP 模型用于CLIP引导。

为了以文本为条件，首先将其编码为 K 个标记序列，并将这些标记输入 Transformer 模型。 该 Transformer 的输出有两种使用方式：首先，使用最终的标记嵌入来代替模型中的类别嵌入； 其次，把最后一层 token 嵌入（K 个特征向量的序列）分别投影到每个注意力层的维度，然后连接到每层的注意力特征上。

直观地比较 CLIP 引导与无分类器引导时，发现来自无分类器引导的样本通常看起来比使用 CLIP 引导生成的样本更真实。

![](https://pic.imgdb.cn/item/6672a638d9c307b7e98290c6.png)

作者认为CLIP 引导效果不好的原因是，在训练过程中模型通过生成对抗攻击样本对CLIP模型进行干扰，而不是优化更匹配的图像生成。