---
layout: post
title: 'Training language models to follow instructions with human feedback'
date: 2022-07-24
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67ff5ebb88c538a9b5d32a6a.png'
tags: 论文阅读
---

> 使用人类反馈训练语言模型遵循指令.

- paper：[Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)

# 0. TL; DR

本文介绍了如何通过人类反馈微调语言模型，使其更好地遵循用户意图。研究者们通过收集人类标注的数据，使用监督学习和强化学习相结合的方法，训练了**InstructGPT**模型。实验表明，**InstructGPT**在多个任务上的表现优于**GPT-3**，尤其是在减少有害输出（如毒性内容）和提高真实性方面。尽管**InstructGPT**仍可能犯简单错误，但该研究表明，通过人类反馈微调是使语言模型更符合人类意图的有前景的方向。

# 1. 背景介绍
大型语言模型（**LMs**）在自然语言处理（**NLP**）任务中表现出色，但它们在生成输出时可能会产生不真实、有毒或对用户无帮助的内容。这些模型的训练目标（即预测网页上的下一个单词）与“帮助用户完成任务”的目标不同，导致模型行为与用户意图不一致。这种不一致性被称为“目标错位”。为了解决这一问题，研究者们提出了一种通过人类反馈微调语言模型的方法，以使模型的行为更符合用户意图。

具体来说，研究者们通过以下步骤实现这一目标：
1. **收集标注数据**：从**OpenAI API**的用户提交的提示中收集人类标注的演示数据，这些数据展示了期望的模型行为。
2. **监督学习微调**：使用这些标注数据对**GPT-3**进行监督学习微调，训练模型生成符合标注行为的输出。
3. **强化学习微调**：进一步收集模型输出的排名数据，并使用这些数据训练奖励模型（**RM**）。然后，通过强化学习（**PPO**算法）进一步微调模型，以最大化奖励模型的输出。

这种结合监督学习和强化学习的方法被称为“从人类反馈中进行强化学习”（**RLHF**），它利用人类偏好作为奖励信号来微调模型。通过这种方法，研究者们训练了**InstructGPT**模型，该模型在多个任务上表现出色，尤其是在减少有害输出和提高真实性方面。

![](https://pic1.imgdb.cn/item/67ff655688c538a9b5d3442e.png)

# 2. 方法介绍

研究者们从**OpenAI API**收集提示制作了一个提示数据集。这些提示涵盖了多种自然语言任务，如生成、问答、对话、总结、提取等。并对这些提示进行了数据标注。

![](https://pic1.imgdb.cn/item/67ff632988c538a9b5d33bc8.png)
![](https://pic1.imgdb.cn/item/67ff633a88c538a9b5d33c0d.png)

基于提示数据集，研究者们设计了以下三个步骤来训练**InstructGPT**模型：

![](https://pic1.imgdb.cn/item/67ff61d088c538a9b5d33696.png)

### 2.1 监督学习微调（SFT）
研究者们使用标注者的演示数据对GPT-3进行监督学习微调。具体步骤如下：
- **数据准备**：将标注者的演示数据作为输入，模型的输出目标是标注者的期望输出。
- **训练过程**：使用Adam优化器，学习率从初始值逐渐衰减到10%。训练16个epoch，选择在验证集上奖励模型分数最高的模型作为最终模型。

### 2.2 奖励模型训练（RM）
奖励模型的目的是预测人类标注者更倾向于哪个输出。训练过程如下：
- **数据准备**：对于每个输入提示，标注者对模型的多个输出进行排名，生成偏好数据。
- **模型结构**：使用GPT-3架构，去掉最后的解码层，添加一个输出标量值的投影层。
- **训练过程**：使用交叉熵损失函数，训练模型预测标注者更倾向于的输出。训练一个epoch，使用余弦学习率调度。

### 2.3 强化学习微调（PPO）
强化学习微调的目的是进一步优化模型，使其输出更符合人类标注者的偏好。具体步骤如下：
- **初始化**：从监督学习微调后的模型初始化PPO模型。
- **训练过程**：使用PPO算法，以奖励模型的输出作为奖励信号，优化模型的输出。训练256k个episode，使用Adam优化器，学习率逐渐衰减。

### 2.4 模型变体
研究者们还尝试了以下几种模型变体：
- **PPO-ptx**：在**PPO**微调过程中，混合预训练数据的更新，以减少在公共**NLP**数据集上的性能退化。
- **不同模型规模**：训练了**1.3B**、**6B**和**175B**参数的模型，以研究模型规模对性能的影响。


# 3. 实验分析

研究者们在多个数据集上评估了**InstructGPT**模型的性能，包括**API**提示分布和公共**NLP**数据集。主要评估指标包括：
- **人类偏好评分**：标注者对模型输出的偏好评分。
- **真实性评估**：使用**TruthfulQA**数据集评估模型生成的真实性和信息量。
- **毒性评估**：使用**RealToxicityPrompts**数据集评估模型生成的毒性内容。

在人类偏好评分方面，**InstructGPT**模型在人类偏好评分上显著优于**GPT-3**。

![](https://pic1.imgdb.cn/item/67ff63fd88c538a9b5d33ef6.png)

在真实性评估方面，**InstructGPT**在**TruthfulQA**数据集上的表现优于**GPT-3**，生成真实且信息量大的答案的频率是**GPT-3**的两倍。

![](https://pic1.imgdb.cn/item/67ff645e88c538a9b5d34083.png)

在毒性评估方面，**InstructGPT**在生成毒性内容方面有所改善，但在某些情况下仍可能生成有害内容。

![](https://pic1.imgdb.cn/item/67ff64b588c538a9b5d341d0.png)