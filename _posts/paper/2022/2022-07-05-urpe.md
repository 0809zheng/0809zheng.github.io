---
layout: post
title: 'Your Transformer May Not be as Powerful as You Expect'
date: 2022-07-05
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/62c64ddf5be16ec74a1f6eb5.jpg'
tags: 论文阅读
---

> 使用通用相对位置编码改进Transformer的通用近似性.

- paper：[Your Transformer May Not be as Powerful as You Expect](https://arxiv.org/abs/2205.13401)

# 1. 相对位置编码的理论缺陷

位置编码是为**Transformer**的输入序列引入位置信息的重要手段。其中绝对位置编码是在输入中添加入绝对位置的表示。对应的完整自注意力机制运算如下

$$ \begin{aligned} q_i &= (x_i+p_i) W^Q , k_j = (x_j+p_j) W^K ,v_j = (x_j+p_j) W^V  \\ \alpha_{ij} &= \text{softmax}\{(x_i+p_i)W^Q ( (x_j+p_j)W^K)^T \} \\ &=  \text{softmax}\{ x_iW^Q (W^K)^T x_j^T+x_iW^Q (W^K)^T p_j^T+p_iW^Q (W^K)^T x_j^T+p_iW^Q (W^K)^T p_j^T \} \\ z_i &= \sum_{j=1}^{n} \alpha_{ij}(x_jW^V+p_jW^V)  \end{aligned} $$

注意到绝对位置编码相当于在自注意力运算中引入了一系列$p_iW^Q,(p_jW^K)^T,p_jW^V$项。而相对位置编码通过将这些项调整为与相对位置$(i,j)$有关的向量$R_{i,j}$，在一系列自然语言任务上取得更好的表现。

然而相对位置编码存在理论缺陷，会使得**Transformer**无法成为通用近似器。不妨构造一个简单的探针实验，以判断一个模型有没有足够的位置识别能力。对于一个有位置识别能力的模型，应该能准确实现如下映射：

$$ \text{输入：} [0,0,\cdots, 0,0] \to \text{输出：} [1,2,\cdots, n-1,n] $$

即输入全$0$序列，模型能有序地输出位置编号。绝对位置编码比较容易实现上述需求，而现有的相对位置编码却几乎都不能完成上述任务。

大部分相对位置编码都只修改了**Softmax**前的**Attention**矩阵，此时带有相对位置信息的**Attention**矩阵依然是一个按行的概率矩阵，位置信息由相对位置矩阵$B$引入：

$$ \begin{aligned} \alpha_{ij} &= \text{softmax}\{x_iW^Q (W^K)^T x_j^T+B \}  \\ z_i &= \sum_{j=1}^{n} \alpha_{ij}x_jW^V  \end{aligned} $$

对于相同的输入，每个$x_j$都是相同的，此时有：

$$ z_i = \sum_{j=1}^{n} \alpha_{ij}x_jW^V = (\sum_{j=1}^{n} \alpha_{ij})xW^V = xW^V $$

此时模型的每个位置都输出相同的结果，无法输出不同的位置编号。上述结果表明相对位置编码会导致**Transformer**出现拟合能力缺陷问题。

# 2. URPE

为了保证使用相对位置编码的**Transformer**仍然是通用近似器，作者设计了通用相对位置编码(**Universal RPE**)。注意到上述问题出现在**Attention**矩阵是一个按行的概率矩阵，因此引入如下约束：

$$ z_i = \sum_{j=1}^{n} \alpha_{ij}c_{ij}x_jW^V $$

其中$C=[c_{ij}]$是一个可训练的参数矩阵，它与**Attention**矩阵逐位相乘。为使模型包含相对位置信息，约束$C$为**Toeplitz**矩阵: $c_{ij}=g(i−j)$。

实验表明，上述**URPE**在预测位置编号和预测偶数位置等特殊任务上都能取得显著的表现。

![](https://pic.imgdb.cn/item/62c64b41493c5f999cf415ac.jpg)