---
layout: post
title: 'On the Role of Bidirectionality in Language Model Pre-Training'
date: 2022-07-12
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67f7389188c538a9b5c8347c.png'
tags: 论文阅读
---

> 探讨语言模型预训练中的双向性.

- paper：[On the Role of Bidirectionality in Language Model Pre-Training](https://arxiv.org/abs/2205.11726)

# 0. TL; DR

本文深入探讨了语言模型预训练中双向性的作用，提出了一个统一框架来研究不同双向性配置在多种任务中的表现。研究发现，双向性在文本填充和微调任务中非常有益，但在下一个词预测和零样本启动任务中效果不佳。此外，模型规模的扩大并未改变这些趋势，表明双向性的作用与应用场景密切相关。这些发现为未来语言模型的设计和优化提供了重要参考。

# 1. 背景介绍

自然语言处理（**NLP**）领域近年来经历了由预训练模型驱动的范式转变。像**GPT**和**BERT**这样的预训练模型在无监督学习中表现出色，并且可以通过微调或少样本启动来适应下游任务。然而，这些模型在架构和学习目标上存在差异，使得它们之间的比较变得困难。本文聚焦于双向性这一关键因素，研究其在不同任务中的作用，并提出了一个新框架来统一不同的预训练方法。

# 2. 方法介绍

本文提出的框架能够同时支持单向和双向注意力机制，并且可以灵活地调整双向性在上下文和注意力中的权重。具体来说，框架通过以下参数来控制双向性：
- $n_{bidir}$：控制使用双向注意力的前缀长度。对于前$n_{bidir}$个标记使用双向注意力，其余标记使用单向注意力。
- $n_{mask}$：控制随机掩码的标记数量。从原始文档中随机选择$n_{mask}$个标记并替换为<mask>，然后将这些掩码标记及其位置嵌入移动到序列末尾。
- $n_{predict}$：控制定义监督信号的后缀长度。定义损失函数时，只对最后$n_{predict}$个标记进行预测，其中最后$n_{mask}$个标记预测掩码标记，其余预测下一个标记。

![](https://pic1.imgdb.cn/item/67f7625e88c538a9b5c84ddd.png)

基于上述参数，本文探索了六种不同的模型变体，涵盖了从完全单向（如**GPT**）到完全双向（如**BERT**）的各种情况，还包括混合模型（如**CM3**和前缀语言模型）。这些变体在训练过程中通过不同的参数配置来实现，从而研究双向性在不同任务中的作用。

![](https://pic1.imgdb.cn/item/67f7638a88c538a9b5c84e99.png)

# 3. 实验分析

实验涵盖了多种模型规模（从**125M**到**6.7B**参数）和四种不同的任务设置：语言建模、文本填充、零样本启动和微调。所有模型均在相同的语料库上进行训练，并使用相同的**BPE**编码。
- 在语言建模任务中，完全单向模型（如**NXTUNI**）表现最佳，而混合模型（如**HYBUNI**）和前缀语言模型（如**NXTPRE**）稍逊一筹。这表明单向注意力在下一个词预测任务中具有优势。分析：混合模型在训练时引入了掩码和双向注意力，这可能会干扰下一个词预测的训练目标。尽管模型规模扩大，这种差距并未缩小，说明不同训练目标之间存在根本的冲突。
- 在文本填充任务中，完全双向模型（如**MSKBI**）表现最佳，且双向注意力的权重（**bidir**）对性能有显著影响。此外，混合模型（如**HYBPRE**）在大规模时接近纯掩码模型（如**MSKBI**）的性能。分析：双向注意力允许模型同时考虑上下文信息，这对文本填充任务非常有益。尽管混合模型在小规模时受到干扰，但随着规模扩大，这种干扰逐渐减弱，表明模型规模对双向性的作用有重要影响。
- 在零样本启动任务中，单向模型（如**NXTUNI**）表现最佳，而混合模型（如**HYBUNI**）和前缀语言模型（如**NXTPRE**）稍逊一筹。这表明单向注意力在零样本启动任务中具有优势。分析：单向模型能够直接生成完整的序列，而双向模型需要额外的推理步骤来处理掩码标记。这使得单向模型在零样本启动任务中更具优势。
- 在微调任务中，双向模型（如**MSKBI**）表现最佳，而单向模型（如**NXTUNI**）表现较差。这表明双向注意力在微调任务中非常有益。分析：双向注意力允许模型在微调时充分利用上下文信息，从而提高任务性能。

![](https://pic1.imgdb.cn/item/67f764c688c538a9b5c84f5d.png)
