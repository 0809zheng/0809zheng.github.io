---
layout: post
title: 'The relativistic discriminator: a key element missing from standard GAN'
date: 2022-02-21
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/634cad5116f2c2beb15b87e1.jpg'
tags: 论文阅读
---

> RGAN：GAN中的相对判别器.

- paper：[The relativistic discriminator: a key element missing from standard GAN](https://arxiv.org/abs/1807.00734)

**GAN**的目标函数为：

$$ \begin{aligned} \mathop{ \min}_{G} \mathop{\max}_{D}  \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{x \text{~} P_{G}(x)}[\log(1-D(x))] \end{aligned} $$

本文作者提出了**Relativistic GAN (RGAN)**，其目标函数为：

$$ \begin{aligned} \mathop{ \min}_{D}  -\Bbb{E}_{x_r \text{~} P_{data}(x), x_f \text{~} P_{G}(x)}[\log \sigma(D(x_r)-D(x_f))] \\ \mathop{ \min}_{G}- \Bbb{E}_{x_r \text{~} P_{data}(x), x_f \text{~} P_{G}(x)}[\log \sigma(D(x_f)-D(x_r))] \end{aligned} $$

下面先求判别器的最优解。判别器的目标函数为：

$$ L(D) = -\iint_{x_r,x_f} P_{data}(x_r) P_{G}(x_f) \log \sigma(D(x_r)-D(x_f)) dx_r dx_f $$

下面求上式的极值。先求$$\frac{\partial L(D(x))}{\partial D}$$：

$$ \begin{aligned} & \nabla_{D} P_{data}(x_r) P_{G}(x_f) \log \sigma(D(x_r)-D(x_f)) \\ = & P_{data}(x_r) P_{G}(x_f) \frac{ \nabla_{D} \sigma  (D(x_r)-D(x_f))}{\sigma(D(x_r)-D(x_f))}   \\ & (\text{according to} \quad \nabla_x \sigma(x) = \sigma(x)\sigma(-x)) \\  =& P_{data}(x_r) P_{G}(x_f) \sigma(D(x_f)-D(x_r)) (\nabla_{D}D(x_r)-\nabla_{D}D(x_f))   \\  =& P_{data}(x_r) P_{G}(x_f) \sigma(D(x_f)-D(x_r)) \nabla_{D}D(x_r) \\ & -P_{data}(x_r) P_{G}(x_f) \sigma(D(x_f)-D(x_r)) \nabla_{D}D(x_f) \\ & (\text{exchange } x_f \text{ and } x_r \text{ in 2nd formula} )  \\  =& P_{data}(x_r) P_{G}(x_f) \sigma(D(x_f)-D(x_r)) \nabla_{D}D(x_r)  \\ & -P_{data}(x_f) P_{G}(x_r) \sigma(D(x_r)-D(x_f)) \nabla_{D}D(x_r)  \end{aligned} $$

极值在$$\frac{\partial L(D(x))}{\partial D}=0$$处求得，此时有：

$$ P_{data}(x_r) P_{G}(x_f) \sigma(D(x_f)-D(x_r))  -P_{data}(x_f) P_{G}(x_r) \sigma(D(x_r)-D(x_f)) =0  $$

整理得：

$$ \frac{P_{data}(x_r) P_{G}(x_f)}{P_{data}(x_f) P_{G}(x_r)} = \frac{\sigma(D(x_r)-D(x_f))}{\sigma(D(x_f)-D(x_r))} = e^{\sigma(D(x_r)-D(x_f))} $$

代入生成器的目标函数：

$$ \begin{aligned} &- \Bbb{E}_{x_r \text{~} P_{data}(x), x_f \text{~} P_{G}(x)}[\log \sigma(D(x_f)-D(x_r))] \\ &=-\iint_{x_r,x_f} P_{data}(x_r) P_{G}(x_f) \log \sigma(D(x_f)-D(x_r)) dx_r dx_f \\ &= -\iint_{x_r,x_f} P_{data}(x_r) P_{G}(x_f) \log  \log \frac{P_{data}(x_f) P_{G}(x_r)}{P_{data}(x_r) P_{G}(x_f)} dx_r dx_f \end{aligned} $$

上式表示优化目标为$$P_{data}(x_f) P_{G}(x_r)$$和$$P_{data}(x_r) P_{G}(x_f)$$之间的[f散度](https://0809zheng.github.io/2020/02/03/kld.html#-f%E6%95%A3%E5%BA%A6-f-divergence)，且$f(x) = \log \log(x)$。

此时**RGAN**的判别器不是一个二分类器，而是一个相对判别器。对于真实样本$x_r$和伪造样本$x_f$，判别器评估把它们两个交换后$$P_{data}(x_f) P_{G}(x_r)$$的变化程度。假如变化程度较小，说明真实样本$x_r$和伪造样本$x_f$的相似程度较高，判别器无法区分它们。

下面给出由**pytorch**实现的**RGAN**的损失函数计算和参数更新过程：

```python
for epoch in range(opt.n_epochs):
    for i, real_imgs in enumerate(dataloader):

        z = torch.randn(real_imgs.shape[0], opt.latent_dim) 
        gen_imgs = generator(z)

        # 训练判别器
        optimizer_D.zero_grad()
        d_loss = -torch.mean(torch.log(torch.sigmoid(
            discriminator(real_imgs)-discriminator(gen_imgs.detach()))))
        d_loss.backward()
        optimizer_D.step()
            
        # 训练生成器
        optimizer_G.zero_grad()
        g_loss = -torch.mean(torch.log(torch.sigmoid(
            discriminator(gen_imgs)-discriminator(real_imgs))))
        g_loss.backward()
        optimizer_G.step()
```


实验表明**RGAN**能够加快生成器的训练速度：

![](https://pic1.imgdb.cn/item/634cc07e16f2c2beb173f6d7.jpg)
