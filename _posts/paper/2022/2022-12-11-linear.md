---
layout: post
title: 'What learning algorithm is in-context learning? Investigations with linear models'
date: 2022-12-11
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67fe178b88c538a9b5d1957f.png'
tags: 论文阅读
---

> 上下文学习算法是什么？使用线性模型的解释.

- paper：[What learning algorithm is in-context learning? Investigations with linear models](https://arxiv.org/abs/2211.15661)

# 0. TL; DR

本文探讨了基于**Transformer**的大型神经序列模型在上下文学习（**ICL**）中的表现，特别是这些模型是否隐式地实现了标准的学习算法。研究通过线性回归问题作为原型，提供了三方面的证据，表明**Transformer**能够实现基于梯度下降和闭式岭回归的学习算法。实验结果表明，训练有素的上下文学习者与通过梯度下降、岭回归和精确最小二乘回归计算的预测器非常接近，并且随着**Transformer**深度和数据集噪声的变化，它们在不同的预测器之间转换，当宽度和深度较大时收敛到贝叶斯估计器。此外，初步证据表明，上下文学习者与这些预测器共享算法特征，即学习者的后期层非线性地编码权重向量和时刻矩阵。

# 1. 背景介绍

上下文学习（**ICL**）是近年来大型神经序列模型（如**Transformer**）表现出的一种能力，即模型能够从输入中提供的标记示例序列构建新的预测器，而无需进一步更新模型参数。这种学习方式在少样本学习问题和大型语言模型中都有所体现。尽管**ICL**在多种任务上表现优异，但其具体的学习机制尚不清楚。

本文旨在探讨**Transformer**是否隐式地实现了标准的学习算法，即模型是否通过其激活中编码的较小模型来实现**ICL**，并在处理上下文示例时更新这些隐式模型。研究聚焦于线性回归问题，这是一个在机器学习和统计估计中被广泛研究的简单问题，为理解**ICL**提供了理想的测试平台。

线性回归问题是机器学习中的一个经典问题，目标是找到一个线性函数 $f(x) = w^\top x$，使得预测值 $f(x)$ 与真实值 $y$ 之间的平方误差最小化。具体来说，给定输入 $X$ 和输出 $Y$，最小化的目标函数为：

$$
\text{minimize} \quad \sum_i (w^\top x_i - y_i)^2 + \lambda \|w\|^2_2
$$

其中，$\lambda$ 是正则化参数。当 $\lambda = 0$ 时，这是普通最小二乘回归（**OLS**）；当 $\lambda > 0$ 时，这是岭回归（**Ridge Regression**）。

# 2. 方法介绍


为了训练**Transformer**模型进行**ICL**，作者定义一个函数类 $F$，一个分布 $p(f)$ 支持在 $F$ 上，一个输入域上的分布 $p(x)$，以及一个损失函数 $L$。目标是最小化自回归目标：

$$
\text{minimize}_\theta \mathbb{E}_{x_1, \ldots, x_n \sim p(x), f \sim p(f)} \left[ \sum_{i=1}^n L(f(x_i), T_\theta([x_1, f(x_1), \ldots, x_i])) \right]
$$

实验比较了**Transformer**与多种标准学习算法（如普通最小二乘回归、岭回归、随机梯度下降等）的预测性能。结果显示，**Transformer**的预测与普通最小二乘回归非常接近，而与其他预测器的差异较大。

![](https://pic1.imgdb.cn/item/67fe1a9e88c538a9b5d198c0.png)


实验进一步探讨了**Transformer**在噪声数据集上的表现。结果显示，**Transformer**的行为与贝叶斯最小风险预测器非常接近，尤其是在数据噪声和先验方差变化时。

![](https://pic1.imgdb.cn/item/67fe1b3888c538a9b5d1999f.png)

实验还探讨了模型深度和隐藏大小对**Transformer**表现的影响。结果显示，模型深度和隐藏大小的变化会导致**Transformer**在不同的学习算法之间转换，表现出算法的“相变”。

![](https://pic1.imgdb.cn/item/67fe1b9188c538a9b5d19a1a.png)

实验还探讨了**Transformer**是否能够编码标准学习算法中计算的中间量。通过训练辅助探针模型，结果显示**Transformer**能够非线性地编码权重向量和时刻矩阵。

![](https://pic1.imgdb.cn/item/67fe1c4188c538a9b5d19aa4.png)