---
layout: post
title: 'Minimax Active Learning'
date: 2022-08-06
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/631a8bf116f2c2beb113c2dc.jpg'
tags: 论文阅读
---

> MAL：最小最大主动学习.

- paper：[Minimax Active Learning](https://arxiv.org/abs/2012.10467)

**最小最大主动学习(minimax active learning, MAL)**框架包括一个熵最小特征编码网络$F$和一个熵最大分类器$C$；该框架旨在减小标注数据和未标注数据之间的分布差异。

![](https://pic.imgdb.cn/item/631a8c5816f2c2beb114235b.jpg)

特征编码网络$F$把样本编码到经过**l2**标准化的$d$维特征空间，假设共有$K$个分类类别，则分类器$C$的权重参数为$W \in \Bbb{R}^{d \times K}$。

特征编码网络$F$和分类器$C$首先在已标注数据集上进行训练，目标函数为分类交叉熵损失：

$$ \mathcal{L}_{CE} = -\Bbb{E}_{(x^l,y)\text{~}\mathcal{X}} \sum_{k=1}^K \Bbb{I}[k=y]\log (\sigma (\frac{1}{T}\frac{W^TF(x^l)}{||F(x^l)||})) $$

在训练未标注数据时，**MAL**采用**minimax**目标函数优化预测概率的熵：

$$ \begin{aligned} \mathcal{L}_{Ent} &= - \sum_{k=1}^K p(y=k|u) \log p(y=k|u) \\ \theta_F^*,\theta_C^* &= \mathop{\min}_F \mathop{\max}_C \mathcal{L}_{Ent} \\ \theta_F &\leftarrow \theta_F - \alpha_1\nabla \mathcal{L}_{Ent} \\ \theta_C &\leftarrow \theta_C + \alpha_2\nabla \mathcal{L}_{Ent} \end{aligned} $$

其中最小化特征编码网络$F$的熵使得具有相似预测标签的样本具有相似的特征；最大化分类器$C$的熵使得预测结果为更均匀的类别分布。

而判别器试图有效地区分标记样本和未标记样本的特征：

$$ \mathcal{L}_{\text{D}} = -\mathbb{E}[\log (D(q_{\phi}(z_L|x_L)))]-\mathbb{E}[\log (1-D(q_{\phi}(z_U|x_U)))] $$

**MAL**的采样策略同时考虑多样性和不确定性：
- 多样性采样：判别器$D$的得分显示出未标注样本与训练中使用的已标注样本之间的相似程度，选择得分趋近于$0$的样本。
- 不确定性采样：分类器$C$的熵可用作样本的不确定性得分，更高的熵得分表示模型预测结果的置信度较低。

作者通过方法对比和消融实验验证了**MAL**的有效性：

![](https://pic.imgdb.cn/item/631a99b916f2c2beb12047e8.jpg)