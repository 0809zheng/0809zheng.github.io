---
layout: post
title: 'DETR：End-to-End Object Detection with Transformers'
date: 2020-06-20
author: 郑之杰
cover: 'https://pic.downk.cc/item/5eedb67d14195aa594885979.jpg'
tags: 论文阅读
---

> 使用Transformer进行目标检测.

- paper：[arXiv](https://arxiv.org/abs/2005.12872)
- code：[github](https://github.com/facebookresearch/detr)
- 教程：[link](https://www.bilibili.com/video/BV1Qg4y1B7rL)

# 1. 模型框架

![](https://pic.downk.cc/item/5eedb73f14195aa5948970e4.jpg)

DETR主要有三部分：
- 一个卷积神经网络backbone，抽取图像特征;
- 一个编码器-解码器结构的Transformer；
- 一个简单的feed forward network (FFN)，进行最终的检测预测。

# 2. 模型细节

![](https://pic.downk.cc/item/5eedb8e214195aa5948bbb07.jpg)

- **backbone卷积网络**:将图像的长宽减小，通道数增加，得到特征图像尺寸为$(C,H,W)$，通常取$C=2048,H=\frac{H_0}{32},W=\frac{W_0}{32}$；
- **Transformer编码器**：首先使用$1×1$卷积降低通道数为$d$；将特征调整为$(d,H×W)$，将其看作$H×W$个长度为$d$的向量，加入位置编码后喂入Transformer的编码器；
- **Transformer解码器**：Transformer的解码器接收$N$个查询向量（对应在图像中检测$N$个目标），结合编码器的输出得到解码器的$N$个输出；
- **前馈神经网络**：（共享参数的）使用ReLU激活函数的3层感知机加一个线性映射层。$N$个预测结果表示为目标置信度和坐标框$(cls,bbox)$

通常模型的预测结果数量$N$远大于图像中实际存在的目标数，为此对图像的标签增加**no object**的边界框直至达到$N$个。

损失函数使用**二分匹配损失（bipartite matching loss）**，即将$N$个预测边界框和补充后的$N$个实际标签框进行一一匹配（由**匈牙利算法**实现匹配），根据匹配后的两个框计算损失。
