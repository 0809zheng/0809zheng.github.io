---
layout: post
title: 'DETR：End-to-End Object Detection with Transformers'
date: 2020-06-20
author: 郑之杰
cover: 'https://pic.downk.cc/item/5eedb67d14195aa594885979.jpg'
tags: 论文阅读
---

> 使用Transformer进行目标检测.

- paper：DETR：End-to-End Object Detection with Transformers
- link：[arXiv](https://arxiv.org/abs/2005.12872)
- code：[github](https://github.com/facebookresearch/detr)


# 1. 模型框架

![](https://pic.downk.cc/item/5eedb73f14195aa5948970e4.jpg)

DETR主要有三部分：
- 一个卷积神经网络backbone，抽取图像特征;
- 一个编码器-解码器结构的Transformer；
- 一个简单的feed forward network (FFN)，进行最终的检测预测。

# 2. 模型细节

![](https://pic.downk.cc/item/5eedb8e214195aa5948bbb07.jpg)

- **backbone卷积网络**:通过卷积神经网络提取特征，将图像的长宽减小，通道数增加，得到特征图像尺寸为$(C,H,W)$，通常取$C=2048,H=\frac{H_0}{32},W=\frac{W_0}{32}$；
- **Transformer编码器**：首先使用$1×1$卷积降低通道数为$d$；将特征调整为$(d,H×W)$，将其看作$H×W$个长度为$d$的向量，加入固定的位置编码后喂入Transformer的编码器；
- **Transformer解码器**：Transformer的解码器接收$N$个查询向量（对应在图像中检测$N$个目标，通常远大于实际目标数），结合编码器的输出得到解码器的$N$个输出；
- **前馈神经网络**：使用ReLU激活函数的3层感知机（共享参数）加一个线性映射层。$N$个预测结果表示为目标置信度和坐标框$(cls,bbox)$

该方法可以看作$N$个边界框的预测，从而避免了引入**anchor**设计与非极大值抑制。

通常模型的预测结果数量$N$远大于图像中实际存在的目标数，为此对图像的标签增加**no object**的边界框直至达到$N$个。

损失函数使用**二分匹配损失（bipartite matching loss）**，即将$N$个预测边界框和补充后的$N$个实际标签框进行一一匹配（由**匈牙利算法**实现匹配），根据匹配后的两个框计算损失：

$$ \mathcal{L}_{Hungarian}(y,\hat{y}) = \sum_{i=1}^{N} [-log \hat{p}_{\hat{\sigma}(i)}(c_i)+1_{ \{ c_i ≠ \Phi \} } \mathcal{L}_{box}(b_i, \hat{b}_{\hat{\sigma}}(i)) ] $$

其中$\hat{\sigma}$表示可选的标签对齐。$c_i$和$\hat{p}_{\hat{\sigma}(i)}(c_i)$分别表示目标类别标签和预测标签，$b_i$和$\hat{b}_{\hat{\sigma}}(i)$分别表示实际边界框参数和预测边界框。

**DETR**方法相比**Faster R-CNN**在**COCO**数据集上取得**comparable**的准确率，但仍存在一些问题。主要体现在更长的训练时间和对小目标较差的表现。
