---
layout: post
title: 'Attention as Activation'
date: 2020-10-29
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63b8ccd3be43e0d30e440589.jpg'
tags: 论文阅读
---

> 使用注意力机制作为激活函数.

- paper：[Attention as Activation](https://arxiv.org/abs/2007.07729)

激活函数和注意力机制都可以表示为非线性门控函数，本文作者受两者相似性的启发，提出了一种新型的激活单元，称**为注意力激活（Attentional Activation, ATAC）**单元，作为激活函数和注意力机制的统一。

**ATAC**是一种同时用于非线性激活和逐元素特征细化的局部通道注意力模块，该模块局部地聚合了逐点跨通道特征上下文信息。

![](https://pic.imgdb.cn/item/63b8cdf2be43e0d30e457c3b.jpg)

通过在卷积网络中用**ATAC**单元替换**ReLU**激活函数，可以构建完全注意力的网络，并在不增加参数量的情况下表现更好。

![](https://pic.imgdb.cn/item/63b8ce45be43e0d30e45f27a.jpg)


```python
import torch.nn as nn

class ATAC(nn.Module):
    def __init__(self, channel, reduction=16):
        super(ATAC, self).__init__()
        self.path = nn.Sequential(
            nn.Conv2d(channel, channel//reduction, 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(channel//reduction,channel, 1, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        return x * self.path(x)
```