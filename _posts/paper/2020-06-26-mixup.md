---
layout: post
title: 'mixup: Beyond Empirical Risk Minimization'
date: 2020-06-26
author: 郑之杰
cover: 'https://pic.downk.cc/item/5f09ac1a14195aa594471088.jpg'
tags: 论文阅读
---

> mixup：一种数据增强的方法.

- paper：mixup: Beyond Empirical Risk Minimization
- arXiv：[link](https://arxiv.org/abs/1710.09412)

由于样本数据集的分布和总体分布有一定的差距，对于样本中的数据点，已知对应的标签；对于不在训练样本中的数据点，标签往往是未知的。

作者提出了**mixup**，一种构造新的数据方法，利用两个样本点的凸组合构造新的样本：

$$ \hat{x} = λx_i + (1-λ)x_j $$

$$ \hat{y} = λy_i + (1-λ)y_j $$

对于下图所示的样本集，绿色表示类别0，橙色表示类别1。通常的经验风险最小化（ERM）实现的是硬分类，蓝色区域代表模型分类为1的区域。**mixup**实现了软分类，对于中间的区域，并不是硬性的分类为0或1，而是给出了一种不确定性：

![](https://pic.downk.cc/item/5f09bd5d14195aa5944cae85.jpg)

使用Pytorch实现**mixup**：

```
# y1, y2 should be one-hot vectors
for (x1, y1), (x2, y2) in zip(loader1, loader2):
    lam = numpy.random.beta(alpha, alpha)
    x = Variable(lam * x1 + (1. - lam) * x2)
    y = Variable(lam * y1 + (1. - lam) * y2)
    optimizer.zero_grad()
    loss(net(x), y).backward()
    optimizer.step()
```