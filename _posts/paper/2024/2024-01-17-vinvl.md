---
layout: post
title: 'VinVL: Revisiting Visual Representations in Vision-Language Models'
date: 2024-01-17
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67a1be91d0e0a243d4fbc6be.png'
tags: 论文阅读
---

> VinVL：重新回归视觉语言模型中的视觉表示.

- paper：[VinVL: Revisiting Visual Representations in Vision-Language Models](https://arxiv.org/abs/2101.00529)

## 0. TL; DR

本文提出了一种名为**VinVL**的视觉语言（**VL**）预训练模型，通过改进目标检测模型来提供更丰富的图像视觉表示。与广泛使用的自底向上和自顶向下的模型相比，**VinVL**更大、更适合**VL**任务，并且在更大的训练数据集上进行了预训练。实验结果表明，**VinVL**在多个**VL**任务上取得了新的最佳性能，包括视觉问答（**VQA**）、图像描述生成和视觉推理等任务。

## 1. 背景介绍

视觉语言预训练（**VLP**）在多种视觉语言（**VL**）任务中已被证明是有效的，如视觉问答（**VQA**）、图像描述生成和视觉推理等。**VLP**通常分为两个阶段：首先，使用目标检测模型对图像及其视觉目标进行编码；其次，使用跨模态融合模型将文本和视觉特征进行融合。尽管现有的**VLP**研究主要集中在改进跨模态融合模型上，但本文关注于改进目标检测模型，以提供更丰富的视觉特征。

## 2. VinVL 模型

**VinVL**的核心是改进目标检测模型，以提供更丰富的视觉目标和概念的表示。**VinVL**基于**ResNeXt-152 C4**架构，使用四个公共目标检测数据集（**COCO、OpenImages、Objects365**和**Visual Genome**）进行预训练，这些数据集在目标类别、数据规模和注释数量上具有互补性。这些数据集提供了更丰富的视觉目标和属性类别，使得**VinVL**能够生成更丰富的视觉特征。

**VinVL**选择**C4**架构而不是**FPN**架构，因为**C4**在**VL**任务中表现更好。**C4**架构的所有层都使用**ImageNet**预训练权重，而**FPN**的**MLP**头没有。此外，**C4**的卷积头在编码视觉信息方面比**FPN**的**MLP**头具有更好的归纳偏差。

**VinVL**使用改进的[<font color=blue>Oscar</font>](https://0809zheng.github.io/2024/01/06/oscar.html)模型进行预训练，以学习图像和文本的联合表示。**OSCAR+**在预训练阶段使用掩码标记损失（**Masked Token Loss**）和三元组对比损失（**3-way Contrastive Loss**）。
- 掩码标记损失与**BERT**中的掩码语言模型类似，用于预测被掩码的标记。在**OSCAR+**中，掩码标记损失应用于图像描述和目标标签的标记上。
- 三元组对比损失用于优化**VQA**和文本-图像匹配任务的目标。它通过构造污染的“描述”和“答案”来生成负样本，并使用全连接层预测三元组是否匹配。


## 3. 实验分析

**VinVL**在多个**VL**任务上进行了评估，包括**VQA、GQA、NLVR2、COCO**图像描述生成、**NoCaps、COCO**文本-图像检索和**NLVR2**。这些任务涵盖了**VL**理解任务和**VL**生成任务。

主要实验结果：
- **VQA**：**VinVL**在**VQA v2.0**数据集上取得了**76.12%**的准确率，超越了之前的最佳模型（**73.67%**）。
- **GQA**：**VinVL**在**GQA**数据集上取得了**64.65%**的准确率，首次超越了专门设计的神经状态机（**NSM**）模型（**61.62%**）。
- 图像描述生成：在**COCO**图像描述生成任务上，**VinVL**取得了**140.6**的**CIDEr**分数，超越了之前的最佳模型（**137.6**）。
- **NoCaps**：在**NoCaps**任务上，**VinVL**取得了**92.46**的**CIDEr**分数，超越了人类表现。
- 文本-图像检索：在**COCO**文本-图像检索任务上，**VinVL**在**IR**和**TR**上分别取得了**58.1**和**74.6**的**R@1**分数，超越了之前的最佳模型。
- **NLVR2**：在**NLVR2**任务上，**VinVL**取得了**83.08%**的准确率，超越了之前的最佳模型（**81.47%**）。

![](https://pic1.imgdb.cn/item/67a1c013d0e0a243d4fbc6d1.png)


