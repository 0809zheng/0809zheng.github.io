---
layout: post
title: 'Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation'
date: 2024-04-05
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/667a7124d9c307b7e947e7a6.png'
tags: 论文阅读
---

> 得分恒等蒸馏：单步生成的预训练扩散模型的指数级快速蒸馏.

- paper：[Score identity Distillation: Exponentially Fast Distillation of Pretrained Diffusion Models for One-Step Generation](https://arxiv.org/abs/2404.04057)

本文提出了一种将扩散模型蒸馏为单步生成模型的方案：得分恒等蒸馏（**Score identity Distillation, SiD**），既不依赖于教师模型的真实训练集，也不需要通过教师模型的多步迭代来生成样本对，而是通过恒等变换来交替训练并稳定训练过程。

在目标数据集上训练好的教师扩散模型$\epsilon_\phi(x_t,t)$通常需要多步采样才能生成高质量图片；**SiD**旨在训练一个单步采样的学生模型$x=g_\theta(z)$，输入指定噪声$z$就可以直接生成符合要求的图像。蒸馏的常规步骤是通过教师扩散模型$\epsilon_\phi(x_t,t)$采样大量输入输出对，来监督训练学生模型。然而扩散模型生成训练数据太费时费力。

**SiD**的思路是既然希望学生模型$x=g_\theta(z)$训练后生成的数据分布与目标数据集相似，则通过学生模型$x=g_\theta(z)$采样数据并训练扩散模型$\epsilon_\psi(x_t,t)$，其表现应该与训练好的教师扩散模型$\epsilon_\phi(x_t,t)$也相似。并且学生模型$x=g_\theta(z)$是单步采样模型，因此其数据生成速度比教师模型快得多。

从学生模型$x=g_\theta(z)$采样数据并训练扩散模型$\epsilon_\psi(x_t,t)$：

$$
\begin{aligned}
\psi^* = \mathop{\arg\min}_\psi\mathbb{E}_{t \sim[1, T], z, \epsilon_t \sim \mathcal{N}(0,I)}\left[\left\|\boldsymbol{\epsilon}_t-\boldsymbol{\epsilon}_\psi\left(\sqrt{\bar{\alpha}_t} g_\theta(z)+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}_t, t\right)\right\|^2\right]
\end{aligned}
$$

最小化扩散模型$\epsilon_\psi(x_t,t)$与教师扩散模型$\epsilon_\phi(x_t,t)$的差异：

$$
\begin{aligned}
\theta^* = \mathop{\arg\min}_\theta\mathbb{E}_{t \sim[1, T], z, \epsilon_t \sim \mathcal{N}(0,I)}\left[\left\|\boldsymbol{\epsilon}_\phi\left(\sqrt{\bar{\alpha}_t} g_\theta(z)+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}_t, t\right)-\boldsymbol{\epsilon}_\psi\left(\sqrt{\bar{\alpha}_t} g_\theta(z)+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}_t, t\right)\right\|^2\right]
\end{aligned}
$$

通过交替优化上述两个损失，可以实现**SiD**的训练过程。然而上述训练过程存在问题：理论上要求先求出$\psi$的最优解，然后才去优化$\theta$；然而在实际中几乎不可能总是将$\psi$求到最优。**SiD**给出的解决方案是通过恒等变换减少$\theta$的优化过程中对$\psi$的依赖。

记$$x_t^{(g)}=\sqrt{\bar{\alpha}_t} g_\theta(z)+\sqrt{1-\bar{\alpha}_t} \boldsymbol{\epsilon}_t$$，把$\theta$的目标函数改写为：

$$
\begin{aligned}
& \mathbb{E}_{t \sim[1, T], z, \epsilon_t \sim \mathcal{N}(0,I)}\left[\left\|\boldsymbol{\epsilon}_\phi\left(x_t^{(g)}, t\right)-\boldsymbol{\epsilon}_\psi\left(x_t^{(g)}, t\right)\right\|^2\right]\\
= & \mathbb{E}_{t \sim[1, T], z, \epsilon_t \sim \mathcal{N}(0,I)}\left[\left<\boldsymbol{\epsilon}_\phi\left(x_t^{(g)}, t\right)-\boldsymbol{\epsilon}_\psi\left(x_t^{(g)}, t\right), \boldsymbol{\epsilon}_\phi\left(x_t^{(g)}, t\right)-\boldsymbol{\epsilon}_\psi\left(x_t^{(g)}, t\right)\right>\right]
\end{aligned}
$$

注意到当$\psi$取得最优值$\psi^*$时，近似有：

$$
\mathbb{E}_{t \sim[1, T], z, \epsilon_t \sim \mathcal{N}(0,I)}\left[\left\|\boldsymbol{\epsilon}_t-\boldsymbol{\epsilon}_{\psi^*}\left(x_t^{(g)},t\right)\right\|^2\right] \to 0
$$

此时近似有$$\boldsymbol{\epsilon}_t\approx \boldsymbol{\epsilon}_{\psi^*}\left(x_t^{(g)},t\right)$$，因此调整$\theta$的目标函数，使其对$\psi$的依赖减少，并且引入了$\psi$取得最优值的条件信息：

$$
\begin{aligned}
\theta^* = \mathop{\arg\min}_\theta\mathbb{E}_{t \sim[1, T], z, \epsilon_t \sim \mathcal{N}(0,I)}\left[\left<\boldsymbol{\epsilon}_\phi\left(x_t^{(g)}, t\right)-\boldsymbol{\epsilon}_\psi\left(x_t^{(g)}, t\right), \boldsymbol{\epsilon}_\phi\left(x_t^{(g)}, t\right)-\boldsymbol{\epsilon}_t\right>\right]
\end{aligned}
$$

**SiD**方法的主要缺点是对显存的需求比较大，因为它同时要维护三个模型$\epsilon_\phi(x_t,t)$、$\epsilon_\psi(x_t,t)$和$g_\theta(z)$。

