---
layout: post
title: 'Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks'
date: 2024-01-31
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67b58d43d0e0a243d400bc16.png'
tags: 论文阅读
---

> 图像作为外语：所有视觉和视觉-语言任务的BEiT预训练.

- paper：[Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks](https://arxiv.org/abs/2208.10442)

## 0. TL; DR

本文提出了一个通用的多模态基础模型**BEiT-3**，通过将图像视为一种“外语”（**Imglish**），在图像、文本和图像-文本对上统一进行掩码“语言”建模，实现了在视觉和视觉-语言任务上的最佳迁移性能。**BEiT-3**在多个基准测试中取得了**SOTA**性能，包括目标检测、语义分割、图像分类、视觉推理、视觉问答、图像描述和跨模态检索等任务。


## 1. 背景介绍

近年来，语言、视觉和多模态预训练领域出现了大融合的趋势。通过在大规模数据上进行预训练，可以轻松地将模型迁移到各种下游任务。**BEiT-3**旨在通过以下三个方面推进这种融合趋势：
- 骨干网络架构：采用**Multiway Transformers**作为通用建模架构，支持深度融合和模态特定编码。
- 预训练任务：通过掩码数据建模（**Masked Data Modeling**）统一处理图像（**Imglish**）、文本（**English**）和图像-文本对（“平行句子”）。
- 模型扩展：通过扩展模型规模和数据规模，提升模型的泛化能力。

**BEiT-3**的核心思想是将图像视为一种“外语”，并通过掩码建模的方式学习图像和文本之间的对齐关系。这种方法不仅简化了预训练任务，还使得模型能够高效扩展。

## 2. BEiT-3 模型

**BEiT-3**通过掩码数据建模在单模态（图像、文本）和多模态（图像-文本对）数据上进行预训练，使用共享的**Multiway Transformer**作为骨干网络。

![](https://pic1.imgdb.cn/item/67b58de5d0e0a243d400bc36.png)

**BEiT-3**采用**Multiway Transformers**作为骨干网络，支持不同模态的编码。每个**Multiway Transformer**块包含一个共享的自注意力模块和一个用于不同模态的前馈网络池（模态专家）。输入的每个**token**根据其模态被路由到相应的专家。这种架构不仅支持模态特定的编码，还通过共享的自注意力模块学习不同模态之间的对齐关系。

输入的数据根据其模态的不同选择不同的模态专家，比如图片数据就使用 **Vision FFN**，文本数据就使用 **Language FFN**。模型前面的一些层是只有 **Vision FFN** 和 **Language FFN**，到了最后**3**层会再有 **Vision-Language FFN**，为了特征融合。
- **(a) BEIT-3** 模型转换为视觉编码器，适配任务：图像分类，目标检测，语义分割 (**ImageNet-1K, COCO, ADE20K**)。
- **(b) BEIT-3** 模型转换为文本编码器。
- **(c)** 单塔结构，联合编码图像-文本对以进行深度交互的融合编码器，适配任务：多模态理解任务 (**VQA, NLVR2**)。
- **(d)** 双塔结构，对每个模态进行单独编码以求高效的图文检索，适配任务：图文检索任务 (**Flickr30k, COCO**)。
- **(e)** 单塔结构，用于图像到文本生成任务，适配任务：多模态生成任务 (**COCO**)。

![](https://pic1.imgdb.cn/item/67b592a2d0e0a243d400bf18.png)

**BEiT-3**通过掩码数据建模进行预训练。在预训练过程中，随机掩码部分文本**token**或图像**patch**，并训练模型恢复原始**token**。这种统一的掩码-预测任务不仅学习了表示，还学习了不同模态之间的对齐关系。具体来说：
- 文本数据通过**SentencePiece**分词器进行分词。
- 图像数据通过**BEIT v2**的分词器进行分词，获得离散的视觉**token**作为重建目标。
- 单模态文本掩码比例为**15%**，图像-文本对中的文本掩码比例为**50%**，图像**patch**掩码比例为**40%**。


## 3. 实验分析

**BEiT-3**是一个包含**19**亿参数的模型，基于**ViT-giant**架构。预训练数据包括：
- 多模态数据：来自**CC12M**、**CC3M**、**SBU**、**COCO**和**VG**的**21M**图像-文本对。
- 单模态数据：来自**ImageNet-21K**的**14M**图像和**160GB**的文本数据。

![](https://pic1.imgdb.cn/item/67b58f55d0e0a243d400bc75.png)

![](https://pic1.imgdb.cn/item/67b58f6dd0e0a243d400bc77.png)

**BEiT-3**在多个视觉和视觉-语言基准测试中进行了评估，包括目标检测、语义分割、图像分类、视觉推理、视觉问答、图像描述和跨模态检索等任务。

视觉-语言下游任务的性能：
- 视觉问答（**VQA**）：在**VQAv2**数据集上，**BEIT-3**达到了**84.03%**的准确率，超越了之前的**SOTA**模型。
- 视觉推理（**NLVR2**）：**BEIT-3**达到了**92.58%**的准确率，首次超过**90%**，显著优于之前的模型。
- 图像描述（**COCO**）：**BEIT-3**在**COCO**数据集上达到了**147.6**的**CIDEr**分数，超越了之前的**SOTA**模型。![](https://pic1.imgdb.cn/item/67b59031d0e0a243d400bcc7.png)
- 跨模态检索（**Flickr30K、COCO**）：**BEIT-3**在图像到文本和文本到图像检索任务上均取得了**SOTA**性能，例如在**Flickr30K**上，图像到文本检索的**R@1**指标达到了**98.0%**。![](https://pic1.imgdb.cn/item/67b590a5d0e0a243d400bcf5.png)

视觉下游任务的性能：
- 目标检测和实例分割（**COCO**）：**BEIT-3**在**COCO**数据集上达到了**63.7**的**AP**和**54.8**的**APmask**，超越了之前的**SOTA**模型。![](https://pic1.imgdb.cn/item/67b590fcd0e0a243d400bd17.png)
- 语义分割（**ADE20K**）：**BEIT-3**达到了**62.8**的**mIoU**，超越了**FD-SwinV2**模型。![](https://pic1.imgdb.cn/item/67b5910fd0e0a243d400bd2b.png)
- 图像分类（**ImageNet**）：**BEIT-3**在**ImageNet-1K**上达到了**89.6%**的**Top-1**准确率，超越了**FD-CLIP**模型。![](https://pic1.imgdb.cn/item/67b5911fd0e0a243d400bd36.png)



