---
layout: post
title: 'ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data'
date: 2024-01-09
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67a07aa2d0e0a243d4f9b551.png'
tags: 论文阅读
---

> ImageBERT：使用大规模弱监督图像文本数据进行跨模态预训练.

- paper：[ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data](https://arxiv.org/abs/2001.07966)

## 0. TL; DR

本文介绍了一种新的视觉-语言预训练模型——**ImageBERT**，旨在学习图像和文本的联合嵌入表示。该模型基于**Transformer**架构，通过同时预训练四个任务（掩码语言建模、掩码目标分类、掩码区域特征回归和图像-文本匹配）来学习视觉和语言内容之间的关系。为了进一步提升预训练质量，作者收集了一个大规模弱监督图像-文本数据集（**LAIT**），并采用多阶段预训练策略，在**LAIT**数据集上进行第一阶段预训练，随后在**Conceptual Captions**和**SBU Captions**数据集上进行第二阶段预训练。实验表明，多阶段预训练策略优于单阶段预训练策略。在图像检索和文本检索任务上，**ImageBERT**在**MSCOCO**和**Flickr30k**数据集上均取得了新的最佳性能。

## 1. 背景介绍

近年来，视觉-语言任务在自然语言处理（**NLP**）和计算机视觉（**CV**）领域受到广泛关注。例如，文本-图像检索旨在根据文本检索最相关的图像，或反之；视觉问答（**VQA**）旨在根据图像和相关问题预测正确答案；视觉常识推理（**VCR**）则要求模型不仅能回答常识问题，还能选择支持答案的理由；图像描述生成则旨在为输入图像生成自然语言描述。以往的方法通常基于分别预训练的语言模型（如**BERT**）和视觉模型（如**ResNet**），并通过特定任务的标注数据进行多模态融合。然而，获取足够的任务标注数据既具有挑战性又成本高昂。

受**NLP**领域预训练模型（如**BERT、XLNet**和**RoBERTa**）成功应用的启发，跨模态预训练成为研究热点。这些模型能够在大规模语料库上学习语言和视觉内容的联合表示，并通过特定任务的微调应用于下游任务。本文提出的**ImageBERT**模型作为跨模态预训练的强基线，在文本到图像和图像到文本检索任务上取得了新的最佳结果。此外，作者还构建了一个包含**1000**万图像-文本对的大规模语料库，以推动跨模态预训练研究的发展。

## 2. ImageBERT 模型

### （1）模型结构

**ImageBERT**模型的整体架构如图所示。该模型基于**Transformer**架构，同时输入图像视觉**token**和文本**token**，并通过多层双向自注意力**Transformer**学习视觉区域和语言**token**之间的关系。

![](https://pic1.imgdb.cn/item/67a07ba5d0e0a243d4f9b56c.png)

与**BERT**类似，输入句子通过**WordPiece**方法分割为子词**token**，并添加特殊**token**（如$[CLS]$放在整个token序列的开头，$[SEP]$用于区分文本和图像或者不同的句子）。每个子词**token**的最终嵌入由原始词嵌入、段嵌入和序列位置嵌入组合而成，所有嵌入均初始化自公开预训练的**BERT**模型。

图像嵌入通过**Faster-RCNN**模型从图像中提取的感兴趣区域（**RoI**）特征生成。每个**RoI**的嵌入包括目标特征和位置嵌入。位置嵌入通过将目标位置编码为5维向量实现，包括目标边界框的相对坐标和相对于整个图像的比例面积。最终，目标嵌入、段嵌入、图像位置嵌入和序列位置嵌入相加，经过层归一化后得到每个**RoI**的表示。

序列位置嵌入用于指示输入**token**的顺序。对于视觉**token**，由于检测到的**RoI**没有顺序，因此使用固定的虚拟位置；而对于文本**token**，则使用递增序列表示文本描述中单词的顺序。此外，段嵌入添加到每个输入**token**中，以区分不同模态。

### （2）预训练任务

**⚪ 掩码语言模型（MLM）**

**MLM**任务与**BERT**中的**MLM**任务相同，随机掩盖输入**token**的**15%**，并基于周围文本和视觉特征进行预测。损失函数为：

$$
L_{MLM}(\theta)=-E_{(v,w)\sim D}\log P_{\theta}(w_{m_T}|w_{/m_T},v)
$$

**⚪ 掩码目标分类（MOC）**

**MOC**任务是**MLM**任务的扩展，对视觉目标**token**进行掩码建模。随机掩盖目标**token**的**15%**，并基于上下文预测掩码目标的类别标签。损失函数为：

$$
L_{MOC}(\theta)=-E_{(v,w)\sim D}\sum_{i=0}^{M-1}CE(l_{\theta}(v_{m_I}^{(i)}),f_{\theta}(v_{m_I}^{(i)}))
$$

**⚪ 掩码区域特征回归（MRFR）**

**MRFR**任务旨在更精确地预测掩码目标的特征嵌入。损失函数为：

$$
L_{MRFR}(\theta)=-E_{(v,w)\sim D}\sum_{i=0}^{M-1}\|h_{\theta}(v_{m_I}^{(i)})-r_{\theta}(v_{m_I}^{(i)})\|_{2}^{2}
$$

**⚪ 图像-文本匹配（ITM）**

**ITM**任务用于学习图像和文本的对齐关系。对于每个训练样本，随机采样负样本（图像或文本），并使用二元分类损失优化模型：

$$
L_{ITM}(θ) = -E_{(v,w)∼D} [y\log S_{\theta}(v,w) + (1-y)\log(1-S_{\theta}(v,w))]
$$


**⚪ 多阶段预训练**

由于不同数据集来自不同来源，可能具有不同的质量水平和噪声分布，因此作者提出了一种多阶段预训练框架。在多阶段预训练中，同一网络结构可以依次利用不同类型的数据集进行预训练。具体而言，**ImageBERT**采用两阶段预训练策略：第一阶段在**LAIT**数据集上进行预训练，第二阶段在**Conceptual Captions**和**SBU Captions**数据集上继续预训练。两个预训练阶段均包括所有四个预训练任务。

![](https://pic1.imgdb.cn/item/67a07de5d0e0a243d4f9b5a5.png)

### （3）LAIT数据集

作者收集了一个比较大的数据集，用于在第一阶段对模型进行预训练，数据的具体收集过程如图所示。
1. 网页收集：首先，研究者从互联网上爬取了数十亿个网页，排除了所有非英语网页，因为后续任务都是以英语进行的。然后，通过解析每个网页来收集图像**URL**，并通过**HTML**标签和**DOM**树特征来检测主导图像。非主导图像被丢弃，因为它们与网页内容的相关性可能较低。
2. 基于图像内容的过滤：研究者进一步根据图像内容进行过滤，只保留宽度和高度都大于**300**像素的图像。同时，排除了包含色情或不雅内容的图像。此外，考虑到后续任务中的图像都是来自现实世界的自然、真实的照片，研究者使用二元分类器来排除非自然、非现实和不可学习的图像。
3. 句子检测与清洗：研究者使用**HTML**中的用户定义元数据（如**Alt**或**Title**属性）和图像周围的文本作为图像的文本描述。通过一系列启发式规则来过滤掉句子中的不良片段和噪声词汇（如垃圾邮件/色情内容），只保留正常长度的句子。最后丢弃那些包含高比例生僻词汇的句子。
4. 图像-文本语义评分：在过滤掉不良图像和清洗嘈杂文本之后，研究者希望确保文本和图像在语义上是相关的。通过少量监督的图像-文本数据，研究者训练了一个弱图像-文本语义模型来预测文本和图像对是否语义相关，并将其应用于大规模的图像-文本对来过滤掉不相关的对。该语义模型是基于数百个特征进行训练的，包括仅文本特征、图像内容特征和文本-图像跨模态特征。
5. 图像-文本聚合：在某些情况下，一个图像可能从多个网页下载，因此具有不同的文本描述。在这种情况下，研究者只选择得分最高的文本和图像对。如果太多图像具有相同的描述，他们将直接从语料库中删除所有这些对。

![](https://pic1.imgdb.cn/item/67a07e70d0e0a243d4f9b5af.png)

## 3. 实验分析

在预训练完成后，作者将模型应用于图像-文本检索任务，并在**MSCOCO**和**Flickr30k**数据集上进行微调。图像检索任务旨在根据输入的描述句子检索正确的图像，而文本检索则相反。微调阶段的输入序列格式与预训练相同，但不包含掩码。作者尝试了三种不同的损失函数进行微调：二元分类损失、多分类损失和三元组损失。

在**Flickr30k**和**MSCOCO**测试集上评估预训练模型的零样本性能。如表所示，**ImageBERT**在**MSCOCO**上取得了新的最佳结果，但在**Flickr30k**上略逊于**UNITER**模型。这表明多阶段预训练策略在预训练阶段学习了更多有用的知识，有助于下游任务的微调。

![](https://pic1.imgdb.cn/item/67a07f35d0e0a243d4f9b5c3.png)

微调后的模型在**Flickr30k**和**MSCOCO**测试集上取得了新的最佳性能，如表所示。这证明了**LAIT**数据集和多阶段预训练策略在跨模态联合学习中的有效性。

![](https://pic1.imgdb.cn/item/67a07f59d0e0a243d4f9b5c5.png)

