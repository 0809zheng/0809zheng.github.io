---
layout: post
title: 'SLIP: Self-supervision meets Language-Image Pre-training'
date: 2024-01-25
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67ab1b91d0e0a243d4fe5794.png'
tags: 论文阅读
---

> SLIP：自监督语言图像预训练.

- paper：[SLIP: Self-supervision meets Language-Image Pre-training](https://arxiv.org/abs/2112.12750)

## 0. TL; DR

本文介绍了一种名为**SLIP**的多任务学习框架，旨在结合自监督学习和语言监督学习，以提升视觉表征学习的效果。**SLIP**通过整合自监督学习和**CLIP**预训练，在零样本迁移、线性分类和端到端微调等多种设置下，均显著优于单独的自监督学习和语言监督学习方法。实验表明，**SLIP**在**ImageNet**等数据集上取得了显著的性能提升，验证了自监督学习在语言监督场景下的有效性。

## 1. 背景介绍

近年来，深度学习领域的发展依赖于强大的通用表征预训练，这些表征能够迁移到多种特定应用中。在计算机视觉领域，监督学习和自监督学习是两种主要的表征学习方法。监督学习依赖于大规模标注数据，如**ImageNet**，而自监督学习则通过无监督的方式从无标签数据中学习视觉特征。自监督学习在语言领域取得了巨大成功，并逐渐在视觉领域取得进展。然而，自监督学习在大规模无标注数据集上的应用仍面临挑战。

**CLIP**是一种新的语言监督学习方法，通过图像和文本的对比学习，无需人工标注即可学习强大的视觉表征。**CLIP**的成功引发了对语言监督在视觉表征学习中潜力的关注。本文探索自监督学习是否能够进一步提升语言监督的效果，提出了**SLIP**框架，将自监督学习和语言监督学习相结合，以充分利用两者的优点。

![](https://pic1.imgdb.cn/item/67ab1c4bd0e0a243d4fe57e3.png)

## 2. SLIP 模型

**SLIP**框架的核心思想是将自监督学习和语言监督学习结合起来，通过多任务学习的方式提升视觉表征的质量。具体而言，**SLIP**在预训练阶段同时使用图像的自监督学习和图像-文本对的语言监督学习。

**SLIP**采用**CLIP**作为语言监督学习的框架。**CLIP**通过对比学习的方式，将图像和文本嵌入到一个共享的嵌入空间中，并通过**InfoNCE**损失函数优化正样本对（图像和对应的文本描述）和负样本对（图像和不匹配的文本描述）。具体而言，**CLIP**的损失函数定义如下：

$$
L_{CLIP}=−\log\frac{\exp(sim(I,T)/τ)}{\sum_{I^′,T^′}\exp(sim(I^′,T^′)/τ)}
$$
 
其中，$I$和$T$分别表示图像和文本的嵌入向量，$sim(⋅)$表示相似度函数（如点积），$τ$是温度参数。

**SLIP**采用**SimCLR**作为自监督学习的框架。**SimCLR**通过对比学习的方式，将同一图像的不同增强视图拉近，将不同图像的视图推远。具体而言，**SimCLR**的损失函数定义如下：

$$
L_{SSL}=−\log\frac{\exp(sim(I_1,I_2)/τ)}{\sum_{I_1,I^′}\exp(sim(I_1,I^′)/τ)}
$$

其中，$I_1,I_2$是同一图像的两个增强视图。

**SLIP**框架将**CLIP**和**SimCLR**的目标函数结合起来，形成一个多任务学习框架。在训练过程中，**SLIP**通过共享的图像编码器同时处理**CLIP**和**SSL**的输入，使得模型能够同时学习来自语言监督和自监督的特征。这种多任务学习方式不仅提高了模型对视觉信息的理解能力，还增强了模型对语言信息的敏感性。

![](https://pic1.imgdb.cn/item/67ab1d68d0e0a243d4fe57f9.png)

## 3. 实验分析

**SLIP**主要在**YFCC15M**数据集上进行预训练，该数据集包含**1500**万张带有标题和描述的图像。然后在**ImageNet**上在以下三种设置下评估模型性能：
- 零样本迁移（**Zero-shot Transfer**）：直接使用预训练模型进行分类，无需微调。
- 线性分类（**Linear Classification**）：冻结预训练模型的权重，仅训练一个线性分类器。
- 端到端微调（**End-to-end Finetuning**）：对整个模型进行微调。

**SLIP**在所有设置下均显著优于**CLIP**和单独的自监督学习方法。例如，在零样本迁移中，**SLIP**的性能比**CLIP**提高了约**5%**，在线性分类中提高了约**5%-10%**。

![](https://pic1.imgdb.cn/item/67ab2e46d0e0a243d4fe5d6b.png)

**SLIP**在大多数零样本测试数据集上均优于**CLIP**，尤其是在与**YFCC15M**数据分布更接近的数据集上，如**Food-101**和**Caltech-101**。

![](https://pic1.imgdb.cn/item/67ab2ebed0e0a243d4fe5d7f.png)



