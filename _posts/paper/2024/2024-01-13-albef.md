---
layout: post
title: 'Align before Fuse: Vision and Language Representation Learning with Momentum Distillation'
date: 2024-01-13
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67a0b394d0e0a243d4f9bc06.png'
tags: 论文阅读
---

> 融合前对齐：使用动量蒸馏进行视觉和语言表示学习.

- paper：[Align before Fuse: Vision and Language Representation Learning with Momentum Distillation](https://arxiv.org/abs/2107.07651)

## 0. TL; DR

本文提出了一种新的视觉-语言预训练框架**ALBEF（ALign BEfore Fuse）**，通过对比学习在融合之前对齐图像和文本表示，并引入动量蒸馏（**Momentum Distillation**）方法来提高模型在噪声数据上的学习能力。**ALBEF**在多个下游任务上取得了**SOTA**性能，同时在推理速度上优于现有方法。

## 1. 背景介绍

视觉-语言预训练（**VLP**）旨在从大规模图像-文本对中学习多模态表示，以提升下游视觉-语言（**V+L**）任务的性能。现有方法大多依赖于基于**Transformer**的多模态编码器来联合建模视觉**token**（基于区域的图像特征）和词**token**。然而，这些方法存在以下局限性：
- 视觉**token**和词**token**未对齐，导致多模态编码器难以学习图像-文本交互。
- 预训练目标检测器既需要边界框注释，又需要高分辨率图像，计算成本高昂。
- 现有的图像-文本数据集（如**Conceptual Captions**和**SBU Captions**）从网络收集，存在噪声，可能导致模型过拟合。

为了解决这些问题，本文提出**ALBEF**框架，通过对比学习对齐图像和文本表示，并引入动量蒸馏方法来提高模型在噪声数据上的学习能力。

## 2. ALBEF 模型

**ALBEF**包含三个主要部分：图像编码器、文本编码器和多模态编码器。图像编码器使用**12**层的**ViT-B/16**，文本编码器和多模态编码器均使用**6**层**Transformer**（预训练**BERT**的前六层作为文本编码器，后六层作为多模态编码器）。图像特征和文本特征通过交叉注意力在多模态编码器中融合。

![](https://pic1.imgdb.cn/item/67a0b4d5d0e0a243d4f9bc1d.png)

**ALBEF**的预训练目标包括三个部分：
- 图像-文本对比学习（**ITC**）：在单模态编码器的表示上引入对比损失，对齐图像和文本表示，使多模态编码器更容易进行跨模态学习。
- 掩码语言建模（**MLM**）：利用图像和上下文文本预测被掩码的词。
- 图像-文本匹配（**ITM**）：预测图像和文本是否匹配，并通过对比相似度挖掘硬负样本。

**⚪ 图像-文本对比学习（ITC）**

**ITC**通过最大化图像和文本的互信息来对齐它们的表示。具体来说，对于每个图像-文本对，计算其在单模态编码器中的表示，并通过对比损失来优化模型：

$$
L_{ITC}=-\frac{1}{2}E_{(I,T)\sim D}\left[\log \frac{\exp \left(s(I, T) / \tau\right)}{\sum_{m=1}^{M} \exp \left(s(I, T_m) / \tau\right)}+\log \frac{\exp \left(s(T, I) / \tau\right)}{\sum_{m=1}^{M} \exp \left(s(T, I_m) / \tau\right)}\right]
$$

其中，$s(I,T)$是图像和文本的相似度函数，$τ$是温度参数，$M$是负样本数量。

**⚪ 掩码语言建模（MLM）**

**MLM**的目标是预测被掩码的词**token**。具体来说，随机掩码输入文本中的**token**，并使用模型的上下文表示来预测被掩码的**token**：

$$
L_{MLM}=−E_{(I,\hat{T})\sim D}\left[\log \frac{\exp \left(\psi\left(y^{msk}\right)^\top f(I, \hat{T})\right)}{\sum_{y \in V} \exp \left(\psi(y)^\top f(I, \hat{T})\right) }\right]
$$

其中，$ψ(y)$是词**token**的嵌入函数，$V$是词汇表，$f(I, \hat{T})$是多模态编码器的输出。

**⚪ 图像-文本匹配（ITM）**

**ITM**的目标是预测图像和文本是否匹配。具体来说，使用多模态编码器的$[CLS]$标记的表示来预测匹配分数。


**⚪ 动量蒸馏（Momentum Distillation）**

为了提高模型在噪声数据上的学习能力，本文提出动量蒸馏方法。动量模型是基模型的指数移动平均版本，用于生成伪目标。具体来说，对于**ITC**和**MLM**任务，使用动量模型的输出作为伪目标，并通过**KL**散度来优化基模型：

$$
L^{mod}_{ITC}=(1-\alpha)L_{ITC}+\frac{\alpha}{2} E_{(I,T)\sim D}\left[\operatorname{KL}\left(q^{i 2 t}(I) \| p^{i 2 t}(I)\right)+\operatorname{KL}\left(q^{t 2 i}(T) \| p^{t 2 i}(T)\right)\right] \\
L^{mod}_{MLM}=(1-\alpha)L_{MLM}+\alpha E_{(I,\hat{T})\sim D}\left[\operatorname{KL}\left(q^{\text {msk }}(I, \hat{T}) \| p^{\text {msk }}(I, \hat{T})\right)\right]
$$

其中，$α$是蒸馏权重，$q$是动量模型的输出，$p$是基模型的输出。



## 3. 实验分析

**ALBEF**在以下数据集上进行预训练：
- **Conceptual Captions (CC)**：包含**2.95M**图像和**2.95M**描述。
- **SBU Captions (SBU)**：包含**860K**图像和**860K**描述。
- **COCO**：包含**113K**图像和**567K**描述。
- **Visual Genome (VG)**：包含**100K**图像和**769K**描述。
- **Conceptual 12M (CC12M)**：包含**10.06M**图像和**10.06M**描述。


**ALBEF**在以下下游任务上进行评估：
- 图像-文本检索：使用**R@1、R@5**和**R@10**作为评估指标。
- 视觉问答（**VQA**）：使用准确率作为评估指标。
- 自然语言视觉推理（**NLVR2**）：使用准确率作为评估指标。
- 视觉蕴含（**SNLI-VE**）：使用准确率作为评估指标。
- 弱监督视觉定位：使用定位准确率作为评估指标。

主要实验结果：
- 图像-文本检索：**ALBEF**在**Flickr30K**和**COCO**数据集上取得了**SOTA**性能，显著优于**CLIP**和**ALIGN**等方法。具体来说，**ALBEF**在**Flickr30K**的**1K**测试集上取得了**95.9%**的**TR R@1**，在**COCO**的**5K**测试集上取得了**77.6%**的**TR R@1**。![](https://pic1.imgdb.cn/item/67a0bbf9d0e0a243d4f9bd48.png)
- 视觉问答（**VQA**）：**ALBEF**在**VQA2.0**数据集上取得了**75.84%**的准确率，比之前的**SOTA**方法**VILLA**高出**2.25**个百分点。
- 自然语言视觉推理（**NLVR2**）：**ALBEF**在**NLVR2**数据集上取得了**83.14%**的准确率，比之前的**SOTA**方法**VILLA**高出**3.84**个百分点。
- 视觉蕴含（**SNLI-VE**）：**ALBEF**在**SNLI-VE**数据集上取得了**80.91%**的准确率，比之前的**SOTA**方法**VILLA**高出**1.88**个百分点。![](https://pic1.imgdb.cn/item/67a0bc0ad0e0a243d4f9bd4d.png)
- 弱监督视觉定位：**ALBEF**在**RefCOCO+**数据集上取得了**65.89%**的定位准确率，显著优于现有方法。![](https://pic1.imgdb.cn/item/67a0bc2ed0e0a243d4f9bd58.png)

