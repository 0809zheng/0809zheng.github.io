---
layout: post
title: 'VL-BEiT: Generative Vision-Language Pretraining'
date: 2024-02-28
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67b594f6d0e0a243d400bfa9.png'
tags: 论文阅读
---

> VL-BEiT：生成式视觉-语言预训练.

- paper：[VL-BEiT: Generative Vision-Language Pretraining](https://arxiv.org/abs/2206.01127)

## 0. TL; DR

本文提出了一个名为**VL-BEiT**的视觉-语言基础模型，通过在图像、文本和图像-文本对上进行掩码预测任务实现多模态预训练。**VL-BEiT**使用统一的双向**Transformer**架构，通过掩码语言建模（**MLM**）、掩码图像建模（**MIM**）和掩码视觉-语言建模（**MVLM**）三种任务进行预训练。实验结果表明，**VL-BEiT**在视觉问答、视觉推理和图像-文本检索等视觉-语言任务上表现出色，同时在图像分类和语义分割等视觉任务上也取得了竞争力的结果。

## 1. 背景介绍

近年来，生成式预训练在自然语言处理（**NLP**）和计算机视觉（**CV**）领域取得了巨大成功。例如，**BERT**通过掩码语言建模（**MLM**）学习文本表示，**BEIT**通过掩码图像建模（**MIM**）学习图像表示。然而，现有的多模态预训练方法大多依赖于复杂的对比学习目标或多种预训练任务的组合，这限制了模型的扩展性和效率。

本文提出**VL-BEiT**，一个基于生成式预训练的视觉-语言基础模型。**VL-BEiT**的核心思想是通过一个统一的掩码预测任务，在单模态（图像和文本）和多模态（图像-文本对）数据上进行预训练。这种方法不仅简化了预训练过程，还使得模型能够高效地扩展到大规模数据。

## 2. VL-BEiT 模型

**VL-BEiT**通过掩码预测任务在单模态和多模态数据上进行预训练，使用共享的双向**Transformer**架构。**VL-BEiT**将图像和文本分别编码为序列化表示，然后将它们组合起来进行多模态建模：
- 图像表示：图像被分割为一系列**patch**，每个**patch**通过线性投影层转换为**patch**嵌入。此外，添加一个特殊的[I_CLS]标记以表示图像序列的起始位置。
- 文本表示：文本被分词为单词嵌入，并添加特殊标记[T_CLS]和[T_SEP]。最终的文本表示是单词嵌入和位置嵌入的和。
- 图像-文本对表示：将图像和文本的表示拼接起来，形成一个联合的输入序列。

**VL-BEiT**使用混合模态专家（**Mixture-of-Modality-Experts, MOME**）**Transformer**作为骨干网络。**MOME Transformer**包含多个**Transformer**块，每个块包含一个跨模态共享的多头自注意力模块和一个模态专家池。这种设计允许模型在不同模态之间对齐内容，同时捕获模态特定的信息。

**VL-BEiT**通过以下三种掩码预测任务进行预训练，能够学习到强大的多模态表示：
- 掩码语言建模（**MLM**）：随机掩码文本中的部分**token**，并训练模型根据上下文预测这些掩码**token**。
- 掩码图像建模（**MIM**）：随机掩码图像中的部分**patch**，并训练模型预测这些掩码**patch**的离散视觉**token**。
- 掩码视觉-语言建模（**MVLM**）：在图像-文本对中，同时掩码文本**token**和图像**patch**，并训练模型根据视觉和语言线索预测掩码内容。

![](https://pic1.imgdb.cn/item/67b59643d0e0a243d400c027.png)

## 3. 实验分析

**VL-BEiT**的预训练数据包括：
- 单模态数据：使用**ImageNet-22K**作为图像数据，英文维基百科和**BookCorpus**作为文本数据。
- 多模态数据：结合**Conceptual Captions、SBU Captions、COCO**和**Visual Genome**四个数据集，包含约**400**万图像和**1000**万图像-文本对。

**VL-BEiT**在多个视觉-语言和视觉任务上取得了优异的性能：
- 在视觉问答（**VQA**）任务上，**VL-BEiT**达到了**77.75%**的准确率，超越了其他同类模型。
- 在视觉推理（**NLVR2**）任务上，**VL-BEiT**达到了**82.66%**的准确率，表现出色。
- 在图像-文本检索任务上，**VL-BEiT**在**COCO**数据集上达到了**79.5%**的图像到文本检索**Top-1**召回率，在**Flickr30K**数据集上达到了**95.8%**的文本到图像检索**Top-1**召回率。![](https://pic1.imgdb.cn/item/67b59832d0e0a243d400c2a6.png)
- 在图像分类（**ImageNet-1K**）任务上，**VL-BEiT**达到了**85.9%**的**Top-1**准确率。
- 在语义分割（**ADE20K**）任务上，**VL-BEiT**达到了**53.1%**的**mIoU**。![](https://pic1.imgdb.cn/item/67b59812d0e0a243d400c25a.png)

**VL-BEiT**通过掩码预测任务有效地学习了多模态表示。实验结果表明，掩码视觉-语言建模（**MVLM**）对于多模态对齐至关重要。此外，**MOME Transformer**的设计使得模型能够更好地捕获模态特定的信息，从而提升性能。

![](https://pic1.imgdb.cn/item/67b598a1d0e0a243d400c2f9.png)
![](https://pic1.imgdb.cn/item/67b59896d0e0a243d400c2f1.png)