---
layout: post
title: 'VMamba: Visual State Space Model'
date: 2024-07-15
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67c95525066befcec6dee995.png'
tags: 论文阅读
---

> VMamba: 视觉状态空间模型.

- paper：[VMamba: Visual State Space Model](https://arxiv.org/abs/2401.10166)

## 0. TL;DR

本文提出了**VMamba**，一种用于视觉表示学习的视觉状态空间模型。**VMamba**通过引入全局感受野和动态权重，实现了线性复杂度，同时保持了视觉建模的高性能。通过引入交叉扫描模块（**CSM**），**VMamba**解决了方向敏感性问题，使其能够有效地处理视觉图像。实验结果表明，**VMamba**在图像分类、目标检测和语义分割等任务上均优于现有的**CNN**和视觉**Transformer**模型，并且在图像分辨率增加时表现出更显著的优势。

## 1. 背景介绍

视觉表示学习是计算机视觉中的一个基本研究课题。自深度学习时代以来，卷积神经网络（**CNNs**）和视觉**Transformer（ViTs）**一直是两种最主流的基础模型。**CNNs**在图像分辨率方面表现出线性复杂度，而**ViTs**则在拟合能力上优于**CNNs**，尽管它们面临二次复杂度的挑战。**ViTs**通过引入全局感受野和动态权重，实现了更优越的视觉建模性能。然而，这种全局感受野和动态权重的引入也带来了计算复杂度的增加，尤其是在处理高分辨率图像时。

为了在保持全局感受野和动态权重的同时提高计算效率，本文提出了一种基于状态空间模型（**SSMs**）的新型架构，即视觉状态空间模型（**VMamba**）。**VMamba**通过引入选择性扫描机制，实现了线性复杂度，同时保持了全局感受野。此外，为了处理非因果视觉图像，**VMamba**引入了交叉扫描模块（**CSM**），使其能够有效地捕获图像中的全局信息。


## 2. VMamba 模型

**VMamba**是一种集成了基于**Mamba**模块的视觉骨干网络，以促进高效的视觉表示学习。**Mamba**的并行选择扫描操作是为处理一维顺序数据而设计的。当尝试将其适用于视觉数据处理时会有问题，因为视觉数据本质上缺乏视觉组件的顺序排列。

为了解决这个问题，作者提出了二维选择扫描（**SS2D**），这是一种为空间域遍历量身定制的四向扫描机制，促进选择性**SSM**在处理视觉数据方面的扩展。与自注意力机制相比，**SS2D**确保每个图像块仅通过沿相应扫描路径计算的压缩隐藏状态获取上下文知识，从而将计算复杂度从二次降低到线性。

![](https://pic1.imgdb.cn/item/67c956d1066befcec6deea7a.png)

**VMamba** 的架构概览如图所示：
1. 输入图像$\mathbf{I} \in \mathbb{R}^{H \times W \times 3}$首先通过一个**patch partition**模块划分成图像块，生成一个空间维度为$H/4 × W/4$的二维特征图；
2. 多个阶段被用来创建分辨率为$H/8 × W/8$、$H/16 × W/16$ 和$H/32 × W/32$ 的分层表示，每个阶段包括一个下采样层和一个视觉状态空间（**VSS**）模块。
3. **VSS**模块设计受**Mamba**模块的启发，通过引入**SS2D**扫描机制来处理二维数据；
4. 为了进一步提高计算效率，作者消除了整个乘法分支，最终**VSS**模块由一个具有两个残差模块的单一网络分支组成。

![](https://pic1.imgdb.cn/item/67c95771066befcec6deeab1.png)

**SS2D**中的数据转发涉及三个步骤：
1. 交叉扫描：给定输入数据，**SS2D**首先沿四条不同的遍历路径（即交叉扫描，分别从二维矩阵的左上角向右$\rightarrow$、左上角向下$↓$、右下角向左$←$、右下角向上$↑$出发）；
2. 选择性扫描：使用单独的**S6**模块并行处理每个序列；
3. 交叉合并：随后重新整形并合并结果序列以形成输出图。

通过采用互补的**1D**遍历路径，**SS2D**使图像中的每个像素能够有效整合来自其他像素的不同方向的信息，从而促进在**2D**空间中建立全局感受野。

![](https://pic1.imgdb.cn/item/67c95dfa066befcec6def525.png)

使用原始**VSS**模块的**VMamba-T**模型实现了**426**张图像/秒的吞吐量，包含**22.9M**参数和**5.6G FLOPs**。尽管在微型级别上实现了**82.2%**的最新分类准确率，但低吞吐量和高内存开销对**VMamba**的实际部署构成了重大挑战。作者通过在**ImageNet-1K**上的图像分类对模型进行评估。测试了一系列渐进改进：
- **step (a)** (**+0.0%,+41img/s**)：通过在**Triton**中重新实现**Cross-Scan**和**Cross-Merge**以适应**float16**输入和**float32**输出；
- **step (b)** (**+0.0%，−3img/s**)：通过用线性变换（`torch.nn.functional.linear`）替换选择性扫描中相对较慢的**einsum**，尽管测试时间略有速度波动，但这显著提高了训练效率（吞吐量从**165**提升到**184**）；
- **step (c) (+0.0%，+174img/s)**：采用$(B, C, H, W)$的张量布局以消除不必要的尺寸转换；
- **step (d) (−0.6%, +175img/s)**：引入**MLP**到**VMamba**中，舍弃了**DWConv**层，并将层配置从$[2,2,9,2]$更改为$[2,2,2,2]$以降低**FLOPs**；
- **step (e) (+0.6%, +366img/s)**：舍弃整个乘法分支，并将参数**sm-ratio**（特征扩展因子）从**2.0**降低到**1.0**，使得可以将层数增加到$[2,2,5,2]$，同时减少**FLOPs**；
- **step (f) (+0.3%, +161 img/s)**：将参数 **d_state**（**SSM** 状态维度）从 **16.0** 降低到 **1.0**，使得可以将 **ssm-ratio** 提回到 **2.0**，并在不增加 **FLOPs** 的情况下引入 **DWConv** 层；
- **step (g) (+0.1%, +346 img/s)**：将 **ssm-ratio** 降低到 **1.0**，同时将层配置从 $[2,2,5,2]$ 更改为$[2,2,8,2]$。

![](https://pic1.imgdb.cn/item/67c95fff066befcec6defc04.png)

## 3. 实验分析

对于图像分类任务，**VMamba**在**ImageNet-1K**数据集上的表现优于现有的**CNN**和视觉**Transformer**模型。具体来说，**VMamba-T**在**224×224**输入尺寸下达到了**82.6%**的**Top-1**准确率，超过了**RegNetY-4G、DeiT-S**和**Swin-T**等模型。

![](https://pic1.imgdb.cn/item/67c96096066befcec6defc71.png)

对于目标检测任务，**VMamba**在**COCO**数据集上的表现同样优于现有模型。在**12**轮和**36**轮训练中，**VMamba-T、VMamba-S**和**VMamba-B**模型在目标检测和实例分割任务上均取得了更高的**AP**值。

对于语义分割任务，**VMamba**在**ADE20K**数据集上的表现同样优于现有模型。在**512×512**和**640×640**输入尺寸下，**VMamba-T、VMamba-S**和**VMamba-B**模型在语义分割任务上均取得了更高的**mIoU**值。

![](https://pic1.imgdb.cn/item/67c96108066befcec6defd6b.png)

**VMamba**的有效感受野（**ERF**）显示出其全局特性，这与其他模型如**ResNet、ConvNeXt、Swin**和**DeiT**进行了对比。**VMamba**在训练后展现出全局**ERF**，表明其对图像的全局感知能力。

![](https://pic1.imgdb.cn/item/67c9613a066befcec6defde9.png)

**VMamba**在不同输入尺寸下的性能表现出更稳定的趋势，随着输入尺寸的增加，**VMamba**的性能逐渐提升，显示了其对输入图像尺寸变化的鲁棒性。

![](https://pic1.imgdb.cn/item/67c96181066befcec6defe88.png)