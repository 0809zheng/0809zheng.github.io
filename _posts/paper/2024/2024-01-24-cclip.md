---
layout: post
title: 'Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese'
date: 2024-01-24
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67a1fd20d0e0a243d4fbcf77.png'
tags: 论文阅读
---

> Chinese CLIP：中文对比视觉语言预训练.

- paper：[Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese](https://arxiv.org/abs/2211.01335)

## 0. TL; DR

本文提出了 **Chinese CLIP**，这是一个专门针对中文的视觉-语言对比预训练模型。与传统的多模态模型不同，**Chinese CLIP** 通过两阶段预训练方法（锁定图像微调和对比微调）在大规模中文图文数据上进行训练，能够有效提升模型在中文场景下的性能。实验表明，**Chinese CLIP** 在多个中文跨模态检索数据集（如 **MUGE**、**Flickr30K-CN** 和 **COCO-CN**）上达到了最先进的性能，并在零样本图像分类任务中表现出色。此外，该模型还支持高效的部署，通过 **NVIDIA TensorRT** 和 **ONNX** 模型加速推理速度。

## 1. 背景介绍

随着预训练技术在自然语言处理（**NLP**）中的成功，多模态预训练模型逐渐成为计算机视觉和多模态表示学习的重要基础。**CLIP** 是一个多模态预训练模型的里程碑，它通过对比学习的方式在大规模图文数据上进行预训练，并在跨模态检索和零样本图像分类任务中取得了显著的性能。然而，将这类模型有效地迁移到特定语言场景中仍然存在挑战，尤其是在中文场景下。主要原因是：
- 语言特异性数据的重要性：直接将 **CLIP** 用于中文数据时，由于其训练数据主要来自英文网站，导致在中文场景下性能较差。
- 预训练数据质量的限制：现有的中文多模态预训练方法由于缺乏大规模高质量的中文图文数据，性能受到限制。

为了解决这些问题，本文提出了 **Chinese CLIP**，这是一个基于大规模中文图文数据预训练的视觉-语言模型。它通过两阶段预训练方法（锁定图像微调和对比微调）来提升模型在中文场景下的性能，并在多个中文跨模态检索数据集上达到了最先进的性能。

![](https://pic1.imgdb.cn/item/67a2044dd0e0a243d4fbd04f.png)

## 2. Chinese CLIP 模型

为了训练 **Chinese CLIP**，作者从 **LAION-5B** 数据集和 **Wukong** 数据集中提取了中文图文对，并添加了经典英文多模态数据集（如 **Visual Genome** 和 **MSCOCO**）的翻译版本。最终构建了一个包含约 **2** 亿图文对的中文多模态预训练数据集。在数据预处理阶段，作者移除了低质量样本和包含广告或文件名等无关内容的样本，并对图像进行了标准化处理。

**Chinese CLIP** 的模型架构基于 **OpenAI CLIP**，包括一个图像编码器和一个文本编码器。图像编码器使用了不同大小的 **Vision Transformer（ViT）**或 **ResNet**，而文本编码器则使用了不同层数的 **RoBERTa** 模型。具体来说，作者开发了 **5** 种不同大小的 **Chinese CLIP** 模型，参数量从 **7700** 万到 **9.58** 亿不等。这些模型在预训练过程中通过对比学习损失函数进行优化，以学习图像和文本之间的语义对齐。
- **CN-CLIPRN50**：使用 **ResNet-50** 作为图像编码器，**RBT3** 作为文本编码器，参数量为 **7700** 万。
- **CN-CLIPViT-B/16**：使用 **ViT-B/16** 作为图像编码器，**RoBERTa-wwm-Base** 作为文本编码器，参数量为 **1.88** 亿。
- **CN-CLIPViT-L/14**：使用 **ViT-L/14** 作为图像编码器，**RoBERTa-wwm-Base** 作为文本编码器，参数量为 **4.06** 亿。
- **CN-CLIPViT-L/14@336px**：使用 **ViT-L/14@336px** 作为图像编码器，**RoBERTa-wwm-Base** 作为文本编码器，参数量为 **4.07** 亿。
- **CN-CLIPViT-H/14**：使用 **ViT-H/14** 作为图像编码器，**RoBERTa-wwm-Large** 作为文本编码器，参数量为 **9.58** 亿。

在预训练过程中，作者使用了随机裁剪、**AutoAugment** 等数据增强技术，并通过对比学习损失函数进行优化。预训练方法分为两个阶段：
- 第一阶段：锁定图像微调（**LiT**）。在这一阶段，作者冻结了图像编码器的参数，仅对文本编码器进行优化。文本编码器基于预训练的中文 **RoBERTa** 模型初始化，而图像编码器则基于 **OpenAI CLIP** 的权重初始化。通过这种方式，模型能够利用预训练的图像编码器提取高质量的视觉特征，同时让文本编码器更好地适应中文数据。
- 第二阶段：对比微调。在这一阶段，作者解冻了图像编码器的参数，并对整个模型进行对比学习优化。这一阶段的目标是让模型能够更好地学习中文数据中的视觉和语言分布，从而提升跨模态检索的性能。


## 3. 实验分析

作者在多个中文跨模态检索数据集上评估了 **Chinese CLIP** 的性能，包括：
- **MUGE-Retrieval**：从中文电商网站提取的图文检索数据集。
- **Flickr30K-CN**：**Flickr30K** 数据集的中文翻译版本。
- **COCO-CN**：**MSCOCO** 数据集的中文翻译版本。

实验结果表明，**Chinese CLIP** 在所有数据集上均达到了最先进的性能。例如，在 **MUGE** 数据集上，**CN-CLIPViT-L/14@336px** 模型在零样本学习和微调设置下分别达到了 **59.0%** 和 **65.3%** 的 **R@1**，显著优于其他基线模型。

![](https://pic1.imgdb.cn/item/67a20486d0e0a243d4fbd053.png)

在 **Flickr30K-CN** 和 **COCO-CN** 数据集上，**Chinese CLIP** 也表现出色，尤其是在零样本学习设置下，**CN-CLIPViT-H/14** 模型在 **Flickr30K-CN** 的文本到图像检索任务中达到了 **71.2%** 的 **R@1**，超过了其他基线模型。

作者还在 **ELEVATER** 基准的 “**Image Classification in the Wild**” 赛道上评估了 **Chinese CLIP** 的零样本图像分类能力。实验结果表明，**Chinese CLIP** 在多个分类数据集上取得了与英文预训练模型相当甚至更好的性能。例如，在 **CIFAR-10** 数据集上，**CN-CLIPViT-H/14** 模型达到了 **96.0%** 的准确率，超过了 **OpenAI CLIP** 的 **94.9%**。

![](https://pic1.imgdb.cn/item/67a204dfd0e0a243d4fbd05c.png)

通过对比不同预训练方法的实验结果，作者发现两阶段预训练方法在多个数据集上均优于从头开始预训练或直接微调预训练模型的方法。特别是在处理翻译数据时，两阶段预训练能够更好地适应中文数据的分布。

![](https://pic1.imgdb.cn/item/67a20539d0e0a243d4fbd067.png)