---
layout: post
title: 'UNITER: UNiversal Image-TExt Representation Learning'
date: 2024-01-07
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/679f62aed0e0a243d4f99747.png'
tags: 论文阅读
---

> UNITER：通用图像-文本表示学习.

- paper：[UNITER: UNiversal Image-TExt Representation Learning](https://arxiv.org/abs/1909.11740)

## 0. TL; DR

本文介绍了一种名为**UNITER（UNiversal Image-TExt Representation）**的模型，旨在通过大规模预训练学习通用的图像-文本表示，以支持多种视觉与语言任务。**UNITER**通过在四个图像-文本数据集（**COCO**、**Visual Genome**、**Conceptual Captions**和**SBU Captions**）上进行预训练，设计了四种预训练任务：掩码语言建模（**MLM**）、掩码区域建模（**MRM**）、图像-文本匹配（**ITM**）和词-区域对齐（**WRA**）。与以往方法不同，**UNITER**采用了条件掩码策略，并通过最优传输（**OT**）技术实现细粒度的词-区域对齐。实验表明，**UNITER**在多个**V+L**任务上取得了新的最佳性能，显著优于现有方法。

## 1. 背景介绍

在视觉与语言（**V+L**）任务中，联合图像-文本嵌入是实现视觉和文本信息融合的基础。以往的研究通常针对特定任务设计专用的多模态表示。然而，这些模型的架构多样且表示高度依赖于特定任务，限制了它们在其他任务中的通用性。因此，一个关键问题被提出：是否可以学习一种通用的图像-文本表示，以支持多种**V+L**任务？

为了解决这一问题，本文提出了**UNITER**模型。该模型基于**Transformer**架构，通过大规模预训练学习通用的图像-文本表示。与**BERT**类似，**UNITER**通过设计多种预训练任务来学习上下文化的表示。这些任务包括掩码语言建模（**MLM**）、掩码区域建模（**MRM**）、图像-文本匹配（**ITM**）和词-区域对齐（**WRA**）。与以往方法不同，**UNITER**采用了条件掩码策略，即在预训练任务中，仅对一种模态进行掩码，而保持另一种模态的完整观察。此外，**UNITER**还引入了基于最优传输（**OT**）的词-区域对齐任务，以实现细粒度的跨模态对齐。


## 2. UNITER 模型

**UNITER**模型由三个主要部分组成：图像嵌入器、文本嵌入器和多层**Transformer**模块。模型的输入是一对图像和句子，输出是联合的多模态嵌入表示。
- 图像嵌入器（**Image Embedder**）：使用**Faster R-CNN**提取图像区域的视觉特征，并将每个区域的位置信息编码为**7**维向量（包括归一化的坐标、宽度、高度和面积）。视觉特征和位置特征通过全连接层投影到同一嵌入空间，并通过层归一化进行处理。
- 文本嵌入器（**Text Embedder**）：将输入句子分词为**WordPieces**，并将每个子词的词嵌入和位置嵌入相加，再通过层归一化处理。
- **Transformer**模块：将图像区域和文本词的嵌入输入到多层**Transformer**中，学习跨模态的上下文化嵌入。

![](https://pic1.imgdb.cn/item/679f66ead0e0a243d4f997fb.png)

**UNITER**设计了四种预训练任务，以学习通用的图像-文本表示：
- 掩码语言建模（**MLM**）：按照**15%**的概率随机掩码输入句子中的部分单词，并用特殊标记$[MASK]$替换。模型的目标是基于周围的单词$w_{/m}$和完整的图像区域$v$预测被掩码的单词$w_m$。损失函数为：

$$
L_{MLM}(\theta)=-E_{(w,v)\sim D}\log P_{\theta}(w_{m}|w_{/m},v)
$$

- 掩码区域建模（**MRM**）：按照**15%**的概率随机掩码图像区域的视觉特征，并用零向量替换。模型的目标是基于剩余的图像区域$v_{/m}$和完整的文本$w$预测被掩码的区域$v_m$。

$$
L_{MRM}(\theta)=E_{(w,v)\sim D}f_{\theta}(v_{m}|v_{/m},w)
$$


**MRM**有三种变体：
1. 掩码区域特征回归（**MRFR**）：学习将**Transformer**输出的掩码区域特征回归到其原始**RoI Pooled**视觉特征。
2. 掩码区域分类（**MRC**）：预测掩码区域的物体语义类别。
3. 掩码区域分类（**KL**散度）（**MRC-kl**）：使用检测器的输出作为软标签，通过最小化**KL**散度进行训练。


- 图像-文本匹配（**ITM**）：模型输入为一个句子和一组图像区域，输出为一个二元标签，表示这对输入是否匹配。正样本为文本-图像对齐的数据对，负样本为将文本或者图像替换之后的数据对。模型通过一个全连接层和**sigmoid**函数预测额外添加的$[CLS]$的匹配分数$s_{\theta}$，并使用二元交叉熵损失进行优化：

$$
L_{ITM}(\theta)=-E_{(w,v)\sim D}[y\log s_{\theta}(w,v)+(1-y)\log(1-s_{\theta}(w,v))]
$$

- 词-区域对齐（**WRA**）：使用最优传输（**OT**）技术，学习单词和图像区域之间的细粒度对齐。**OT**的目标是最小化将一个分布传输到另一个分布的成本。在**UNITER**中，**OT**用于最小化单词嵌入和图像区域嵌入之间的传输成本，从而优化跨模态对齐。损失函数为：

$$
L_{WRA}(\theta)=D{ot}(μ,ν)=\min_{T∈Π(a,b)}\sum_{i=1}^{T}\sum_{j=1}^{K}T_{ij}\cdot c(w_{i},v_{j})
$$

其中，$μ$和$ν$分别表示单词和图像区域的离散分布，$c(w_i,v_j)$表示单词和图像区域之间的距离（如余弦距离），$T$表示传输计划。

与以往方法不同，**UNITER**采用了条件掩码策略。在预训练任务中，仅对一种模态进行掩码，而保持另一种模态的完整观察。例如，在**MLM**任务中，仅掩码文本中的单词，而保持图像区域的完整观察；在**MRM**任务中，仅掩码图像区域，而保持文本的完整观察。这种策略避免了同时掩码两种模态可能导致的对齐问题，从而学习到更好的跨模态表示。

## 3. 实验分析

**UNITER**的预训练数据集由四个现有的**V+L**数据集组成：C**OCO、Visual Genome、Conceptual Captions**和**SBU Captions**。为了确保预训练数据与下游任务的评估数据不重叠，作者对这些数据集进行了清理，排除了下游任务中出现的验证和测试图像。最终，预训练数据集包含**560**万图像-文本对。

![](https://pic1.imgdb.cn/item/679f6ad0d0e0a243d4f9989f.png)

作者在多个**V+L**任务上对**UNITER**进行了评估，包括**VQA**、视觉常识推理（**VCR**）、**NLVR2**、视觉蕴含（**Visual Entailment**）、图像-文本检索和指代表达理解（**Referring Expression Comprehension**）。实验结果表明，**UNITER**在所有任务上均取得了新的最佳性能，显著优于现有方法。主要实验结果：
- **VQA**任务：**UNITER-large**在**test-dev**和**test-std**数据集上分别达到了**73.82%**和**74.02%**的准确率，优于现有的**SOTA**方法。
- **VCR**任务：**UNITER-large**在**Q→A、QA→R**和**Q→AR**三个子任务上分别达到了**77.30%、80.80%**和**62.80%**的准确率，优于**VLBERT**和**ViLBERT**。
- **NLVR2**任务：**UNITER-large**在**dev**和**test-P**数据集上分别达到了**79.12%**和**79.98%**的准确率，优于**VisualBERT**和**LXMERT**。
- 图像-文本检索任务：**UNITER-large**在**Flickr30K**数据集上，图像检索（**IR**）和文本检索（**TR**）的**R@1**指标分别达到了**75.56%**和**87.30%**，在**COCO**数据集上，**IR**和**TR**的**R@1**指标分别达到了**52.93%**和**65.68%**。
- 指代表达理解任务：**UNITER-large**在**RefCOCO+**数据集上达到了**86.34%**的准确率，优于现有的**MAttNet**。

![](https://pic1.imgdb.cn/item/679f6be5d0e0a243d4f998d6.png)

通过广泛的消融研究，作者发现所有四种预训练任务都对模型性能有显著贡献。特别是，词-区域对齐（**WRA**）任务在需要细粒度区域识别和推理的任务（如**VQA**和指代表达理解）上表现出了显著的性能提升。

![](https://pic1.imgdb.cn/item/679f6c1bd0e0a243d4f998e0.png)

作者还对**UNITER**模型的注意力机制进行了可视化。结果显示，模型能够学习到多种注意力模式，包括垂直注意力（关注特殊标记$[CLS]$或$[SEP]$）、对角线注意力（关注自身或相邻的词/区域）、块状注意力（模态内自注意力）和反向块状注意力（模态间注意力）。这些注意力模式表明，**UNITER**能够有效地学习跨模态对齐。

![](https://pic1.imgdb.cn/item/679f6c6ad0e0a243d4f998eb.png)