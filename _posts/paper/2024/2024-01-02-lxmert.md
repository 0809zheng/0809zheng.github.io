---
layout: post
title: 'LXMERT: Learning Cross-Modality Encoder Representations from Transformers'
date: 2024-01-02
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/679dc2fad0e0a243d4f971c4.png'
tags: 论文阅读
---

> LXMERT：学习Transformer中的跨模态编码表示.

- paper：[LXMERT: Learning Cross-Modality Encoder Representations from Transformers](https://arxiv.org/abs/1908.07490)

## 0. TL; DR
**LXMERT**是一种用于学习视觉与语言之间联系的框架。它通过构建一个包含三个编码器（目标关系编码器、语言编码器和跨模态编码器）的大规模**Transformer**模型，并通过五个预训练任务（掩码语言建模、掩码目标预测、跨模态匹配和图像问答）学习视觉和语言之间的对齐关系。**LXMERT**在视觉问答（**VQA**和**GQA**）和视觉推理（**NLVR2**）任务上均取得了最先进的结果，并通过详细的消融研究证明了其模型组件和预训练策略的有效性。

## 1. 背景介绍

视觉与语言推理需要理解视觉概念、语言语义以及两者之间的对齐和关系。尽管在单一模态（视觉或语言）的模型开发方面已经取得了显著进展，但针对视觉与语言模态对的预训练和微调研究仍处于发展阶段。**LXMERT**框架旨在填补这一空白，通过构建一个能够理解视觉和语言交互的预训练模型，为视觉问答、视觉推理等任务提供更强大的基础。


## 2. LXMERT模型

**LXMERT**框架的核心是一个包含三个编码器的**Transformer**模型：目标关系编码器、语言编码器和跨模态编码器。这些编码器通过自注意力和交叉注意力层实现视觉和语言的深度融合。

![](https://pic1.imgdb.cn/item/679dc3fed0e0a243d4f971d5.png)

**LXMERT**将输入（图像和句子）转换为两个序列：单词级别的句子嵌入和目标级别的图像嵌入:
- 输入的文本经过**WordPiece**分词器分割为**sub-word**序列，这些序列及其索引值经过不同的**Embedding Layer**得到不同的**word embedding** 向量和**position embedding**向量，相加得到**sub word**的最终向量；
- 输入的图像则是通过**Faster R-CNN**检测对应的物体框，每一个物体包含**2048**维的经过**ROI Pooling**之后的特征向量以及空间位置（检测到的物体的包围框的坐标值），这两个信息经过全连接层之后的输出取平均得到最终区域特征描述。

得到的文本和图像特征序列，文本序列在开始处添加特殊的$[CLS]$ **token**，然后分别通过目标关系编码器和语言编码器提取特征，再通过跨模态编码器交换特征信息。
- 目标关系编码器和语言编码器都是单模态编码器，每个编码器都由多个**Transformer**层组成，每层包含自注意力子层和前馈子层。这些编码器分别处理语言和视觉输入，生成各自的特征表示。
- 跨模态编码器是**LXMERT**的核心，包含多个跨模态层，每层由两个自注意力子层和一个双向交叉注意力子层组成。交叉注意力子层允许语言和视觉特征之间进行信息交换和对齐，从而生成联合的跨模态表示。

**LXMERT**通过五个预训练任务学习视觉与语言之间的联系：
- 掩码跨模态语言建模：类似于**BERT**的掩码语言建模任务，但允许模型从视觉模态中推断掩码单词。按照**15%**的概率随机**mask**部分文本**token**，让模型对这些**token**进行预测。
- 掩码目标预测：包括**RoI**特征回归和检测标签分类两个子任务，允许模型从语言模态中推断掩码目标的属性。将区域特征序列按照**15%**的概率随机将部分区域对应的特征值设置全零，然后模型对这些**mask**的区域进行预测。**RoI**特征回归以**Faster R-CNN**经过**ROI Pooling**之后的特征序列为真值进行预测；检测标签分类将输出的特征经过一个全连接层预测该物体的类别概率分布值，然后以**Faster R-CNN**预测标签值为真值计算交叉熵损失。
- 跨模态匹配：判断图像和句子是否匹配，类似于**BERT**中的“下一句预测”任务。将$[CLS]$ **token**对应的模型输出的特征向量经过全连接层，预测该文本-图像是否是匹配的，负样本为按照**50%**的概率将文本句子替换为和图像不匹配的文本。
- 图像问答：通过让模型预测和图像相关的问题的答案，进一步增强视觉与语言之间的对齐。

![](https://pic1.imgdb.cn/item/679dc47dd0e0a243d4f971dc.png)

## 3. 实验分析

**LXMERT**在三个主要数据集上进行了评估：
- **VQA v2.0**：包含约**110**万条图像相关问题，任务是生成准确的答案。
- **GQA**：包含**2200**万条需要推理技能的问题，任务是生成准确的答案。
- **NLVR2**：包含**86K**训练数据，任务是判断一个自然语言陈述是否正确描述了两张图像。

主要实验结果：
- **LXMERT**在**VQA v2.0**数据集上取得了**72.5%**的准确率，比之前的最佳方法（**BAN+Counter**）提高了**2.1%**。在二元问题和非二元问题上均取得了显著提升。
- **LXMERT**在**GQA**数据集上取得了**60.0%**的准确率，比之前的最佳方法（**BAN**）提高了**3.2%**。在开放域问题上，LXMERT的准确率提升了**4.6%**。
- **LXMERT**在**NLVR2**数据集上取得了**76.2%**的准确率，比之前的最佳方法（**MaxEnt**）提高了**22%**。此外，**LXMERT**在一致性指标上也取得了显著提升，从**12%**提高到**42.1%**。
 
![](https://pic1.imgdb.cn/item/679dc566d0e0a243d4f971ef.png)

此外，作者还进行了一系列消融实验：
- **BERT**与**LXMERT**的对比：实验表明，仅使用**BERT**进行预训练无法充分利用视觉模态的信息，而**LXMERT**通过跨模态预训练显著提升了性能。
- 图像问答任务的重要性：通过对比实验，**LXMERT**证明了图像问答任务在预训练中的重要性，该任务显著提升了模型在**NLVR2**等复杂任务上的表现。
- 视觉预训练任务的效果：实验表明，**RoI**特征回归和检测标签分类两个视觉预训练任务均对最终性能有显著贡献。

![](https://pic1.imgdb.cn/item/679dc67bd0e0a243d4f971fe.png)