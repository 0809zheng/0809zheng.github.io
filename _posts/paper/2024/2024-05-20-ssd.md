---
layout: post
title: 'Learning Spatial Similarity Distribution for Few-shot Object Counting'
date: 2024-05-20
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6685f25ed9c307b7e97f179e.png'
tags: 论文阅读
---

> 学习少样本目标计数的空间相似度分布.

- paper：[Learning Spatial Similarity Distribution for Few-shot Object Counting](https://arxiv.org/abs/2405.11770)


现有少样本目标计数方法通过计算查询图像与匹配样本在二维空间域的相似度，并进行回归得到计数结果。然而这些方法忽略了样本图像上丰富的相似度空间分布信息，从而对匹配精度产生了重大影响。

本文提出了一种用于少样本目标计数的网络空间相似度分布(**Spatial Similarity Distribution, SSD**)，该方法保留了样本特征的空间结构，并计算了查询特征与样本特征之间点对点的四维相似金字塔，获取了四维相似空间中每个点的完整分布信息。**SSD**通过相似学习模块(**SLM**)在相似金字塔上应用高效的中心-枢轴四维卷积，将不同的相似分布映射到不同的预测密度值，从而获得准确的计数。此外还通过特征交叉增强模块(**FCE**)相互增强查询特征和样本特征，以提高特征匹配的准确性。

![](https://pic.imgdb.cn/item/66860156d9c307b7e9943675.png)

作者观察到对于匹配样本，每个部分之间的相似性分布特征是不同的，例如对于物体中心、边缘和背景，样本中目标中心的相似度分布从中心位置向周围区域逐渐减小，而边缘的相似度分布在不同位置呈现变化。另一方面，背景在除背景区域外的所有位置上的相似性值通常较低。

![](https://pic.imgdb.cn/item/66860252d9c307b7e996b269.png)

**SSD**框架包括特征提取模块、特征交叉增强模块、相似度金字塔计算模块、相似度学习模块与回归解码器模块组成。

![](https://pic.imgdb.cn/item/668608bcd9c307b7e9a05e37.png)

特征提取模块使用冻结的**ResNet-50**从查询图像中提取金字塔特征；对于特征金字塔中的每一层，使用**RoI Align**方法提取$K$个目标样本的特征。将$K$个样本特征$F_l^q$与同一层次的查询特征$F_l^s$一起输入到特征交叉增强模块，得到增强后的特征$E_l^q,E_l^s$。

同一类别查询特征中目标特征的分布往往是不均匀的，使用原始特征直接匹配和计数会导致每个目标的密度值不同。特征交叉增强模块(**Feature Cross Enhancement, FCE**)旨在使查询中的目标特征更接近样本特征，同时也使样本特征更接近所有目标特征的中心位置。通过增强特定类别的物体特征的相似度，该模型能够生成更准确的密度值。

在**FCE**模块中，输入特征$F_l^q,F_l^s$通过卷积层联合变换为$V^q,V^s$；再经过另外两个卷积层分别变换为$Q,K$，并计算注意力矩阵$A_l=softmax(Q^TK)$。之后分别增强特征$F_l^q,F_l^s$：

$$
E_l^q = F_l^q + MLP(F_l^q) \odot Conv(V^sA_l^T) \\
E_l^s = F_l^s + MLP(F_l^s) \odot Conv(V^qA_l)
$$

对$K$个样本进行划分，每个样本对应一组特征金字塔组合$E_l^q,E_{l,k}^s$，进行余弦乘法得到相似矩阵:

$$
S_{l,k}(x^q,x^s) = ReLU\left( \frac{E_l^q(x^q)\cdot E_{l,k}^s(x^s)}{||E_l^q(x^q)||\cdot ||E_{l,k}^s(x^s)||} \right)
$$

将同一大层$L_p$的相似度矩阵串联起来，然后将它们输入到相似度学习模块中，产生一个融合的特征图$S_{L_p}$。

![](https://pic.imgdb.cn/item/668608f0d9c307b7e9a0af08.png)

相似度学习模块（**Similarity Learning Module, SLM**）采用中心-枢轴**4D**卷积进行特征提取$S_{L_p}^\prime=f_{L_p}(S_{L_p})$。**4D**卷积仅关注与卷积中心相关的信息，在保持有效性的同时减少了计算开销。**4D**卷积基于卷积核权值对每个四维位置进行张量融合，在四维空间中积分附近的信息，并将该位置的向量转换为相应的输出维数。然后将不同层的特征图进行融合：

$$
S_{L_{p-1}} = f_{L_{p-1}}(upsample(S_{L_p}^\prime)+S_{L_{p-1}}^\prime)
$$

最后将融合特征图输入到回归解码器模块中，得到预测的密度图。回归解码器模块由多个组件模块组成，这些组件模块由卷积层、**ReLU**激活层和上采样层组成；最后通过1 × 1卷积层和**ReLU**激活层。损失函数采用广义损失（**Generalized Loss**）。广义损失通过不平衡最优传输问题直接测量了预测密度图$a$和真实点图$b$之间的距离：
UOT为

$$
\mathcal{L}_{\mathbf{C}} = \min_{\mathbf{P}\in\mathbb{R}_+^{n\times m}} \left\langle \mathbf{C},\mathbf{P}\right\rangle -\epsilon H\left(\mathbf{P}\right) + \tau ||\mathbf{P}\mathbf{1}_m-\mathbf{a}||_2^2 +\tau |\mathbf{P}^T\mathbf{1}_n-\mathbf{b}|_1
$$

其中$$\mathbf{C}\in\mathbb{R}_+^{n\times m}$$是传输代价矩阵,$C_{i,j}$为将密度图从$$\mathbf{x}_i$$搬运到$$\mathbf{y}_j$$的距离。$$\mathbf{P}$$为传输矩阵。令$$\hat{\mathbf{a}} = \mathbf{P}\mathbf{1}_m, \hat{\mathbf{b}}=\mathbf{P}^T\mathbf{1}_n$$。

这个损失包括四部分：
1. 传输损失，目的是将预测的密度图往真实标注靠拢；
2. 熵正则化项，目的是控制密度图的稀疏程度；
3. 希望$$\hat{\mathbf{a}}$$靠近$$\mathbf{a}$$；
4. 希望$$\hat{\mathbf{b}}$$靠近$$\mathbf{b}$$。

某些样本图像可能包含小尺寸或密集分布的物体，导致难以有效区分单个物体，这将导致预测密度图中的重叠密度，从而影响模型性能。为了解决这个问题，在进入主干之前根据样本框的大小动态调整输入图像的大小。这种调整大小是按样本框的尺寸成比例执行的，允许模型更好地识别包含较小目标的样本。计算$K$个样本边界框$B$的平均长度和宽度$\overline{B}$，如果平均长度或宽度低于阈值$γ$，计算图像扩展的尺度:

$$
scale = \frac{\gamma - \min(\overline{B})}{\eta} + 1
$$

最后在输入模型之前，将图像尺寸和样本边界框$B$同时按比例值**scale**进行调整。

实验结果表明，**SSD**方法在**FSC-147**数据集上的少样本计数性能始终优于现有的方法。

![](https://pic.imgdb.cn/item/66860ec2d9c307b7e9a9d822.png)

