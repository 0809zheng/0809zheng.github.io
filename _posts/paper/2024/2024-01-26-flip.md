---
layout: post
title: 'Scaling Language-Image Pre-training via Masking'
date: 2024-01-26
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67adba06d0e0a243d4fedb3c.png'
tags: 论文阅读
---

> 通过掩码提升语言-图像预训练效率.

- paper：[Scaling Language-Image Pre-training via Masking](https://arxiv.org/abs/2212.00794)

## 0. TL; DR

本文介绍了 **Fast Language-Image Pre-training（FLIP）**，这是一种用于训练 **CLIP** 模型的高效方法。**FLIP** 通过在训练过程中随机遮盖图像块，显著减少了计算量，同时允许在相同的训练时间内处理更多的图像-文本对。实验表明，**FLIP** 在零样本分类、图像检索等下游任务上均优于或接近原始 **CLIP** 模型，且训练速度提升超过 **3** 倍。此外，**FLIP** 还探索了模型规模、数据规模和训练时长的扩展行为，为未来大规模视觉-语言学习的研究提供了有价值的参考。

![](https://pic1.imgdb.cn/item/67addd8ad0e0a243d4fee948.png)

## 1. 背景介绍

语言监督的视觉预训练方法（如 **CLIP**）已成为学习视觉表示的强大工具。这些模型通过对比学习的方式，将图像和文本嵌入到同一空间中，从而实现零样本分类、图像生成等多种下游任务。

然而，大规模训练是提升这些模型性能的关键，而训练时间和计算资源的限制成为研究的瓶颈。例如，**CLIP** 原始模型的训练需要数千个 **GPU** 天。本文提出的 **FLIP** 方法通过引入遮盖机制，有效减少了计算量，同时保持了模型性能，显著提升了训练效率。

## 2. FLIP 模型

**FLIP** 的核心思想是在训练过程中随机遮盖图像块，从而减少计算量并提高训练速度。

![](https://pic1.imgdb.cn/item/67adddbed0e0a243d4fee94b.png)

**FLIP** 使用 **Vision Transformer（ViT）**作为图像编码器，将图像划分为非重叠的块。在训练时，随机遮盖掉大部分（如 **50%** 或 **75%**）的块，仅对可见块进行编码。这种遮盖方式将图像编码的时间复杂度从 $O(n)$ 降低到 $O(n×(1−m))$，其中 $m$ 是遮盖比例。例如，遮盖 **50%** 的块可以将计算量减少一半，同时允许使用两倍的批量大小，而不会显著增加内存占用。

**FLIP** 也尝试对文本进行遮盖，但效果有限。文本编码器通常较小，且文本序列较短，因此遮盖文本带来的计算量减少并不显著。实验表明，遮盖 **50%** 的文本会降低模型性能，而优先遮盖填充词可以减少这种性能下降。

**FLIP** 的训练目标是最小化对比损失函数，通过比较图像和文本嵌入的相似性来学习表示。与 **MAE** **不同，FLIP** 不使用重建损失，因为对比学习本身已经足够有效。实验表明，去掉重建损失可以进一步提升训练速度，同时保持性能。

尽管 **FLIP** 在训练时使用遮盖，但在推理时可以直接应用于完整的图像。为了减少训练和推理之间的分布差异，**FLIP** 还引入了无遮盖微调步骤，即在训练的最后阶段，将遮盖比例设置为 **0%**，继续训练少量步骤。这一策略可以进一步提升模型性能。

![](https://pic1.imgdb.cn/item/67ade0c7d0e0a243d4feea15.png)

## 3. 实验分析

**FLIP** 在 **ImageNet-1K** 零样本分类任务上达到了 **68.0%** 的准确率，优于或接近原始 **CLIP** 模型。在其他多个分类数据集上，**FLIP** 也表现出色，例如在 **Food101** 上达到了 **89.3%** 的准确率。

![](https://pic1.imgdb.cn/item/67ade03bd0e0a243d4fee9f1.png)
![](https://pic1.imgdb.cn/item/67ade047d0e0a243d4fee9f6.png)


在 **Flickr30k** 和 **COCO** 数据集上，**FLIP** 的图像/文本检索性能优于所有 **CLIP** 对照组，**R@1** 指标分别达到了 **89.1%** 和 **60.2%**。

![](https://pic1.imgdb.cn/item/67ade072d0e0a243d4fee9fd.png)

**FLIP** 探索了模型规模、数据规模和训练时长的扩展行为。结果表明，模型规模和数据规模的扩展均能显著提升性能，而训练时长的扩展收益较小。例如，使用更大的 **ViT-H** 模型和 **2B** 数据时，零样本分类准确率提升至 **78.8%**。

![](https://pic1.imgdb.cn/item/67ade013d0e0a243d4fee9ea.png)
![](https://pic1.imgdb.cn/item/67ade029d0e0a243d4fee9ef.png)