---
layout: post
title: 'GIT: A Generative Image-to-text Transformer for Vision and Language'
date: 2024-01-15
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67a18d9ad0e0a243d4fbc082.png'
tags: 论文阅读
---

> GIT：视觉和语言的通用图像到文本Transformer.

- paper：[GIT: A Generative Image-to-text Transformer for Vision and Language](https://arxiv.org/abs/2205.14100)

## 0. TL; DR

本文介绍了一种名为**GIT（Generative Image-to-text Transformer）**的模型，旨在统一处理视觉-语言任务，如图像/视频描述生成和问答。**GIT**通过简化架构，仅使用一个图像编码器和一个文本解码器，并在大规模图像-文本对上进行预训练，取得了多项任务的最新最佳性能。此外，**GIT**还首次在**TextCaps**任务上超越了人类表现，并提出了一种基于生成的图像分类新方案。

## 1. 背景介绍

近年来，视觉-语言（**Vision-Language，VL**）预训练取得了显著进展，尤其是在大规模图像-文本对数据上的学习。然而，现有的生成模型通常结构复杂，依赖于外部模块（如目标检测器、**OCR**等）。这些模型在预训练和微调阶段的网络架构不一致，导致任务特定的适配变得复杂。为了克服这些问题，本文提出了**GIT**模型，它通过一个图像编码器和一个文本解码器，将输入图像映射到相关的文本描述，并通过语言建模任务进行预训练。

## 2. GIT 模型

**GIT**的架构包括一个图像编码器和一个文本解码器。图像编码器基于对比学习预训练的模型（如**Florence/CoSwin**），输出一个紧凑的二维特征图，然后被展平并投影到文本解码器的输入维度。文本解码器是一个自回归**Transformer**模块，用于预测文本描述。在预训练阶段，模型的任务是将输入图像映射到完整的相关文本描述。

![](https://pic1.imgdb.cn/item/67a1922ed0e0a243d4fbc10e.png)

**GIT**的预训练任务是语言建模（**LM**），即预测图像相关的文本描述。与掩码语言建模（**MLM**）不同，**LM**在每次迭代中可以预测所有文本标记，这在大规模预训练数据上更为高效。预训练的目标是最小化以下损失函数：

$$
L=\frac{1}{N+1}\sum_{i=1}^{N+1}CE(y_i,p(y_i|I,\{y_j,j=0,\ldots,i-1\}))
$$

其中，$CE$是带有标签平滑的交叉熵损失，$I$ 是输入图像，$y_i$是文本标记。

需要注意的是，在文本解码器中，图像**token**之间可以交换信息，但是看不到文本**token**，文本**token**只可以看到视觉**token**以及之前的文本**token**，而看不到之后的文本**token**，即如图所示，**1**代表“可看见”，**0**代表“不可看见”。

![](https://pic1.imgdb.cn/item/67a1931ed0e0a243d4fbc129.png)

作者指出，除了视觉和文本**token**连接之外，也可以通过**cross attention**的方式将视觉信息注入文本。实验表明在数据较小时，**cross-attention**更有效，但是数据量较大的时候，图像**token**和文本**token**连接效果更好。

在微调阶段，**GIT**可以根据不同的下游任务进行调整。对于图像描述任务，模型直接使用语言建模任务进行微调。对于视觉问答（**VQA**）任务，问题被作为文本前缀，答案以自回归方式生成。此外，**GIT**还提出了一种基于生成的图像分类方案，将类别名称作为图像的“描述”，并直接预测类别标签。

## 3. 实验分析

**GIT**在多个图像/视频描述和问答任务上进行了评估，包括**COCO、nocaps、TextCaps、VizWiz-Captions、VQAv2、TextVQA、VizWiz-VQA、ST-VQA、OCR-VQA、MSVD、MSRVTT、VATEX、TVC、MSVD-QA、MSRVTT-QA**和**TGIF-Frame**等。评估指标包括**CIDEr、BLEU、METEOR、ROUGE-L、SPICE**等。

![](https://pic1.imgdb.cn/item/67a194acd0e0a243d4fbc148.png)

对于图像描述任务，在**COCO**数据集上，**GIT**取得了**148.8**的**CIDEr**分数，超越了之前的最佳模型（**138.7**）。在**nocaps**数据集上，**GIT**的**CIDEr**分数达到了**123.4**，超越了人类表现（**120.6**）。在**TextCaps**任务上，**GIT**首次超越了人类表现，**CIDEr**分数为**138.2**，比人类的**125.5**高出**12.7**分。

![](https://pic1.imgdb.cn/item/67a19514d0e0a243d4fbc152.png)

对于视觉问答任务，在**VQAv2**任务上，**GIT**的准确率为**78.81%**，略低于最佳模型（**82.3%**）。然而，**GIT**在**VizWiz-VQA**任务上取得了**67.5%**的准确率，超越了之前的最佳模型（**65.4%**）。在**ST-VQA**任务上，**GIT**的准确率为**69.6%**，与最佳模型持平。

![](https://pic1.imgdb.cn/item/67a1958ad0e0a243d4fbc15b.png)

对于视频描述任务，在**MSVD**数据集上，**GIT**的**CIDEr**分数为**180.2**，超越了之前的最佳模型（**120.6**）。在**MSRVTT**数据集上，**GIT**的**CIDEr**分数为**73.9**，也超越了之前的最佳模型（**60**）。在视频问答任务**MSVD-QA**上，**GIT**的准确率为**56.8%**，超越了之前的最佳模型（**48.3%**）。

![](https://pic1.imgdb.cn/item/67a195ecd0e0a243d4fbc15f.png)

![](https://pic1.imgdb.cn/item/67a195fcd0e0a243d4fbc161.png)

实验表明，随着预训练数据规模的增加，模型性能显著提升。例如，在**TextCaps**任务上，使用**0.8B**图像-文本对预训练的**GIT**模型（**CIDEr**分数**138.2**）显著优于使用**10M**对预训练的模型（**CIDEr**分数**92.7**）。此外，更大的模型（如**GIT2，5.1B**参数）在更大规模的数据（**10.5B**图像-文本对）上预训练时，性能进一步提升（**CIDEr**分数**145.0**）。

![](https://pic1.imgdb.cn/item/67a19659d0e0a243d4fbc164.png)