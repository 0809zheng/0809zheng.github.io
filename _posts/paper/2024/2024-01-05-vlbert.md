---
layout: post
title: 'VL-BERT: Pre-training of Generic Visual-Linguistic Representations'
date: 2024-01-05
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/679e0cfbd0e0a243d4f977ca.png'
tags: 论文阅读
---

> VL-BERT：通用视觉-语言表示的预训练.

- paper：[VL-BERT: Pre-training of Generic Visual-Linguistic Representations](https://arxiv.org/abs/1908.02265)

## 0. TL; DR

**VL-BERT（Visual-Linguistic BERT）**是一种用于视觉与语言任务的预训练通用表示模型。它基于**Transformer**架构，能够同时处理视觉和语言输入，并通过大规模数据集（如**Conceptual Captions**）进行预训练。**VL-BERT**在多个视觉与语言任务（如视觉常识推理、视觉问答和指代表达理解）上取得了最先进的性能，并在视觉常识推理（**VCR**）基准测试中获得了单模型第一名的成绩。

## 1. 背景介绍

深度网络在计算机视觉和自然语言处理领域取得了巨大成功，其中预训练通用特征表示是关键因素之一。例如，**ImageNet**预训练的卷积神经网络（**CNN**）在图像识别任务中表现出色，而**BERT**等基于**Transformer**的模型在自然语言处理任务中也取得了优异成绩。然而，对于视觉与语言交叉领域的任务（如图像字幕生成、视觉问答和视觉常识推理），缺乏类似的预训练通用特征表示。

以往的方法通常是将针对图像识别和自然语言处理分别预训练的基础网络以任务特定的方式组合起来，然后直接对特定目标任务进行微调。这种方法可能会导致在目标任务数据稀缺时出现过拟合问题，且由于任务特定模型设计，难以从预训练中受益。因此，研究视觉与语言任务的特征设计和预训练具有重要意义。


## 2. VL-BERT 模型

**VL-BERT**扩展了**BERT**模型，使其能够同时处理图像和文本输入。**VL-BERT**的核心是一个多模态**Transformer**注意力模块，输入包括视觉和语言嵌入特征。每个输入元素可以是输入句子中的一个单词，也可以是输入图像中的一个感兴趣区域（**RoI**）。**VL-BERT**的设计目标是适应大多数视觉与语言下游任务。

**VL-BERT**的输入包括视觉和语言**token**。每个**token**的嵌入特征是四种嵌入的总和：标记嵌入、视觉特征嵌入、段嵌入和序列位置嵌入。
- 标记嵌入：语言单词使用**WordPiece**嵌入，视觉元素使用特殊的$[IMG]$标记。在每一个输入序列的开头加入额外的$[CLS]$，文本和图像之间用$[SEP]$来分开，**mask**的文本**token**处用特殊的$[MASK]$来代替，每一个输入序列的结尾处用$[END]$来表示。
- 视觉特征嵌入：视觉元素的外观特征通过**Fast R-CNN RoI Pooling**提取，非视觉元素的区域统一用全局的图像**RoI Pooling**特征来代替；几何嵌入通过计算用图像长宽归一化之后的**RoI**区域左上角、右下角坐标值的正弦和余弦函数得到。
- 段嵌入：用于区分输入元素的来源，例如，**A**和**B**分别表示第一和第二输入句子的单词，**C**表示输入图像的**RoI**。
- 序列位置嵌入：表示输入元素在序列中的顺序，所有视觉元素的序列位置嵌入相同。

![](https://pic1.imgdb.cn/item/679e0eefd0e0a243d4f977fd.png)

**VL-BERT**通过两个预训练任务学习视觉与语言的联合表示：
- 掩码语言建模与视觉线索：类似于**BERT**的掩码语言建模任务，但加入了视觉线索。按照**15%**的概率随机**mask**部分文本，模型需要根据未掩盖的单词和视觉特征预测被掩盖的单词。
- 掩码**RoI**分类与语言线索：模型需要根据其他线索预测被掩盖的**RoI**的类别标签。按照15%的概率随机**mask**部分图像区域，为了避免视觉线索泄露，被掩盖**RoI**的像素在输入图像中被设置为零。

**VL-BERT**在大规模的视觉与语言数据集（如**Conceptual Captions**）和纯文本数据集（如**BooksCorpus**和英文维基百科）上进行预训练。预训练的目标是通过预测随机掩盖的单词或**RoI**来优化模型，从而更好地对齐视觉与语言线索。预训练过程包括以下步骤：
- 输入格式：对于视觉与语言数据集，输入格式为$<Caption, Image>$；对于纯文本数据集，输入格式为$<Text, ∅>$。
- 损失函数：视觉与语言数据集的损失函数包括掩码语言建模和掩码**RoI**分类的损失；纯文本数据集的损失函数为标准的掩码语言建模损失。
- 优化器：使用**Adam**优化器，学习率在前**8000**步内线性升温，然后线性衰减。

**VL-BERT**可以轻松地微调以适应各种下游任务。微调过程包括以下步骤：
- 输入格式：根据任务需求，输入格式可以是$<Caption, Image>$、$<Question, Answer, Image>$等。
- 输出预测：通常使用$[CLS]$元素的最终输出特征进行句子-图像关系级别的预测，或者使用单词或**RoI**的最终输出特征进行单词级别或**RoI**级别的预测。
- 损失函数和训练策略：根据任务需求调整任务特定的损失函数和训练策略。


## 3. 实验分析

**VL-BERT**在**Conceptual Captions**数据集和**BooksCorpus &** 英文维基百科数据集上进行预训练。预训练过程中，**VL-BERT**的参数初始化为**BERT**模型的参数，新添加的参数随机初始化。视觉内容嵌入由**Fast R-CNN + ResNet-101**生成，初始化为在**Visual Genome**上预训练的参数。

实验在三个下游任务上进行微调：
- 视觉常识推理（**VCR**）：要求模型根据给定的图像和问题选择正确的答案并提供理由。**VL-BERT**在**VCR**任务上的输入格式为$<Question, Answer, Image>$。微调过程中，**VL-BERT**的$[CLS]$元素的最终输出特征用于预测答案的正确性。
- 视觉问答（**VQA**）：要求模型根据给定的图像和问题生成或选择正确的答案。**VL-BERT**在**VQA v2.0**数据集上的输入格式为$<Question, Answer, Image>$。微调过程中，**VL-BERT**的$[MASK]$元素的最终输出特征用于预测答案。
- 指代表达理解：要求模型根据给定的自然语言描述定位图像中的对象。**VL-BERT**在**RefCOCO+**数据集上的输入格式为$<Query, Image>$。微调过程中，**VL-BERT**为所有输入**RoI**计算分类分数，并选择得分最高的**RoI**作为指代对象。

![](https://pic1.imgdb.cn/item/679e1262d0e0a243d4f97845.png)

主要实验结果：
- **VL-BERT**在**VCR**任务上取得了**75.8%**的准确率，超过了其他同期工作，如**ViLBERT**和**VisualBERT**。![](https://pic1.imgdb.cn/item/679e12b0d0e0a243d4f9784c.png)
- **VL-BERT**在**VQA**任务上取得了**72.22%**的准确率，接近同期工作的最佳性能。![](https://pic1.imgdb.cn/item/679e12c8d0e0a243d4f97852.png)
- **VL-BERT**在指代表达理解任务上取得了**83.62%**的准确率，超过了其他同期工作。![](https://pic1.imgdb.cn/item/679e12dbd0e0a243d4f97855.png)

