---
layout: post
title: 'Resurrecting Recurrent Neural Networks for Long Sequences'
date: 2024-07-06
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/678778bfd0e0a243d4f480a4.png'
tags: 论文阅读
---

> 复活长序列的循环神经网络.

- paper：[Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/abs/2303.06349)

本文提出了**LRU**（**Linear Recurrent Unit**，线性循环单元），它是既可以并行又可以串行的线性**RNN**，训练和推断都具备高效的优势。**RNN**的隐状态更新可以简略地表示为：

$$
x_t=f(Ax_{t-1}+u_t)
$$

其中$x$是隐状态，$u$是输入。激活函数$f$通常是非线性的。作者发现如果将**Transformer**的**Self Attention**替换为**RNN**的话，线性**RNN**效果才是最好的：

![](https://pic1.imgdb.cn/item/6787796ed0e0a243d4f480d1.png)

去掉激活函数后，**RNN**的隐状态更新公式为：

$$
\begin{aligned}
x_t&=Ax_{t-1}+u_t \\
&=A^2x_{t-2}+Au_{t-1}+u_t \\
& \cdots \\
&= \sum_{k=0}^t A^{t-k}u_k
\end{aligned}
$$

矩阵$A$可以在复数域对角化：

$$
A = P\Lambda P^{-1}
$$

其中$\Lambda$是特征值对角矩阵，$P$是特征向量矩阵。此时有：

$$
\begin{aligned}
x_t&= \sum_{k=0}^t A^{t-k}u_k= \sum_{k=0}^t (P\Lambda P^{-1})^{t-k}u_k\\
& = \sum_{k=0}^t P\Lambda^{t-k} P^{-1}u_k
\end{aligned}
$$

由于特征向量矩阵$P$可以在参数化的学习过程中获得，因此上式简化为：

$$
\begin{aligned}
x_t&=  \sum_{k=0}^t \lambda^{t-k} u_k
\end{aligned}
$$

