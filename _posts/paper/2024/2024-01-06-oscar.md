---
layout: post
title: 'Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks'
date: 2024-01-06
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/679e1699d0e0a243d4f978b8.png'
tags: 论文阅读
---

> Oscar：视觉-语言任务的目标语义对齐预训练.

- paper：[Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks](https://arxiv.org/abs/1908.02265)

## 0. TL; DR

**Oscar（Object-Semantics Aligned Pre-training）**是一种新型的视觉与语言预训练方法，通过在图像中检测到的目标标签作为锚点，显著简化了图像和文本语义对齐的学习过程。**Oscar**在大规模的文本-图像对数据集上进行预训练，并在多个视觉与语言任务（如视觉问答、图像-文本检索和图像字幕生成）上取得了新的最佳性能。该模型通过引入目标标签作为语义对齐的桥梁，有效地解决了现有方法中图像区域和文本之间对齐信息缺失的问题。

## 1. 背景介绍

视觉与语言任务（**V+L**）的目标是让计算机能够理解图像内容并生成或响应自然语言。近年来，预训练方法在自然语言处理和计算机视觉领域取得了巨大成功，但在视觉与语言交叉领域，预训练方法的发展相对滞后。

现有的视觉与语言预训练（**VLP**）模型通常将图像区域特征和文本特征简单拼接后输入模型，并依赖自注意力机制来学习图像与文本之间的语义对齐。然而，这种方法存在两个主要问题：
1. 图像区域特征通常存在重叠和噪声，导致对齐信息模糊；
2. 缺乏显式的对齐监督，使得学习过程成为一种弱监督学习任务。

为了解决这些问题，**Oscar**提出了一种新的预训练方法，通过引入图像中的目标标签作为锚点，来简化图像和文本之间的语义对齐学习。

![](https://pic1.imgdb.cn/item/679e197fd0e0a243d4f97912.png)


## 2. Oscar 模型

**Oscar**的核心是一个基于**Transformer**的多层自注意力模型，它将输入的图像-文本对表示为一个三元组：单词序列、目标标签序列和图像区域特征。这种表示方式允许模型通过目标标签作为锚点，学习图像和文本之间的语义对齐。

![](https://pic1.imgdb.cn/item/679e19bed0e0a243d4f97914.png)

**Oscar**的输入表示：
- 单词序列$w$：来自文本的单词嵌入。
- 目标标签序列$q$：从图像中检测到的目标标签的单词嵌入。
- 图像区域特征$v$：通过**Faster R-CNN**提取的图像区域的视觉和位置特征。

输入模型的三元组可以进一步以两种方式进行表示:

![](https://pic1.imgdb.cn/item/679e1abcd0e0a243d4f97926.png)

其中$x$是**modality view**，用于区分文本和图像的表征，$x^\prime$则是**dictionary view**，用以区分两种不同的语义空间。在两种不同的**view**下，可以设计不同的训练任务。**Oscar**的预训练目标包括两个部分：
- 掩码标记损失（**Masked Token Loss**）：类似于**BERT**中的掩码语言建模任务，按照**15%**的概率随机掩盖输入序列中的某些单词或目标标签（替换为$[MASK]$），并训练模型根据上下文预测这些被掩盖的标记。
- 对比损失（**Contrastive Loss**）：按照**50%**的概率用从数据集中采样的目标标签序列来代替$q$作为负样本，通过比较目标标签和图像区域特征之间的相似性，训练模型区分图像和文本是否匹配。

**Oscar**在包含**650**万文本-图像对的公共语料库上进行预训练。预训练过程中，模型通过上述两个目标函数进行优化，学习图像和文本之间的语义对齐。预训练完成后，模型可以在特定的下游任务上进行微调，以适应不同的视觉与语言任务。

![](https://pic1.imgdb.cn/item/679e1b7fd0e0a243d4f97932.png)

## 3. 实验分析

**Oscar**在多个视觉与语言任务上进行了评估，包括视觉问答（**VQA**）、图像-文本检索、图像字幕生成和自然语言视觉推理（**NLVR2**）等。评估指标根据不同任务而有所不同，例如**VQA**任务使用准确率（**Accuracy**），图像-文本检索任务使用召回率（**R@1, R@5, R@10**）等。


主要实验结果：
- 视觉问答（**VQA**）: **Oscar**在**VQA v2.0**数据集上取得了**73.82%**的准确率，超过了现有的最佳模型（**UNITER large，73.40%**）。
- 图像-文本检索:在**COCO**数据集的图像检索和文本检索任务上，**Oscar**分别取得了**82.8%**和**92.2%**的**R@5**召回率，显著优于现有的最佳模型（**UNITER large，78.4%**和**89.4%**）。
- 图像字幕生成:在**COCO**图像字幕生成任务上，**Oscar**取得了**41.7**的**BLEU@4**分数和**140.0**的**CIDEr**分数，超过了现有的最佳模型（**AoANet，39.5**和**129.3**）。
- 自然语言视觉推理（**NLVR2**）:在**NLVR2**任务上，**Oscar**取得了**80.37%**的准确率，超过了现有的最佳模型（**UNITER large，79.50%**）。

![](https://pic1.imgdb.cn/item/679e1bc8d0e0a243d4f97939.png)

通过**t-SNE**可视化技术，**Oscar**的特征空间显示了图像和文本特征之间的紧密对齐，尤其是在使用目标标签后，同一目标的图像和文本特征之间的距离显著减小。

![](https://pic1.imgdb.cn/item/679e1cf4d0e0a243d4f9794c.png)