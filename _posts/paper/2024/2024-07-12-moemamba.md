---
layout: post
title: 'MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts'
date: 2024-07-12
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67c94358066befcec6decf1e.png'
tags: 论文阅读
---

> MoE-Mamba: 通过混合专家实现高效选择状态空间模型.

- paper：[MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts](https://arxiv.org/abs/2401.04081)

## 0. TL;DR

本文介绍了**MoE-Mamba**，一种结合了**Mamba**状态空间模型和混合专家（**MoE**）技术的高效模型。**MoE-Mamba**在训练效率和性能上均优于原始的**Mamba**模型和**Transformer-MoE**模型。具体来说，**MoE-Mamba**在**2.35**倍更少的训练步数内达到了与**Mamba**相同的性能，同时保持了**Mamba**在推理性能上的优势。通过全面的研究，作者证实了**MoE-Mamba**的改进在不同的模型大小、设计选择和专家数量下都是稳健的。

## 1. 背景介绍

近年来，状态空间模型（**SSMs**）在序列建模领域逐渐崭露头角，成为**Transformer**架构的有力竞争者。**SSMs**的优势在于其线性时间推理、高度并行化的训练以及在长上下文处理任务中的强大性能。**Mamba**作为一种基于**SSM**的模型，通过其选择性机制和硬件感知设计，实现了令人印象深刻的性能，成为挑战注意力机制的**Transformer**架构的有力候选者。

与此同时，混合专家（**MoE**）技术在基于**Transformer**的大规模语言模型中取得了显著的改进。**MoE**通过稀疏激活，能够在增加模型参数的同时，对计算需求的影响最小化。这种特性使得**MoE**模型能够高效地扩展到数万亿参数。鉴于**SSMs**和**MoE**技术各自的优势，作者提出将两者结合，以释放**SSMs**在扩展方面的潜力。


## 2. MoE-Mamba 模型

### （1）Mamba架构

[<font color=blue>Mamba</font>](https://0809zheng.github.io/2024/07/10/mamba.html)是一种基于**SSM**的模型，通过采用高效的工作并行扫描，减轻了递归性质的影响。同时，通过融合**GPU**操作，**Mamba**消除了扩展状态的存储需求。在反向传播过程中，不保存中间状态，而是重新计算，从而降低了内存需求。**Mamba**的优势在推理过程中尤为明显，因为其计算复杂度较低，且内存使用不依赖于上下文长度。

![](https://pic1.imgdb.cn/item/67c9445f066befcec6ded20d.png)

**Mamba**模块如图所示。通过卷积映射和门控映射把输入特征维度扩大$E$倍，然后通过下列**SSM**计算：

$$
\begin{aligned}
\mathbf{x}^\prime(t) &= A \mathbf{x}(t) + B \mathbf{u}(t) \\
\mathbf{y}(t) &= C \mathbf{x}(t) \\
\end{aligned}
$$

### （2）MoE架构

作者采用了**Switch Transformer MoE**设计。**MoE**层由多个专家网络组成，每个专家网络是一个可训练的前馈网络。对于每个**token**嵌入，计算得分并使用**softmax**进行归一化。然后，选择得分最高的专家来处理该**token**。在批量执行中，如果某个专家被超过一定数量的**token**选择，则会丢弃多余的**token**。为了鼓励**token**在专家之间的均匀分布，添加了负载平衡损失。

### （3）MoE-Mamba架构

**MoE-Mamba**通过在**Mamba**层之间交错插入**MoE**层，实现了无条件处理和条件处理的分离。**Mamba**层负责高效地将整个序列上下文集成到内部表示中，而**MoE**层则应用最相关的专家（即参数子集）来处理每个**token**。这种设计借鉴了一些基于**MoE**的模型，通过交替使用原始层和**MoE**层来实现条件和无条件处理的结合。

![](https://pic1.imgdb.cn/item/67c94566066befcec6ded3da.png)


## 3. 实验分析

作者使用了**C4**数据集进行模型训练，任务是下一个**token**预测，使用交叉熵作为损失函数，使用了训练对数困惑度作为比较指标。

实验结果显示，**MoE-Mamba**在训练效率和性能上均优于原始的**Mamba**模型和**Transformer-MoE**模型。具体来说，**MoE-Mamba**在**2.35**倍更少的训练步数内达到了与**Mamba**相同的性能。此外，**MoE-Mamba**的表现优于对应的**Transformer-MoE**，这进一步证明了**Mamba**是**Transformer**的有竞争力的替代品。

![](https://pic1.imgdb.cn/item/67c9440a066befcec6ded10d.png)

实验结果表明，随着专家数量的增加，模型的性能单调提高。当专家数量达到**32**时，模型取得了最佳性能。

![](https://pic1.imgdb.cn/item/67c94604066befcec6ded4b4.png)

作者研究了**Mamba**层和**MoE**层之间活动参数的最佳比例。结果显示，增加**Mamba**层的活动参数可以提高性能，但在达到一定比例后，收益递减。

![](https://pic1.imgdb.cn/item/67c94640066befcec6ded54a.png)

作者比较了不同的**MoE-Mamba**设计，包括交错设计和平行设计。结果显示，交错设计在所有测试设置中均优于平行设计。

![](https://pic1.imgdb.cn/item/67c9467f066befcec6ded589.png)

