---
layout: post
title: 'Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers'
date: 2024-01-08
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67a03a6fd0e0a243d4f9a5e6.png'
tags: 论文阅读
---

> Pixel-BERT：使用深度多模态Transformer对齐图像像素和文本.

- paper：[Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers](https://arxiv.org/abs/1909.11740)

## 0. TL; DR

本文提出了一种名为**Pixel-BERT**的模型，旨在通过深度多模态**Transformer**架构直接从图像像素和文本对中学习视觉和语言的联合嵌入表示。与以往依赖于区域级图像特征的方法不同，**Pixel-BERT**通过像素级对齐解决了视觉和语言任务中语义信息不匹配的问题，同时避免了边界框注释的依赖。实验表明，**Pixel-BERT**在视觉问答（**VQA**）、图像-文本检索和自然语言视觉推理（**NLVR2**）等下游任务中均取得了最先进的性能。

## 1. 背景介绍

随着自然语言处理领域自监督学习的成功，跨模态学习尤其是视觉与语言的结合逐渐成为研究热点。早期的视觉问答（**VQA**）和图像描述生成任务通常使用预训练的**CNN**模型提取图像特征，但这种方法存在语义信息丢失的问题。后来的研究引入了基于目标检测模型（如**Faster R-CNN**）提取的区域级视觉特征，虽然性能有所提升，但这些特征仍然受限于特定任务的类别，无法充分利用图像中的视觉信息。例如，区域检测模型可能无法准确描述物体的形状、空间关系以及场景情感等更广泛的语义信息。因此，本文提出**Pixel-BERT**模型，直接从图像像素出发，学习视觉和语言的联合表示，以解决上述问题。


## 2. Pixel-BERT 模型

### （1）模型结构

**Pixel-BERT**模型由三部分组成：基于**CNN**的视觉编码器、基于**BERT**的词级嵌入和多模态**Transformer**模块，整体架构如图所示。

![](https://pic1.imgdb.cn/item/67a03bd3d0e0a243d4f9a60d.png)

**⚪ 视觉特征嵌入**

**Pixel-BERT**使用**CNN**作为视觉编码器，直接从图像像素中提取特征。给定输入图像$I$，首先通过**CNN**骨干网络（如**ResNet**）提取特征，然后将特征沿空间维度展平，得到像素级特征表示。假设展平后的特征为$$v= \{v_1, v_2, ..., v_k\} ∈ R^d$$，其中$k$为特征像素的数量。最终的视觉嵌入特征通过添加语义嵌入向量$s_v$来区分视觉和语言嵌入，表示为：

$$
\hat{v}_i = v_i + s_v
$$
 
其中，$s_v$是所有像素共享的语义嵌入向量，可以看作是与**CNN**骨干网络结合的偏置项。

**⚪ 语言特征嵌入**

语言特征的处理与**BERT**类似。首先将句子分割为单词序列，然后使用**WordPiece**工具将每个单词分割为子词（**token**），并通过嵌入矩阵将每个**token**嵌入为向量。假设嵌入后的序列为$$w= \{w_1, w_2, ..., w_n\} ∈ R^d$$，其中$n$为序列长度，$d$为嵌入维度。为了编码位置信息，还添加了位置嵌入$p_i$，最终的语言表示为：

$$
\hat{w}_i = LayerNorm(w_i + p_i+s_w)
$$

其中，$p_i$是第$i$个单词的位置嵌入向量。$s_w$是语义嵌入向量。在实际实现中，$s_w$项被省略。

**⚪ 多模态Transformer模块**

将视觉特征和语言特征组合后，输入到**Transformer**模块中进行联合学习。**Transformer**模块能够建立视觉和语言模态之间的密集连接，包括模态内部（图像-图像、句子-句子）和模态之间（图像-句子）的连接。最终输入到**Transformer**的序列形式为：

$$
\{[CLS],\hat{w}^1,\hat{w}^2,...,\hat{w}^n,[SEP],\hat{v}^1,\hat{v}^2,...,\hat{v}^k\}
$$

其中，$[CLS]$和$[SEP]$是特殊标记，分别用于学习联合分类特征和指定**token**长度。

### （2）预训练任务

为了学习通用的视觉和语言表示，**Pixel-BERT**采用了两种预训练任务：掩码语言模型（**MLM**）和图像-文本匹配（**ITM**）。

**⚪ 掩码语言模型（MLM）**

**MLM**任务的目标是预测被随机掩盖的**token**。具体来说，以**0.15**的概率随机掩盖语言**token**，并要求模型根据其他非掩盖的**token**和视觉**token**来预测掩盖的**token**。**MLM**的损失函数为：

$$
L_{MLM}(θ) = -E_{(w,I)∼D} \log P_{\theta}(w_i | w_{/m}, I)
$$

其中，$w_m$表示掩盖的**token**，$θ$是模型参数，$P$是生成的似然函数。与单模态**BERT**不同，**Pixel-BERT**可以利用视觉信息来帮助预测掩盖的**token**，从而建立视觉和语言模态之间的映射。

**⚪ 图像-文本匹配（ITM）**

**ITM**任务的目标是判断图像和文本是否匹配。在训练过程中，将数据集中的图像-文本对作为正样本，随机打乱后得到的不匹配对作为负样本。通过二分类器对$[CLS]$标记的联合嵌入特征进行分类，判断输入的图像和文本是否匹配。**ITM**的损失函数为：

$$
L_{ITM}(θ) = -E_{(w,I)∼D} [y\log S_{\theta}(w,I) + (1-y)\log(1-S_{\theta}(w,I))]
$$


**⚪ 随机像素采样机制**

为了提高视觉特征学习的鲁棒性并避免过拟合，**Pixel-BERT**引入了随机像素采样机制。在每次迭代中，从提取的像素特征中随机采样一部分输入到**Transformer**中。这种机制可以鼓励模型从不完整的视觉输入中学习语义知识，从而增强鲁棒性。此外，减少输入元素的数量还可以降低计算成本并加速训练过程。在实验中，每个输入图像随机采样**100**个像素特征。

## 3. 实验分析

**Pixel-BERT**在两个大规模图像-文本数据集上进行预训练：**MS-COCO**和**Visual Genome**。**MS-COCO**包含**106K**张图像和**533K**个句子，**Visual Genome**包含**101K**张图像和**5.06M**个句子。下游任务包括视觉问答（**VQA**）、自然语言视觉推理（**NLVR2**）和图像-文本检索任务，分别在**VQA 2.0**、**NLVR2**、**Flickr30K**和**MS-COCO**数据集上进行评估。详细的实验数据集统计信息如表所示。

![](https://pic1.imgdb.cn/item/67a04434d0e0a243d4f9a6ea.png)


在预训练阶段，**Pixel-BERT**使用**ResNet-50**或**ResNeXt-152**作为视觉骨干网络，并采用**SGD**和**AdamW**优化器分别优化**CNN**骨干网络和**Transformer**模块。预训练在**64**个**NVIDIA Tesla V100 GPU**上进行，共**40**个**epoch**。

**VQA**任务的目标是根据输入的图像和问题预测答案。**Pixel-BERT**将该任务建模为分类问题，并通过二元交叉熵损失进行训练。实验结果如表所示，**Pixel-BERT**在**VQA**任务中取得了显著的性能提升。使用**ResNet-50**作为视觉骨干网络时，**Pixel-BERT**在**test-dev**数据集上取得了**71.35**的分数，超过了**ViLBERT**和**VisualBERT**等方法。当使用**ResNeXt-152**时，**Pixel-BERT**在**test-dev**和**test-std**数据集上分别取得了**74.45**和**74.55**的分数，甚至超过了使用**24**层**Transformer**的**UNITER（Large）**模型。这表明**Pixel-BERT**通过像素级对齐能够更好地学习视觉和语言的联合表示。

**NLVR2**任务要求模型判断语言描述是否与给定的图像对相关。**Pixel-BERT**通过将两个图像-语言对输入到模型中，从$[CLS]$标记中提取两个嵌入向量，并将它们的连接用于学习分类器。实验结果如表所示，**Pixel-BERT**在**dev**和**test-P**数据集上分别取得了**76.5**和**77.2**的准确率，超过了**LXMERT**和**UNITER**等方法。这表明**Pixel-BERT**能够适应类似的输入格式，并在复杂的视觉推理任务中表现出色。

![](https://pic1.imgdb.cn/item/67a044c9d0e0a243d4f9a6ee.png)

图像-文本检索任务包括图像到文本（**TR**）和文本到图像（**IR**）两个子任务。**Pixel-BERT**将该任务建模为排名问题，并通过**softmax**交叉熵损失进行训练。实验结果如表所示，在**Flickr30K**数据集上，**Pixel-BERT**在**TR**和**IR**任务中分别取得了**75.7**和**59.8**的**Recall@1**分数；在**MS-COCO**数据集上，**Pixel-BERT**在**1K**测试集和**5K**测试集上均取得了显著的性能提升，与**Unicoder-VL**和**UNITER**相比，**Recall@1**分数分别提高了至少**0.6**和**1.9**。这表明**Pixel-BERT**能够更好地学习图像和文本之间的全局描述关系。

![](https://pic1.imgdb.cn/item/67a04563d0e0a243d4f9a6f3.png)
![](https://pic1.imgdb.cn/item/67a04570d0e0a243d4f9a6f4.png)

为了验证**Pixel-BERT**中各个组件的有效性，进行了消融实验。实验结果如表所示。实验表明，**MLM**和**ITM**预训练任务对下游任务的性能提升至关重要。**MLM**任务在**VQA**任务中带来了**7.6**的性能提升，在**NLVR2**任务中不可或缺；**ITM**任务在检索任务中贡献了至少**13.0**的性能提升。此外，随机像素采样机制在**VQA**、检索任务和**NLVR2**任务中分别带来了**0.5、2.0**和**0.4**的性能提升，证明了其有效性。使用更强大的**ResNeXt-152**作为视觉骨干网络时，**Pixel-BERT**的性能进一步提升，表明视觉特征的表示能力对跨模态学习的重要性。

![](https://pic1.imgdb.cn/item/67a045b5d0e0a243d4f9a6fc.png)

为了进一步验证**Pixel-BERT**是否能够通过跨模态注意力学习到准确的视觉表示，作者对**MS-COCO**验证集中的部分样本进行了注意力图可视化。如图所示，尽管没有使用空间监督（例如边界框注释）来指导注意力学习，但**Pixel-BERT**能够很好地学习到与文本对应的视觉区域。例如，在案例（**A**）中，“**dog**”、“**grass**”和“**frisbee**”等**token**的响应区域正确地分布在图像的相应位置；在案例（**B**）中，“**cutting**”这一动词能够关注到与“**cutting**”动作相关的区域；在案例（**C**）中，“**room**”这一**token**能够关注到图像中难以用边界框表示的区域。这表明**Pixel-BERT**通过像素级对齐能够有效地学习到视觉和语言之间的语义关联。

![](https://pic1.imgdb.cn/item/67a045f8d0e0a243d4f9a702.png)