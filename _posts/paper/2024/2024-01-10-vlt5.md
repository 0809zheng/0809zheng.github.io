---
layout: post
title: 'Unifying Vision-and-Language Tasks via Text Generation'
date: 2024-01-10
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67b59f1ed0e0a243d400c48e.png'
tags: 论文阅读
---

> 通过文本生成统一视觉和语言任务.

- paper：[Unifying Vision-and-Language Tasks via Text Generation](https://arxiv.org/abs/2102.02779)

## 0. TL; DR

本文提出了一种统一的框架，通过文本生成的方式解决视觉-语言任务。该框架基于**T5**和**BART**预训练语言模型，通过扩展其文本编码器以包含图像区域嵌入，将多种视觉-语言任务（如视觉问答、指代表达理解、图像描述生成等）统一为多模态条件文本生成任务。在**7**个流行的视觉-语言基准测试中，该方法达到了与任务特定的最新模型相当的性能，并且在多任务学习设置下，使用单一架构和参数集实现了与单独优化的单任务模型相似的性能。

## 1. 背景介绍

视觉-语言学习通常需要为每个任务设计特定的架构和目标。例如，视觉问答（**VQA**）需要一个多标签答案分类器，指代表达理解需要一个区域评分器，图像描述生成需要一个语言解码器等。这些任务所需的推理技能有显著重叠，且其标签都可以用文本轻松表达。然而，现有的方法通常需要为每个预训练或下游任务设计特定的架构，这增加了任务的复杂性。

为了简化这一过程，本文提出了一种统一的框架，通过文本生成的方式解决不同的视觉-语言任务。该框架基于预训练的语言模型**T5**和**BART**，通过扩展其文本编码器以包含图像区域嵌入，将所有任务统一为多模态条件文本生成任务。

![](https://pic1.imgdb.cn/item/67a0827fd0e0a243d4f9b605.png)

## 2. 模型介绍

本文提出的框架将视觉和语言问题统一为多模态条件文本生成任务。基于两个预训练的**Transformer**语言模型**T5Base**和**BARTBase**，分别扩展为**VL-T5**和**VL-BART**。具体来说，通过将图像区域嵌入作为额外输入，扩展了它们的文本编码器。整体架构如图所示。

![](https://pic1.imgdb.cn/item/67a0819bd0e0a243d4f9b5f0.png)

输入图像$v$通过**Faster R-CNN**模型表示为$n=36$个目标区域，该模型在**Visual Genome**数据集上训练，用于目标和属性分类。每个图像区域编码为四种特征的和：(i) **RoI**目标特征；(ii) **RoI**边界框坐标；(iii) 图像**id**；(iv) 区域**id**。**RoI**特征和边界框坐标通过线性层编码，而图像**id**和区域**id**通过学习嵌入编码。图像**id**用于区分不同图像的区域，区域**id**用于标记特定区域。

为了适应不同任务，作者在原始输入文本前添加不同的前缀。例如，对于视觉问答任务，输入文本为“**vqa: [问题]**”，对于指代表达理解任务，输入文本为“**visual grounding: [指代表达]**”。这些增强后的输入文本被标记化并编码为学习嵌入。

使用**Transformer**编码器-解码器架构编码视觉和文本输入，并生成标签文本。双向多模态编码器由多个**Transformer**块组成，每个块包含自注意力层和全连接层。解码器与编码器类似，但每个块额外包含一个交叉注意力层。编码器将文本和视觉嵌入的连接作为输入，并输出它们的上下文化联合表示。然后，解码器迭代地关注之前生成的**token**和编码器输出，并预测未来文本**token**的概率。

预训练在多任务设置下进行，包括多模态语言建模、视觉问答、图像-文本匹配、视觉定位和基于区域的描述生成等任务。这些任务的输入和输出格式如表所示。
- 多模态语言建模：对于**VL-T5**，随机掩盖15%的输入文本**token**，并用[TEXT_i]替换连续文本片段。对于**VL-BART**，随机掩盖30%的输入文本**token**，并用[Mask]标记替换。然后预测被掩盖的文本。
- 视觉问答：直接生成问题的答案文本，而不是将其视为预定义答案候选集上的分类任务。
- 图像-文本匹配：模型需要验证文本是否对应于图像。将图像及其标题视为正样本对，以**50%**的概率随机采样另一个训练图像的标题以创建负样本对。模型预测对应关系为“**true**”或“**false**”。
- 视觉定位：给定区域描述，模型预测相关目标区域的**id**，这使得任务自然地符合文本生成目标。
- 基于区域的描述生成：给定目标区域的**id**（表示图像区域）作为文本输入，模型生成该图像区域的文本描述。

![](https://pic1.imgdb.cn/item/67a0837ad0e0a243d4f9b61e.png)

## 3. 实验分析

预训练数据从**MS COCO**和**Visual Genome**图像中聚合而来。这些数据集的标题用于多模态语言建模任务，**COCO**标题还用于图像-文本匹配任务。此外，还使用了三个视觉问答数据集（**VQA v2.0**、**GQA**平衡版本和**Visual7W**）作为预训练任务。预训练数据集包含**9.18M**图像-文本对，覆盖**180K**不同的图像。

主要实验结果：
- 视觉问答（**VQA**和**GQA**）：**VL-T5**和**VL-BART**在**VQA**和**GQA**任务上达到了与现有方法相当的性能。与基于分类的方法相比，生成式方法在开放性问题上表现更好，尤其是在训练中未见过的答案上。实验表明，生成式模型在开放性问题上的性能比基于分类的模型高出**6-6.2**个百分点。
- 自然语言视觉推理（**NLVR2**）：**NLVR2**任务要求模型判断自然语言陈述是否适用于两幅图像。**VL-T5**在最简单的编码设置（**Triplet**）下与**UNITER**模型表现相当，但在更复杂的编码设置（**Pair**和**Pair-biattn**）下仍有差距。![](https://pic1.imgdb.cn/item/67a08444d0e0a243d4f9b630.png)
- 指代表达理解（**RefCOCOg**）：在**RefCOCOg**数据集上，**VL-T5**达到了与**UNITER**模型相当的性能，尽管未达到最佳性能，但表明指代表达理解可以有效地表述为文本生成任务。
- 视觉常识推理（**VCR**）：**VCR**任务要求模型进行常识推理以选择正确答案和理由。**VL-T5**在经过第二阶段领域预训练后，达到了与**UNITER**模型相当的性能。
- 图像描述生成（**COCO Captioning**）：在**COCO Captioning**任务上，**VL-T5**和**VL-BART**达到了与现有方法相当的性能。实验表明，使用目标标签作为额外文本输入可以显著提升性能。
- 多模态机器翻译（**Multi30K En-De**）：在**Multi30K**数据集上，**VL-T5**和**VL-BART**在多模态机器翻译任务上优于现有方法，尽管未观察到视觉-语言预训练带来的显著提升。

![](https://pic1.imgdb.cn/item/67a08418d0e0a243d4f9b62b.png)

在多任务微调设置下，**VL-T5**使用单一架构和参数集同时训练**7**个任务，达到了与单独优化的单任务模型相当的性能。这表明该框架可以有效地进行多任务学习。