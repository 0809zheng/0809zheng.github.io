---
layout: post
title: 'VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts'
date: 2024-01-14
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67a1857ad0e0a243d4fbb28a.png'
tags: 论文阅读
---

> VLMo：使用模态混合专家的统一视觉语言预训练.

- paper：[VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts](https://arxiv.org/abs/2111.02358)

## 0. TL; DR

本文介绍了一种名为**VLMo（Vision-Language pre-trained Model）**的统一视觉-语言预训练模型。**VLMo**通过引入**Mixture-of-Modality-Experts（MOME）Transformer**架构，联合学习了双编码器和融合编码器，能够在视觉-语言分类任务中作为融合编码器使用，在检索任务中作为双编码器使用。此外，**VLMo**采用了分阶段预训练策略，利用大规模的纯图像和纯文本数据，显著提升了模型的泛化能力。实验结果表明，**VLMo**在多个视觉-语言任务上均取得了最先进的性能。

## 1. 背景介绍

视觉-语言（**Vision-Language，VL**）预训练的目标是从大规模的图像-文本对中学习通用的跨模态表示。这些预训练模型通常通过图像-文本匹配、对比学习、掩码区域分类等任务来聚合和对齐视觉和语言信息。预训练完成后，模型可以直接在下游的视觉-语言任务上进行微调，如视觉问答（**VQA**）、视觉推理等。

现有的**VL**预训练模型主要分为两大类：双编码器架构和融合编码器架构。双编码器架构（如**CLIP**和**ALIGN**）分别对图像和文本进行编码，通过计算图像和文本特征向量的余弦相似性来实现模态间的交互。这种架构在检索任务中表现出色，但对于复杂的**VL**分类任务，其浅层的模态交互能力不足。另一类融合编码器架构（如**ViLT**）通过多层**Transformer**网络融合图像和文本表示，虽然在分类任务上表现优异，但在检索任务中由于需要联合编码所有可能的图像-文本对，导致推理速度较慢。

为了结合两种架构的优势，本文提出了**VLMo**模型。**VLMo**通过**MOME Transformer**实现了对不同模态（图像、文本和图像-文本对）的编码，能够在预训练阶段灵活切换为双编码器或融合编码器，从而在检索和分类任务中均表现出色。

## 2. VLMo 模型

**VLMo**的核心是**MOME Transformer**，它通过引入模态专家池来替代标准**Transformer**中的前馈网络（**FFN**）。**MOME Transformer**包含三种模态专家：视觉专家（**V-FFN**）、语言专家（**L-FFN**）和视觉-语言专家（**VL-FFN**）。视觉专家用于编码图像，语言专家用于编码文本，而视觉-语言专家则用于处理图像-文本对，以捕捉更深层次的模态交互。

![](https://pic1.imgdb.cn/item/67a1891dd0e0a243d4fbba48.png)

**MOME Transformer**的计算过程如下：
- 多头自注意力模块（**MSA**）：对输入向量进行自注意力计算，以对齐视觉和语言信息。

$$
H_l^′=MSA(LN(H_{l-1}))+H_{l-1}
$$

- 模态专家选择（**MoME-FFN**）：根据输入向量的模态类型选择相应的专家进行处理。

$$
H_l=MoME-FFN(LN(H_l^′))+H_l^′
$$

 
其中，**MoME-FFN**会根据输入是纯图像、纯文本还是图像-文本对，分别选择**V-FFN**、**L-FFN**或**VL-FFN**进行处理。

**VLMo**通过以下三种预训练任务联合优化：
- 图像-文本对比学习（**Image-Text Contrastive Learning**）：预测图像和文本对是否匹配。通过计算图像和文本的特征向量的相似性，并使用交叉熵损失进行优化。
- 掩码语言建模（**Masked Language Modeling, MLM**）：随机选择文本序列中的部分单词并替换为$[MASK]$标记，模型需要根据上下文和视觉线索预测这些被掩盖的单词。
- 图像-文本匹配（**Image-Text Matching**）：预测图像和文本是否匹配。通过计算$[T_CLS]$标记的最终隐藏向量，并使用交叉熵损失进行二分类。

**VLMo**采用了分阶段预训练策略，充分利用大规模的纯图像和纯文本数据：
- 视觉预训练：在纯图像数据上预训练**MOME Transformer**中的视觉专家和自注意力模块，使用掩码图像建模任务。
- 语言预训练：在纯文本数据上预训练语言专家，使用掩码语言建模任务。
- 视觉-语言预训练：在图像-文本对数据上进行联合预训练，优化上述三种预训练任务。

![](https://pic1.imgdb.cn/item/67a18a28d0e0a243d4fbbc30.png)


## 3. 实验分析

**VLMo**在多个视觉-语言任务上进行了评估，包括视觉问答（**VQA**）、自然语言视觉推理（**NLVR2**）、图像-文本检索等。

![](https://pic1.imgdb.cn/item/67a18ae8d0e0a243d4fbbd8e.png)

在**VQA**和**NLVR2**任务上，**VLMo**取得了最先进的性能。例如，在**NLVR2**任务上，**VLMo-Large**模型达到了**86.86%**的准确率，显著优于其他同类模型。

![](https://pic1.imgdb.cn/item/67a18b17d0e0a243d4fbbdd4.png)

在**COCO**和**Flickr30K**数据集上，**VLMo**作为双编码器使用，展现了与融合编码器模型相当的性能，同时推理速度更快。例如，在**Flickr30K**数据集上，**VLMo-Large**模型的文本检索**R@1**指标达到了**95.3%**，图像检索**R@1**指标达到了**84.5%**。

![](https://pic1.imgdb.cn/item/67a18b62d0e0a243d4fbbe44.png)