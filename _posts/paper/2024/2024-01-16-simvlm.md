---
layout: post
title: 'SimVLM: Simple Visual Language Model Pretraining with Weak Supervision'
date: 2024-01-16
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67a1a53cd0e0a243d4fbc2e8.png'
tags: 论文阅读
---

> SimVLM：弱监督的简单视觉语言模型预训练.

- paper：[SimVLM: Simple Visual Language Model Pretraining with Weak Supervision](https://arxiv.org/abs/2205.14100)

## 0. TL; DR

本文提出了一种名为**SimVLM（Simple Visual Language Model）**的视觉-语言预训练模型。**SimVLM**通过利用大规模弱监督数据，仅使用单一的前缀语言建模（**PrefixLM**）目标进行端到端训练，显著简化了视觉-语言预训练（**VLP**）的复杂性。**SimVLM**在多个视觉-语言任务上取得了最新的最佳性能，包括视觉问答（**VQA**）、图像描述生成、视觉推理等，并且展示了强大的零样本（**zero-shot**）泛化能力。

## 1. 背景介绍

近年来，视觉-语言预训练（**VLP**）在多模态任务上取得了显著进展。然而，现有的**VLP**方法通常需要昂贵的标注数据，如干净的图像描述和区域标签，这限制了模型的可扩展性。此外，这些方法通常需要多个数据集特定的目标函数，使得预训练过程复杂化。为了克服这些问题，本文提出了**SimVLM**，它通过大规模弱监督数据进行预训练，仅使用单一的前缀语言建模目标，显著简化了预训练过程。

## 2. SimVLM 模型

**SimVLM**采用**Transformer**作为骨干网络，支持处理图像和文本输入。图像输入通过卷积层提取上下文化的**patch**特征，然后输入到**Transformer**中。文本输入则通过标准的子词标记化和嵌入层处理。模型结构如下图所示：

![](https://pic1.imgdb.cn/item/67a1a635d0e0a243d4fbc2f7.png)

**SimVLM**的核心是前缀语言建模（**PrefixLM**）。与传统的掩码语言建模（**MLM**）和单向语言建模（**LM**）不同，**PrefixLM**允许模型在前缀序列内进行双向注意力计算，同时对剩余的标记进行自回归分解。具体来说，对于一个图像-文本对，将图像特征序列作为前缀，文本序列作为生成目标。训练目标是最小化以下损失函数：

$$
L_{\text{PrefixLM}}(\theta)=-E_{x\sim D}[\log P_{\theta}(x_{\geq T_p}|x_{<T_p})]
$$

其中，$T_p$是随机选择的前缀长度，$x_{≥T_p}$是需要生成的文本部分，$x_{<T_p}$是前缀部分。

**SimVLM**使用大规模弱监督数据进行预训练，包括**ALIGN**数据集（约**18**亿图像-文本对）和**C4**文本数据集（约**800GB**文本）。这些数据无需额外的预处理或过滤，直接用于训练。

## 3. 实验分析

**SimVLM**在多个视觉-语言任务上进行了评估，包括视觉问答（**VQA**）、图像描述生成（**COCO、nocaps**）、视觉推理（**NLVR2**）、视觉蕴含（**SNLI-VE**）和多模态翻译（**Multi30k**）。评估指标包括**VQA**分数、**CIDEr**分数、**BLEU**分数等。

对于视觉问答（**VQA**），**SimVLM**在**VQA v2**任务上取得了**80.34%**的准确率，超越了之前的最佳模型（**76.60%**）。这表明**SimVLM**在处理复杂的视觉和语言推理任务上具有显著优势。对于图像描述生成，在**COCO**数据集上，**SimVLM**取得了**143.3**的**CIDEr**分数，超越了之前的最佳模型（**140.9**）。在**nocaps**数据集上，**SimVLM**的**CIDEr**分数达到了**110.3**，展示了其在生成新颖概念描述方面的能力。

![](https://pic1.imgdb.cn/item/67a1a7a6d0e0a243d4fbc315.png)

**SimVLM**在零样本图像描述生成任务上表现出色。例如，在**COCO**数据集上，**SimVLM**的零样本性能达到了**32.2**的**CIDEr**分数，少样本微调后提升到**131.3**。

![](https://pic1.imgdb.cn/item/67a1a892d0e0a243d4fbc32e.png)

此外，**SimVLM**还展示了跨模态零样本转移能力，例如在**SNLI-VE**任务上，仅使用文本数据进行微调后，**SimVLM**在视觉-语言任务上取得了**73.56%**的准确率。

![](https://pic1.imgdb.cn/item/67a1a8a1d0e0a243d4fbc331.png)

