---
layout: post
title: 'BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation'
date: 2024-01-22
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67a1d5f0d0e0a243d4fbca23.png'
tags: 论文阅读
---

> BLIP：引导式语言-图像预训练实现统一的视觉-语言理解和生成.

- paper：[BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://arxiv.org/abs/2201.12086)

## 0. TL; DR

**BLIP** 是一种新型的视觉语言预训练框架，能够同时处理视觉语言理解任务（如图像-文本检索、视觉问答）和生成任务（如图像标题生成）。通过引入“多模态混合编码器-解码器”（**Multimodal Mixture of Encoder-Decoder, MED**）架构和“标题生成与过滤”（**Captioning and Filtering, CapFilt**）数据增强方法，**BLIP** 在多个视觉语言任务上取得了 **SOTA** 性能。此外，**BLIP** 在零样本学习场景下表现出色，能够直接迁移到视频语言任务中。

## 1. 背景介绍

视觉语言预训练（**Vision-Language Pre-training, VLP**）近年来在多模态下游任务中取得了显著进展。然而，现有的预训练模型存在两大局限性：
- 模型架构方面：大多数方法要么采用基于编码器的模型（如 **CLIP**），要么采用编码器-解码器模型（如 **SimVLM**）。前者在文本生成任务（如图像标题生成）上不够直接，后者则未被成功应用于图像-文本检索任务。
- 数据方面：大多数 **SOTA** 方法（如 **CLIP、ALBEF、SimVLM**）使用从网络爬取的图像-文本对进行预训练。尽管通过扩大数据集规模获得了性能提升，但网络文本的噪声问题被忽视了，这些噪声数据对视觉语言学习来说并非最优。

为了解决这些问题，**BLIP** 提出了一个统一的 **VLP** 框架，通过引入新的模型架构和数据增强方法，同时提升理解任务和生成任务的性能。

## 2. BLIP 模型

**BLIP** 的核心是一个多模态混合编码器-解码器（**MED**）架构，能够以三种方式运行：单模态编码器、图像引导的文本编码器和图像引导的文本解码器。
- 单模态编码器：分别对图像和文本进行编码，训练目标是图像-文本对比学习（**Image-Text Contrastive Learning, ITC**）。
- 图像引导的文本编码器：通过交叉注意力层（**Cross-Attention, CA**）将视觉信息注入文本编码器，训练目标是图像-文本匹配（**Image-Text Matching, ITM**）。
- 图像引导的文本解码器：将双向自注意力层替换为因果自注意力层（**Causal Self-Attention, CSA**），用于给定图像生成文本描述，训练目标是语言建模（**Language Modeling, LM**）。

![](https://pic1.imgdb.cn/item/67a1d9c1d0e0a243d4fbcad7.png)

**BLIP** 在预训练阶段联合优化三个目标：
- 图像-文本对比学习损失（**ITC**）：对齐视觉和文本特征空间，使正样本对的特征更接近，负样本对的特征更远离。
- 图像-文本匹配损失（**ITM**）：学习图像和文本之间的细粒度对齐，预测图像-文本对是否匹配。
- 语言建模损失（**LM**）：给定图像，生成文本描述，最大化文本序列的似然。

**BLIP** 提出了一种新的数据增强方法 **CapFilt**，通过生成合成标题并过滤噪声标题来提升数据质量。具体步骤如下：
1. 标题生成器（**Captioner**）：使用图像引导的文本解码器，给定网络图像生成合成标题。
2. 过滤器（**Filter**）：使用图像引导的文本编码器，过滤掉噪声标题（包括原始网络文本和合成文本）。
3. 数据增强：将过滤后的图像-文本对与人工标注的数据结合，形成新的预训练数据集。

![](https://pic1.imgdb.cn/item/67a1da4fd0e0a243d4fbcaff.png)

## 3. 实验分析

**BLIP** 在多个视觉语言任务上进行了评估，包括：
- 图像-文本检索：使用 **COCO** 和 **Flickr30K** 数据集，评估指标为 **Recall@1、Recall@5** 和 **Recall@10**。
- 图像标题生成：使用 **COCO** 和 **NoCaps** 数据集，评估指标为 **BLEU@4、CIDEr** 和 **SPICE**。
- 视觉问答（**VQA**）：使用 **VQA2.0** 数据集，评估指标为 **VQA** 分数。
- 自然语言视觉推理（**NLVR2**）：评估指标为准确率。
- 视觉对话（**VisDial**）：评估指标为 **MRR、R@1、R@5、R@10** 和 **MR**。

主要实验结果：
- 图像-文本检索：**BLIP** 在 **COCO** 和 **Flickr30K** 数据集上取得了 **SOTA** 性能，平均 **Recall@1** 提升了 **2.7%**。![](https://pic1.imgdb.cn/item/67a1dbc2d0e0a243d4fbcb28.png)
- 图像标题生成：**BLIP** 在 **COCO** 数据集上取得了 **129.7** 的 **CIDEr** 分数，提升了 **2.8%**，在 **NoCaps** 数据集上取得了 **105.1** 的 **CIDEr** 分数。![](https://pic1.imgdb.cn/item/67a1dc12d0e0a243d4fbcb2d.png)
- 视觉问答：**BLIP** 在 **VQA2.0** 数据集上取得了 **78.25** 的 **VQA** 分数，提升了 **1.6%**。
- 自然语言视觉推理：**BLIP** 在 **NLVR2** 数据集上取得了 **82.24** 的准确率，达到 **SOTA** 水平。![](https://pic1.imgdb.cn/item/67a1dc72d0e0a243d4fbcb32.png)
- 视觉对话：**BLIP** 在 **VisDial v1.0** 验证集上取得了 **69.41** 的 **MRR**，达到了 **SOTA** 性能。![](https://pic1.imgdb.cn/item/67a1dc95d0e0a243d4fbcb36.png)

**BLIP** 在零样本学习场景下表现出色，能够直接迁移到视频语言任务中，如文本到视频检索和视频问答。在 **MSRVTT** 数据集上，零样本 **BLIP** 在文本到视频检索任务上取得了 **43.3%** 的 **Recall@1**，显著优于其他方法。

![](https://pic1.imgdb.cn/item/67a1dcfdd0e0a243d4fbcb3b.png)