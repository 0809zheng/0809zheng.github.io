---
layout: post
title: 'Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN'
date: 2024-12-18
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67e500480ba3d5a1d7e50ee4.png'
tags: 论文阅读
---

> Mix-LN：通过结合Pre-LN与Post-LN释放深层网络的能力.

- paper：[Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN](https://arxiv.org/abs/2411.14347)

# 0. TL; DR

本文提出了一种新的归一化技术**Mix-LN**，旨在解决大型语言模型（**LLMs**）中深度层训练不足的问题。研究表明，常用的**Pre-LN**会导致深度层的梯度减小，降低其有效性，而**Post-LN**虽然能保留深度层的梯度，但会导致早期层梯度消失。**Mix-LN**通过在模型的早期层应用**Post-LN**，在深度层应用**Pre-LN**，实现了更均匀的梯度分布，使网络的浅层和深层都能有效地参与训练。

# 1. 背景介绍

大型语言模型（**LLMs**）在理解和生成类似人类文本方面取得了巨大成功，推动了多个行业和学术领域的进步。然而，最近的研究发现，**LLMs**的深度层往往贡献较小，甚至可以被剪枝而不影响整体性能。这一现象被许多人视为模型压缩的机会，但本文作者认为这反映了训练过程中的不足，主要是由于广泛使用的**Pre-LN**导致的。**Pre-LN**在模型如**GPT**和**LLaMA**中被广泛使用，但会导致深度层的梯度减小，降低其有效性。相比之下，**Post-LN**虽然在深度层保留了较大的梯度，但会导致早期层的梯度消失。为了解决这一问题，本文提出了**Mix-LN**，一种结合了**Pre-LN**和**Post-LN**优势的新归一化技术。

# 2. Mix-LN技术

层归一化（**LN**）是一种在现代语言模型中广泛使用的归一化技术，它通过估计隐藏层中神经元输入的分布来确保输入分布的稳定性。原始**Transformer**架构中使用的是**Post-LN**，即在残差连接后应用**LN**。然而，后续研究表明，将**LN**放在残差连接前（**Pre-LN**）可以实现更稳定的性能，尤其是在大型语言模型中。本文通过实验验证了**Pre-LN**和**Post-LN**对**LLMs**深度层有效性的影响，并提出了**Mix-LN**技术。

**Mix-LN**的核心思想是结合**Pre-LN**和**Post-LN**的优势。具体来说，**Mix-LN**在模型的早期层（前$aL$层）应用**Post-LN**，在深度层（后$(1-a)L$层）应用**Pre-LN**。这样做的目的是利用**Post-LN**在深度层增强梯度流动的优势，同时利用**Pre-LN**在早期层稳定梯度的优势。通过这种方式，**Mix-LN**在中间和深度层实现了更健康的梯度范数，促进了整个网络的平衡训练，从而提高了模型的整体性能。

![](https://pic1.imgdb.cn/item/67e501590ba3d5a1d7e50f60.png)

超参数$a$控制应用**Post-LN**的层的比例。通过在**LLaMA-1B**模型上进行超参数搜索，发现$a=0.25$时性能最佳，因此在所有模型大小上均使用了这一值。

![](https://pic1.imgdb.cn/item/67e502180ba3d5a1d7e50f98.png)

# 3. 实验分析

实验使用了基于**LLaMA**架构的不同大小模型（从**1.1B**到**5B**参数），并比较了**Post-LN、DeepNorm、Pre-LN**和**Mix-LN**的性能。结果表明，**Mix-LN**在所有模型大小上均优于其他归一化技术，显著降低了困惑度，表明**Mix-LN**在预训练阶段就展现出了更好的性能。

![](https://pic1.imgdb.cn/item/67e508f00ba3d5a1d7e512a5.png)

为了验证**Mix-LN**在更大模型上的有效性，实验还使用了**LLaMa-7B**架构。尽管由于计算限制，训练只进行了**13,000**步，但**Mix-LN**在早期训练阶段就显示出了一致的性能提升，表明其优势在更大规模模型上也成立。

![](https://pic1.imgdb.cn/item/67e509130ba3d5a1d7e512c4.png)

在监督微调阶段，**Mix-LN**预训练的模型在多个下游任务上的表现优于使用**Pre-LN**和**Post-LN**的模型。这表明**Mix-LN**训练的深度层能够捕捉到更丰富和多样化的特征，从而在复杂任务中实现更好的泛化能力。

![](https://pic1.imgdb.cn/item/67e509380ba3d5a1d7e512ce.png)

