---
layout: post
title: 'CoCa: Contrastive Captioners are Image-Text Foundation Models'
date: 2024-01-18
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67a1c221d0e0a243d4fbc705.png'
tags: 论文阅读
---

> CoCa：对比描述器是图像文本基础模型.

- paper：[CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/abs/2205.01917)

## 0. TL; DR

本文介绍了一种名为 **Contrastive Captioners (CoCa)** 的新型图像-文本基础模型。**CoCa** 结合了对比学习和标题生成的双重目标，通过一个简化的编码器-解码器架构，同时学习单模态和多模态表示。该模型在多种下游任务（包括视觉识别、跨模态检索、多模态理解和图像标题生成）上表现出色，无需针对特定任务进行大量调整。**CoCa** 的设计不仅统一了单编码器、双编码器和编码器-解码器三种范式，还通过对比学习和生成学习的结合，实现了高效的预训练和任务迁移能力。

## 1. 背景介绍

近年来，随着深度学习的发展，预训练语言模型（如 **BERT、T5、GPT-3**）在自然语言处理领域取得了巨大成功。这些模型通过在大规模数据上进行预训练，展示了强大的多任务能力。类似地，计算机视觉领域也开始探索大规模预训练模型，以实现对视觉和视觉-语言任务的快速迁移和适应。

然而，现有的视觉预训练模型存在一些局限性。例如，单编码器模型（如基于 **ImageNet** 的分类模型）虽然能够学习通用的视觉表示，但缺乏对自然语言的理解能力。双编码器模型（如 **CLIP**）通过对比学习实现了图像和文本的对齐，但在多模态理解任务（如视觉问答）上表现有限。编码器-解码器模型（如 **SimVLM**）能够生成详细的文本描述，但缺乏单模态文本表示与图像嵌入的对齐。

为了克服这些局限性，本文提出了 **Contrastive Captioners (CoCa)**，一个统一的图像-文本基础模型，结合了对比学习和标题生成的优势，同时学习单模态和多模态表示，以实现对多种下游任务的高效迁移。

## 2. CoCa 模型

**CoCa** 的核心是一个编码器-解码器架构，其中编码器负责提取图像特征，解码器负责生成文本描述。

![](https://pic1.imgdb.cn/item/67a1c352d0e0a243d4fbc77d.png)

与传统的编码器-解码器模型不同，**CoCa** 的解码器被分为两部分：单模态解码器 和 多模态解码器。
- 单模态解码器：这部分解码器不使用交叉注意力，仅对输入文本进行编码，生成单模态文本表示。这些表示用于对比学习，以实现图像和文本的对齐。
- 多模态解码器：这部分解码器在单模态解码器的基础上，通过交叉注意力机制与图像编码器的输出进行交互，生成多模态图像-文本表示。这些表示用于标题生成任务。

**CoCa** 的训练目标包括两个部分：对比损失 和 标题生成损失。
- 对比损失：通过对比学习，**CoCa** 学习将图像和文本嵌入到同一个潜在空间中，使得匹配的图像-文本对在空间中更接近，不匹配的对则更远离。对比损失的公式如下：

$$
L_{con} =-\frac{1}{N}\left(\sum_{i}\log\frac{\exp(x_i^Ty_i/\sigma)}{\sum_{j}\exp(x_i^Ty_j/\sigma)}+\log\frac{\exp(y_i^Tx_i/\sigma)}{\sum_{j}\exp(y_i^Tx_j/\sigma)}\right)
$$

其中，$x_i$ 和 $y_i$ 分别是第 $i$ 对图像和文本的归一化嵌入，$N$ 是批量大小，$\sigma$ 是温度参数。

- 标题生成损失：通过自回归的方式，**CoCa** 学习预测文本序列中的下一个词。标题生成损失的公式如下：

$$
L_{cap} =- \sum_{t=1}^{T} \log P_{\theta}(y_{t} | y_{<t}, x)
$$

其中$y_t$ 是目标文本序列中第 $t$ 个词，，$y_{<t}$ 是目标文本序列中前 $t-1$ 个词，$x$ 是图像特征。

**CoCa** 在预训练阶段使用了大规模的图像-文本对数据，包括标注图像数据和噪声化的网页文本数据。为了处理这些数据，**CoCa** 采用了注意力池化：为了适应不同的任务需求，**CoCa** 引入了任务特定的注意力池化机制。通过多头注意力层，模型可以学习到不同长度的视觉表示，以满足不同任务的需求。例如，对比学习的特征序列数量为$1$，针对**caption**任务的特征序列数量为$256$。

## 3. 实验分析

**CoCa** 在多个下游任务上进行了评估，包括视觉识别、跨模态检索、多模态理解和图像标题生成。具体数据集和评估指标如下：
- 视觉识别：使用 **ImageNet** 数据集进行图像分类，评估指标为 **Top-1** 准确率。
- 跨模态检索：使用 **MSCOCO** 和 **Flickr30K** 数据集进行图像-文本检索，评估指标为 **Recall@1、Recall@5** 和 **Recall@10**。
- 多模态理解：使用 **VQA v2、SNLI-VE** 和 **NLVR2** 数据集进行视觉问答和视觉推理，评估指标为准确率。
- 图像标题生成：使用 **MSCOCO** 和 **NoCaps** 数据集进行图像标题生成，评估指标为 **BLEU、METEOR、CIDEr** 和 **SPICE**。

![](https://pic1.imgdb.cn/item/67a1c4dad0e0a243d4fbc7c8.png)

**CoCa** 在 **ImageNet** 数据集上展示了强大的性能。在零样本分类中，**CoCa** 达到了 **86.3%** 的 **Top-1** 准确率。在冻结编码器的情况下，**CoCa** 达到了 **90.6%** 的准确率。经过微调后，**CoCa** 进一步达到了 **91.0%** 的准确率，刷新了 **ImageNet** 分类任务的最新记录。

![](https://pic1.imgdb.cn/item/67a1c53ed0e0a243d4fbc7d3.png)

在跨模态检索任务中，**CoCa** 在 **MSCOCO** 和 **Flickr30K** 数据集上均取得了优异的性能。例如，在 **MSCOCO** 数据集上，**CoCa** 的图像到文本检索 **Recall@1** 达到了 **66.3%**，文本到图像检索 **Recall@1** 达到了 **51.2%**，显著优于其他方法。

![](https://pic1.imgdb.cn/item/67a1c585d0e0a243d4fbc7dd.png)

在多模态理解任务中，**CoCa** 在 **VQA v2** 数据集上达到了 **82.3%** 的准确率，在 **SNLI-VE** 数据集上达到了 **87.0%** 的准确率，在 **NLVR2** 数据集上达到了 **86.1%** 的准确率，均刷新了这些任务的最新记录。

![](https://pic1.imgdb.cn/item/67a1c5c0d0e0a243d4fbc7ef.png)

在图像标题生成任务中，**CoCa** 在 **MSCOCO** 数据集上达到了 **33.9** 的 **METEOR** 和 **143.6** 的 **CIDEr**，在 **NoCaps** 数据集上达到了 **122.4** 的 **CIDEr**，刷新了 **NoCaps** 数据集的最新记录。

![](https://pic1.imgdb.cn/item/67a1c5e7d0e0a243d4fbc7f2.png)

为了验证 **CoCa** 的设计合理性，作者进行了多项消融实验，包括对比损失和标题生成损失的权重调整、单模态解码器层数的调整等。实验结果表明，**CoCa** 的设计能够有效地平衡对比学习和标题生成任务的需求，同时提升模型在多种下游任务上的性能。

![](https://pic1.imgdb.cn/item/67a1c61ed0e0a243d4fbc7f7.png)