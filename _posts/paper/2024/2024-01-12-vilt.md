---
layout: post
title: 'ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision'
date: 2024-01-12
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67a0ab5fd0e0a243d4f9bade.png'
tags: 论文阅读
---

> ViLT：无卷积或区域监督的视觉语言Transformer.

- paper：[ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334)

## 0. TL; DR

本文提出了一种名为**ViLT（Vision-and-Language Transformer）**的视觉-语言预训练模型，该模型不依赖于卷积网络或区域监督，而是通过简单的线性投影将图像和文本嵌入到同一个空间中，然后通过**Transformer**模块进行交互学习。**ViLT**在多个视觉-语言下游任务中表现出色，同时在推理速度上比现有模型快数十倍。此外，**ViLT**首次在不使用区域特征或深度卷积视觉嵌入器的情况下实现了视觉-语言任务的竞争力。

## 1. 背景介绍

视觉-语言预训练（**VLP**）模型在多种视觉和语言的联合任务中取得了显著的性能提升。现有的**VLP**方法大多依赖于复杂的图像特征提取过程，这些过程通常涉及区域监督（如目标检测）和卷积架构（如**ResNet**）。然而，这些方法在效率和速度方面存在问题，因为仅提取输入特征所需的计算量就远大于多模态交互步骤。此外，这些方法在表达能力上也受到限制，因为它们的性能上限取决于视觉嵌入器及其预定义的视觉词汇表。

为了克服这些问题，本文提出了一种简化的**VLP**模型——**ViLT**。**ViLT**的核心思想是将视觉输入的处理方式简化为与文本输入相同的无卷积方式，从而显著减少模型大小和运行时间，同时保持对下游任务的竞争力。

![](https://pic1.imgdb.cn/item/67a0ac7cd0e0a243d4f9bb13.png)

## 2. ViLT 模型

### （1）模型结构

**ViLT**的架构设计简洁明了，主要由以下几个部分组成：

![](https://pic1.imgdb.cn/item/67a0ace0d0e0a243d4f9bb20.png)

**⚪ 视觉特征**

**ViLT**将输入图像划分为固定大小的图像块（**patch**），然后通过线性投影将这些图像块嵌入到一个与文本嵌入相同维度的空间中。具体来说，输入图像$I∈R^{C×H×W}$被划分为$N=HW/P^2$个大小为$P×P$的图像块，每个图像块被展平为一个向量，然后通过线性投影$V∈R^{P^2×H}$嵌入到维度为$H$的空间中。此外，**ViLT**还为每个图像块添加了位置嵌入$V^{pos}∈R^{N×H}$，以保留图像的空间信息。


**⚪ 文本嵌入**

文本嵌入部分与**BERT**类似，使用预训练的**BERT tokenizer**将文本分割为**token**序列，然后通过词嵌入矩阵$T∈R^{\mid V\mid ×H}$和位置嵌入矩阵$T^{pos}∈R^{\mid V\mid ×H}$将文本嵌入到维度为$H$的空间中。

**⚪ 多模态交互**

**ViLT**采用单流（**single-stream**）的**Transformer**模块来处理视觉和文本嵌入序列。具体来说，将视觉嵌入序列和文本嵌入序列拼接在一起，形成一个联合的输入序列$z_0$，然后通过$D$层**Transformer**层进行交互学习。每一层**Transformer**层包括一个多头自注意力（**MSA**）模块和一个**MLP**模块。最终，通过一个池化层将联合表示压缩为一个固定大小的向量$p$，用于下游任务的分类或检索。

### （2）预训练目标

**ViLT**在预训练阶段采用了两个常见的目标：图像-文本匹配（**ITM**）和掩码语言建模（**MLM**）。

**⚪ 图像-文本匹配（ITM）**

**ITM**任务的目标是判断给定的图像和文本是否匹配。**ViLT**通过随机替换对齐的图像来生成负样本，然后使用一个二分类头来预测图像和文本是否匹配。此外，**ViLT**还引入了一个词块对齐（**WPA**）目标，通过计算文本子集和视觉子集之间的对齐分数来进一步优化模型。

**⚪ 掩码语言建模（MLM）**

**MLM**任务的目标是预测被掩码的文本**token**。**ViLT**随机掩码文本序列中的**token**，并使用模型的上下文表示来预测被掩码的**token**。**ViLT**采用了**BERT**中的整词掩码（**whole word masking**）技术，即同时掩码一个词的所有子词**token**，以提高模型对下游任务的适应能力。


## 3. 实验分析

**ViLT**在以下四个数据集上进行预训练：
- **Microsoft COCO (MSCOCO)**：包含**113K**张图像和**567K**个描述。
- **Visual Genome (VG)**：包含**108K**张图像和**5.41M**个描述。
- **SBU Captions (SBU)**：包含**867K**张图像和**867K**个描述。
- **Google Conceptual Captions (GCC)**：包含**3.01M**张图像和**3.01M**个描述。

**ViLT**在以下两个类型的视觉-语言下游任务上进行评估：
- 分类任务：使用**VQAv2**和**NLVR2**数据集，评估指标为准确率。
- 检索任务：使用**MSCOCO**和**Flickr30K**数据集，评估指标为召回率（**R@1, R@5, R@10**）。

**ViLT**在**VQAv2**任务上的表现略低于其他使用深度视觉嵌入器的**VLP**模型，但考虑到其显著的推理速度优势，这一结果仍然具有竞争力。具体来说，**ViLT**在**VQAv2**的**test-dev**集上取得了**71.26%**的准确率。**ViLT**在**NLVR2**任务上取得了**75.70%**的准确率，与使用深度视觉嵌入器的模型相当。

![](https://pic1.imgdb.cn/item/67a0afaed0e0a243d4f9bb59.png)

**ViLT**在零样本文本检索和图像检索任务上表现出色，特别是在**Flickr30K**数据集上，**ViLT**的零样本文本检索**R@1**达到了**73.2%**，零样本图像检索**R@1**达到了**55.0%**。

![](https://pic1.imgdb.cn/item/67a0aff9d0e0a243d4f9bb66.png)

在微调后的检索任务中，**ViLT**在**MSCOCO**和**Flickr30K**数据集上均取得了较高的召回率。具体来说，在**MSCOCO**数据集上，**ViLT**的文本检索**R@1**达到了**61.5%**，图像检索**R@1**达到了**42.7%**；在**Flickr30K**数据集上，**ViLT**的文本检索**R@1**达到了**83.5%**，图像检索**R@1**达到了**64.4%**。

![](https://pic1.imgdb.cn/item/67a0b03dd0e0a243d4f9bb6e.png)

若干消融实验：
- 训练步数：增加训练步数可以显著提升**ViLT**的性能。例如，将训练步数从**100K**增加到**200K**，**ViLT**在**VQAv2**任务上的准确率从**70.85%**提升到**71.26%**，在**NLVR2**任务上的准确率从**75.57%**提升到**76.13%**。
- 整词掩码（**Whole Word Masking**）：在**MLM**任务中使用整词掩码可以提高下游任务的性能。例如，在**VQAv2**任务上，使用整词掩码后，**ViLT**的准确率从**70.16%**提升到**70.33%**。
- 图像增强（**Image Augmentation**）：在微调阶段使用图像增强可以进一步提升**ViLT**的性能。例如，在**VQAv2**任务上，使用图像增强后，**ViLT**的准确率从**70.33%**提升到**70.85%**。

![](https://pic1.imgdb.cn/item/67a0b0f8d0e0a243d4f9bbad.png)

