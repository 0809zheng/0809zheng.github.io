---
layout: post
title: 'mT5: A massively multilingual pre-trained text-to-text transformer'
date: 2021-01-10
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ed30c35132923bf8bb3f4b.jpg'
tags: 论文阅读
---

> mT5：多语言版本的预训练语言模型T5.

- paper：mT5: A massively multilingual pre-trained text-to-text transformer
- arXiv：[link](https://arxiv.org/abs/2010.11934)

**mT5(Multilingual T5)**是多语言版本的预训练语言模型T5，其采用[T5.1.1](https://0809zheng.github.io/2021/01/09/t511.html)结构设计，并构建了多国语言版的训练数据集**mC4**，其技术路线并无太大创新。其实验结果如下：

![](https://pic.imgdb.cn/item/60ed31525132923bf8c04be6.jpg)

其中**Cross-lingual zero-shot transfer**是指只在英语上进行预训练，在其他语言上测试性能。即在跨语种任务上的**zero shot**表现。