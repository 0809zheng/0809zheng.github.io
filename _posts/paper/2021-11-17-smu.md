---
layout: post
title: 'SMU: smooth activation function for deep networks using smoothing maximum technique'
date: 2021-11-17
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6194da882ab3f51d91608eef.jpg'
tags: 论文阅读
---

> SMU：基于光滑最大值技术的光滑激活函数.

- paper：[SMU: smooth activation function for deep networks using smoothing maximum technique](https://arxiv.org/abs/2111.04682)

本文使用最大值函数的光滑化方法对**Leaky ReLU**进行光滑近似，提出了一种新的激活函数，称为光滑最大单元(**smooth maximum unit, SMU**)，将其替换**shufflenet v2**模型中的**ReLU**，在**CIFAR100**数据集上取得$6.22\%$的准确率提高。

作者从绝对值函数$\|x\|$的光滑逼近出发，找到一个一般的最大值函数逼近，进而光滑逼近**Leaky ReLU**等激活函数。最大值函数可以定义为：

$$ \max(x_1,x_2) = \frac{x_1+x_2+|x_1-x_2|}{2} $$

绝对值函数$\|x\|$常用的光滑近似包括$x \text{erf}(\mu x)$和$\sqrt{x^2+\mu^2}$。前者从下面逼近$\|x\|$($\mu$越大越逼近)，后者从上面逼近$\|x\|$($\mu$越小越逼近)。

使用上述近似替换最大值函数的表达式，可以得到最大值函数的两种近似：

$$ f_1(x_1,x_2;\mu) = \frac{x_1+x_2+(x_1-x_2) \text{erf}(\mu (x_1-x_2))}{2} $$

$$ f_2(x_1,x_2;\mu) = \frac{x_1+x_2+\sqrt{(x_1-x_2)^2+\mu^2}}{2} $$

使用上述形式可以求得许多基于最大值函数的激活函数的光滑近似，比如：
- **maxout**：
$$ f_1(ax,bx;\mu) = \frac{(a+b)x+(a-b)x \text{erf}(\mu (a-b)x)}{2} $$
- **ReLU**（**GELU**当$\mu=\frac{1}{\sqrt{2}}$）：
$$ f_1(x,0;\mu) = \frac{x+x \text{erf}(\mu x)}{2} $$
- **Leaky ReLU**：
$$ f_1(x,\alpha x;\mu) = \frac{(1+\alpha)x+(1-\alpha)x \text{erf}(\mu (1-\alpha)x)}{2} $$

作者选择**Leaky ReLU**的光滑近似作为**SMU**激活函数的形式，其超参数$\alpha$和$\mu$通过梯度下降学习得到：

$$ \frac{\partial f}{\partial \alpha} = \frac{1}{2}[x-x \text{erf}(\mu (1-\alpha)x)-(1-\alpha)\mu x^2 \frac{2}{\sqrt{\pi}}e^{-(\mu (1-\alpha)x)^2}]  $$

$$ \frac{\partial f}{\partial \mu} = \frac{1}{2}[(1-\alpha)^2x^2 \frac{2}{\sqrt{\pi}}e^{-(\mu (1-\alpha)x)^2}]  $$

其中：

$$ \text{erf}(x)=\frac{2}{\sqrt{\pi}}\int_{0}^{x}e^{-t^2}dt, \quad \frac{d}{dx}\text{erf}(x)=\frac{2}{\sqrt{\pi}}e^{-x^2} $$

![](https://pic.imgdb.cn/item/6195bee12ab3f51d91ee4287.jpg)

作者也设置了使用$\sqrt{x^2+\mu^2}$的近似，称为**SMU-1**。

在实验中设置$\alpha=0.25$，$\mu$为可训练参数，初始值$1000000$(**SMU**)或$4.352665993287951e^{-9}$(**SMU-1**)。

实验结果如下：

![](https://pic.imgdb.cn/item/6195c2702ab3f51d91f0a39e.jpg)