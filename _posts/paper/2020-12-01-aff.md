---
layout: post
title: 'Attentional Feature Fusion'
date: 2020-12-01
author: 郑之杰
cover: 'https://pic.downk.cc/item/5fed80673ffa7d37b3310432.jpg'
tags: 论文阅读
---

> AFF：特征通道注意力融合.

- paper：Attentional Feature Fusion
- arXiv：[link](https://arxiv.org/abs/2009.14082)

**特征融合(Feature Fusion)**是指将来自不同层或不同分支的特征进行组合，一些常见的融合方法是**求和(sum)**或**串联(concatenate)**。本文作者通过通道注意力机制提出了一种注意力特征融合方法**AFF**，可以应用到**skip connection**、**multi-path**、**FPN**等需要进行特征融合的地方。

![](https://pic.downk.cc/item/5fed83123ffa7d37b3364435.jpg)

# Multi-Scale Channel Attention Module (MS-CAM)
作者首先提出了一种多尺度通道特征模块**MS-CAM**，该模块通过并行的全局通道注意力和逐点通道注意力重构输入特征：

![](https://pic.downk.cc/item/5fed84063ffa7d37b3384f5c.jpg)

**Pytorch**代码如下：

```
class ResGlobLocaChaFuse(HybridBlock):
    def __init__(self, channels=64):
        super(ResGlobLocaChaFuse, self).__init__()
        with self.name_scope():
            self.local_att = nn.HybridSequential(prefix='local_att')
            self.local_att.add(nn.Conv2D(channels, kernel_size=1, strides=1, padding=0))
            self.local_att.add(nn.BatchNorm())
            self.global_att = nn.HybridSequential(prefix='global_att')
            self.global_att.add(nn.GlobalAvgPool2D())
            self.global_att.add(nn.Conv2D(channels, kernel_size=1, strides=1, padding=0))
            self.global_att.add(nn.BatchNorm())
            self.sig = nn.Activation('sigmoid')

    def hybrid_forward(self, F, x, residual):
        xa = x + residual
        xl = self.local_att(xa)
        xg = self.global_att(xa)
        xlg = F.broadcast_add(xl, xg)
        wei = self.sig(xlg)
        xo = 2 * F.broadcast_mul(x, wei) + 2 * F.broadcast_mul(residual, 1-wei)
        return xo
```

# Attentional Feature Fusion (AFF)
作者提出注意力融合模块**AFF**，将两个不同特征进行融合：

![](https://pic.downk.cc/item/5fed84ca3ffa7d37b339e7c8.jpg)

**Pytorch**代码如下：

```
class AXYforXplusYAddFuse(HybridBlock):
    def __init__(self, channels=64):
        super(AXYforXplusYAddFuse, self).__init__()
        with self.name_scope():
            self.local_att = nn.HybridSequential(prefix='local_att')
            self.local_att.add(nn.Conv2D(channels, kernel_size=1, strides=1, padding=0))
            self.local_att.add(nn.BatchNorm())
            self.global_att = nn.HybridSequential(prefix='global_att')
            self.global_att.add(nn.GlobalAvgPool2D())
            self.global_att.add(nn.Conv2D(channels, kernel_size=1, strides=1, padding=0))
            self.global_att.add(nn.BatchNorm())
            self.sig = nn.Activation('sigmoid')

    def hybrid_forward(self, F, x, residual):
        xi = x + residual
        xl = self.local_att(xi)
        xg = self.global_att(xi)
        xlg = F.broadcast_add(xl, xg)
        wei = self.sig(xlg)
        xo = F.broadcast_mul(wei, residual) + x
        return xo
```

# iterative Attentional Feature Fusion (iAFF)
作者提出迭代注意力融合模块**iAFF**，将两个不同特征进一步融合：

![](https://pic.downk.cc/item/5fed85453ffa7d37b33b0159.jpg)


**Pytorch**代码如下：

```
class AXYforXYAddFuse(HybridBlock):
    def __init__(self, channels=64):
        super(AXYforXYAddFuse, self).__init__()
        with self.name_scope():
            self.local_att = nn.HybridSequential(prefix='local_att')
            self.local_att.add(nn.Conv2D(channels, kernel_size=1, strides=1, padding=0))
            self.local_att.add(nn.BatchNorm())

            self.global_att = nn.HybridSequential(prefix='global_att')
            self.global_att.add(nn.GlobalAvgPool2D())
            self.global_att.add(nn.Conv2D(channels, kernel_size=1, strides=1, padding=0))
            self.global_att.add(nn.BatchNorm())
            self.sig = nn.Activation('sigmoid')

    def hybrid_forward(self, F, x, residual):
        xi = x + residual
        xl = self.local_att(xi)
        xg = self.global_att(xi)
        xlg = F.broadcast_add(xl, xg)
        wei = self.sig(xlg)
        xo = F.broadcast_mul(wei, xi)
        return xo
```
