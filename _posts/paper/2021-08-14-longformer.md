---
layout: post
title: 'Longformer: The Long-Document Transformer'
date: 2021-08-14
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/61179b1b5132923bf808f170.jpg'
tags: 论文阅读
---

> Longformer: 适用于长文本的Transformer.

- paper：Longformer: The Long-Document Transformer
- arXiv：[link](https://arxiv.org/abs/2004.05150)

自注意力运算的复杂度呈输入序列长度的平方项增长。
现有的方法是将上下文缩短或者划分成为较小的序列，如限制这些序列在$512$的长度以内；但是这种划分可能导致重要的信息丢失。

作者提出了**Longformer**，它将注意力分解成多个不同的模式。其中窗口化的局部注意力能够捕捉上下文的局部信息；由下游任务激活的全局注意力能够捕捉输入的全局信息。

![](https://pic.imgdb.cn/item/61179cb95132923bf8100609.jpg)

下面分别介绍这些注意力模式：
1. **滑动窗口注意力**(**Sliding Window**)：对每一个**token**采用固定大小的窗口计算局部注意力。假设窗口大小是$w$，序列长度是$n$，则计算复杂度是$O(n*w)$，通常$w<n$。如果对每一层设置不同的$w$窗口值对可以平衡效率和模型的表示能力。
2. **空洞滑动注意力**(**Dilated Sliding**)：类似于空洞卷积，假设窗口大小是$w$，空洞间隔是$d$，网络层数是$l$，则窗口能覆盖到的范围是$l*d*w$。在多头自注意力中，一些头没有设置空洞，用于捕捉局部上下文信息；另一些头设置空洞用于捕捉较长的上下文信息。
3. **全局滑动注意力**(**Global+Sliding**)：为了捕捉全局信息，预先选择一些位置添加全局注意力，这些位置会在整个序列上计算注意力。位置的选择根据具体的下游任务决定。如对于分类任务位置选择[**CLS**]标签，对于问答任务选择整个问句。

通过上述形式将自注意力矩阵稀疏化，从而降低计算复杂度，使得处理较长序列时也不会有太大的内存占用与处理速度：

![](https://pic.imgdb.cn/item/6117aac95132923bf84d652c.jpg)

模型采用字符级别的自回归语言模型，数据集采用**text8**和**enwik8**。训练采用阶段式的训练方式，在第一阶段使用较短的序列长度和窗口大小，在每个后续阶段将窗口大小和序列长度增加一倍，并将学习率减半。一共训练$5$个阶段，第一阶段的序列长度是$2048$，最后一个阶段是$23040$。评估时将数据集分为大小为$32256$的重叠序列，步长为$512$，并评估序列最后$512$个**token**的性能。

本文设置了两种不同的模型大小，小模型设置$12$层，大模型设置$30$层。实验结果如下表所示：

![](https://pic.imgdb.cn/item/6117ab525132923bf84fcfb7.jpg)