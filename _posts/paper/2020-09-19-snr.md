---
layout: post
title: 'Spectral Norm Regularization for Improving the Generalizability of Deep Learning'
date: 2020-09-19
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/63466bcc16f2c2beb14fbd5e.jpg'
tags: 论文阅读
---

> 使用谱正则化改进深度学习的泛化性.

- paper：[Spectral Norm Regularization for Improving the Generalizability of Deep Learning](https://arxiv.org/abs/1705.10941)

# 1. 神经网络的Lipschitz约束

一般地，一个实值函数$f$是$K$阶[Lipschitz连续](https://0809zheng.github.io/2022/10/11/lipschitz.html)的，是指存在一个实数$K\geq 0$，使得对$$\forall x_1,x_2 \in \Bbb{R}$$，有：

$$ || f(x_1)-f(x_2) || ≤K || x_1-x_2 || $$

通常一个连续可微函数满足**Lipschitz**连续，这是因为其微分(用$\frac{\|f(x_1)-f(x_2)\|}{\|x_1-x_2\|}$近似)是有界的。但是一个**Lipschitz**连续函数不一定是处处可微的，比如$f(x) = \|x\|$。

若神经网络具有**Lipschitz**连续性，意味着该网络对输入扰动不敏感，具有更好的泛化性。下面讨论如何对神经网络施加**Lipschitz**约束。

假设神经网络$f(x)$具有参数$W$，则**Lipschitz**常数$K$通常是由参数$W$决定的，此时**Lipschitz**约束为：

$$ || f_W(x_1)-f_W(x_2) ||\leq K(W) || x_1-x_2 || $$

首先考虑单层全连接层$f_W(x)=\sigma(Wx)$，其中$\sigma$是激活函数，对应**Lipschitz**约束：

$$ || \sigma(Wx_1)-\sigma(Wx_2) || \leq K(W) || x_1-x_2 || $$

对$\sigma(Wx)$进行[Taylor展开](https://0809zheng.github.io/2021/08/20/taylor.html)并取一阶近似可得：

$$ ||  \frac{\partial \sigma}{\partial Wx} W(x_1-x_2) || \leq K(W) || x_1-x_2 || $$

$\frac{\partial \sigma}{\partial Wx}$表示激活函数的导数。通常激活函数的导数是有界的，比如**ReLU**函数的导数范围是$[0,1]$；因此这一项可以被忽略。则全连接层的**Lipschitz**约束为：

$$ ||  W(x_1-x_2) || \leq K(W) || x_1-x_2 || $$

上式对全连接层的参数$W$进行了约束。在实践中全连接网络是由全连接层组合而成，而卷积网络、循环网络等也可以表示为特殊的全连接网络，因此上述分析具有一般性。

# 2. 矩阵范数问题

全连接层的**Lipschitz**约束可以转化为一个矩阵范数问题（由向量范数诱导出来的矩阵范数，作用相当于向量的模长），定义为：

$$ ||W||_2 = \mathop{\max}_{x \neq 0} \frac{||Wx||}{||x||} $$

当$W$为方阵时，上述矩阵范数称为**谱范数(spectral norm)**。此时问题转化为：

$$ ||  W(x_1-x_2) || \leq ||W||_2 \cdot || x_1-x_2 || $$

谱范数$\|\|W\|\|_2$等于$W^TW$的最大特征值(主特征值)的平方根；若$W$为方阵，则$\|\|W\|\|_2$等于$W$的最大特征值的绝对值。

### ⚪ 谱范数的证明

谱范数$\|\|W\|\|_2$的平方为：

$$ ||W||_2^2 = \mathop{\max}_{x \neq 0} \frac{x^TW^TWx}{x^Tx} $$

上式右端为[瑞利商(Rayleigh Quotient)](https://0809zheng.github.io/2021/06/22/rayleigh.html)，取值范围是：

$$ \lambda_{min}≤\frac{x^TW^TWx}{x^Tx}≤\lambda_{max} $$

因此谱范数$\|\|W\|\|_2$的平方的取值为$W^TW$的最大特征值。

### ⚪ 谱范数的计算：幂迭代

$W^TW$的最大特征值可以通过**幂迭代(power iteration)**方法求解。

迭代格式1：

$$ u \leftarrow \frac{(W^TW)u}{||(W^TW)u||}, ||W||_2^2 ≈ u^TW^TWu $$

迭代格式2：

$$ v \leftarrow \frac{W^Tu}{||W^Tu||},u \leftarrow \frac{Wv}{||Wv||}, ||W||_2 ≈ u^TWv $$

其中$u,v$可以初始化为全$1$向量。下面以迭代格式1为例简单证明迭代过程收敛，记$A=W^TW$，初始化$u^{(0)}$，若$A$可对角化，则$A$的特征向量$$\{v_1 v_2  \cdots v_n\}$$构成一组完备的基，$u^{(0)}$可由这组基表示：

$$ u^{(0)} = c_1v_1+c_2v_2+\cdots c_nv_n $$

先不考虑迭代中分母的归一化，则迭代过程$u \leftarrow Au$经过$t$次后为：

$$ A^tu^{(0)} = c_1A^tv_1+c_2A^tv_2+\cdots c_nA^tv_n $$

注意到$Av=\lambda v$，则有：

$$ A^tu^{(0)} = c_1\lambda_1^tv_1+c_2\lambda_2^tv_2+\cdots c_n\lambda_n^tv_n $$

不失一般性地假设$\lambda_1$为最大特征值，则有：

$$ \frac{A^tu^{(0)}}{\lambda_1^t} = c_1v_1+c_2(\frac{\lambda_2}{\lambda_1})^tv_2+\cdots c_n(\frac{\lambda_n}{\lambda_1})^tv_n $$

注意到当$t \to \infty$时，$(\frac{\lambda_2}{\lambda_1})^t,\cdots (\frac{\lambda_n}{\lambda_1})^t \to 0$。则有：

$$ \frac{A^tu^{(0)}}{\lambda_1^t} ≈ c_1v_1 $$

上述结果表明当迭代次数$t$足够大时，$A^tu^{(0)}$提供了最大特征根对应的特征向量的近似方向，对其归一化后相当于单位特征向量：

$$ \begin{aligned} u &= \frac{A^tu^{(0)}}{||A^tu^{(0)}||} \\ A u &≈ \lambda_1 u \end{aligned} $$

因此可求$A=W^TW$的最大特征值：

$$ u^T A u ≈ \lambda_1  $$

# 3. 谱正则化 Spectral Norm Regularization

**谱正则化（Spectral Norm Regularization）**是指把谱范数的平方作为正则项：

$$ \mathcal{L}(x,y;W) + \lambda ||W||_2^2 $$

谱正则化能够增强网络的**Lipschitz**连续性，减少网络对输入扰动的敏感程度，增强网络的泛化性。

下面给出**pytorch**中实现谱正则化的方法，采用迭代过程：

$$ v \leftarrow \frac{W^Tu}{||W^Tu||},u \leftarrow \frac{Wv}{||Wv||}, ||W||_2 ≈ u^TWv $$

```python
def spectral_norm(w, t=5):
    w = w.view(-1, w.shape[-1]) # [m, n]
    u = torch.ones(1, w.shape[0]) # [1, m]
    for i in range(t):
        v = torch.mm(u, w) # [1, n]
        v = v/torch.norm(v)
        u = torch.mm(v, w.T) # [1, m]
        u = u/torch.norm(u)
    return torch.sum(torch.mm(torch.mm(u, w), v.T))
```