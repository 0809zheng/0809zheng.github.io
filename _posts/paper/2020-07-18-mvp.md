---
layout: post
title: 'Movement Pruning: Adaptive Sparsity by Fine-Tuning'
date: 2020-07-18
author: 郑之杰
cover: 'https://pic.downk.cc/item/5f12c02914195aa5946e279a.jpg'
tags: 论文阅读
---

> 讨论迁移学习中的权重剪枝.

- paper：Movement Pruning: Adaptive Sparsity by Fine-Tuning
- arXiv：[link](https://arxiv.org/abs/2005.07683)

模型的权重剪枝中最常用的一种方法是**Magnitude pruning**。该方法旨在对模型中数值较小的权重进行剪枝。具体地，设置权重得分为$$S=(\mid W_{i,j} \mid)_{1≤i,j≤n}$$，保留得分排序前$v%$的权重，其余权重置零。实践中引入mask矩阵$M$，在前向传播时计算：

$$ a = WMx $$

作者认为，对于迁移学习中的权重剪枝，直接使用**magnitude pruning**是不合适的。权重的数值在预训练任务中可以一定程度上代表重要性，但是对于迁移任务，其数值并不能代表重要性。

作者提出了一种**movement pruning**的方法，用权重的移动变化作为其重要性衡量。
- 当权重本身为正且其梯度为负或者权重本身为负且其梯度为正时，模型试图让该权重的数值变得更大，表明其重要性更大；
- 当权重本身为正且其梯度为正或者权重本身为负且其梯度为负时，模型试图让该权重的数值变得更小，表明其重要性较小。

累计训练过程中的梯度变化，定义模型的权重得分为：

$$ S^T_{i,j}=-α_S \sum_{t<T}^{} {(\frac{\partial L}{\partial W_{i,j}})^{(t)} W_{i,j}^{(t)} } $$

比较两种剪枝方法：

![](https://pic.downk.cc/item/5f12dae114195aa594758840.jpg)

![](https://pic.downk.cc/item/5f12db1f14195aa59475a7cb.jpg)

作者通过实验发现，在剪枝程度较大的情况下（保留权重较少），**movement pruning**相比于**magnitude pruning**表现更好:

![](https://pic.downk.cc/item/5f12db5414195aa59475b482.jpg)
