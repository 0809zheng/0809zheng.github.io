---
layout: post
title: 'Large Batch Training of Convolutional Networks'
date: 2020-12-15
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/620c9bb72ab3f51d91dfa8fa.jpg'
tags: 论文阅读
---

> LARS：层级自适应学习率缩放.

- paper：[Large Batch Training of Convolutional Networks](https://arxiv.org/abs/1708.03888)


加速大型卷积神经网络训练的一种常见方法是增加计算单元(如**GPU**)，然后使用数据并行的方式同步训练。随着计算节点数量增加，数据的批量大小也会增加。大批量训练往往导致模型精度降低；目前普遍采用带**warmup**的线性学习率缩放策略，即增大批量的同时线性增大学习率。作者提出了一种基于层级自适应学习率缩放(**layer-wise adaptive rate scaling, LARS**)的优化方法，降低了大批量训练时的优化困难。

## 1. 大批量训练

线性学习率缩放策略(**linear scaling rule**)是大批量训练时常用的学习率调整策略，即当批量增大$k$倍时，学习率也增大$k$倍。直观上，批量增大$k$倍时，等效更新轮数减小$k$倍，因此更新步长应该增大$k$倍。

使用较大学习率训练模型会导致不稳定。为了减少训练初始阶段的不稳定性，**warmup**被用于调整学习率，即训练从较小的学习率开始，逐渐增大到正常学习率。

线性学习率和缩放与**warmup**共同被用作大批量训练的常用技术。作者尝试训练了一个**Alexnet**模型，实验观察到增大数据批量时，通过增大学习率能够维持模型一定的准确率，但仍然有一定的泛化差距(精度降低$2.2\%$)。然而从损失曲线上发现两个批量的训练程度并无明显的差距。因此作者认为，大批量训练中只使用这些策略还不够充分。

![](https://pic.imgdb.cn/item/620cb1a02ab3f51d910d5f45.jpg)

## 2. LARS

注意到标准的梯度下降算法对深度网络的每一层使用相同的学习率$\lambda$。当学习率过大时，参数更新值$\|\|\lambda *\nabla L(w_t)\|\|$可能会比参数值本身$\|\|w\|\|$还要大，从而导致参数更新不收敛。

作者发现每一层参数及其梯度的范数之比$\|\|w\|\| / \|\|\nabla L(w_t)\|\|$显著变化。如果学习率$\lambda$显著大于这个比值，则可能导致训练不稳定；如果学习率$\lambda$比这个比值小得多，则参数更新速度会很慢。

![](https://pic.imgdb.cn/item/620cb5422ab3f51d9112d644.jpg)

由于这个比例在不同层之间差异很大，所以有必要对神经网络中的每一层设置不同的学习率$\lambda^l$。则第$l$层的参数更新量计算为：

$$ \Delta w_{t}^{l} = \gamma * \lambda^l * \Delta L(w_{t}^{l}) $$

其中$\Delta L(w_{t}^{l})$是反向传播计算得到的梯度，$\gamma$是全局学习率；$\lambda^l$是局部学习率，计算为：

$$ \lambda^l = \eta \times \frac{|| w^l ||}{|| \Delta L(w^{l}) ||} $$

其中系数$\eta$表明对该层会在一次更新中改变其参数的信任程度。注意到此时参数更新的幅值与梯度本身的幅值无关。

权重衰减也可以方便地引入局部学习率的计算：

$$ \lambda^l = \eta \times \frac{|| w^l ||}{|| \Delta L(w^{l}) ||+ \beta * || w^l ||} $$

**LARS**可以与其他优化算法结合使用。下表给出了结合**LARS**与**momentum**的优化算法：

![](https://pic.imgdb.cn/item/620cb7782ab3f51d91181378.jpg)

作者对不同层的不同参数的局部学习率$\lambda^l$进行可视化，结果说明局部学习率的变化依赖于批量大小。

![](https://pic.imgdb.cn/item/620cb8dd2ab3f51d911b315c.jpg)

## 3. 实验结果

作者使用**LARS**将**Alexnet**的训练扩展到$8$k批量大小，**ResNet-50**的训练扩展到$32$k批量大小，两个模型的准确率几乎没有下降。

![](https://pic.imgdb.cn/item/620cb87a2ab3f51d911a45a1.jpg)

![](https://pic.imgdb.cn/item/620cb88a2ab3f51d911a6449.jpg)