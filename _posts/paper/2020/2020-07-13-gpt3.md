---
layout: post
title: 'Language Models are Few-Shot Learners'
date: 2020-07-13
author: 郑之杰
cover: 'https://pic.downk.cc/item/5f0bf6c814195aa5946976af.jpg'
tags: 论文阅读
---

> GPT3：预训练语言模型.

- paper：Language Models are Few-Shot Learners
- arXiv：[link](https://arxiv.org/abs/2005.14165v2)

作者训练了一个非常大的预训练语言模型**GPT3**。其最大参数量可达1750亿。模型继承了**GPT2**的结构，使用了更深的层数和更多的自注意力头数：

![](https://pic.downk.cc/item/5f0bf9cb14195aa5946a3562.jpg)

训练是在庞大的语料库上进行的，训练集如下：

![](https://pic.downk.cc/item/5f0bf9e014195aa5946a3b04.jpg)

通常预训练模型对于不同的任务会进行微调，微调过程如下（以机器翻译为例）：

![](https://pic.downk.cc/item/5f0bfaa314195aa5946a70e1.jpg)

而作者应用**GPT3**模型并没有微调，而是尝试了三种任务：
1. **zero-shot**：输入问题描述，输出答案：
![](https://pic.downk.cc/item/5f0bfeed14195aa5946b992b.jpg)
2. **one-shot**：输入一个问题和答案的例子，再输入一个问题，输出答案：
![](https://pic.downk.cc/item/5f0bff1414195aa5946ba32e.jpg)
3. **few-shot**：输入一些问题和答案的例子，再输入一个问题，输出答案：
![](https://pic.downk.cc/item/5f0bff2714195aa5946ba83b.jpg)

作者在大量的**NLP**任务上进行实验，通过实验发现预训练后没有微调的**GPT3**可以达到甚至超过经过微调的**BERT**等模型的实验结果。