---
layout: post
title: 'Learning Sparse Neural Networks through L0 Regularization'
date: 2020-08-31
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6496be3d1ddac507cc9991fc.jpg'
tags: 论文阅读
---

> 通过L0正则化学习稀疏神经网络.

- paper：[Learning Sparse Neural Networks through L0 Regularization](https://arxiv.org/abs/1712.01312)

本文作者提出了一种神经网络的$L_0$范数正则化方法，即在训练过程中鼓励网络的权重为零。若训练数据为$$\{(x_i,y_i)\}_{i=1}^N$$，则对神经网络$h(\theta)$引入$L_0$范数正则化：

$$
\begin{aligned}
\mathcal{R}(\theta) &= \frac{1}{N} \left( \sum_{i=1}^N\mathcal{L}(h(x_i;\theta),y_i) \right)+\lambda ||\theta||_0 \\
||\theta||_0 &= \sum_{j=1}^{|\theta|} \mathbb{I}[\theta_j \neq 0]
\end{aligned}
$$

![](https://pic.imgdb.cn/item/6496cba41ddac507ccacf050.jpg)

考虑到$L_0$范数无法直接求导，因此上述正则化过程无法直接应用到神经网络中。需要寻找该正则化目标的一个连续近似。

对参数$\theta$进行重参数化：

$$
\theta_j = \tilde{\theta}_jz_j,z_j = \{0,1\},||\theta||_0 = \sum_{j=1}^{|\theta|} z_j
$$

该重参数化过程相当于为参数$\theta$的每一个元素$\theta_j$引入了一个二元门控标量$z_j$，则$L_0$范数衡量参数中所有开启的门的数量。

$$
\begin{aligned}
\mathcal{R}(\tilde{\theta}) &= \frac{1}{N} \left( \sum_{i=1}^N\mathcal{L}(h(x_i;\tilde{\theta} \odot z),y_i) \right)+\lambda \sum_{j=1}^{|\theta|} z_j
\end{aligned}
$$

给定任意连续随机变量$s$，则可以构造$z$如下：

$$
z = \min(1, \max(0, s))
$$

连续随机变量$s$的累积分布函数$Q(s)$是可求的，则对$z$的非零约束可以转化为：

$$
q(z\neq 0) = 1-Q(s \leq 0)
$$

至此$L_0$范数正则化表示为：

$$
\begin{aligned}
\mathcal{R}(\tilde{\theta}) &= \frac{1}{N} \left( \sum_{i=1}^N\mathcal{L}(h(x_i;\tilde{\theta} \odot \min(1, \max(0, s))),y_i) \right)+\lambda \sum_{j=1}^{|\theta|} (1-Q(s_j \leq 0))
\end{aligned}
$$

连续随机变量$s$被设置为服从**hard concrete**分布：首先从均匀分布$U[0,1]$中采样$u$，经过一系列变换获得$s$与$z$：

$$
\begin{aligned}
u & \sim U[0,1] \\
s &= \text{sigmoid}((\log u - \log(1-u) + \log \alpha)/\beta) \\
\overline{s} &= s(\zeta - \gamma) + \gamma \\
z &= \min(1, \max(0, \overline{s}))
\end{aligned}
$$

其中$\log \alpha$是位置参数，$\beta$是温度参数，当$\beta \to 0$时$s$服从伯努利分布。$\gamma < 0, \zeta > 1$把分布的取值拉伸到$[\gamma,\zeta]$，并应用**hard-sigmoid**构造$z$。

![](https://pic.imgdb.cn/item/64bf84651ddac507cce638cc.jpg)

此时$L_0$范数损失项计算为：

$$
\begin{aligned}
1-Q(\overline{s} \leq 0) &= Q(\overline{s} > 0) = Q(s(\zeta - \gamma) + \gamma > 0) \\
&= Q(s > \frac{- \gamma}{\zeta-\gamma}) \\
&= Q(\text{sigmoid}((\log u - \log(1-u) + \log \alpha)/\beta)  > \frac{- \gamma}{\zeta-\gamma}) \\
&= Q(u > \left(1+e^{\log \alpha - \beta \log \frac{-\gamma}{\zeta}}\right)^{-1}) \\
&= 1-\left(1+e^{\log \alpha - \beta \log \frac{-\gamma}{\zeta}}\right)^{-1} \\
&= \frac{1}{1 + e^{-(\log \alpha - \beta \log \frac{-\gamma}{\zeta})}}\\
&= \text{sigmoid}(\log \alpha - \beta \log \frac{-\gamma}{\zeta})
\end{aligned}
$$

至此$L_0$范数正则化表示为：

$$
\begin{aligned}
\mathcal{R}(\tilde{\theta}) &= \frac{1}{N} \left( \sum_{i=1}^N\mathcal{L}(h(x_i;\tilde{\theta} \odot z),y_i) \right)+\lambda \sum_{j=1}^{|\theta|} \text{sigmoid}(\log \alpha_j - \beta \log \frac{-\gamma}{\zeta})
\end{aligned}
$$