---
layout: post
title: 'TAPAS: Weakly Supervised Table Parsing via Pre-training'
date: 2020-07-01
author: 郑之杰
cover: 'https://pic.downk.cc/item/5efbf74a14195aa594dd7a64.jpg'
tags: 论文阅读
---

> 用BERT解决表格问答任务.

- TAPAS: Weakly Supervised Table Parsing via Pre-training
- arXiv：[link](https://arxiv.org/abs/2004.02349v1)

# 问题阐述
本文提出了一个模型，用来解决表格的问答问题。下面是一个训练样本：

![问题例子](https://pic.downk.cc/item/5efbf81614195aa594ddd678.jpg)

样本由**表格table**和**问题question**组成，其中：
- 表格包含一个表头
- 问题分成两种：
1. **cell selection**：答案在某一个表格格子中，通过选择格子作答；
2. **scalar/ambiguous answer**：答案是一个标量，可能出现在表格中，也可能需要计算得到。

# 模型介绍

![模型](https://pic.downk.cc/item/5efbf87214195aa594de03bf.jpg)

模型的主体是一个BERT结构，通过对问题和表格进行编码作为输入；通过模型得到两组输出：
- **Cell selection**：对表格进行选择，输出选择哪一列以及该列每一行被选中的概率；
- **Aggregation prediction**：输出问题属于哪一类，有四种类别：
1. **NONE**：不做计算任务，相当于仅进行表格选择；
2. **COUNT**：统计选中元素个数
3. **SUM**：选中元素求和
4. **AVE**：选中元素求均值

上述计算均按照概率加权的方法进行。最终的预测结果再次按照概率求和的方法得到。

模型训练采用端到端的训练方式，并不指定具体的任务，而是比较上述结果和最终答案之间的差别（包括格子选择的误差和计算数值误差）。

# 编码

![编码](https://pic.downk.cc/item/5efbf84014195aa594ddeb24.jpg)

BERT的输入需要对问题和表格进行编码，使用了六种编码相加：
- **Token Embeddings**：占位符编码，$\[CLS\]$标记问题类型，$query$是问题占位符，后接问题的编码；$\[SEP\]$分隔符，$col$是列占位符，后接列名称，最后是表格元素从上到下、从左到右的编码
- **Position Embeddings**：位置编码，同BERT
- **Segment Embeddings**：分割编码，区分编码的是问题$SEG_0$还是表格$SEG_1$
- **Column Embeddings**：列编码，标记表格元素属于哪一列，依次标记$COL_1$、$COL_2$...
- **Row Embeddings**：行编码，标记表格元素属于哪一行，依次标记$ROW_1$、$ROW_2$...
- **Rank Embeddings**：排序编码，表示表格每一列中元素的大小顺序，$RANK_0$无意义，在每一列中依次标记$RANK_1$、$RANK_2$...

