---
layout: post
title: 'Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution'
date: 2020-09-08
author: 郑之杰
cover: 'https://pic.downk.cc/item/5f56d210160a154a67833feb.jpg'
tags: 论文阅读
---

> LapSRN：多尺度超分辨率的拉普拉斯金字塔网络.

- paper：Deep Laplacian Pyramid Networks for Fast and Accurate Super-Resolution
- arXiv：[link](https://arxiv.org/abs/1704.03915)
- code：[github](https://github.com/twtygqyy/pytorch-LapSRN)

![](https://pic.downk.cc/item/5f56d2ae160a154a6783566d.jpg)

现存的超分辨率模型存在三个主要的问题：
1. 采用预定义的上采样操作（例如双三次插值）会产生不必要的计算代价，并且结果可能会有重建伪影。而使用反卷积层操作来替换预定义的上采样操作，网络结构又比较简单，性能较差，不能很好地学习复杂的映射；
2. 使用$L2$损失函数时，不可避免地会产生模糊的预测，恢复出的高分辨图片往往会过于平滑；
3. 大部分的方法都只有一次上采样的步骤，这就会使得对于更高倍数的训练变得困难。

为解决上述问题，本文提出了**LapSRN**，与其他模型的对比如下：

![](https://pic.downk.cc/item/5f570304160a154a678e6c25.jpg)

# 网络结构

![](https://pic.downk.cc/item/5f570401160a154a678ea2fb.jpg)

模型由特征提取分支和图像重构分支构成。
- **特征提取分支（Feature Extraction Branch）**：采用转置卷积对图像逐级放大，并将每一级放大结果传递给图像重构分支；
- **图像重构分支（Image Reconstruction Branch）**：使用双线性插值放大图像，并结合特征提取分支得到的图像特征重构图像。

模型学习的是每一个尺度（**phase**）下原图像与插值图像之间的残差。

# 损失函数
作者使用**Charbonnier Loss**作为对$L1$损失函数的近似：

![](https://pic.downk.cc/item/5f570684160a154a678f2465.jpg)

作者分析该损失函数相比于$L2$损失函数，对**outlier**不敏感，能获得更好的效果：

![](https://pic.downk.cc/item/5f5707d9160a154a678f68c6.jpg)

`Pytorch`实现如下：

```
class L1_Charbonnier_loss(torch.nn.Module):
    """L1 Charbonnierloss."""
    def __init__(self):
        super(L1_Charbonnier_loss, self).__init__()
        self.eps = 1e-3

    def forward(self, X, Y):
        diff = torch.add(X, -Y)
        error = torch.sqrt(diff * diff + self.eps)
        loss = torch.mean(error)
        return loss
```