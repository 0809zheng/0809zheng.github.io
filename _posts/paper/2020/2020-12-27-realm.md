---
layout: post
title: 'REALM: Retrieval-Augmented Language Model Pre-Training'
date: 2020-12-27
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60d9884f5132923bf8559514.jpg'
tags: 论文阅读
---

> REALM：通过检索增强预训练语言模型.

- paper：REALM: Retrieval-Augmented Language Model Pre-Training
- arXiv：[link](https://arxiv.org/abs/2002.08909)

预训练语言模型(如**ELMo,BERT,GPT**)广泛应用于自然语言处理任务中，**预训练(pre-training)**加**微调(fine-tuning)**逐渐成为建模的新范式。针对预训练语言模型的改进包括对模型结构的改进、对预训练策略的调整、预训练结合多任务学习等方法。本文提出了一种使用检索技术增强预训练模型的方法**REALM**，通过中间步骤的检索任务来增强模型的学习能力，同时增强模型的可解释性。

**REALM**在预训练和微调阶段都增加了**知识检索(knowledge retrieval)**的过程。
- 预训练时，模型在预测**mask**区域的正确内容前，首先通过检索模型从检索语料库中检索相关的文档，然后使用检索结果中的**top-k**文档中的内容进行正确的预测。
- 微调时，模型同样先从完整的检索语料库中进行相应的检索，最后利用检索结果来完成下游任务。

由于检索过程的存在，模型使用了一种间接且直观化的学习过程，即先检索再预测，而且检索模型和预训练模型是共同训练的。当检索模型可以正确的检索到对应的文档时，模型就可以更好的进行预测；反之模型利用检索的结果很好的实现了下游任务，则就会给检索模型一个正反馈。

![](https://pic.imgdb.cn/item/60d98cf85132923bf871acc6.jpg)

**REALM**使用简单的检索方法：**Maximum Inner Product Search**，即将输入句子和语料库中的文档转换为对应的表示向量，通过两者的内积计算相似度，选用相似度最大的前$k$个文档作为检索文档。

## 预训练

预训练采用和**BERT**一样的**mask**训练策略，即在一个句子中随即给某些单词加上**mask**，训练模型对这个单词进行正确的预测。假设经过**mask**后的句子为$x$，待预测的单词为$y$，则问题建模为：

$$ p(y|x) $$

**REALM**引入了检索过程，若检索得到的句子为$z$，则问题可以被拆分成：

$$ p(y|x) = \sum_{z \in Z}^{} p(z|x)p(y|x,z) $$

在预训练时检索模型$p(z\|x)$和语言模型$p(y\|x,z)$一起训练。

![](https://pic.imgdb.cn/item/60d98d1d5132923bf8727f7f.jpg)

## 微调
本文选择的下游任务为**开放领域问答(Open QA)**，即给定一个问题，模型需要给出对应的答案。由于引入了检索语料库，模型需要从包含大量文档的该语料库中找到答案。由于训练所使用的检索语料库(**wikipedia**)规模巨大，在检索阶段会引入大量计算。因此预先使用一些方法来减少计算量：
- 预先计算好语料库中所有文档的嵌入向量以及构建有效的索引机制；
- 使用嵌入向量的内积对检索到的文档进行排序；
- 在训练几百步后再异步的更新嵌入向量所对应的索引。

为了减少模型在训练过程中的偏差，额外引入了几种策略来进行处理：
- **Salient span masking**：采用和**ERNIE**类似的方式，通过**mask**比较重要的部分来使模型学习到更重要的东西；
- **Null document**：当预测的部分不重要时，将检索结果设置为空文档$\phi$来减少计算量；
- **Prohibiting trivial retrievals**：避免检索结果和完整文档集一致；
- **Initialization**：采用更好的初始化方式来期望得到更好的嵌入向量。

通过在不同的**Open-QA**数据集上进行实验，和已有的预训练模型进行比较，证明了**REALM**的优越性(其中$X$表示预训练语料库;$Z$表示检索语料库)：

![](https://pic.imgdb.cn/item/60de83b25132923bf8d72b43.jpg)
