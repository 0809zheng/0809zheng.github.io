---
layout: post
title: 'Faster Neural Network Training with Data Echoing'
date: 2020-07-06
author: 郑之杰
cover: 'https://pic.downk.cc/item/5f02b76714195aa594db2e68.jpg'
tags: 论文阅读
---

> Data Echoing：一种加速模型训练的方法.

- paper：Faster Neural Network Training with Data Echoing
- arXiv：[link](https://arxiv.org/abs/1907.05550v3)

# 方法介绍
训练一个深度学习模型的典型过程如下：

![](https://pic.downk.cc/item/5f02b7f414195aa594db75a4.jpg)

1. 从实际数据集中读取数据，表示成张量；
2. 打乱数据集；
3. 进行数据增强；
4. 取出一个数据批次；
5. 通过梯度下降更新模型参数。

在这个流水线中，通常**上游任务（upstream）**是在**CPU**上进行的，而**下游任务（downstream）**是在**GPU**或**TPU**上进行的；模型对一批数据进行预处理的时间要长于使用数据进行学习的时间；

![](https://pic.downk.cc/item/5f02b93e14195aa594dc3048.jpg)

作者提出了一种**data echoing**的方法，将预处理的数据放入一个**buffer**中，使用数据更新参数时从这个**buffer**直接或再进行一些处理来获得一个批量的数据，不断从**buffer**中采样直到**upstream**处理好了一批新的数据，更新**buffer**。

作者讨论了该方法的一些优势：
1. **data echoing**降低了达到相同精度时所需的**upstream**计算量；
2. **data echoing**加速了算法运行的**墙上时间(wall time)**，即进程运行的时间总量；
3. **data echoing**提供了一系列**echoing factor**，即可选择每读入一组样本后的重复利用次数；
4. **data echoing**的有效性决定于将其插入**pipeline**的位置；
5. 从**buffer**中采样后进行数据打乱效果会更好；
6. **data echoing**可以达到与**baseline**相同的实验效果。

# 实验
作者选择了四个模型，测试分别需要多少新样本才能够达到给定的目标值；

![](https://pic.downk.cc/item/5f02bcfd14195aa594de0da4.jpg)

使用**data echoing**的位置非常关键，这个位置往往选择整个模型的**瓶颈 bottleneck**，用于平衡较慢的**upstream**和较快的**downstream**。在**pipeline**中有以下几个可插入的位置：

![](https://pic.downk.cc/item/5f02bfe014195aa594df761b.jpg)

1. **example echoing before augmentation**
2. **example echoing after augmentation**
3. **batch echoing**

### 插入位置的实验

![](https://pic.downk.cc/item/5f02c11614195aa594e020d1.jpg)

当**echoing factor**设置为$2$时，理论上只需要一半的数据量（图中虚线）；由于重复使用这些样本打破了独立同分布的假设，实际上需要的样本数会更多一些。**data echoing**插入**pipeline**的位置越靠前，实际上经过后续的预处理的样本更符合独立同分布，因此所需的等效样本数越少；但所需的**upstream**计算量越多。

### echoing factor 的实验

![](https://pic.downk.cc/item/5f02c19b14195aa594e05c21.jpg)

使用**data echoing**的位置在**example echoing before augmentation**；当逐渐增大**echoing factor**，由于增加了重复样本的可能性，实际所需要的样本量比理论值更多，实际减少的运行时间也比理论值更少。

### batch size 的实验

![](https://pic.downk.cc/item/5f02c38d14195aa594e12785.jpg)

大的**batch size**能够缓解**echoing factor**增大带来的数据重复问题。

![](https://pic.downk.cc/item/5f02c35314195aa594e10b4f.jpg)

**batch size**越大，**data echoing**的效果越明显。

### shuffle 的实验

![](https://pic.downk.cc/item/5f02c45514195aa594e17525.jpg)

打乱**buffer**中的数据再采样能够显著减小所需的样本量。

### 验证集的实验

![](https://pic.downk.cc/item/5f02c4ce14195aa594e1a472.jpg)

通过实验证明，使用**data echoing**能够用更少的数据样本实现给定的模型精度。