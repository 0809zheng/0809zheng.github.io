---
layout: post
title: 'A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm'
date: 2020-12-07
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/61f4cd3f2ab3f51d91ffa89a.jpg'
tags: 论文阅读
---

> RProp：一种快速反向传播学习的直接自适应方法.

- paper：[A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.1417)

作者提出了一种快速实现反向传播的梯度更新算法**RProp(resilient propagation)**，它根据参数的每个维度的偏导数符号来更新参数，与标准的梯度下降算法相比，学习过程易于实现且计算量小。对于一些简单的任务，不需要选择超参数便可以快速获得接近最优的收敛值。

在**RProp**中，参数$w_{ij}$在第$t$次更新时的更新量$\Delta_{ij}^{t}$计算为：

$$ \Delta_{ij}^{t} = \begin{cases} \eta_{+} * \Delta_{ij}^{t-1}, & \text{if } \frac{\partial E}{\partial w_{ij}}^{t-1} * \frac{\partial E}{\partial w_{ij}}^{t} >0 \\ \eta_{\_} * \Delta_{ij}^{t-1}, & \text{if } \frac{\partial E}{\partial w_{ij}}^{t-1} * \frac{\partial E}{\partial w_{ij}}^{t} <0 \\ \Delta_{ij}^{t-1}, & \text{else} \end{cases} $$

上式表示，如果误差函数$E$对参数$w_{ij}$的偏导数改变符号，则表明上一次更新的步长太大，使得算法跳过了局部最小值，因此需要减小更新的步长($$0<\eta_{\_}<1$$)。另一方面，如果偏导数的符号没有改变，则表明还没有到达局部最小值，因此增加更新值($\eta_+>1$)，以加速收敛。更新量的初始值通常设置为初始学习率$\Delta_{ij}^0$。

参数更新的规则是沿着梯度的反方向改变参数值：

$$ w_{ij}^{t+1} = w_{ij}^{t} + \begin{cases} - \Delta_{ij}^{t}, & \text{if } \frac{\partial E}{\partial w_{ij}}^{t} >0 \\ \Delta_{ij}^{t}, & \text{if } \frac{\partial E}{\partial w_{ij}}^{t} <0 \\ 0, & \text{else} \end{cases} \\ =  w_{ij}^{t} - \text{sign}(\frac{\partial E}{\partial w_{ij}}^{t})*\Delta_{ij}^{t} $$

在实际中，如果偏导数的符号改变，表明上一次更新步长太大，错过了局部最小值，因此也可以恢复之前的参数数值：

$$ w_{ij}^{t+1} = w_{ij}^{t}, \text{  if } \frac{\partial E}{\partial w_{ij}}^{t-1} * \frac{\partial E}{\partial w_{ij}}^{t} <0 $$

**RProp**算法的流程图为：

![](https://pic.imgdb.cn/item/61f504c82ab3f51d913845ef.jpg)

下图表明**RProp**在不同的任务中对参数初始化不敏感，且能够快速收敛到最优值附近。

![](https://pic.imgdb.cn/item/61f4fc372ab3f51d912f68ee.jpg)

下图对比了几种不同的梯度算法在各种任务中收敛所需要的更新轮数，结果表明**RProp**需要最少的轮数。

![](https://pic.imgdb.cn/item/61f4fc752ab3f51d912fa604.jpg)
