---
layout: post
title: 'Weighted Channel Dropout for Regularization of Deep Convolutional Neural Network'
date: 2020-10-19
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63b2a0ce5d94efb26f100019.jpg'
tags: 论文阅读
---

> 卷积神经网络中的加权通道丢弃正则化.

- paper：[Weighted Channel Dropout for Regularization of Deep Convolutional Neural Network](https://ojs.aaai.org//index.php/AAAI/article/view/4858)

卷积神经网络的卷积层特征是由之前的层生成的，其中某些特征比其它特征更有用。对于每张输入图像，更深的卷积层中仅有少量通道被激活，同时其它通道中的神经元响应接近于零。

本文作者提出了一种根据激活的相对幅度来选择通道的方法：加权通道丢弃（**Weighted Channel Dropout, WCD**），这是一种卷积层的正则化方法，能够建模通道之间的依赖关系。

**WCD**首先通过全局平均池化**GAP**为输入特征的每个通道分配一个分数；然后生成一个二元掩码来指示每个通道是否被选中，分数更高的通道有更高的概率得到保留；最后使用一个额外的随机数生成器来进一步为下一层过滤通道（可选）。

![](https://pic.imgdb.cn/item/63b2a1155d94efb26f105ac4.jpg)

构建二元掩码的过程采用加权式随机选择（**WRS**）。对于每个通道的分数$score_i$，生成介于$0$和$1$之间的一个随机数$r_i$，从而得到一个键值$key_ir_i^{-score_i}$。选择其中$M$个最大的键值并将对应的$mask_i$设置为$1$。

![](https://pic.imgdb.cn/item/63b2a3045d94efb26f12d1ca.jpg)

对于更小的数据集，可以额外地使用一个随机数生成器，即使$mask_i$已被设置为$1$，对应的通道仍有可能不被选择。这样做的理由是对于某个预训练模型，在较深卷积层中仅有少量通道被分配了较大的激活值，其它激活值很小。如果网络仅根据这些层中的分数选择通道，那么有可能对于每张图像，被选择的通道序列在每次前向通过时都一样。

使用**WRS**后在收敛之前会造成更高的训练样本误差，因此收敛速度更慢；同时所得到的测试误差更低，即可以降低训练阶段中的过拟合。

![](https://pic.imgdb.cn/item/63b2a4d45d94efb26f1524b5.jpg)
