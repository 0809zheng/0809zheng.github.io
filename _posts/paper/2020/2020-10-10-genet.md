---
layout: post
title: 'Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks'
date: 2020-10-10
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63ad79f908b6830163c2a93e.jpg'
tags: 论文阅读
---

> GENet：在通道注意力中利用特征上下文.

- paper：[Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks](https://arxiv.org/abs/1810.12348)

**GENet**是对通道注意力网络[<font color=blue>SENet</font>](https://0809zheng.github.io/2020/10/01/senet.html)的改进。**SENet**包括**Squeeze**过程和**Excitation**过程。**Squeeze**过程对特征$x$沿着通道维度进行全局平均池化，**Excitation**过程通过两层全连接层学习通道之间的相关性。

本文作者指出，**SENet**的**Squeeze**过程对通道的空间维度统计量估计是粗略的（仅考虑了均值这个一阶统计量）。**GENet**把**Squeeze**过程替换为**Gather**过程，即对每个局部的空间位置提取一个统计量，用于捕捉特征之间的上下文信息；对应的**Excite**操作则用于将其进行缩放还原回原始尺寸。

![](https://pic.imgdb.cn/item/63ad7a4808b6830163c30e7b.jpg)

统计量的提取可以通过具有较大卷积核尺寸的通道卷积实现，引入可学习的参数；空间尺寸的还原通过插值操作实现。

```python
import torch.nn as nn
import torch.nn.functional as F

class GEModule(nn.Module):
    def __init__(self, channels, kernel_size):
        super(GEModule, self).__init__()
        self.downop = nn.Sequential(
            nn.Conv2d(channels, channels, groups=channels,
                      stride=1, kernel_size=kernel_size, padding=0,
                      bias=False,),
            nn.BatchNorm2d(channels),)
        self.mlp = nn.Sequential(
            nn.Conv2d(
                channels, channels // 16,
                kernel_size=1, padding=0, bias=False),
            nn.ReLU(),
            nn.Conv2d(
                channels // 16, channels,
                kernel_size=1, padding=0, bias=False),)

    def forward(self, x):
        out = self.downop(x)
        out = self.mlp(out)
        shape_in = x.shape[-1]
        out = F.interpolate(out, shape_in)
        out = torch.sigmoid(out)
        out = x * out
        return out

x = torch.rand((16, 256, 64, 64))
genet = GEModule(256, 32)
print(genet(x).shape) # torch.Size([16, 256, 64, 64])
```
