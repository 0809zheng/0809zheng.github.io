---
layout: post
title: 'ADADELTA: An Adaptive Learning Rate Method'
date: 2020-12-06
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/61f3e9ff2ab3f51d912b41f2.jpg'
tags: 论文阅读
---

> Adadelta：一种自适应学习率方法.

- paper：[ADADELTA: An Adaptive Learning Rate Method](https://arxiv.org/abs/1212.5701)

梯度下降算法可以表示为：

$$ x_{t+1} = x_t - \eta g_t $$

其中学习率$\eta$需要手动调整。过大的学习率可能会使过程不收敛甚至发散，过低的学习率会导致收敛速度缓慢。本文提出了一种动态学习率算法**Adadelta**，主要特点如下：
- 不需要手动设置学习率；
- 对超参数不敏感；
- 对参数的每个维度单独设置动态的学习率；
- 为梯度下降引入最少的计算量；
- 对较大的梯度、噪声和结构选择具有鲁棒性；
- 适用于本地或分布式环境中。

**Adadelta**是对**Adagrad**算法的改进。**Adagrad**算法的梯度更新公式如下：

$$ x_{t+1} = x_t - \frac{\eta}{\sqrt{\sum_{\tau=1}^{t}g_{\tau}^2}} g_t $$

**Adagrad**算法为参数的每个维度设置了不同的动态学习率。梯度较大的参数维度具有较小的学习率，梯度较小的参数维度具有较大的学习率，这使得不同维度上的参数更新是平衡的。此外，随着训练轮数的推移，学习率逐渐衰减。

**Adagrad**算法也存在一些缺点：
- 学习率在整个训练过程中持续衰减：由于分母中平方梯度的不断累积，动态学习率将在整个训练过程中持续下降，最终变得非常小。
- 该方法对参数的初始值和相应的梯度非常敏感：如果初始梯度较大，则整个训练过程中的学习率普遍较低，因此需要手动选择合适的全局学习率。

### 改进①：沿窗口累积

不累计所有轮数的平方梯度，而是指定累计最近$w$次迭代中的平方梯度，通过这种窗口式累计，总平方梯度不会累积到无穷大，而是使用最近一些轮数的梯度进行局部估计。

由于存储最近的$w$次平方梯度需要额外的内存和运算时间，因此采用指数衰减平均的方式累计平方梯度：

$$ G_t = \rho G_{t-1} + (1-\rho)g_t^2 $$

超参数$\rho$为衰减率，此时梯度更新公式如下：

$$ x_{t+1} = x_t - \frac{\eta}{\sqrt{G_t+\epsilon}} g_t $$

### 改进②：使用Hessian近似校正单位

参数更新时应保证具有相同的单位，如果参数$x$具有一些假设单位，则对该参数的更新量$\Delta x$也应该具有相同的单位。梯度更新属于一阶优化方法，在更新时无法保证更新量$\Delta x$与原参数$x$具有相同的单位：

$$ \Delta x ∝ g  = \frac{\partial f}{\partial x} ∝ \frac{1}{\partial x} $$

注意到使用**Hessian**矩阵的二阶优化方法(如牛顿法)具有正确的参数更新单位：

$$ \Delta x ∝ H^{-1}g  = \frac{\frac{\partial f}{\partial x}}{\frac{\partial^2 f}{\partial x^2}} ∝ x $$

因此考虑使用与$H^{-1}$具有相同单位(二阶导数的逆)的数值对梯度$g$的单位进行修正，注意到：

$$ \Delta x  = \frac{\frac{\partial f}{\partial x}}{\frac{\partial^2 f}{\partial x^2}} \to \frac{1}{\frac{\partial^2 f}{\partial x^2}} = \frac{\Delta x}{\frac{\partial f}{\partial x}} $$

由于梯度的方均根已经在改进①中出现在分母上校正，因此在分子上添加参数的更新量进行校正。具体地，采用指数衰减平均的方式累计平方参数更新量：

$$ ΔX_t = \rho ΔX_{t-1} + (1-\rho)\Delta x_{t}^2 $$

**Adagrad**算法最终的参数更新量和梯度更新公式如下：

$$ Δx_t = -\frac{\sqrt{ΔX_{t-1}+\epsilon}}{\sqrt{G_t+\epsilon}} g_t $$

$$ x_{t+1} = x_t +Δx_t $$

