---
layout: post
title: 'Neural Architecture Search for Lightweight Non-Local Networks'
date: 2020-11-24
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6433ea610d2dde5777faacdb.jpg'
tags: 论文阅读
---

> 轻量级非局部网络的神经结构搜索.

- paper：[Neural Architecture Search for Lightweight Non-Local Networks](https://arxiv.org/abs/2004.01961)

**Non-Local**模块引入自注意力机制提取全局特征，但是计算量巨大，引入大量的矩阵乘法，因此**Non-Local**的位置相对固定，一般只放置在网络高层用于高维特征编码。

本文作者通过神经结构搜索技术来设计包含**Non-Local**模块的卷积神经网络，为了实现搜索，首先设计了一种轻量级的**Non-Local**模块：**LightNL**。

![](https://pic.imgdb.cn/item/6433eb340d2dde5777fb6a43.jpg)

**LightNL**相比于原始的**Non-Local**模块，主要有三点差异：
- $Q,K,V$的生成去掉了$1\times 1$卷积，使得特征图更依赖于前层网络的学习能力；
- 输出卷积从$1\times 1$卷积替换为深度卷积，模型的**Flops**会减小；
- 使用更小的特征图计算相关性。

对于第三点，作者简单的使用采样的方法，对于输入的特征$(H, W, C)$，在**spatial**维度和**channel**维度上进行降采样，具体实现的方法是:
- 在空间维度上按照步长$s$进行采样，`f[:, ::nl_stride, ::nl_stride, :]`
- 在通道维度上取前`int(nl_ratio * n_in)`个通道，`f[:, :, :, :int(nl_ratio * n_in)]`

于是这里引入了两个可搜索参数`nl_stride`和`nl_ratio`(小于$1$)；另外还有一个隐式的搜索参数：插入**LightNL**的位置`Location`。

在实现时，对$Q$仅进行通道下采样$x_c$，对$K$同时进行通道和空间下采样$x_{sc}$，对$V$仅进行空间下采样$x_s$。$x_c,x_{sc},x_s$这三者之间的矩阵相乘是可以通过结合律等价实现的，但是两种计算方法的总计算量是不同的：
- $(x_cx_{sc}^T)x_s$和$x_c(x_{sc}^Tx_s)$的计算量不同，按照二阶矩阵相乘时的乘加运算计算次数是一个只和$HW$与$C$相关的比值（比如乘法次数之比为$HW:C$）
- 因此代码实现的时候可以通过比较$H×W$与$C$的大小，决定结合律使用的方式，减少计算量

作者设置**IBN(Inverted bottleneck network)**作为主要搜索的模块，在每一个**IBN**的第二个**pw-conv**之后加入待搜索的**LightNL**模块，而是否使用该模块则通过一个隐变量$t$决定，当满足**dw-conv**权重的二范数大于阈值$t$时，**LightNL**才会被使用。否则**LightNL**输出端的**dw-conv**的权重会被置为全$0$。

于是，整个搜索空间的可搜索参数为:
- **IBN**的卷积核大小：$$k \in \{3,5\}$$
- **IBN**通道扩张率：$$\alpha \in \{3,6\}$$
- **LightNL**的空间压缩率：$$\text{nl_stride} \in \{1,2\}$$ 
- **LightNL**的通道压缩率：$$\text{nl_ratio} \in \{0.25,0.125\}$$ 
- **LightNL**的位置阈值$t$：设置为可学习的参数，通过梯度下降更新

搜索得到的网络结构如下：

![](https://pic.imgdb.cn/item/6433eff70d2dde57770127e1.jpg)