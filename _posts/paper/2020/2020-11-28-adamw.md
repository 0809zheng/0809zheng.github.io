---
layout: post
title: 'Decoupled Weight Decay Regularization'
date: 2020-11-28
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/62668f93239250f7c5a54d81.jpg'
tags: 论文阅读
---

> AdamW：解耦梯度下降与权重衰减正则化.

- paper：[Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)

在标准的随机梯度下降算法中，**L2**正则化与权重衰减正则化是等价的；但是对于自适应梯度算法(如**Adam**)并不适用。本文将自适应梯度下降算法中的权重衰减解耦出来，提高了优化算法的泛化能力。本文的主要结论如下：
1. **L2**正则化与权重衰减不等价；
2. **L2**正则化对**Adam**无效；
3. 权重衰减在**SGD**和**Adam**中同样有效；
4. 最优权重衰减值取决于权重更新次数；
5. **Adam**能够从全局学习率乘子中受益。

# 1. 解耦权重衰减

在损失函数中引入**L2**正则化的形式如下：

$$ f_t^{reg}(\theta) = f_t(\theta)+\frac{\lambda}{2\alpha} ||\theta_t||_2^2 $$

当应用标准的随机梯度下降算法**SGD**时，参数的更新过程如下：

$$ \theta_{t+1} = \theta_t-\alpha\nabla f_t^{reg}(\theta_t) =  \theta_t-\alpha\nabla f_t(\theta_t)- \lambda \theta_t  = (1-\lambda)\theta_t-\alpha\nabla f_t(\theta_t) $$

因此在**SGD**中，**L2**正则化也被等价地称为权重衰减正则化。然而这种等价性在自适应梯度算法中不成立。为了解耦学习率$\alpha$与正则化因子$\lambda$，**SGD**算法中在最后进行参数的梯度更新时加入权重衰减项，即**SGDW**算法。

![](https://pic.imgdb.cn/item/6266974b239250f7c5b7bb4e.jpg)

在**Adam**等自适应梯度算法中，使用梯度的二阶矩进行梯度缩放。因此对于具有较大梯度的权重，其**L2**正则化项会被缩小，从而与权重衰减的正则化不等价。因此将权重衰减从梯度更新过程中解耦，使得所有权重以相同的正则化程度进行衰减，即**AdamW**算法。

![](https://pic.imgdb.cn/item/62669760239250f7c5b7f4a9.jpg)

# 2. 实验分析

下图展示了当应用权重衰减正则化时，调整学习率策略能够获得更大的参数搜索空间，取得更好的表现。

![](https://pic.imgdb.cn/item/62669d50239250f7c5c7bb64.jpg)


下图展示了调整学习率$\alpha$与正则化因子$\lambda$对损失函数产生的改变。从图中可以看出，若采用**L2**正则化，则两个超参数的调整具有一定的相关性；若采用权重衰减正则化，则两个超参数被解耦，从而具有可分离的参数空间。

![](https://pic.imgdb.cn/item/62669d6b239250f7c5c837ba.jpg)

**AdamW**使得测试误差降低了约$15\%$。

![](https://pic.imgdb.cn/item/62669db6239250f7c5c98cd2.jpg)

