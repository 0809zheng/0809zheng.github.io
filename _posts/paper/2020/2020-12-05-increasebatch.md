---
layout: post
title: 'Don’t Decay the Learning Rate, Increase the Batch Size'
date: 2020-12-05
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/61f00bd32ab3f51d91b4fab7.jpg'
tags: 论文阅读
---

> 通过增加批量大小代替学习率衰减.

- paper：[Don't Decay the Learning Rate, Increase the Batch Size](https://arxiv.org/abs/1711.00489)

通常在训练深度学习模型时会衰减学习率，作者认为在训练时增加批量的大小也可以获得类似的学习曲线。这两种方法在相同的训练次数下能够取得相似的测试精度，但后者所进行的参数更新次数更少，并行性更好，缩短了训练时间。作者在**ImageNet**上训练**ResNet50**模型，仅用$0.5 h$达到$76.1 \%$的验证集准确率。

# 1. 随机梯度下降中的噪声建模

在强凸函数的优化过程中，学习率应满足：

$$ \sum_{i}^{∞}\epsilon_i = ∞, \quad \sum_{i}^{∞}\epsilon_i^2 < ∞ $$

其中$\epsilon_i$表示第$i$次梯度更新时的学习率。左式确保训练过程能够达到最小值，无论参数初始化在什么位置；右式表示学习率衰减足够快，确保训练过程能够收敛到最小值，而不是在最小值周围震荡。

若用$C$表示代价函数，$w$表示待优化参数，这些参数随时间$t$(即训练轮数)的变化可以建模为随机微分方程：

$$ \frac{dw}{dt} = -\frac{dC}{dw} + \eta(t) $$

其中$\eta(t)$表示高斯随机噪声，是由批量梯度估计全量梯度造成的。若总样本数为$N$，批量大小为$B$，则噪声尺度$g$估计为：

$$ g = \epsilon(\frac{N}{B}-1) $$

从上式可以观察到，衰减学习率会使噪声尺度减小，从而收敛到代价函数的最小值。若增加批量大小，也可以在固定的学习率下实现相同速率的噪声降低。因此在训练期间可以固定学习率，逐渐增加批量大小，直至$B=N/10$后再衰减学习率。

当随机梯度下降引入动量时，噪声尺度$g$估计为：

$$ g = \frac{\epsilon}{1-m}(\frac{N}{B}-1) $$

因此也可以协调地调整动量系数$m$和批量大小$B$，尽管实验发现这样会轻微降低精度。

# 2. 实验分析

为了验证衰减学习率和增加批量大小之间的等价性，作者设置了三种不同的学习策略：

![](https://pic.imgdb.cn/item/61f0b6a02ab3f51d913bea3d.jpg)

作者绘制了训练过程中的损失和精度随**epoch**变化的曲线，发现三种学习策略得到的学习曲线几乎是完全相同的：

![](https://pic.imgdb.cn/item/61f0b6f52ab3f51d913c23b6.jpg)

作者进一步绘制了训练过程中的损失和精度随更新轮数变化的曲线，注意到此时每轮更新中增大批量大小能够处理更多的样本，因此增加批量大小能够更快的收敛。若增大批量的同时调整学习率和动量，则收敛速度还会更快。

![](https://pic.imgdb.cn/item/61f0bc542ab3f51d913fba01.jpg)