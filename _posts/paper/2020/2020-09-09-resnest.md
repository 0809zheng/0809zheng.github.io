---
layout: post
title: 'ResNeSt: Split-Attention Networks'
date: 2020-09-09
author: 郑之杰
cover: 'https://pic.downk.cc/item/5fec3b333ffa7d37b3606820.jpg'
tags: 论文阅读
---

> ResNeSt：拆分注意力网络.

- paper：[ResNeSt: Split-Attention Networks](https://arxiv.org/abs/2004.08955)
- code：[github](https://github.com/zhanghang1989/ResNeSt)

作者指出，目前卷积神经网络的结构设计主要有两种思路。
1. 第一种是模块化设计，通过设计模块进行堆叠，组成深度网络，在分类任务上预训练后可以应用于各种计算机视觉任务中。这类方法的主要优点是具有较好的泛化性和可替代性，新的网络只需要替换掉基本模块就可以提高性能；主要缺点是含有较多的超参数，需要人工选择。这类方法的代表作是**GoogLeNet**、**ResNet**、**Inception ResNet**、**ResNeXt**。
2. 第二种是神经结构搜索，这类方法预先设定网络的模块结构，通过自动搜索选定合适的超参数。这类方法优点是无需人工选择，缺点是计算量较大，且**tailored to one task**，即在分类任务上训练的模型通常只适合分类，很难迁移到其他任务中。

目前计算机视觉领域的下游工作（如目标检测、图像分割）仍选择结构简单通用的**ResNet**为**backbone**网络。在本文中作者按照第一种思路设计了一个新的卷积网络**backbone**：**ResNeSt**，该网络将**SKNet**中的注意力机制引入**ResNeXt**中，经过预训练后替换掉**ResNet**网络，在各项任务中刷新**SOTA**的结果：

![](https://pic.downk.cc/item/5fec3def3ffa7d37b365d9e0.jpg)

**ResNeSt**网络提出的模块结构如下图所示。该网络的两个核心思想是**multi-path**和**channel attention**。前者参考**ResNeXt**，引入超参数**cardinality** $k$控制分组卷积的分支数；后者参考**SKNet**，引入超参数**radix** $r$控制注意力计算的分支数。

![](https://pic.downk.cc/item/5fec3e5a3ffa7d37b366a46a.jpg)

作者提出的**拆分注意力(split attention)**计算过程如下。

![](https://pic.imgdb.cn/item/6432759c0d2dde577737f9aa.jpg)

作者给出了两种实现思路，分别是**cardinality-major**方法（下图左）和**radix-major**方法（下图右），两者可由下图中进行等价变换。通过变换能够并行计算，加速模型训练。

![](https://pic.downk.cc/item/5fec52fa3ffa7d37b38b8655.jpg)

除此之外，作者训练模型时还使用了大量数据增强方法，包括：
- **Large Mini-batch Distributed Training**
- **Label Smoothing**
- **Auto Augmentation**
- **Mixup Training**
- **Large Crop Size**
- **Regularization**
