---
layout: post
title: 'Image Super-Resolution Using Very Deep Residual Channel Attention Networks'
date: 2020-08-01
author: 郑之杰
cover: 'https://pic.downk.cc/item/5f43acf4160a154a67279065.jpg'
tags: 论文阅读
---

> RCAN：残差通道注意力网络.

- paper：Image Super-Resolution Using Very Deep Residual Channel Attention Networks
- arXiv：[link](https://arxiv.org/abs/1807.02758)


作者认为在图像超分辨率任务中，卷积神经网络的深度是非常重要的，但是深度网络的训练困难。因此作者提出了一种**Residual in Residual**结构，可以训练深层网络，从而保留更多的低频信息。另一方面，作者认为卷积网络提取的特征不同通道的重要形式性是不同的，因此引入了**Channel Attention**。

![](https://pic.downk.cc/item/5f43b369160a154a672d7dd4.jpg)

# 网络结构

### 1. Channel attention (CA)

![](https://pic.downk.cc/item/5f43b3fa160a154a672e025c.jpg)

通道注意力机制**CA**首先将尺寸为$H×W×C$的特征映射通过全局平均池化压缩为$1×1×C$，再通过通道下采样卷积压缩通道为$1×1×\frac{C}{r}$，通过**ReLU**激活函数后进行通道上采样卷积恢复到$1×1×C$，通过**Sigmoid**激活函数后得到每个通道的权重系数，再与原特征映射相乘得到处理后的特征映射$H×W×C$。

### 2. Residual channel attention block (RCAB)

![](https://pic.downk.cc/item/5f43b40b160a154a672e146c.jpg)

**RCAB**在**CA**的基础上引入了残差连接，使得低频特征信息得到一定程度的保留。

### 3. Residual Group (RG)

![](https://pic.downk.cc/item/5f43b426160a154a672e2ec2.jpg)

**RG**由若干个**RCAB**和卷积组成，并引入了较短的残差连接（**short skip connection，SSC**）；**SSC**用于学习局部的残差信息，并加速了训练。

### 4. Residual in Residual (RIR)

![](https://pic.downk.cc/item/5f43b46e160a154a672e76e0.jpg)

**RIR**由若干个**RG**和卷积组成，并引入了较长的残差连接（**long skip connection，LSC**））；**LSC**用于学习全局的残差信息，并加速了网络训练。

# 实验结果
实验设置：
- 训练集：DIV2K中的800张图像；
- 测试集：Set5, Set14, B100, Urban100, Manga109
- 评估指标：PSNR, SSIM,  top-1 and top-5 recognition errors
- 训练尺寸：裁剪48×48

作者首先通过实验验证了残差连接和通道注意力对实验结果的提升：

![](https://pic.downk.cc/item/5f43b857160a154a6731ddc3.jpg)

在测试集上的评估指标均达到了SOTA，其中**RCAN+**表示多个模型的集成：

![](https://pic.downk.cc/item/5f43ba6d160a154a6733c5d0.jpg)

评估指标和参数量的折中：

![](https://pic.downk.cc/item/5f43cdd5160a154a67451165.jpg)

![](https://pic.downk.cc/item/5f43bc36160a154a673580f3.jpg)