---
layout: post
title: 'Residual Attention Network for Image Classification'
date: 2020-10-30
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/643267830d2dde57772290d3.jpg'
tags: 论文阅读
---

> 图像分类的残差注意力网络.

- paper：[Residual Attention Network for Image Classification](https://arxiv.org/abs/1704.06904)

**Attention**的出发点是将注意力集中在部分显著或者是感兴趣的图像点上。其实卷积网络本身就自带**Attention**效果，以分类网络为例，高层**feature map**所激活的**pixel**也恰好集中在分类任务相关的区域。

![](https://pic.imgdb.cn/item/643268630d2dde577723b088.jpg)

本文提出一种可堆叠的**Residual Attention Module**模块，在普通的**ResNet**网络中，增加侧分支，侧分支通过一系列的卷积和池化操作，逐渐提取高层特征并增大模型的感受野。

高层特征的激活对应位置能够反映**attention**的区域，然后再对这种具有**attention**特征的**feature map**进行上采样，使其大小回到原始**feature map**的大小。

**attention map**对应到原始图片的每一个位置上，与原来的**feature map**进行**element-wise product**的操作，相当于一个权重器，增强有意义的特征，抑制无意义的信息。

![](https://pic.imgdb.cn/item/64326a660d2dde5777271374.jpg)

**Attention Module**分为两个分支，右边的分支就是普通的卷积网络，即主干分支，叫做**Trunk Branch**。左边的分支是为了得到一个掩码**mask**，该掩码的作用是得到输入特征的**attention map**，所以叫做**Mask Branch**，这个**Mask Branch**包含**down sample**和**up sample**的过程，目的是为了保证和右边分支的输出大小一致。

![](https://pic.imgdb.cn/item/64326add0d2dde577727c6d8.jpg)

得到**Attention map**的**mask**以后，可以直接用**mask**和主干分支进行一个**element-wise product**的操作，即$M(x) \cdot T(x)$，来对特征做一次权重操作。但是这样导致的问题就是：$M(x)$的掩码是通过最后的**sigmoid**函数得到的，$M(x)$值在$[0, 1]$之间，连续多个**Module**模块直接相乘的话会导致**feature map**的值越来越小，同时也有可能打破原有网络的特性，使得网络的性能降低。比较好的方式就借鉴**ResNet**恒等映射的方法：

$$
H(x) = (1+M(x)) \cdot T(x)
$$

其中$M(x)$为**Soft Mask Branch**的输出，$T(x)$为**Trunk Branch**的输出，那么当$M(x)=0$时，该层的输入就等于$T(x)$，因此该层的效果不可能比原始的$T(x)$差，这一点也借鉴了**ResNet**中恒等映射的思想，同时这样的加法，也使得**Trunk Branch**输出的**feature map**中显著的特征更加显著，增加了特征的判别性。经过这种残差结构的堆叠，能够很容易的将模型的深度达到很深的层次，具有非常好的性能。

```python
def attention_block(input, input_channels=None, output_channels=None, encoder_depth=1):
    p = 1
    t = 2
    r = 1
    if input_channels is None:
        input_channels = input.get_shape()[-1].value
    if output_channels is None:
        output_channels = input_channels
    # First Residual Block
    for i in range(p):
        input = residual_block(input)
    # Trunk Branch
    output_trunk = input
    for i in range(t):
        output_trunk = residual_block(output_trunk)
        
    # Soft Mask Branch
    ## encoder
    ### first down sampling
    output_soft_mask = MaxPool2D(padding='same')(input)  # 32x32
    for i in range(r):
        output_soft_mask = residual_block(output_soft_mask)
 
    skip_connections = []
    for i in range(encoder_depth - 1):
        ## skip connections
        output_skip_connection = residual_block(output_soft_mask)
        skip_connections.append(output_skip_connection)
        ## down sampling
        output_soft_mask = MaxPool2D(padding='same')(output_soft_mask)
        for _ in range(r):
            output_soft_mask = residual_block(output_soft_mask)
 
    ## decoder
    skip_connections = list(reversed(skip_connections))
    for i in range(encoder_depth - 1):
        ## upsampling
        for _ in range(r):
            output_soft_mask = residual_block(output_soft_mask)
        output_soft_mask = UpSampling2D()(output_soft_mask)
        ## skip connections
        output_soft_mask = Add()([output_soft_mask, skip_connections[i]])
 
    ### last upsampling
    for i in range(r):
        output_soft_mask = residual_block(output_soft_mask)
    output_soft_mask = UpSampling2D()(output_soft_mask)
 
    ## Output
    output_soft_mask = Conv2D(input_channels, (1, 1))(output_soft_mask)
    output_soft_mask = Conv2D(input_channels, (1, 1))(output_soft_mask)
    output_soft_mask = Activation('sigmoid')(output_soft_mask)
 
    # Attention: (1 + output_soft_mask) * output_trunk
    output = Lambda(lambda x: x + 1)(output_soft_mask)
    output = Multiply()([output, output_trunk])  #
 
    # Last Residual Block
    for i in range(p):
        output = residual_block(output)
 
    return output
```