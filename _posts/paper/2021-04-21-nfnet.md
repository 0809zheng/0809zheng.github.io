---
layout: post
title: 'High-Performance Large-Scale Image Recognition Without Normalization'
date: 2021-04-21
author: 郑之杰
cover: 'https://img.imgdb.cn/item/607fffa8af7e6730a78dc149.jpg'
tags: 论文阅读
---

> NFNet：不使用BatchNorm的大尺度图像分类网络.

- paper：High-Performance Large-Scale Image Recognition Without Normalization
- arXiv：[link](https://arxiv.org/abs/2102.06171)

**Batch Norm**作为深度学习的常用优化方法之一，几乎被应用于所有模型中。但是**Batch Norm**也存在一些问题：
1. 它带来昂贵的计算代价，增加内存开销，并显著增加了计算梯度所需的时间（前向传播和反向传播都需要额外计算，并且需要暂存中间值）；
2. 它带来模型训练和推理时的差异，引入需要微调的超参数（训练时不同**batch**独立计算，推理时使用训练时的滑动平均）；
3. 它打破了训练样本不同**minibatch**之间的独立性（使用多块**GPU**计算时，每块**GPU**处理不同的**minibatch**，前向传播时独立计算**Batch Norm**，但反向传播时需要交互）。

作者提出了一种**自适应梯度裁剪(Adaptive Gradient Clipping,AGC)**策略，通过应用该策略，模型即使不使用**Batch Norm**也取得最好的性能。

![](https://img.imgdb.cn/item/60800a17af7e6730a7dabd9e.jpg)

梯度裁剪是指限制梯度的范数值。通常的梯度裁剪表示如下：

$$ G → \begin{cases} \lambda \frac{G}{|| G ||} \quad \text{if } ||G|| > \lambda \\ G \quad \text{otherwise} \end{cases} $$

作者提出的自适应梯度裁剪，不是限制梯度的范数值，而是限制参数的梯度和参数值的范数值之比：

$$ G → \begin{cases} \lambda \frac{|| W ||}{|| G ||}G \quad \text{if } ||G|| > \lambda \\ G \quad \text{otherwise} \end{cases} $$

