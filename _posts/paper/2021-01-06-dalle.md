---
layout: post
title: 'Learning Transferable Visual Models From Natural Language Supervision'
date: 2021-01-06
author: 郑之杰
cover: 'https://pic.downk.cc/item/5ff555b83ffa7d37b381af0e.jpg'
tags: 论文阅读
---

> DALL·E：从文本生成图像.

- 论文：[Learning Transferable Visual Models From Natural Language Supervision](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf)
- 模型：[DALL·E: Creating Images from Text](https://openai.com/blog/dall-e/)
- 方法：[CLIP: Connecting Text and Images](https://openai.com/blog/clip/)

# DALL·E: Creating Images from Text
**OpenAI**提出了**DALL·E**模型，该模型可以从包含大量概念的文本描述中生成相关图像。其名称**DALL·E**致敬了艺术家**Salvador Dalí**和皮克斯动画角色**WALL·E**。

**DALL·E**采用了**GPT-3**结构，共有$1200$万参数。该模型同时接收图像和其文本描述作为输入，输入使用$1280$个**token**，包括$256$个词汇量是$16384$的文本**token**和$1024$个词汇量是$8192$的图像**token**。对于文本**token**，使用标准的随机**mask**；对于图像**token**，使用稀疏注意力。根据极大似然算法进行自回归训练。

训练时图像尺寸被调整为$256 \times 256$，并参考**VQVAE**模型将其压缩为$32 \times 32$的特征，使用字典对每个特征进行编码。该模型能够从文本描述中生成图像：

![](https://pic.downk.cc/item/5ff558353ffa7d37b383298b.jpg)

上述结果是在生成的$512$个样本中选择的前$32$个质量最好的样本，选择过程使用了**CLIP**模型。

# CLIP: Connecting Text and Images
**Contrastive Language-Image Pre-training (CLIP)**方法用于在图像和文本数据集中进行匹配。具体地，训练一个文本编码器和图像编码器，分别得到文本和图像的编码，并计算两者的匹配程度。通过训练使得匹配样本的编码内积最大，不匹配的样本内积减小。模型完成训练后，对于一张图像，可以选出匹配程度最高的一个文本：

![](https://pic.downk.cc/item/5ff55f393ffa7d37b3878e17.jpg)
