---
layout: post
title: 'Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet'
date: 2021-08-06
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/610bae695132923bf854f012.png'
tags: 论文阅读
---

> 使用全连接层替换**ViT**中的自注意力层.

- paper：Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet
- arXiv：[link](https://arxiv.org/abs/2105.02723)

作者认为目前流行的视觉**transformer**中的自注意力层并不是影响模型性能的最重要部分，而是由其他部分决定的，如由**patch**嵌入引入的归纳偏置或精心设计的数据增强。

为了验证自注意力层对模型性能的影响，作者将自注意力层换成全连接层。具体地，先对**patch**嵌入序列按照特征通道进行全连接运算，再将特征转置后按照**patch**进行全连接运算，并恢复转置。

![](https://pic.imgdb.cn/item/610bae875132923bf8551a14.jpg)

通过简单的实验，未经过调参的模型获得了**74.9%**的**top-1**准确度，相比之下，**ViT**和**DeiT**分别获得了**77.9%**和**79.9%**的准确度。这表明自注意力层并不是视觉**transformer**结构中最重要的部分。

![](https://pic.imgdb.cn/item/610bae9e5132923bf8553cd7.jpg)