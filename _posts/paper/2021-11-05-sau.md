---
layout: post
title: 'SAU: Smooth activation function using convolution with approximate identities'
date: 2021-11-05
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6184dab12ab3f51d9116ca04.jpg'
tags: 论文阅读
---

> SAU：使用Dirac函数构造激活函数的光滑近似.

- paper：[SAU: Smooth activation function using convolution with approximate identities](https://arxiv.org/abs/2109.13210)

作者提出了使用**Dirac**函数对现有的非光滑激活函数(如**ReLU,Leaky ReLU**)进行光滑近似，并进一步提出了光滑激活单元(**smooth activation unit, SAU**)。

# 1. Dirac函数
**Dirac**函数又称**Dirac**-$\delta$函数、单位冲激函数，是一种广义函数(泛函)，表达式如下：

$$ \delta(x) = \begin{cases} +∞, & x =0 \\ 0, & x≠0 \end{cases}, \quad \int_{-∞}^{+∞}\delta(x)dx = 1 $$

**Dirac**函数可以看成一个连续型概率密度函数，仅在$x=0$处取值，均值为$0$，方差也为$0$。

**Dirac**函数本身没有明显的意义，但将其作用于其他函数时，具有重要意义。此时主要性质包括：

**Dirac**函数的采样空间为$\Bbb{R}$，但只在$x=0$处概率为$1$；因此**Dirac**函数可以通过积分采样到其他函数在$x=0$处的值：

$$ \int_{-∞}^{+∞}f(x)\delta(x)dx = f(0) $$

**Dirac**函数也可以采样任意点$x$处的函数值（**Dirac**函数构造任意函数光滑近似的保证）：

$$ \int_{-∞}^{+∞}f(y)\delta(x-y)dy = f(x) $$

**Dirac**函数的导数可以采样其他函数的导数：

$$ \int_{-∞}^{+∞}f(x)\delta'(x)dx = \int_{-∞}^{+∞}f(x)d\delta(x) \\ = f(x)\delta(x)|_{-∞}^{+∞}-\int_{-∞}^{+∞}\delta(x)df(x) \\ = -\int_{-∞}^{+∞}f'(x)\delta(x)dx =-f'(0) $$

也可以提取高阶导数：

$$ \int_{-∞}^{+∞}f(x)\delta^{(n)}(x)dx = (-1)^{n}f^{(n)}(0) $$

# 2. Dirac函数的光滑近似
**Dirac**函数没有显式表达式，因此常采用一些连续函数作为**Dirac**函数的光滑近似。

一种寻找**Dirac**函数的光滑近似的方法是首先构造类似于正态分布的“钟形曲线”，之后设法让钟形曲线的宽度趋近于$0$，并保持积分为$1$。此时常用的近似包括：

$$ \delta(x) = \mathop{\lim}_{\sigma \to 0} \frac{e^{-x^2/2\sigma^2}}{\sqrt{2\pi}\sigma} \quad ① $$

$$ \delta(x) = \frac{1}{\pi}\mathop{\lim}_{a \to 0} \frac{a}{x^2+a^2} \quad ② $$

另一种光滑近似思路是注意到**Dirac**函数的积分是单位阶跃函数$\theta(x)$：

$$ \int_{-∞}^{x}\delta(x)dx = \theta(x) = \begin{cases} 1, & x >0 \\ 0, & x<0 \end{cases} $$

若能找到$\theta(x)$的光滑近似，其导数即为**Dirac**函数的光滑近似。$\theta(x)$的光滑近似通常是一些“S”型曲线，如**sigmoid**函数$\sigma(x)$，则**Dirac**函数的一个光滑近似为：

$$ \delta(x) =  \mathop{\lim}_{t \to ∞} \frac{d}{dx}\sigma(tx) \\= \mathop{\lim}_{t \to ∞}t\sigma(x)(1-\sigma(x))\\ = \mathop{\lim}_{t \to ∞} \frac{te^{tx}}{(1+e^{tx})^2} \quad ③ $$

# 3. SAU：使用Dirac函数近似激活函数
根据**Dirac**函数的性质：

$$ f(x) = \int_{-∞}^{+∞}f(y)\delta(x-y)dy  $$

如果能找到**Dirac**函数的光滑近似$\phi(x)≈\delta(x)$，当$f(x)$是一个具有可数个间断点的连续函数时，可以构造$f(x)$的光滑近似：

$$ g(x)≈ \int_{-∞}^{+∞}f(y)\phi(x-y)dy = (f* \phi)(x) $$

注意到上式表示为两个函数的卷积，其中$f$为近似的原函数，$\phi$为**Dirac**函数的光滑近似(文中称**approximate identity**)。预先指定一个激活函数，则其光滑近似可以由该激活函数与一个**Dirac**函数的光滑近似通过卷积操作得到。

**SAU**是指选择**Leaky ReLU**激活函数，选择正态分布的极限(式①)作为**Dirac**函数的光滑近似，从而构造一个光滑的激活函数。其参数$\alpha$(来自**Leaky ReLU**)和参数$\sigma$(来自**Dirac**函数的光滑近似)通过梯度下降法学习得到。

下面介绍几种使用**Dirac**函数近似的激活函数：

### ① 近似Leaky ReLU
**Leaky ReLU**激活函数表示为$f(x)=\max(x,\alpha x)$。若选用式①作为**Dirac**函数的光滑近似，则有：

$$ \max(x,\alpha x) ≈ \int_{-∞}^{+∞}\max(y,\alpha y)\frac{e^{-(x-y)^2/2\sigma^2}}{\sqrt{2\pi}\sigma}dy \\ = \int_{-∞}^{0}\alpha y\frac{e^{-(x-y)^2/2\sigma^2}}{\sqrt{2\pi}\sigma}dy+\int_{0}^{+∞}y\frac{e^{-(x-y)^2/2\sigma^2}}{\sqrt{2\pi}\sigma}dy \\ = \frac{\alpha}{\sqrt{2\pi}\sigma} \int_{-∞}^{0} ye^{-\frac{(x-y)^2}{2\sigma^2}}dy +  \frac{1}{\sqrt{2\pi}\sigma} \int_{0}^{+∞} ye^{-\frac{(x-y)^2}{2\sigma^2}}dy   $$

注意到上面的积分：

$$  \int_{}^{} ye^{-\frac{(x-y)^2}{2\sigma^2}}dy = \int_{}^{} (y-x+x)e^{-\frac{(y-x)^2}{2\sigma^2}}dy \\ = \int_{}^{} (y-x)e^{-\frac{(y-x)^2}{2\sigma^2}}dy + x\int_{}^{} e^{-\frac{(y-x)^2}{2\sigma^2}}dy \\ = -\sigma^2 e^{-\frac{(y-x)^2}{2\sigma^2}} + x\sqrt{2}\sigma\frac{\sqrt{\pi}}{2} \text{erf}(\frac{y-x}{\sqrt{2}\sigma}) $$

代入原式得：

$$ \max(x,\alpha x) ≈ \frac{\alpha}{\sqrt{2\pi}\sigma} [-\sigma^2 e^{-\frac{(y-x)^2}{2\sigma^2}} + x\sqrt{2}\sigma\frac{\sqrt{\pi}}{2} \text{erf}(\frac{y-x}{\sqrt{2}\sigma})] |_{-∞}^{0}  \\ +  \frac{1}{\sqrt{2\pi}\sigma} [-\sigma^2 e^{-\frac{(y-x)^2}{2\sigma^2}} + x\sqrt{2}\sigma\frac{\sqrt{\pi}}{2} \text{erf}(\frac{y-x}{\sqrt{2}\sigma})] |_{0}^{+∞}  \\ = -\frac{\alpha\sigma}{\sqrt{2\pi}}  e^{-\frac{x^2}{2\sigma^2}} - \frac{\alpha x}{2}\text{erf}(\frac{x}{\sqrt{2}\sigma}) \\ +  \frac{ x}{2} + \frac{\sigma}{\sqrt{2\pi}}  e^{-\frac{x^2}{2\sigma^2}} + \frac{ x}{2}\text{erf}(\frac{x}{\sqrt{2}\sigma}) \\ =  \frac{(1-\alpha)\sigma}{\sqrt{2\pi}}  e^{-\frac{x^2}{2\sigma^2}}+  \frac{ x}{2} +  \frac{(1-\alpha) x}{2}\text{erf}(\frac{x}{\sqrt{2}\sigma}) $$

### ② 近似ReLU
**ReLU**激活函数表示为$f(x)=\max(x,0)$。若选用式③作为**Dirac**函数的光滑近似，则有：

$$ \max(x,0) ≈ \int_{-∞}^{+∞}\max(y,0)\frac{te^{t(x-y)}}{(1+e^{t(x-y)})^2}dy \\ = \int_{0}^{+∞}\frac{yte^{t(x-y)}}{(1+e^{t(x-y)})^2}dy =  \int_{0}^{+∞}yd\frac{1}{1+e^{t(x-y)}} \\ = \frac{y}{1+e^{t(x-y)}}|_{y=0}^{y=+∞}-\int_{0}^{+∞}\frac{1}{1+e^{t(x-y)}}dy \\ = \frac{y}{1+e^{t(x-y)}}|_{y=+∞}-\frac{1}{t}\ln(e^{tx}+e^{ty})|_{y=0}^{y=+∞} \\ = \frac{\ln(e^{tx}+1)}{t} $$

当$t=1$时，上式即为**SoftPlus**激活函数。

另一方面，**ReLU**激活函数也可表示为$f(x)=x\theta(x)$，其中$\theta(x)$是单位阶跃函数。也可用单位阶跃函数的光滑近似来近似**ReLU**激活函数。

若取单位阶跃函数$\theta(x)$近似**sigmoid**函数$\sigma(x)$，则$f(x)=x\sigma(x)$，即为**Swish**激活函数。

若用式①作为单位阶跃函数的光滑近似，则有：

$$ \max(x,0) = x\theta(x) ≈ x \int_{-∞}^{+∞}\theta(y)\frac{e^{-(x-y)^2/2\sigma^2}}{\sqrt{2\pi}\sigma}dy \\ = x \int_{0}^{+∞}\frac{e^{-(x-y)^2/2\sigma^2}}{\sqrt{2\pi}\sigma}dy  = \frac{x}{2}[1+\text{erf}(\frac{x}{\sqrt{2}\sigma})] $$

当$\sigma=1$时，上式即为**GELU**激活函数。

![](https://pic.imgdb.cn/item/618613912ab3f51d919935a2.jpg)

### ③ 近似取整函数
本文以向下取整函数为例，记为：

$$ \lfloor x \rfloor = n, \quad x\in [n,n+1) $$

若记$\phi(x)$为**Dirac**函数的光滑近似，则有：

$$ \lfloor x \rfloor ≈ \int_{-∞}^{+∞}\lfloor y \rfloor \phi(x-y)dy = \sum_{n=-∞}^{+∞}\int_{n}^{n+1}n\phi(x-y)dy $$

若$\Phi(y)$为$\phi(y)$的原函数，则$\phi(x-y)$的原函数为$-\Phi(x-y)$，则有：

$$ \lfloor x \rfloor ≈ \sum_{n=-∞}^{+∞}-n \Phi(x-y)|_{n}^{n+1} \\ = \sum_{n=-∞}^{+∞} n[\Phi(x-n)-\Phi(x-n+1)] \\ = \mathop{\lim}_{M,N\to ∞} \sum_{n=-M}^{N} [(n-1)\Phi(x-n)-n\Phi(x-n+1)+\Phi(x-n)] \\ = \mathop{\lim}_{M,N\to ∞}  -(M+1)\Phi(x+M)-N\Phi(x-N+1)+\sum_{n=-M}^{N}\Phi(x-n)  $$

注意到$\Phi(x+M)_{M\to ∞} ≈\Phi(∞) = 1$，$\Phi(x-N+1)_{N\to ∞} ≈\Phi(-∞) = 0$，因此上式进一步化简为：

$$ \lfloor x \rfloor ≈  \mathop{\lim}_{M,N\to ∞}  -(M+1)+\sum_{n=-M}^{N}\Phi(x-n) \\ = \mathop{\lim}_{M,N\to ∞} \sum_{n=-M}^{0}[\Phi(x-n)-1]+\sum_{n=0}^{N}\Phi(x-n)  $$

单位阶跃函数$\theta(x)$是**Dirac**函数的一个原函数，其可由**Sigmoid**函数光滑近似，即$\Phi(y)=\sigma(ty)$。

下图展示了一个$t=10,M=5,N=10$时对取整函数的近似情况：

![](https://pic.imgdb.cn/item/6186339f2ab3f51d91c9d9e8.jpg)

# 4. 实验分析
将**Leaky ReLU**激活函数的光滑近似作为新的激活函数**SAU**，其表达式如下：

$$ \text{SAU}(x) =  \frac{(1-\alpha)\sigma}{\sqrt{2\pi}}  e^{-\frac{x^2}{2\sigma^2}}+  \frac{ x}{2} +  \frac{(1-\alpha) x}{2}\text{erf}(\frac{x}{\sqrt{2}\sigma}) $$

![](https://pic.imgdb.cn/item/61860fb12ab3f51d91943de0.jpg)

激活函数**SAU**关于输入$x$和参数$\alpha$的梯度计算如下：

$$ \frac{\partial }{\partial x}\text{SAU}(x) = \frac{-x(1-\alpha)}{\sqrt{2\pi}\sigma}  e^{-\frac{x^2}{2\sigma^2}}+  \frac{1}{2} +  \frac{(1-\alpha) }{2}\text{erf}(\frac{x}{\sqrt{2}\sigma})+  \frac{(1-\alpha) x}{\sqrt{2\pi}\sigma}e^{-\frac{x^2}{2\sigma^2}}   $$

$$ \frac{\partial }{\partial \alpha}\text{SAU}(x) = \frac{-\sigma}{\sqrt{2\pi}}  e^{-\frac{x^2}{2\sigma^2}}- \frac{x}{2}\text{erf}(\frac{x}{\sqrt{2}\sigma}) $$

实验时设初值$\alpha =0.15$，通过梯度更新其参数。参数$\sigma$固定为$5\times 10^{-5}$。实验结果如下：

![](https://pic.imgdb.cn/item/61860d992ab3f51d919145a4.jpg)