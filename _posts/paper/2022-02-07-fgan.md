---
layout: post
title: 'f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization'
date: 2022-02-07
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/6344cf4f16f2c2beb19594f3.jpg'
tags: 论文阅读
---

> fGAN：通过f散度构造GAN.

- paper：[f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization](https://arxiv.org/abs/1606.00709)

**GAN**的生成器在优化时试图减小真实的数据分布$$P_{data}$$和生成分布$$P_G$$之间的**KL散度**。**KL散度**并不是唯一的选择，本文作者通过一般的[f散度](https://0809zheng.github.io/2020/02/03/kld.html#-f%E6%95%A3%E5%BA%A6-f-divergence)来构造**GAN**。

# ① f散度

一般地，$p(x)$和$q(x)$之间的**f散度**定义为：

$$ D_f(P || Q) = \int_{x}^{} {q(x)f(\frac{p(x)}{q(x)})dx} $$

其中函数$f(\cdot)$的性质：
- $f(\cdot)$是非负实数到实数的映射$$\Bbb{R}^{*} \to \Bbb{R}$$；
- $f(1)=0$；对应$p(x)=q(x)$时散度为$0$;
- $f(\cdot)$是凸函数：该性质使**f**散度恒大于等于零(根据[Jenson不等式](https://0809zheng.github.io/2022/07/20/jenson.html))：

$$ D_f(P || Q) = \int_{x}^{} {q(x)f(\frac{p(x)}{q(x)})dx} ≥ f(\int_{x}^{} {q(x)\frac{p(x)}{q(x)}dx}) = f(1) = 0 $$

当函数$f(\cdot)$选择不同时，**f**散度对应到不同的散度：
- **KL散度**：$f(x) = x \log x$

$$ D_f(P || Q) = \int_{x}^{} {q(x) \frac{p(x)}{q(x)} \log(\frac{p(x)}{q(x)})dx} = \int_{x}^{} {p(x) \log(\frac{p(x)}{q(x)})dx} $$

- **Reverse KL散度**：$f(x) = -\log x$

$$ D_f(P || Q) = \int_{x}^{} {q(x) (-\log(\frac{p(x)}{q(x)}))dx} = \int_{x}^{} {q(x) \log(\frac{q(x)}{p(x)})dx} $$

- $\chi^2$**散度**：$f(x) = (x-1)^2$

$$ D_f(P || Q) = \int_{x}^{} {q(x) (\frac{p(x)}{q(x)}-1)^2dx} = \int_{x}^{} {\frac{(p(x)-q(x))^2}{q(x)}dx} $$


下表给出了不同的散度对应的凸函数$f(x)$：
![](https://pic1.imgdb.cn/item/6344d10216f2c2beb19864b8.jpg)

# ② 凸共轭 Fenchel Conjugate

对于凸函数$f(x)$，选择任意一个点$\xi$，计算$y=f(x)$在$x=\xi$处的切线：

$$ y = f(\xi) + f'(\xi)(x-\xi) $$

对于凸函数，函数总在其切线上方，因此有：

$$ \begin{aligned} f(x) & \geq f(\xi) + f'(\xi)(x-\xi) \\ & = f(\xi) - f'(\xi)\xi  + f'(\xi)x \end{aligned} $$

对于定义域内的所有点$$\xi \in \Bbb{D}$$，$f(x)$可以用其所有切线簇表示：

$$ f(x) = \mathop{\max}_{\xi \in \Bbb{D}} \{ f(\xi) - f'(\xi)\xi  + f'(\xi)x \} $$

记$t=f'(\xi)$，$f^{\*}(t) = -f(\xi) + f'(\xi)\xi$，则有：

$$ f(x) = \mathop{\max}_{t \in f'(\Bbb{D})} \{ xt - f^{*}(t) \} $$

上式的几何意义为，对于凸函数$f(x)$，任意一点$x$处的取值为其所有切线簇在该点取值的最大值。

![](https://pic.downk.cc/item/5ebcfd25c2a9a83be542cee6.jpg)

式中$f^{\*}(t)$称为凸函数$f(x)$的**共轭函数（conjugate function）** ，具有如下性质：
- $f^*$也是凸函数；
- $(f^{\*})^{\*}=f$，因此有：

$$ f^{*}(t) = \mathop{\max}_{x \in \Bbb{D}} \{ xt - f(x) \} $$

直观上，通过共轭函数$f^*(t)$给出了凸函数$f(x)$的线性近似。对于任意$x$，通过共轭函数给出$f(x)$的一个下界$xt-f^{\*}(t)$，并且该下界关于$x$是线性的；通过在每一个$x$处最大化$t$才能取得$f(x)$的结果，因此该方法称为**局部变分**方法，$t$可以看作$x$的函数$t(x)$：

$$ f(x) = \mathop{\max}_{t \in f'(\Bbb{D})} \{ xt(x) - f^{*}(t(x)) \} $$

### 例题：**KL散度**的$f(x) = x\log x$对应 $f^*(t) = e^{t-1}$

求解下式：

$$ f^*(t) = \mathop{\max}_{x \in \Bbb{D}} \{ xt - x\log x \} $$

令$$\frac{\partial(xt-x\log x)}{\partial x}=0$$得$$t-\log x-1=0$$，即$$x=e^{t-1}$$,代入得：

$$ f^*(t) = e^{t-1}t - e^{t-1}(t-1) = e^{t-1} $$



# ③ f-GAN

根据**f散度**的计算公式：

$$ D_f(P || Q) = \int_{x}^{} {q(x)f(\frac{p(x)}{q(x)})dx} $$

注意到只有$p(x)$和$q(x)$的解析形式均已知时上式才能求解。在实际应用中，有些概率分布形式未知，只能通过采样获得有限的样本；比如对于生成对抗网络，将$p(x)$视为数据真实分布，$q(x)$视为生成分布，这两种分布的概率形式都是未知的。

此时可以通过共轭函数估算**f散度**。根据共轭函数给出的凸函数$f(x)$表达式：

$$ f(x) = \mathop{\max}_{t \in f'(\Bbb{D})} \{ xt(x) - f^{*}(t(x)) \} $$

**f散度**可以表示为：

$$ \begin{aligned} D_f(P || Q)& = \int_{x}^{} {q(x)f(\frac{p(x)}{q(x)})dx} \\ &= \int_{x}^{} {q(x)(\mathop{\max}_{t \in f'(\Bbb{D})} \{ \frac{p(x)}{q(x)} t(x) - f^*(t(x)) \})dx} \\ &= \mathop{\max}_{t \in f'(\Bbb{D})}\int_{x}^{} {q(x)( \frac{p(x)}{q(x)} t(x) - f^*(t(x)) )dx} \\ &= \mathop{\max}_{t \in f'(\Bbb{D})} \int_{x}^{} p(x)t(x)- {q(x)f^*(t(x))dx} \\&= \mathop{\max}_{t \in f'(\Bbb{D})} \{ \Bbb{E}_{x\text{~}p(x)}[t(x)]- \Bbb{E}_{x\text{~}q(x)}[f^*(t(x))] \} \end{aligned} $$

其中$t(x)$可以用神经网络$D(x)$拟合，相当于**GAN**中的“判别器”；生成分布$q(x)$是由生成器构造的。生成器的目标是减小两个分布的差异，而判别器的目标是增大两个分布的差异，两者的目标函数合并为：

$$ \mathop{\min}_{G}\mathop{\max}_{D \in f'(\Bbb{D})} \{ \Bbb{E}_{x\text{~}p(x)}[D(x)]- \Bbb{E}_{x\text{~}q_G(x)}[f^*(D(x))] \} $$

其中凸函数$f(x)$需自行选择。注意到选定$f$后，$t(x)$（或$D(x)$）的值域是有限制的。在实践时可以通过施加激活函数约束$D(x)$的输出范围。激活函数的选择应满足以下几点：
1. 激活函数的定义域为$$\Bbb{R}$$，值域为$f'(x)$的值域；
2. 最好选择全局光滑的函数；如值域要求为$$\Bbb{R}^+$$，则优先考虑$e^x$而不是$$\text{ReLU}(x)$$；
3. 选择激活函数时，应使其与$f^*(\cdot)$的复合运算比较简单。

下面列出一些凸函数对应的共轭函数及其激活函数选择：

![](https://pic1.imgdb.cn/item/6345194e16f2c2beb11096c4.jpg)

### 例题：JS散度对应的目标函数

原目标函数为：

$$ \mathop{\min}_{G}\mathop{\max}_{D \in f'(\Bbb{D})} \{ \Bbb{E}_{x\text{~}p(x)}[t(x)]- \Bbb{E}_{x\text{~}q(x)}[f^*(t(x))] \} $$

不妨取凸函数$f(x)=-(x+1)\log \frac{1+x}{2}+x \log x$，对应**JS**散度。其共轭函数为$f^{\*}(t)=-\log(2-\exp(t))$，此时目标函数为：

$$  \Bbb{E}_{x\text{~}p(x)}[t(x)]- \Bbb{E}_{x\text{~}q(x)}[-\log(2-\exp(t(x)))]  $$

不妨取$D(x) = \frac{\exp(t(x))}{2}$，则上式简化为：

$$ \begin{aligned} \Bbb{E}_{x\text{~}p(x)}[\log 2D(x)]- \Bbb{E}_{x\text{~}q(x)}[-\log(2-2D(x))] \\ = \Bbb{E}_{x\text{~}p(x)}[\log D(x)]+\Bbb{E}_{x\text{~}q(x)}[\log(1-D(x))] + 2\log 2 \end{aligned} $$

此即标准**GAN**的目标函数。