---
layout: post
title: 'Revisiting ResNets: Improved Training and Scaling Strategies'
date: 2021-03-18
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63ac289408b6830163fd8412.jpg'
tags: 论文阅读
---

> ResNet-RS：改进ResNet的训练和策略.

- paper：[Revisiting ResNets: Improved Training and Scaling Strategies](https://arxiv.org/abs/2103.07579)

一个模型的综合性能，一般是由网络结构、训练方法及缩放策略三方面综合决定的。大部分工作一般都着重强调其结构的改进，而很少提及同时被引入的非常重要的训练方式及缩放策略等问题。另一方面，这些采用先进训练策略的新模型，总是拿来与旧训练方式下的经典模型（如**ResNet**）进行比较，这是不公平的对比。

本文作者通过对经典模型**ResNet**加入目前新的训练及正则方式，并通过合理的缩放方式，提出了改进版的**ResNet**网络系列：**ResNet-RS**。

实验结果表明，通过添加更新的训练方法及小的结果改进，可以使得**ResNet**网络精度提高至**sota**级别；通过所提缩放策略，可以获得更优的精度-速度权衡的系列网络，整体性能全面超越由神经结构搜索获得的**EfficientNet**。

![](https://pic.imgdb.cn/item/63ac295a08b6830163fecc50.jpg)

### ⚪ 改进网络结构

- 将**ResNet**主干网络中的**7x7**卷积变为**3**个**3x3**卷积；
- 交换下采样模块的支路中前两个卷积的步长；
- 将下采样模块的跳跃连接中**s=2**的**1x1**卷积换成**s=2**的**2x2**池化+**1x1**卷积；
- 移除**s=2**的**3x3**最大池化并在下一个下采样模块中进行下调分辨率。
- 加入**ratio=0.25**的**SE-Block**。

### ⚪ 改进训练方法

作者引入一系列改进的训练方法，可以把**ResNet**网络的精度提升到$82.2\%$；再加入结构的微小改进，可以将其精度提升到$83.4\%$。此外，实验指出当与其他正则化方法一起使用时，需要降低权重衰减率。

![](https://pic.imgdb.cn/item/63ac2aa808b683016300ea72.jpg)

### ⚪ 改进缩放策略

模型缩放主要有三个维度：**深度**，**宽度**，**分辨率**。作者对这三个维度进行一定范围内的采点试验，来模拟完整的模型缩放-精度变化趋势，并证明了**EfficientNet**中自动搜索所得的三个维度均衡扩增的方法其实是次优的。此外还在三种不同的训练轮数下进行了试验。实验参数设置组合空间如下：
- 宽度 $[0.25, 0.5, 1.0, 1.5, 2.0]$
- 深度 $[26, 50, 101, 200, 300, 350, 400]$
- 分辨率 $[128, 160, 224, 320, 448]$
- 训练轮数 $[10, 100, 350]$

![](https://pic.imgdb.cn/item/63ac2b7608b6830163026ef4.jpg)

实验结果表明：
- 不同的训练轮数下，最优的模型缩放策略并不相同：大训练轮数下增加深度效果优于宽度，但在较小的训练轮数下变化宽度效果比深度更突出。
- 增大图像分辨率会出现收益递减的现象。
- 利用较小模型、较低的训练轮数来估计最终精度并不安全。
- 从一个次优的架构，通过单个维度来进行缩放实验，可能会导致最终策略的次优。

作者给出了推荐的缩放策略：
- 在可能出现过拟合问题的大训练轮数下首选扩大深度；否则扩大宽度。
- 缓慢扩大分辨率。