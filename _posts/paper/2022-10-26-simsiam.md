---
layout: post
title: 'Exploring Simple Siamese Representation Learning'
date: 2022-10-26
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63e1b9974757feff339decf6.jpg'
tags: 论文阅读
---

> SimSiam：探索简单的孪生表示学习.

- paper：[Exploring Simple Siamese Representation Learning](https://arxiv.org/abs/2011.10566)

**SimSiam**是一种不依赖于负样本的对比学习方法，使用孪生网络$f$从图像$x$的两个增强版本$x_1,x_2$中提取特征$z_1,z_2$，并使用预测头$h$根据一个特征预测另一个特征。

![](https://pic.imgdb.cn/item/63e1bb6f4757feff33a0d545.jpg)

损失函数设置为负余弦相似度：

$$ \begin{aligned} \mathcal{L}_{\text{SimSiam}} = -\frac{1}{2} \frac{h(z_1)}{||h(z_1)||_2} \cdot \frac{sg(z_2)}{||sg(z_2)||_2} -\frac{1}{2} \frac{h(z_2)}{||h(z_2)||_2} \cdot \frac{sg(z_1)}{||sg(z_1)||_2} \end{aligned}  $$

**SimSiam**的实现过程如下：

![](https://pic.imgdb.cn/item/63e1bc794757feff33a2b19e.jpg)

作者对比了若干种使用孪生网络的对比学习方法。**SimCLR**同时使用正样本和负样本；**SwAV**通过**SK**算法构造聚类中心；**BYOL**与**SimSiam**类似，主要区别在于前者使用移动平均更新矩编码器参数。

![](https://pic.imgdb.cn/item/63e1bcc54757feff33a3227e.jpg)

实验结果表明，在不使用负样本的对比学习中，损失函数中的梯度停止操作$sg(\cdot)$非常重要，否则可能使网络学习到平凡解。

![](https://pic.imgdb.cn/item/63e1bdda4757feff33a50034.jpg)

对于这种基于孪生网络的对比学习方法，作者提出了一种基于期望最大算法(**Expectation-Maximization**)的解释。对于数据样本$x$，通过数据增强变换为$$\mathcal{T}(x)$$，通过任意变换转换为$\eta_x$，则上述对比学习损失为：

$$ \mathcal{L}(\theta,\eta) = \Bbb{E}_{x,\mathcal{T}}[||\mathcal{F}_{\theta}(\mathcal{T}(x))-\eta_x||_2^2] $$

最小化上式可以通过迭代地求解以下两个子问题：

$$ \begin{aligned} \theta^t & \leftarrow \mathop{\arg \max}_{\theta} \mathcal{L}(\theta,\eta^{t-1}) \\ \eta^t & \leftarrow \mathop{\arg \max}_{\theta} \mathcal{L}(\theta^t,\eta) \end{aligned}  $$

固定$\theta^t$时，最优$\eta$为：

$$ \eta^t  \leftarrow \mathcal{F}_{\theta^t}(\mathcal{T}(x)) $$

代回原式得：

$$ \theta^{t+1}  \leftarrow \mathop{\arg \max}_{\theta} \Bbb{E}_{x,\mathcal{T}}[||\mathcal{F}_{\theta}(\mathcal{T}(x))-\mathcal{F}_{\theta^t}(\mathcal{T}'(x))||_2^2] $$