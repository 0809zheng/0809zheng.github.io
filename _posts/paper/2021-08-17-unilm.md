---
layout: post
title: 'Unified Language Model Pre-training for Natural Language Understanding and Generation'
date: 2021-08-17
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/611e1b964907e2d39cd309d9.jpg'
tags: 论文阅读
---

> UniLM：使用BERT实现序列到序列的预训练.

- paper：Unified Language Model Pre-training for Natural Language Understanding and Generation
- arXiv：[link](https://arxiv.org/abs/1905.03197)

自然语言处理中的大多数任务都可以表示为序列到序列**Seq2Seq**任务，
在**Seq2Seq**任务中通常使用编码器-解码器结构(如完整的**Transformer**)，这是因为标准的自注意力机制对于输入是无序的，不适合于**Seq2Seq**任务。作者通过为注意力矩阵加上合适的**mask**，使得仅用**Transformer**的编码器(如**BERT**)也能够实现**Seq2Seq**任务。


![](https://pic.imgdb.cn/item/611e30c64907e2d39c0d3273.jpg)

通过为注意力矩阵施加不同形状的**mask**，可以实现不同的任务类型。
- 若不施加**mask**，则相当于自编码式的双向语言模型，对应标准的**BERT**；
- 若施加右上角的**mask**，则相当于自回归式的语言模型，对应**GPT**；
- 作者施加上图所示的**mask**，使得第一个句子的交互是双向的，第二个句子的交互是单向的，满足**Seq2Seq**任务的要求。

该思路没有额外约束。只要添加上述形状的**mask**，不需要修改模型架构，可以直接沿用**BERT**的预训练权重，收敛更快。