---
layout: post
title: 'When Do You Need Billions of Words of Pretraining Data?'
date: 2021-03-31
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67f774d788c538a9b5c85fdc.png'
tags: 论文阅读
---

> 什么时候需要数十亿单词的预训练数据？

- paper：[When Do You Need Billions of Words of Pretraining Data?](https://arxiv.org/abs/2011.04946)


# 0. TL; DR

本文通过多种探测方法研究了不同预训练数据量对**Transformer**语言模型（**LMs**）学习语言特征和解决自然语言理解（**NLU**）任务能力的影响。研究发现，**LMs**仅需约**1000**万至**1**亿词汇的预训练数据即可学习到可靠的大部分句法和语义特征，但要掌握典型的下游**NLU**任务所需的常识知识和其他技能，则需要数十亿词汇的数据。这表明，尽管编码语言特征的能力对于语言理解至关重要，但其他形式的知识可能是大型预训练模型在大规模数据上性能提升的主要驱动力。

![](https://pic1.imgdb.cn/item/67f779e588c538a9b5c86863.png)

# 1. 背景介绍

预训练语言模型（**LMs**）如**BERT**和**RoBERTa**在自然语言处理（**NLP**）领域取得了显著的成果，这些模型通过在数十亿甚至数千亿词汇的大型数据集上进行预训练来学习语言特征和世界知识，并能在许多下游任务上取得良好表现。

然而，关于这些模型从大规模预训练中究竟学到了哪些知识或技能，以及这些技能如何随着预训练数据量的增加而变化，仍有许多问题尚未得到解答。例如，大规模预训练模型所掌握的知识与仅使用较少数据预训练的模型有何不同？预训练数据量对语言模型学习不同语法特征和语言现象的影响如何？哪些技能会在预训练数据量超过**300**亿词汇时得到提升？哪些语法特征可以从与人类学习者输入相当的数据量（约**1000**万至**1**亿词汇）中学习？

为了解答这些问题，本文通过多种探测方法对不同预训练数据量的**RoBERTa**模型进行了研究。

# 2. 方法介绍

本文采用了四种探测方法来评估语言模型的学习能力：分类器探测、信息论探测、无监督相对可接受性判断和在**NLU**任务上的微调。这些方法分别从不同的角度衡量语言模型对语言特征的编码能力和解决实际**NLU**任务的能力。

研究使用了**MiniBERTas**，这是一组从头开始预训练的**RoBERTa**模型，分别在**100**万、**1000**万、**1**亿和**10**亿词汇的数据集上进行预训练。这些数据集是从**Wikipedia**和**Smashwords**中采样得到的，这两个数据源也是**BERT**和**RoBERTa**预训练时使用的数据源。此外，还测试了在约**300**亿词汇上预训练的**RoBERTaBASE**模型以及**3**个随机初始化参数的**RoBERTaBASE**模型。

在每种探测实验中，所有**16**个模型都在涉及的任务上进行了测试，并使用**min-max**归一化将结果调整到[0, 1]范围内，其中0表示任何模型在任务上的最差分数（通常是随机初始化的模型），1表示任何模型的最佳分数（通常是**RoBERTaBASE**）。为了展示整体改进趋势，使用非线性最小二乘法拟合了逻辑函数，并将**x**值对数变换后的点拟合到该函数上。

## （1）分类器探测  Classifier Probing

分类器探测是一种广泛采用的探测方法，用于测试语言模型的表示中编码了多少语言特征，如词性标注和共指。在这些实验中，冻结语言模型的表示，并为**10**个探测任务训练**MLP**分类器。

对于每个任务$T$，如果$T$是成对任务，则训练两个注意力池化函数$f^1_T$和$f^2_T$，并为每个跨度对$(S^1_i, S^2_i$)生成表示对$(r^1_i, r^2_i) = (f^1_T(S^1_i), f^2_T(S^2_i))$。然后，对于$T$的每个标签$L_j$，探测器（即**MLP**）将$(r^1_i, r^2_i)$作为输入，并执行二元分类以预测$L_j$是否是正确标签。对于仅涉及单个跨度的任务（词性标注、短语结构和实体），省略$S^2_i$和$f^2_T$。

采用“混合”表示方法，因此每个标记表示$(t^k_i)_p$是从$S^k_i = \{(t^k_i)_0, (t^k_i)_1, ...\}$中的**RoBERTa**层激活的线性组合，并投影到**256**维空间。

对于每个任务，将验证间隔固定为**1000**步，早停耐心为**20**步，学习率耐心为**5**步，并随机采样**5**种批量大小和学习率的组合，以使用**Adam**优化器调整每个预训练规模下具有最低**MLM**困惑度的模型的超参数。使用最佳超参数设置训练该规模的所有模型。

实验结果表明，语言模型在预训练数据量少于**1**亿词汇时，大部分语言特征的学习已经完成。例如，在句法和语义任务中，**90%**的改进可以在少于**2000**万词汇的预训练数据中实现。然而，对于涉及常识知识的任务，如**Winograd**任务，性能的显著提升出现在**10**亿到**300**亿词汇的预训练数据之间。这表明，常识知识和其他技能需要更多的数据来学习。

![](https://pic1.imgdb.cn/item/67f7787088c538a9b5c8663e.png)

## （2）信息论探测 Minimum Description Length Probing

信息论探测通过最小描述长度（**MDL**）来衡量语言模型表示中语言特征信息的可访问性。**MDL**衡量在发送方和接收方都拥有预训练模型的数据编码的情况下，传输给定任务标签所需的最少位数。实验中，将训练数据分为**11**部分，分别训练**MLP**分类器，并计算其在下一部分上的损失。最终，计算在线代码长度作为**10**个损失值的总和以及第一个数据部分在均匀先验下的代码长度。

信息论探测结果显示，大多数特征信息的减少在少于**1**亿词汇的预训练数据中实现。句法特征的**MDL**减少得更早，表明句法特征的学习曲线比语义特征更早达到饱和。然而，对于**Winograd**任务，**MDL**的变化并不明显，这可能是由于探测器未能很好地学习该任务。

![](https://pic1.imgdb.cn/item/67f778e488c538a9b5c866f3.png)

## （3）无监督语法可接受性判断 Unsupervised Grammaticality Judgement

无监督语法可接受性判断使用**BLiMP**基准测试模型对英语中各个语法现象的知识。**BLiMP**包含**67**个任务，每个任务包含**1000**对最小句子对，突出特定的形态、句法或语义现象。最小句子对仅在单个编辑上有所不同，但在语法可接受性上形成对比。模型通过为可接受的句子分配更高的似然来正确分类最小句子对。

实验结果表明，从**100**万到**1**亿词汇的预训练数据中，**BLiMP**的整体性能得到了最大的提升。在**1**亿词汇时，对可接受性对比的敏感性总体上与人类相差**9**个百分点，并且在额外数据的帮助下仅提高了**6**个百分点。这表明，从**1**亿词汇的原始文本中可以获取大量语法现象的知识。

![](https://pic1.imgdb.cn/item/67f7796888c538a9b5c867b4.png)

## （4）在NLU任务上的微调

在**NLU**任务上的微调使用**SuperGLUE**基准测试语言模型在五个**NLU**任务上的性能。在**NLU**任务上的微调结果显示，**SuperGLUE**性能的提升需要相对较大的预训练数据量。对于大多数任务，最快改进点出现在超过**10**亿词汇的预训练数据中，并且在**300**亿词汇时没有显示出饱和迹象。这表明，一些关键的**NLU**技能不能在少于数十亿词汇的数据上学习，并且模型可能会在**10**到**100**倍更多的预训练数据上继续改进这些任务。

![](https://pic1.imgdb.cn/item/67f779a088c538a9b5c86807.png)