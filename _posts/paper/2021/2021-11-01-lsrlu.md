---
layout: post
title: 'Improving Deep Learning by Inverse Square Root Linear Units (ISRLUs)'
date: 2021-11-01
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/6822ff8458cb8da5c8f0186d.png'
tags: 论文阅读
---

> 使用逆平方根线性单元(ISRLU)改进深度学习.

- paper：[Improving Deep Learning by Inverse Square Root Linear Units (ISRLUs)](https://arxiv.org/abs/1710.09967)

# 0. TL; DR

本文介绍了一种新的激活函数——逆平方根线性单元（**ISRLU**），它在深度神经网络中表现出色，能够加速学习过程并提高性能。**ISRLU**与指数线性单元（**ELU**）具有相似的曲线特性，但计算复杂度更低，尤其在传统**CPU**和硬件实现上效率更高。实验表明，**ISRLU在**卷积神经网络（**CNN**）中的表现优于**ReLU**，并且其变体逆平方根单元（**ISRU**）在循环神经网络（**RNN**）中也具有潜在优势。

# 1. 背景介绍

在深度神经网络中，激活函数起着至关重要的作用，它决定了神经元的输出是否能够有效地传递信息并促进学习。

目前，常用的激活函数包括修正线性单元（**ReLU**）和指数线性单元（**ELU**）。**ReLU**因其简单高效而被广泛应用，但在某些情况下会导致梯度消失或神经元死亡的问题。**ELU**通过引入负值来解决这些问题，使得网络的均值激活更接近零，从而减少偏差偏移，加速学习过程。

然而，**ELU**的计算复杂度较高，尤其是在处理指数运算时。因此，研究者们一直在寻找一种既能保持**ELU**优点又能在计算效率上有所提升的激活函数。本文提出的**ISRLU**正是为了解决这一问题。

# 2. ISRLU与ISRU

## 2.1 ISRLU (inverse square root linear unit)

**ISRLU**的数学表达式如下：

$$
f(x) = \begin{cases} 
x & \text{if } x \geq 0 \\
x \cdot \frac{1}{\sqrt{1 + \alpha x^2}} & \text{if } x < 0 
\end{cases} 
$$

其导数为：

$$
f'(x) = \begin{cases} 
1 & \text{if } x \geq 0 \\
\left(\frac{1}{\sqrt{1 + \alpha x^2}}\right)^3 & \text{if } x < 0 
\end{cases}
$$

其中，超参数 $\alpha$ 控制了**ISRLU**在负输入时的饱和值。当 $\alpha = 1$ 时，**ISRLU**的饱和值接近 -1；当 $\alpha = 3$ 时，饱和值减小，使得反向传播的误差信号中有更少的部分传递到下一层。这种特性使得网络能够输出稀疏激活，同时保留重新激活死亡神经元的能力。

![](https://pic1.imgdb.cn/item/6823019158cb8da5c8f01a27.png)

**ISRLU**与**ELU**的曲线非常相似，但**ISRLU**具有更平滑且连续的一阶和二阶导数，而**ELU**仅在第一导数上连续。此外，**ISRLU**的计算复杂度更低，因为它基于逆平方根运算，而逆平方根的计算速度通常比指数运算更快。

**ISRLU**的主要优势在于其计算效率。在现代**CPU**架构中，逆平方根的计算速度比指数运算快得多。例如，在**Intel Xeon Platinum 8160（Skylake）**上，逆平方根的计算速度是指数运算的2.2倍。这种计算优势使得**ISRLU**在大规模深度学习任务中能够显著减少训练时间。

![](https://pic1.imgdb.cn/item/682301ce58cb8da5c8f01a47.png)

此外，**ISRLU**的超参数 $\alpha$ 可以在训练过程中通过反向传播直接学习，类似于参数化**ReLU（PReLU）**。这为网络提供了一种自适应调整激活函数形状的能力，从而进一步提高性能。

## 2.2 ISRU (inverse square root unit)

**ISRLU**的变体——逆平方根单元（**ISRU**）被提出用于循环神经网络（**RNN**）。**ISRU**的定义如下：

$$
f(x) = x \cdot \frac{1}{\sqrt{1 + \alpha x^2}} 
$$

其导数为：

$$
f'(x) = \left(\frac{1}{\sqrt{1 + \alpha x^2}}\right)^3
$$

**ISRU**的曲线与双曲正切函数（**tanh**）和**Sigmoid**函数相似，但计算复杂度更低。在**RNN**中，常用的**LSTM**和**GRU**单元通常使用**tanh**和**Sigmoid**作为激活函数，而**ISRU**可以作为一种更高效的替代方案。

![](https://pic1.imgdb.cn/item/682302b558cb8da5c8f01a84.png)

# 3. 实验分析

实验使用了**TensorFlow**框架，在**MNIST**数据集上训练了多个卷积神经网络（**CNN**），分别使用**ISRLU、ELU**和**ReLU**作为激活函数。实验结果表明，**ISRLU**在训练过程中表现出更快的收敛速度和更低的交叉熵损失。

![](https://pic1.imgdb.cn/item/682302fc58cb8da5c8f01a8c.png)


实验还对不同激活函数在**CPU**上的性能进行了评估。结果显示，**ISRLU**在**AVX2**指令集上的性能优势显著。例如，**ISRLU**（$\alpha = 1.0$）的计算速度是**ELU**的2.63倍，而其快速近似版本的计算速度与**ReLU**相当，仅相差1%。这表明**ISRLU**在保持学习性能的同时，能够显著减少计算时间。

![](https://pic1.imgdb.cn/item/6823035f58cb8da5c8f01aa4.png)