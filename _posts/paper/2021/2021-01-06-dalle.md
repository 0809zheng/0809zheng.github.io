---
layout: post
title: 'Learning Transferable Visual Models From Natural Language Supervision'
date: 2021-01-06
author: 郑之杰
cover: 'https://pic.downk.cc/item/5ff555b83ffa7d37b381af0e.jpg'
tags: 论文阅读
---

> DALL·E：从文本生成图像.

- 论文：[Learning Transferable Visual Models From Natural Language Supervision](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language.pdf)
- 模型介绍：[DALL·E: Creating Images from Text](https://openai.com/blog/dall-e/)
- 方法介绍：[CLIP: Connecting Text and Images](https://openai.com/blog/clip/)
- 预训练模型：[Contrastive Language-Image Pretraining (github)](https://github.com/OpenAI/CLIP)

# ⚪ DALL·E: Creating Images from Text
**OpenAI**提出了**DALL·E**模型，该模型可以从包含大量概念的文本描述中生成相关图像。其名称**DALL·E**致敬了艺术家**Salvador Dalí**和皮克斯动画角色**WALL·E**。

**DALL·E**采用了基于**Transformer**的预训练结构，共有$120$亿参数。该模型同时接收图像和其文本描述作为输入，输入使用$1280$个**token**，包括$256$个词汇量是$16384$的文本**token**和$1024$个词汇量是$8192$的图像**token**。对于文本**token**，使用标准的随机**mask**，通过**GPT-3**构造；对于图像**token**，使用稀疏注意力(只计算某行、某列或局部)，训练时图像尺寸被调整为$256 \times 256$，并参考**VQ-VAE**模型将其压缩为$32 \times 32$的特征，使用字典对每个特征进行编码。整体模型采用极大似然算法进行自回归训练。

该模型能够从不同的文本描述中生成对应的图像：

![](https://pic.downk.cc/item/5ff558353ffa7d37b383298b.jpg)

上述结果是在生成的$512$个样本中选择的前$32$个质量最好的样本，选择过程使用了**CLIP**模型。值得一提的是，目前的模型对纹理、颜色等性质生成的图像质量好，但对于数量、逻辑等性质生成的图像质量较差。

# ⚪ CLIP: Connecting Text and Images
**Contrastive Language-Image Pre-training (CLIP)**方法用于在图像和文本数据集中进行匹配。具体地，训练一个文本编码器和图像编码器，分别得到文本和图像的编码，并计算两者的匹配程度。

![](https://pic.imgdb.cn/item/63e2f48f4757feff3376d5d7.jpg)

给定$N$个图像-文本对，首先计算任意一个图像和文本之间的余弦相似度矩阵，尺寸为$N \times N$；通过交叉熵损失使得匹配的$N$个图像-文本对的相似度最大，其余$N(N-1)$个相似度最小。

![](https://pic.imgdb.cn/item/63e2ef0e4757feff33709a0c.jpg)

**CLIP**在训练时使用了$4$亿张从互联网收集的图像-文本对(**WebImageText, WIT**)，文本字典存储了在**Wikipedia**中出现超过100次的所有单词。实验结果表明，对于文本编码器，采用**bag-of-words (BoW)**模型比**Transformer**的效率能提高3倍；采用对比损失比直接预测图像对应的文本效率能提高4倍。

![](https://pic.imgdb.cn/item/63e2f0304757feff3371c2ed.jpg)

**CLIP**训练完成后可以实现**zero-shot**的推理，即不经过微调的迁移学习，该过程是通过**prompt templete**实现的。以**ImageNet**数据集的分类任务为例，对于一千个类别标签，分别生成一千个对应的文本(如**A photo of a #Class**)；通过**CLIP**匹配相似度最高的图像和文本，即可确定图像中出现目标的类别。

![](https://pic.imgdb.cn/item/63e2f4e34757feff33774857.jpg)        