---
layout: post
title: 'R-Drop: Regularized Dropout for Neural Networks'
date: 2021-07-10
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60e8f1bc5132923bf8797acd.jpg'
tags: 论文阅读
---

> R-Drop：正则化的Dropout方法.

- paper：R-Drop: Regularized Dropout for Neural Networks
- arXiv：[link](https://arxiv.org/abs/2106.14448)


本文提出了一种借助**Dropout**的正则化方法**R-Drop**，通过**Dropout**从同一个模型中获取若干子模型，使得这些子模型的输出分布足够接近。具体地，通过“**Dropout**两次”的方式来得到同一个输入样本的两个不同输出特征向量，并使其落入同一个特征分布中。作者在$5$个深度学习任务中进行实验，表明方法的优越性。

# 1. R-Drop

![](https://pic.imgdb.cn/item/60e8f2d15132923bf883b3e8.jpg)

以使用**Transformer**进行文本分类任务为例。对于输入文本$x$，将其输入**Transformer**两次分别得到输出$P_1(y\|x)$和$P_2(y\|x)$，如上图左所示。由于**Transformer**中存在**Dropout**，同一个输入的每次前向传播过程都是不完全相同的，相当于使用了两个子模型，如上图右所示。

文本分类任务属于监督学习任务，因此对于输入样本$x_i$的两次输出$P_1(y_i\|x_i)$和$P_2(y_i\|x_i)$通过负对数似然计算分类损失：

$$ \mathcal{L}_{NLL}^i = -logP_1(y_i|x_i) -logP_2(y_i|x_i) $$

损失函数的另一部分是两个子模型输出的对称**KL**散度：

$$ \mathcal{L}_{KL}^i = \frac{1}{2}(\mathcal{D}_{KL}(P_1(y_i|x_i) || P_2(y_i|x_i))+\mathcal{D}_{KL}(P_2(y_i|x_i) || P_1(y_i|x_i))) $$

因此输入样本$x_i$的总损失函数为：

$$ \mathcal{L}^i = \mathcal{L}_{NLL}^i + \alpha \mathcal{L}_{KL}^i $$

# 2. 理论分析
**Dropout**的问题在于训练与测试的不一致性。训练时通过**Dropout**对每一次输入生成一个子模型(部分神经元置零)；而测试时直接关闭**Dropout**进行确定性的预测，即“**权重平均**”。理论上，测试应对同一个输入多次传入没有关闭**Dropout**的模型中，然后把多次的预测结果的平均值作为最终的预测结果，即实现“**模型平均**”。

**R-Drop**通过增加正则化，强化模型对**Dropout**的鲁棒性，即使得不同的**Dropout**下模型的输出落入同样的分布中，进而降低“模型平均”与“权重平均”的不一致性，从而使得简单关闭**Dropout**的效果等价于多**Dropout**模型融合的结果，提升模型性能。

# 3. 实验分析
作者在$5$种深度学习任务中测试了**R-Drop**的性能。

- 神经机器翻译 Neural Machine Translation

![](https://pic.imgdb.cn/item/60e9055b5132923bf83adff7.jpg)

- 文本摘要总结 Abstractive Summarization

![](https://pic.imgdb.cn/item/60e9058d5132923bf83cdc8a.jpg)

- 语言理解 Language Understanding

![](https://pic.imgdb.cn/item/60e905aa5132923bf83e00ba.jpg)

- 语言模型 Language Modeling

![](https://pic.imgdb.cn/item/60e905ce5132923bf83f6d8d.jpg)

- 图像分类 Image Classification

![](https://pic.imgdb.cn/item/60e905ed5132923bf840a882.jpg)