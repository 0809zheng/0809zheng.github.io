---
layout: post
title: 'Improving Language Understanding by Generative Pre-Training'
date: 2021-01-03
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ec11fd5132923bf808ee56.jpg'
tags: 论文阅读
---

> GPT：使用生成式预训练模型提高对语言的理解.

- paper：[Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

# 0. TL; DR

本文提出了一种通过生成式预训练提升自然语言理解能力的方法。该方法首先在大规模无标签文本上预训练一个语言模型，然后在特定的监督学习任务上进行微调。通过使用 **Transformer** 架构，并在预训练阶段学习长文本的表示，该方法在多种自然语言理解任务上取得了显著的性能提升，包括自然语言推理、问答、语义相似性和文本分类等任务。在 **12** 个任务中，该方法在 **9** 个任务上超越了之前的最佳水平。

# 1. 背景介绍

自然语言理解涵盖了多种任务，如文本蕴含、问答、语义相似性评估和文档分类等。尽管有大量的无标签文本数据，但针对这些特定任务的标注数据却很稀缺，这使得仅依赖监督学习的方法难以取得良好的效果。因此，如何从无标签文本中学习有效的语言表示，以便在多种任务中实现有效的迁移学习，成为了一个重要的研究方向。

传统的半监督学习方法主要关注词级或短语级的统计信息，而近年来，随着词嵌入技术的发展，人们开始探索如何从无标签数据中学习更高层次的语义信息。然而，如何选择最有效的优化目标以及如何将这些表示有效地迁移到目标任务上，仍然是一个开放性问题。

本文提出了一种结合无监督预训练和有监督微调的半监督学习方法。该方法的目标是学习一个通用的表示，能够以最小的调整迁移到多种任务中。具体来说，作者使用了一个两阶段的训练过程：首先在无标签数据上使用语言建模目标进行预训练，然后在特定的监督任务上进行微调。

# 2. GPT 模型

**Generative Pre-Training (GPT)**的结构采用**Transformer**的**Decoder**，首先在无标注的大规模语料库上训练模型，之后利用标注数据进行微调解决下游任务。

![](https://pic.imgdb.cn/item/60ec12765132923bf80b0317.jpg)

**GPT**采用**language modeling**作为预训练任务，即在预训练时极大化似然函数：

$$ L_1(\mathcal{U}) = \sum_{i}^{} logP(u_i | u_{i-k},...,u_{i-1};\Theta) $$

其中概率语言模型使用**Transformer**的**Decoder**。原输入可以表示成：

$$ h_0 = UW_e + W_p $$

其中$U$是词的上下文向量，$W_e$是词嵌入矩阵，$W_p$是位置编码。则使用多层**Transformer**如下：

$$ h_l = \text{transformer_block}(h_{l-1}) $$

预测结果可以表示为：

$$ P(u) = \text{softmax}(h_LW_e^T) $$

在预训练完成后，作者将模型参数迁移到特定的监督任务上。**GPT**采用有监督的分类任务进行微调：

$$ P(y | x^1,...,x^m) = \text{softmax}(h_L^mW_y) $$

极大似然函数如下：

$$ L_2(\mathcal{C}) = \sum_{(x,y)}^{} logP(y | x^1,...,x^m) $$

此外，作者发现将语言建模作为辅助目标加入微调阶段有助于学习，最终的优化目标函数为：

$$ L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda \cdot L_1(\mathcal{C}) $$

对于某些任务，如文本分类，可以直接对模型进行微调。而对于其他任务，如问答或文本蕴含，输入数据具有结构化形式，例如有序的句子对或文档、问题和答案的三元组。由于预训练模型是在连续文本序列上训练的，因此需要对这些任务的输入进行转换，以便模型能够处理。作者采用了一种遍历式的方法，将结构化输入转换为模型可以处理的有序序列。例如：
- 文本蕴含：将前提 $p$ 和假设 $h$ 的标记序列连接起来，中间用分隔符 $\$$ 分隔。
- 语义相似性：由于两个句子的比较没有固有的顺序，因此将两种可能的句子顺序都包含在输入序列中，并在分隔符之间独立处理。
- 问答和常识推理：将文档上下文 $z$、问题 $q$ 和每个可能的答案 $a_k$ 连接起来，中间用分隔符分隔，然后独立处理每个序列。

# 3. 实验分析

模型使用 **12** 层的 **Transformer** 解码器，隐藏层维度为 **768**，**12** 个自注意力头，位置感知前馈网络的内层维度为 **3072**。使用 **BooksCorpus** 数据集进行语言模型的预训练，该数据集包含超过 **7000** 本不同类型的未出版书籍。在微调阶段，对大多数任务使用了 **6.25e-5** 的学习率和 **32** 的批量大小，训练 **3** 个周期即可。

在自然语言推理任务中，模型在五个数据集上取得了显著的性能提升，包括 **SNLI、MultiNLI、QNLI、RTE** 和 **SciTail**。

![](https://pic1.imgdb.cn/item/67f624d588c538a9b5c76a7f.png)

在问答和常识推理任务中，模型在 **Story Cloze Test** 和 **RACE** 数据集上取得了显著的性能提升。

![](https://pic1.imgdb.cn/item/67f624ec88c538a9b5c76a8a.png)

在文本分类任务中，模型在 **CoLA** 和 **SST-2** 数据集上取得了显著的性能提升。在语义相似性任务中，模型在 **MSR Paraphrase Corpus、Quora Question Pairs** 和 **STS Benchmark** 三个数据集上取得了良好的性能。

![](https://pic1.imgdb.cn/item/67f6250788c538a9b5c76a9a.png)

作者分析了微调时迁移层数的影响，实验表明从预训练模型中迁移更多的层可以显著提升性能。例如，在 **MultiNLI** 上，迁移全部 **12** 层的 **Transformer** 模型比仅迁移 **6** 层的模型性能提升了 **9%**。

作者分析了模型的零样本学习行为，通过一系列启发式方法，作者分析了预训练模型在不同任务上的零样本学习能力。结果表明，随着预训练的进行，模型在这些任务上的性能稳步提升，这表明预训练过程有助于模型学习多种任务相关的功能。

![](https://pic1.imgdb.cn/item/67f6247d88c538a9b5c76a4d.png)

作者进行了多项消融研究，包括移除辅助语言建模目标、使用 **LSTM** 替代 **Transformer** 以及直接在监督任务上训练 **Transformer** 模型。结果表明，辅助语言建模目标在较大数据集上有助于性能提升，而 **Transformer** 架构比 **LSTM** 更适合处理长文本依赖关系。

![](https://pic1.imgdb.cn/item/67f624a188c538a9b5c76a68.png)


