---
layout: post
title: 'Improving Language Understanding by Generative Pre-Training'
date: 2021-01-03
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ec11fd5132923bf808ee56.jpg'
tags: 论文阅读
---

> GPT：使用生成式预训练模型提高对语言的理解.

- paper：[Improving Language Understanding by Generative Pre-Training](http://www.nlpir.org/wordpress/2019/06/16/improving-language-understanding-by-generative-pre-training/)

**Generative Pre-Training (GPT)**的结构采用**Transformer**的**Decoder**，层数$L=12$,特征维度$H=768$,激活函数使用**GeLU**。首先在无标注的大规模语料库上训练模型，之后利用标注数据进行微调解决下游任务。

![](https://pic.imgdb.cn/item/60ec12765132923bf80b0317.jpg)

**GPT**采用**language modeling**作为预训练任务，即在预训练时极大化似然函数：

$$ L_1(\mathcal{U}) = \sum_{i}^{} logP(u_i | u_{i-k},...,u_{i-1};\Theta) $$

其中概率语言模型使用**Transformer**的**Decoder**。原输入可以表示成：

$$ h_0 = UW_e + W_p $$

其中$U$是词的上下文向量，$W_e$是词嵌入矩阵，$W_p$是位置编码。则使用多层**Transformer**如下：

$$ h_l = \text{transformer_block}(h_{l-1}) $$

预测结果可以表示为：

$$ P(u) = \text{softmax}(h_LW_e^T) $$

另一方面，**GPT**采用有监督的分类任务进行微调：

$$ P(y | x^1,...,x^m) = \text{softmax}(h_L^mW_y) $$

极大似然函数如下：

$$ L_2(\mathcal{C}) = \sum_{(x,y)}^{} logP(y | x^1,...,x^m) $$

最终的优化目标函数为：

$$ L_3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda \cdot L_1(\mathcal{C}) $$
