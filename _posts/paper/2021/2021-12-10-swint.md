---
layout: post
title: 'Swin Transformer: Hierarchical Vision Transformer using Shifted Windows'
date: 2021-12-10
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/61e75fec2ab3f51d91b1881d.jpg'
tags: 论文阅读
---

> Swin Transformer: 基于移动窗口的分层视觉Transformer.

- paper：[Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)

本文设计了**Swin Transformer**，可以作为视觉任务的通用主干网络。**Transformer**已被广泛应用于自然语言处理任务中，将其应用到视觉任务时面临两个挑战：第一个挑战是视觉中目标尺寸存在巨大差异，图像中同时存在大目标和小目标；第二个挑战是图像具有更高的分辨率，一幅图像中的像素数通常远大于一段文本中的单词数。

为了解决这些问题，**Swin Transformer**采用移动窗口处理图像。移动窗口将注意力的计算限制在不重叠的局部窗口中，同时可以跨窗口连接。这种分层结构可以提取不同尺度的特征，并且相对于图像尺寸具有线性计算复杂度。作者在图像分类和密集任务上验证了该方法的有效性。

# 1. Swin Transformer的特点

**Swin Transformer**提出的分层结构如下图所示。通常的视觉**Transformer**(如**ViT**)把图像划分成若干个图像块，每个图像块看作一个**token**，通过自注意力机制进行全局交互。而**Swin Transformer**把不同阶段处理的特性划分成不同尺寸的窗口，自注意力机制在每个窗口内独立计算。每个窗口的尺寸是逐渐变大的，其中的**token**数量划分是固定的($7\times 7$)。

![](https://pic.imgdb.cn/item/61e7c9492ab3f51d9114f48f.jpg)

**Swin Transformer**中的每一个阶段都采用连续两层应用移动窗口(**shifted window**)的设置。对于划分窗口的自注意力，无法捕捉全局的特征信息，因此在下一层将窗口的划分整体向右下平移两个像素块，此时新窗口包含之前窗口的边界，能够建立不同窗口的连接。

![](https://pic.imgdb.cn/item/61e7c9c22ab3f51d91155e48.jpg)

移动窗口后原本的四个区域变成了九个区域，为了不增加多余的计算，作者采取了如下的高效计算过程。首先把九个区域调整为四个区域，则其中的三个区域包含来自不同区域的**token**，为了防止不同区域的交流，对每个区域计算自注意力时设置不同的**mask**。

![](https://pic.imgdb.cn/item/61e7c96f2ab3f51d9115122a.jpg)

对于左上角区域，其中的**token**来自同一个区域，因此可以计算自注意力。对于其他区域，当两个来自不同区域的**token**交互时，在其位置上增加一个比较大的负值。后续在进行**softmax**时该位置便会趋于$0$，相当于为其加上了**mask**。

![](https://pic.imgdb.cn/item/61e7c9902ab3f51d91152dbb.jpg)

# 2. Swin Transformer的整体结构

**Swin Transformer**的整体结构如下图所示。首先把输入图像拆分成不重叠的图像块，每个图像快的大小是$4\times 4$，因此该图像块的**token**可以用一个$4\times 4 \times 3 =48$维的向量表示。因此输入网络的特征维度是$\frac{H}{4} \times \frac{W}{4} \times 48$。**Swin Transformer**一共包括四个阶段。

![](https://pic.imgdb.cn/item/61e7c9e02ab3f51d91157f41.jpg)

在第一个阶段中，首先使用线性嵌入(**linear embedding**)层将每个**token**的特征维度映射为$C$，然后使用两个连续的**Swin Transformer**模块处理。值得一提的是**Swin Transformer**堆叠的块总是偶数个，这是因为每次划分窗口后总接着使用一次移动窗口。两个连续的**Swin Transformer**块如上图(b)所示。两层分别使用基于窗口(**Window**)和移动窗口(**Shifted-Window**)的多头自注意力机制，以及包含**GELU**激活函数的**MLP**层。在这些层之前应用**LayerNorm**，并设置跳跃连接。

在计算自注意力时，作者增加了相对位置偏差$B$，

在后三个阶段中，每个阶段首先使用图像块合并(**patch merging**)层产生分层表示。在第二个阶段中合并层连接相邻的$2\times 2$个图像块，使得图像中**token**的数量减少$4$倍($\frac{H}{8} \times \frac{W}{8}$)，同时**token**的特征维度增大$4$倍($4C$)，再使用一个线性层将特征维度从$4C$调整为$2C$。之后使用两个连续的**Swin Transformer**块处理特征。类似地，在第三阶段和第四阶段中的特征尺寸分别为$\frac{H}{16} \times \frac{W}{16} \times 4C$和$\frac{H}{32} \times \frac{W}{32} \times 8C$。图像块合并层使得网络的不同阶段能够处理不同尺寸的特征。

上述结构被称为**Swin-B**，其模型大小和计算复杂度与**ViT-B**相似。作者进一步设计了模型大小为$0.25$倍、$0.5$倍和$2$倍的模型**Swin-T**、**Swin-S**和**Swin-L**。
- **Swin-T**：$C=96$，每阶段层数$$=\{2,2,6,2\}$$
- **Swin-S**：$C=96$，每阶段层数$$=\{2,2,18,2\}$$
- **Swin-B**：$C=128$，每阶段层数$$=\{2,2,18,2\}$$
- **Swin-L**：$C=192$，每阶段层数$$=\{2,2,18,2\}$$

# 3. 实验分析

下表展示了**Swin Transformer**在图像分类任务和密集预测任务（目标检测和语义分割）上的性能。在图像分类**ImageNet-1K**上的**top-1**准确率为$87.3\%$；在目标检测**COCO testdev**上的**box AP**为$58.7$(提高$2.7$)，**mask AP**为$51.1$(提高$2.6$)；在语义分割**ADE20K val**上的**mIoU**为$53.5$(提高$3.2$)。

![](https://pic.imgdb.cn/item/61e7ca5a2ab3f51d911608ae.jpg)