---
layout: post
title: 'SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation'
date: 2021-03-27
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/642fdbd3a682492fcc6f496a.jpg'
tags: 论文阅读
---

> SegNeXt：重新思考语义分割中的卷积注意力设计.

- paper：[SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation](https://arxiv.org/abs/2209.08575)

语义分割任务的主流模型逐渐采用**Vision Transformer**结构。本文对已有成功分割模型进行了重审视并发现了几个有助于性能提升的关键成分，进而设计了一种新型的卷积注意力架构方案**SegNeXt**。

本文对语义分割领域代表性方案(**DeepLabV3+, HRNet, SETR, SegFormer**)进行重审视，总结出成功的语义分割方案应具有的几点关键属性：
- 采用强骨干网络作为编码器；
- 多尺度信息交互；
- 空间注意力；
- 低计算复杂度。

![](https://pic.imgdb.cn/item/642fdf9da682492fcc7e1c22.jpg)

基于上述考量，本文对卷积注意力设计进行了重思考并提出了一种简单而有效的编码器-解码器架**构SegNeXt**。不同于已有**Transformer**方案，**SegNeXt**对编码器模块采用传统卷积模块设计但引入了多尺度卷积注意力，对解码器模块采用了**Hamberger**(自注意力的一种替代方案)进一步提取全局上下文信息。因此**SegNeXt**能够从局部到全局提取多尺度上下文信息，能在空域与通道维度达成自适应性，能从底层到高层进行信息聚合。

![](https://pic.imgdb.cn/item/642fdfb1a682492fcc7e5d94.jpg)

# 1. 编码器

**Encoder**部分采用了金字塔架构，每个构成模块采用了类似**ViT**的结构，但不同之处在于：自注意力模块通过一种多尺度卷积注意力模块**MSCA**实现。

![](https://pic.imgdb.cn/item/642fedb9a682492fcca82557.jpg)

**MSCA**由三部分构成：深度卷积用于聚合局部信息、多分支深度卷积用于捕获多尺度上下文信息、$1\times 1$卷积用于在通道维度进行相关性建模。卷积的输出将作为注意力权值对**MSCA**的输入进行重加权。此外主要注意的是：**MSCAN**的每个模块采用的是**BN**，而非**LN**。

上表给出了通过堆叠**MSCA**而得到的不同**MSCAN**骨干信息以及**SegNeXt**架构信息。**MSCA**的实现参考如下。

![](https://pic.imgdb.cn/item/642fef64a682492fccab553c.jpg)

```python

class AttentionModule(BaseModule):
    def __init__(self, dim):
        super().__init__()
        self.conv0 = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)
        self.conv0_1 = nn.Conv2d(dim, dim, (1, 7), padding=(0, 3), groups=dim)
        self.conv0_2 = nn.Conv2d(dim, dim, (7, 1), padding=(3, 0), groups=dim)
 
        self.conv1_1 = nn.Conv2d(dim, dim, (1, 11), padding=(0, 5), groups=dim)
        self.conv1_2 = nn.Conv2d(dim, dim, (11, 1), padding=(5, 0), groups=dim)
 
        self.conv2_1 = nn.Conv2d(dim, dim, (1, 21), padding=(0, 10), groups=dim)
        self.conv2_2 = nn.Conv2d(dim, dim, (21, 1), padding=(10, 0), groups=dim)
        self.conv3 = nn.Conv2d(dim, dim, 1)
 
    def forward(self, x):
        u = x.clone()
        attn = self.conv0(x)
 
        attn_0 = self.conv0_1(attn)
        attn_0 = self.conv0_2(attn_0)
 
        attn_1 = self.conv1_1(attn)
        attn_1 = self.conv1_2(attn_1)
 
        attn_2 = self.conv2_1(attn)
        attn_2 = self.conv2_2(attn_2)
        attn = attn + attn_0 + attn_1 + attn_2
 
        attn = self.conv3(attn)
        return attn * u
```

# 2. 解码器

常规语义分割模型的骨干往往在**ImageNet**上预训练得到，为捕获高级语义信息，通常需要一个**Decoder**模块。本文则对以下三种简单**Decoder**架构进行了探索：
- a：源自**SegFormer**的解码器，是一种纯**MLP**架构；
- b：常被**CNN**方案使用，如**ASPP、PSP、DANet**等；
- c：本文采用的解码器，它采用轻量型[**Hamberger**模块]((https://0809zheng.github.io/2021/03/28/hambuger.html))对后三个阶段的特性进行聚合以进行全局上下文建模。

![](https://pic.imgdb.cn/item/642ff036a682492fccacd498.jpg)

需要注意的是，**SegFormer**的解码器对**Stage1**到**Stage4**的特征进行聚合，而本文方案则仅对**Stage2-Stage4**的特征进行聚合。这是因为：
- **SegNeXt**的**Encoder**采用了卷积架构，使得**Stage1**部分特征包含过多底层信息，进而导致其会影响语义分割性能。
- 对**Stage1**部分特征进行处理会带来过多的计算负载。