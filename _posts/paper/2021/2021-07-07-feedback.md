---
layout: post
title: 'Addressing Some Limitations of Transformers with Feedback Memory'
date: 2021-07-07
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60e52ec95132923bf80b16ff.jpg'
tags: 论文阅读
---

> Feedback Transformer：改进Transformer的序列信息提取能力.

- paper：Addressing Some Limitations of Transformers with Feedback Memory
- arXiv：[link](https://arxiv.org/abs/2002.09402v3)

循环神经网络**RNN**被用来处理序列数据，它根据当前时刻的输入和前一时刻的隐状态生成当前时刻的隐状态。**RNN**面临着**长程依赖(Long-Term Dependencies)**问题，即早期的某个序列信息，需要经过很长的路径才能传递给后面的序列，从而导致较难建立起和较早时间步信息的依赖关系。

**Transformer**引入了自注意力机制，该机制使得每个序列的中间特征都是由所有序列共同计算得到的，从而可以捕捉全局信息。然而**Transformer**使得序列的顺序不再重要(尽管引入位置编码一定程度上缓解了这个问题)，其结构因其并行计算的特性有很高的计算效率，但是这种特性限制了**Transformer**提取序列信息的能力，这体现在底层表示无法获得高层表示信息。值得一提的是，本文主要讨论**Transformer**的解码器结构(如**GPT-3**)，其输入是单向的，即每个位置的输出由当前位置及之前所有位置的输入决定。

**Transformer**中的自注意力如下图左所示。对于第$t$个输入序列的第$l$层特征$x_t^{l}$，其是由第$l-1$层的前$\tau$个输入序列特征(设定一个**memory size**的窗口长度$\tau$)共同决定的：

$$ x_t^{l} = \text{FF}( \text{Attn}(x_t^{l-1},\{x_{t-\tau}^{l-1},...,x_{t-1}^{l-1}\})) $$

显然前面输入序列的高层特征(如$x_{t-1}^{l+1}$)无法传递给后面输入序列的低层特征$x_{t}^{l}$。由于高层特征含有更高级的语义信息，因此在某些任务中较为重要。作者提出一种**Feedback Memory**结构，能够将前面输入序列的不同层次特征传递给后面输入序列，其实现过程如下图右所示。具体地，对于第$t$个输入序列，使用可学习的参数对其所有层次特征进行**softmax**加权(类似于注意力机制)，并将该特征作为后面序列的特征输入：

$$ x_t^{l} = \text{FF}( \text{Attn}(x_t^{l-1},\{m_{t-\tau},...,m_{t-1}\})) $$

$$ m_t = \sum_{l=0}^{L} \text{softmax}(w^l)x_t^l $$

![](https://pic.imgdb.cn/item/60e58cae5132923bf82dc7bb.jpg)

![](https://pic.imgdb.cn/item/60e591c05132923bf8423363.jpg)

下图表明，当**memory size**较小时(即自注意力只能在较小的窗口区域内计算)，**Feedback Transformer**仍然能取得较好的性能：

![](https://pic.imgdb.cn/item/60e591165132923bf83f7483.jpg)
