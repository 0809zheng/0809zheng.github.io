---
layout: post
title: 'Language Models are Unsupervised Multitask Learners'
date: 2021-01-11
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ed3fc15132923bf843d532.jpg'
tags: 论文阅读
---

> GPT2：语言模型是无监督的多任务模型.

- paper：[Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

# 0. TL; DR

本文探讨了语言模型在无监督多任务学习中的潜力。研究者们发现，当在大规模、多样化的文本数据集 **WebText** 上训练时，语言模型能够无需显式监督即可学习多种自然语言处理任务，如问答、阅读理解、文本摘要等。这种能力在零样本（**zero-shot**）设置中尤为显著，即模型在未见过特定任务数据的情况下直接进行测试。研究还表明，模型的容量（即参数数量）对零样本任务转移的成功至关重要，且随着容量的增加，性能呈对数线性增长。最大的模型 **GPT-2** 拥有 **15** 亿参数，在 **8** 个语言建模数据集中的 **7** 个上达到了零样本设置下的最佳性能，但仍未完全拟合 **WebText**。此外，**GPT-2** 生成的样本反映了这些改进，能够生成连贯的文本段落。这些发现为构建能够从自然语言演示中学习任务的语言处理系统提供了一条有希望的途径。

# 1. 背景介绍

自然语言处理（**NLP**）任务通常依赖于针对特定任务的数据集进行监督学习。然而，这种方法存在局限性：一方面，对于每个任务都需要手动创建和标注大量训练数据，这既耗时又昂贵；另一方面，这些系统往往对数据分布的微小变化和任务定义的改变非常敏感，缺乏泛化能力。因此，研究者们希望构建更通用的系统，能够执行多种任务，甚至无需为每个任务单独创建和标注数据集。

多任务学习（**multitask learning**）是一种有潜力提升模型泛化能力的框架，但在 **NLP** 领域仍处于起步阶段。以往的多任务训练尝试在有限的（数据集，目标）对上进行训练，但这些尝试在性能提升上相对有限。此外，随着任务数量的增加，创建数据集和设计目标的难度也呈指数级增长，这使得传统的多任务学习方法难以扩展。

本文提出了一种新的方法，通过在大规模、多样化的文本数据集上训练语言模型，使其能够无监督地学习多种任务。这种方法的核心在于，语言模型在学习预测文本序列时，能够隐式地学习到多种任务的模式和结构，从而在没有显式监督的情况下执行这些任务。

# 2. GPT2 模型

**GPT2**模型是对预训练语言模型**GPT**的改进，模型更大，训练数据更多。它在大规模无监督语料库上进行语言模型学习，可以微调到不同的下游自然语言处理任务中。

**GPT2**模型采用语言建模作为无监督训练任务，即建立输入序列的条件概率模型$p(x)=\prod_{i=1}^{n}p(s_i\|s_1,s_2,...,s_{i-1})$。作者认为这种建模可以自然地适配自然语言处理中的各种任务，因为这些下游无监督任务可以被表示成$p(\text{output}\|\text{input},\text{task})$(本质还是序列到序列问题)。而**BERT**那种双向建模模型$p(x)=\prod_{i=1}^{n}p(s_i\|s_1,...,s_{i-1},s_{i+1},...,s_{n})$，则无法直接解决上述问题。

**GPT2**模型使用基于 **Transformer** 的架构作为语言模型。该模型在 **OpenAI GPT** 模型的基础上进行了少量修改，包括将层归一化（**Layer Normalization**）移至每个子块的输入端，并在最终的自注意力块后添加了额外的层归一化。此外，模型的词汇表扩展到了 **50,257**，上下文大小从 **512** 增加到 **1024**，批量大小设置为 **512**。

![](https://pic.imgdb.cn/item/60ed45375132923bf8751d1c.jpg)

为了训练一个能够学习多种任务的语言模型，研究者们构建了一个名为 **WebText** 的新数据集。**WebText** 包含从 **Reddit** 链接到的网页文本，这些链接至少获得了 **3** 个赞。这种数据集的构建方式确保了文本的质量和多样性，涵盖了多种领域和上下文中的自然语言任务演示。

为了使语言模型能够处理任何字符串，研究者们采用了字节对编码（**Byte Pair Encoding, BPE**）作为输入表示。**BPE** 是一种介于字符级和单词级之间的语言建模方法，能够有效地处理高频符号序列和低频符号序列。通过这种方式，模型可以在保持单词级语言模型的经验优势的同时，具备处理任何 **Unicode** 字符串的能力。

在零样本任务转移中，研究者们将 **WebText** 上训练的语言模型直接应用于多种自然语言处理任务，无需对模型参数或架构进行任何修改。这种方法的核心在于，语言模型通过学习预测文本序列，能够隐式地学习到多种任务的模式和结构，从而在没有显式监督的情况下执行这些任务。

# 3. 实验分析

研究者们在多个语言建模数据集上评估了 **WebText** 语言模型的性能。这些数据集包括 **Penn Treebank、WikiText-2、LAMBADA、Children’s Book Test** 等。结果显示，**GPT-2** 在 8 个数据集中的 7 个上达到了零样本设置下的最佳性能，显著优于之前的最佳结果。

![](https://pic1.imgdb.cn/item/67f633e288c538a9b5c783c4.png)

在阅读理解任务中，研究者们使用了 **Conversation Question Answering（CoQA）**数据集。**CoQA** 包含来自 7 个不同领域的文档以及与文档相关的自然语言对话。**GPT-2** 在条件于文档和对话历史的情况下，通过贪婪解码生成的答案在开发集上达到了 **55 F1** 分数，这一性能超过了 3 个基线系统，且未使用这些基线系统训练时所依赖的 **127,000+** 手动收集的问题答案对。

研究者们在 **CNN** 和 **Daily Mail** 数据集上测试了 **GPT-2** 的文本摘要能力。通过在文章末尾添加 “TL;DR:” 并生成 100 个标记，**GPT-2** 能够生成具有一定连贯性的摘要。然而，根据 **ROUGE 1、2、L** 指标，**GPT-2** 的性能仅略高于经典神经基线，且在去除任务提示时性能显著下降，这表明 **GPT-2** 在摘要任务上的表现仍有待提高。

![](https://pic1.imgdb.cn/item/67f6346f88c538a9b5c784ae.png)

研究者们还测试了 **GPT-2** 的机器翻译能力。通过在模型的上下文中添加示例对（如 “英文句子 = 法文句子”），并以 “英文句子 =” 作为提示进行贪婪解码，**GPT-2** 能够生成翻译。在 **WMT-14** 英法测试集上，**GPT-2** 的 **BLEU** 分数为 5，略低于基于双语词典的逐词翻译。而在 **WMT-14** 法英测试集上，**GPT-2** 的 **BLEU** 分数达到了 11.5，超过了多个无监督机器翻译基线。

为了测试语言模型中包含的信息量，研究者们使用了 **Natural Questions** 数据集。通过在模型上下文中添加示例问答对，**GPT-2** 能够在开发集上正确回答 **4.1%** 的问题。这一比例虽然低于人类水平，但显著高于最小模型，表明模型容量对这类任务的性能有重要影响。

![](https://pic1.imgdb.cn/item/67f6350288c538a9b5c785c3.png)

研究者们还分析了 **WebText** 训练数据与特定评估数据集之间的重叠情况。结果显示，测试集与训练集之间存在一定程度的重叠，但这种重叠对模型性能的影响相对较小。此外，**GPT-2** 在 **WebText** 的训练集和测试集上的表现相似，且随着模型大小的增加，两者的表现共同提升，这表明 **GPT-2** 仍在 **WebText** 上欠拟合。

![](https://pic1.imgdb.cn/item/67f6351e88c538a9b5c78601.png)
