---
layout: post
title: 'Searching for Activation Functions'
date: 2021-09-04
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/613080ac44eaada739a7b3b9.jpg'
tags: 论文阅读
---

> Swish：自动搜索得到的一种自门控的激活函数.

- paper：Searching for Activation Functions
- arXiv：[link](https://arxiv.org/abs/1710.05941)

在本文中作者使用**自动搜索**(**automated search**)技术寻找更好的激活函数，并提出了一种新的激活函数：**Swish**。

# 1. 搜索激活函数
需要为激活函数设计合适的搜索空间。作者设计的搜索空间如下：

![](https://pic.imgdb.cn/item/6130830e44eaada739ac37e3.jpg)

该搜索空间由一个**core unit**递归地构造而成。**core unit**接收两个输入（其中一个输入可以是上一个输出），分别经过两次一元(**unary**)操作后使用一个二元(**binary**)操作进行组合，并得到输出。作者选用的一元操作和二元操作如下：

![](https://pic.imgdb.cn/item/6130871444eaada739b40655.jpg)

作者使用一个**RNN**控制器进行搜索。控制器在每个时间步长搜索激活函数的一个组成部分。当搜索完成一个激活函数后，对应地构造一个**ResNet-20**子网络，在**CIFAR-10**上训练$10K$轮并记录验证准确率。准确率作为**reward**通过强化学习训练控制器。由于搜索空间较大，作者采用并行搜索策略，

![](https://pic.imgdb.cn/item/6130842044eaada739ae3850.jpg)

# 2. 搜索结果

![](https://pic.imgdb.cn/item/613087bd44eaada739b54fd9.jpg)

搜索得到的表现较好的若干激活函数如上图所示。这些激活函数的特点如下：
- 复杂的激活函数表现不如简单的激活函数，可能是因为复杂函数导致优化更加困难；表现最好的激活函数通常具有$1$-$2$个**core unit**。
- 表现较好的激活函数通常会使用$x$作为输入的一部分。
- 一些表现较好的激活函数使用到了周期函数，如**sin,cos**，且以加减的形式出现；这类函数之前研究较少。
- 使用除法的激活函数通常表现较差，由于分母接近$0$时数值爆炸。只有当分子分母都接近$0$时才具有较好的表现，如**cosh**。

激活函数的搜索过程是在小模型上进行的，作者通过进一步实验证明这些函数可以成功泛化到较大的模型上，并保持或超过**ReLU**激活函数的准确率。

![](https://pic.imgdb.cn/item/61308a6a44eaada739ba33f1.jpg)

# 3. Swish
作者将搜索得到表现最好的激活函数称为**Swish**，**Swish**的函数表达式如下，其中$β$是一个常数或可学习的参数：

$$ \text{Swish}(x)=x·\text{sigmoid}(βx)=\frac{x}{1+e^{-\beta x}} $$

**Swish**及其一阶导数的图像如下：

![](https://pic.imgdb.cn/item/61308bb544eaada739bfda73.jpg)

当$β=0$时，**Swish**函数退化成线性函数$\frac{x}{2}$；当$β\to ∞$时，**Swish**函数退化成**ReLU**函数。因此**Swish**函数可以看作是线性函数和**ReLU**函数之间的光滑非线性插值结果。

与**ReLU**函数相似，**Swish**函数无上界、有下界。不同于**ReLU**函数，**Swish**函数是光滑的，而且是非单调的。**Swish**函数的导数计算如下（记$\sigma$为**sigmoid**函数）：

$$ f'(x) = \sigma(\beta x) + x \cdot \sigma'(\beta x) \\ = \sigma(\beta x) + \beta x \cdot \sigma(\beta x) \cdot (1-\sigma(\beta x)) \\ = \beta x \cdot \sigma(\beta x) + \sigma(\beta x)(1-\beta x \cdot \sigma(\beta x)) \\ = \beta f(x) + \sigma(\beta x)(1-\beta f(x))  $$

$\beta$控制着**Swish**函数的导数在$0$-$1$之间变化的程度。不同于**ReLU**的导数在$x>0$时恒为$1$，**Swish**函数的导数是变化的。当$x<0$时，**Swish**函数是非单调的。

受**LSTM**中**门控(gating)**机制的启发，**Swish**函数可以看作一种软性的自门控机制(**self-gating**)，使用自身的值作为门控，当$\text{sigmoid}(βx)$接近于$1$时门“开”；当$\text{sigmoid}(βx)$接近于$0$时门“关”。

参数$\beta$可以通过学习得到，则网络中每一个神经元的激活函数参数都可以是不同的。作者展示了在一个网络上参数$\beta$的取值分布，大多数参数取值在$1$附近。

![](https://pic.imgdb.cn/item/61308f8744eaada739cdd1a7.jpg)

实验结果表明**Swish**函数在大多数任务上超越了其他人工设计的激活函数：

![](https://pic.imgdb.cn/item/61308fda44eaada739cee4da.jpg)
