---
layout: post
title: 'Next-Generation Pose Detection with MoveNet and TensorFlow'
date: 2021-10-14
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/678e119ed0e0a243d4f5e2fb.png'
tags: 论文阅读
---

> 通过MoveNet和TensorFlow实现下一代姿态检测.

- paper：[Next-Generation Pose Detection with MoveNet and TensorFlow](https://blog.tensorflow.org/2021/05/next-generation-pose-detection-with-movenet-and-tensorflowjs.html)

**MoveNet**是**Google**推出的一款轻量化姿态估计模型，是一个**bottom-up**的单人姿态估计模型。

**MoveNet**整体的结构如图所示，**Backbone**部分是比较经典的带三层**deconv**的**MobileNetv2**。**Neck**部分使用了目标检测中常见的**FPN**来做不同尺度的特征学习和融合。

**Head**部分有四个预测头，分别是：
- **Center Heatmap** $[B, 1, H, W]$：预测每个人的几何中心，主要用于存在性检测，用**Heatmap**上一个锚点来代替目标检测的**bbox**；
- **Keypoint Regression** $[B, 2K, H, W]$：基于中心点来回归**17**个关节点坐标值；
- **Keypoint Heatmap** $[B, K, H, W]$：每种类型的关键点使用一张**Heatmap**进行检测，这意味着多人场景下这张**Heatmap**中会出现多个高斯核；
- **Offset Regression** $[B, 2K, H, W]$：回归**Keypoint Heatmap**中各高斯核中心跟真实坐标的偏移值，用于消除**Heatmap**方法的量化误差。

![](https://pic1.imgdb.cn/item/678e1248d0e0a243d4f5e366.png)

对于四个头部预测的结果，会按照以下流程进行处理：
1. 选择距离画面中心最近的人体目标的中心点。对于画面中的每一个像素，预先计算好一张权重**Mask**，每一个像素上的权重等于这个像素到画面中心的集合距离的倒数，用它对**Center Heatmap**加权，把画面中其他人的响应值压低，从而通过**Argmax**拿到离画面最近的人的最大值点。
2. 通过**Keypoint Regression**的结果获得一个人的所有关键点的坐标。
3. 假设距离**Regression**位置最近的高斯核是感兴趣目标的。通过**Regression**的坐标和全图的像素计算出一个权重**Mask**，用它来对**Keypoint Heatmap**加权，从而抑制掉其他距离较远的高斯核响应值，从而通过**Argmax**拿到感兴趣目标的关键点。
4. 通过**Offset Regression**分支来预测**Heatmap**到**GT**点的偏移，从而消除量化误差。

![](https://pic1.imgdb.cn/item/678e12ddd0e0a243d4f5e3a7.png)

**MoveNet** 在两个数据集上进行了训练：**COCO**和一个名为 **Active** 的内部 **Google** 数据集。虽然 **COCO** 是标准基准数据集，但它并不适合健身和舞蹈等场景应用，因为这些应用表现出具有挑战性的姿势和明显的运动模糊。**Active** 是通过在 **YouTube** 上的瑜伽、健身和舞蹈视频上标记关键点（采用 **COCO** 的标准 **17** 个身体关键点）生成的。每个视频中选择的训练帧不超过三帧，以促进场景和个人的多样性。

