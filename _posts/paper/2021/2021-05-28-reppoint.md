---
layout: post
title: 'RepPoints: Point Set Representation for Object Detection'
date: 2021-05-28
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6534a37fc458853aeff6566e.jpg'
tags: 论文阅读
---

> RepPoints：目标检测中的点集表示.

- paper：[RepPoints: Point Set Representation for Object Detection](https://arxiv.org/abs/1904.11490)

矩形**bbox**表示方法非常多，常用的可以是**xywh**代表中心点和宽高，也可以**x1y1x2y2**，代表左上和右下点坐标。这些**bbox**表示方式过于粗糙(**bbox**内部所有点都表征该**bbox**)，无法表示不同物体形状和姿态，采用这种方式进行特征提取会带来大量噪声和无关背景，最终导致性能下降。

针对这种情况，作者提出了采用语义关键点来表征**bbox**。设每个**bbox**最多需要$9$个语义点，这$9$个关键点是物体独有语义点，会分布在目标的语义位置。假设对数据采用了$9$个语义点的标注方法，那么在网络训练过程中，可以采用[<font color=blue>CenterNet</font>](https://0809zheng.github.io/2021/03/23/centernet.html)做法，**head**分成$2$个分支输出，第一个分支输出是中心点回归热图，第二个分支输出$18$个通道的$9$个语义点坐标。

![](https://pic.imgdb.cn/item/6534a63fc458853aefff5cec.jpg)

$9$个语义点的表示方法看起来会比**xywh**和**x1y1x2y2**更加靠谱，但是因为不同物体$9$个语义点标注方式很难确定，而且标注工作量太大了的原因，直接标注是肯定不行的。本文的核心亮点就在于仅仅需要原始**bbox**标注的监督就可以自动学习出$9$个语义点坐标。为了能够对$9$个语义点坐标进行弱**bbox**监督训练，作者提出了转换函数$T$即将预测的$9$个语义点通过某个可微函数转换得到**bbox**，然后对预测**bbox**进行**Loss**监督即可。

# 1. 网络结构

整体算法流程如图所示，是采用多阶段**refine**方式。其核心思想是：对特征图上面任何一点都学习出$9$个语义关键点坐标**offset**，同时将**offset**解码和转换得到原始**bbox**，即可进行**bbox**监督了；然后将预测输出**offset**作为可变形卷积的**offset**输入进行特征重采样捕获几何位置得到新的特征图；最后对该特征图进行分类和下一步**offset**精细**refine**即可，第二步**refine**分支输出的是相对于第一阶段**offset** $9$个点的偏移值。

![](https://pic.imgdb.cn/item/6534a6f7c458853aef01c34d.jpg)

**FPN**模块输出是**5**个不同大小的特征图，都需要经过同一个**head**网络进行分类和回归。**head**模块输出**3**个分支：分类分支、初始表征点回归分支和**refine**表征点回归分支。
1. 对**FPN**输出的某个特征图，分成分类特征图和回归特征图两条分支，然后分别经过**3**个卷积进行特征提取
2. 对**pts_feat**进行**3x3+1x1**的卷积，输出通道为$18$的**offset**,即特征图上每个点都回归$9$个语义点的**xy**坐标
3. 初始**pts_out_init**分支梯度乘上系数$0.1$，目的是希望降低该分支的梯度权重
4. 利用**offset**预测值，减掉每个特征图上**kernel**所对应的$9$个点**base**坐标作为**dcn**的**offset**输入值
5. 应用**dcn**对分类分支和**refine**回归分支进行特征自适应，得到新的特征图，然后经过两个**1x1**卷积得到最终输出，分类分支**cls_out**输出通道是**num_class**，而**refine**回归分支**pts_out_refine**是$18$
6. **refine**加上初始预测就可以得到**refine**后的输出$9$个点坐标

![](https://pic.imgdb.cn/item/6534a8adc458853aef079b14.jpg)

# 2. 训练细节

## （1）正负样本定义

在得到**cls_out, pts_out_init, pts_out_refine**输出后，需要对每个特征图位置的三个输出分支都定义正负样本。

### ⚪ pts_out_init的标签分配

对于回归问题而言，其仅仅是对正样本进行训练即可。**pts_out_init**分支采用的正负样本分配配置为**PointAssigner**，其核心操作是：遍历每个**gt bbox**，将该**gt bbox**映射到特定特征图层，其中心点所处位置即为正样本，其余位置全部忽略。

① 计算**gt bbox**宽高落在哪个尺度，公式为：

$$
s(B) = \lfloor \log_2(\sqrt{w_Bh_B}/4) \rfloor
$$

```python
gt_bboxes_lvl = ((torch.log2(gt_bboxes_wh[:, 0] / scale) +
                          torch.log2(gt_bboxes_wh[:, 1] / scale)) / 2).int()
points_lvl = torch.log2(points_stride).int() 
lvl_min, lvl_max = points_lvl.min(), points_lvl.max()                    
gt_bboxes_lvl = torch.clamp(gt_bboxes_lvl, min=lvl_min, max=lvl_max)
```

② 遍历每个**gt bbox**，找到其所属的特征图层；为了通用性，先计算特征图上任何一点距离**gt bbox**中心点坐标的距离；然后利用**topk**算法选择出前**pos_num**个距离**gt bbox**最近的特征图点，这**pos_num**个都算正样本。

```python
points_gt_dist = ((lvl_points - gt_point) / gt_wh).norm(dim=1)
min_dist, min_dist_index = torch.topk(
                points_gt_dist, self.pos_num, largest=False)
min_dist_points_index = points_index[min_dist_index]
```

③ 还需要特别考虑的是：假设**topk**的**k**为**1**，也就是仅仅**gt bbox**落在特征图的位置为正样本，假设有两个**gt bbox**的中心点重合且映射到同一个输出层，那么会出现后遍历的**gt bbox**覆盖前面**gt bbox**；对于这类样本的处理办法是其距离哪个**gt bbox**中心最近就负责预测谁。

```python
less_than_recorded_index = min_dist < assigned_gt_dist[
                min_dist_points_index]
min_dist_points_index = min_dist_points_index[
                less_than_recorded_index]
assigned_gt_dist[min_dist_points_index] = min_dist[
                less_than_recorded_index]
```

### ⚪ pts_out_refine的标签分配

第二阶段**offset**回归采用**MaxIoUAssigner**，输入是经过第一个阶段预测的**offset**解码还原后的初始**bbox**和**gt bbox**，然后基于最大**iou**原则定义正负样本。

### ⚪ cls_out的标签分配

分类分支采用的第二阶段**offset**回归里面的**MaxIoUAssigner**准则。

## （2）边界框编解码

为了能够对预测的$9$个语义点坐标进行**loss**监督，需要将$9$个语义点坐标转化得到**bbox**，作者提出三种做法，性能非常类似：**minmax**、**partial_minmax**和**moment**。
- **minmax**：对$9$个**offset**沿**xy**方向的最大和最小构成**bbox**
- **partial_minmax**：仅仅选择前$4$个点进行**minmax**操作
- **moment**：通过这$9$个点先求均值得到**xy**方向的均值即为**gt bbox**的中心坐标；对$9$个点求标准差操作然后通过可学习的**transfer**参数进行指数还原：

```python
# 均值和方差就是gt bbox的中心点
pts_y_mean = pts_y.mean(dim=1, keepdim=True)
pts_x_mean = pts_x.mean(dim=1, keepdim=True)
pts_y_std = torch.std(pts_y - pts_y_mean, dim=1, keepdim=True)
pts_x_std = torch.std(pts_x - pts_x_mean, dim=1, keepdim=True)
# self.moment_transfer也进行梯度增强操作
moment_transfer = (self.moment_transfer * self.moment_mul) + (
            self.moment_transfer.detach() * (1 - self.moment_mul))
moment_width_transfer = moment_transfer[0]
moment_height_transfer = moment_transfer[1]
# 解码代码
half_width = pts_x_std * torch.exp(moment_width_transfer)
half_height = pts_y_std * torch.exp(moment_height_transfer)
bbox = torch.cat([
            pts_x_mean - half_width, pts_y_mean - half_height,
            pts_x_mean + half_width, pts_y_mean + half_height
   ],dim=1)
```

