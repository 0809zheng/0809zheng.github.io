---
layout: post
title: 'Padé Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks'
date: 2021-10-24
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/617767902ab3f51d91a2196f.jpg'
tags: 论文阅读
---

> PAU：基于Padé近似的可学习激活函数.

- paper：[Padé Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks](https://arxiv.org/abs/1907.06732v1)

作者提出了一种基于**Padé**近似的可学习激活函数，称为**Padé**激活单元(**Padé Activation Unit, PAU**)，即使用给定阶数的有理函数实现对任意函数的通用近似。

# 1. Padé近似

给定任意函数$f(x)$，**Padé**近似是指使用给定阶数的有理函数$F(x)$(分式)对其进行近似。给定有理函数分子和分母的阶$m$和$n$，近似表达式为：

$$ F(x) = \frac{P(x)}{Q(x)} = \frac{\sum_{j=0}^{m}a_jx^j}{1+\sum_{k=1}^{n}b_kx^k} \\ = \frac{a_0+a_1x+a_2x^2+...+a_mx^m}{1+b_1x+b_2x^2+...+b_nx^n} $$

通常**Padé**近似能够给出比**Taylor**近似更好的近似结果，且在**Taylor**级数不收敛的情况下仍然有效。然而**Padé**近似的灵活性可能导致它对函数的极点等进行建模，从而导致网络学习和推理时的不稳定性。作者使用了一种安全的**Padé**近似，能够保证分母的值不小于$1$：

$$ F(x) = \frac{P(x)}{Q(x)} = \frac{\sum_{j=0}^{m}a_jx^j}{1+\sum_{k=1}^{n}|b_k||x|^k} \\ = \frac{a_0+a_1x+a_2x^2+...+a_mx^m}{1+|b_1||x|+|b_2||x|^2+...+|b_n||x|^n} $$

# 2. PAU

一般的**Padé**近似是通过对预定义函数的微分与代数操作进行求解的。将**Padé**近似用作激活函数，可以通过梯度下降进行优化，从数据中学习有理分式的系数。

首先求分子与分母多项式的梯度：

$$ \frac{\partial P(x)}{\partial x} = a_1+2a_2x+...+ma_mx^{m-1} $$

$$ \frac{\partial Q(x)}{\partial x} = \frac{x}{|x|}(|b_1|+2|b_2||x|+...+n|b_n||x|^{n-1}) $$

则**PAU**的梯度为：

$$ \frac{\partial F}{\partial x} = \frac{\partial P(x)}{\partial x} \frac{1}{Q(x)}-\frac{\partial Q(x)}{\partial x}\frac{P(x)}{Q(x)^2} $$

$$ \frac{\partial F}{\partial a_j} = \frac{x^j}{Q(x)} $$

$$ \frac{\partial F}{\partial b_k} = -x^k\frac{P(x)}{Q(x)^2}\frac{b_k}{|b_k|} $$

为了减少参数量，作者设置每一层的所有神经元共享**PAU**的参数。**PAU**的初始化采用标准的**Padé**近似对已知激活函数拟合。下面展示了**Padé**近似对常用的激活函数进行近似(虚线表示)：

![](https://pic.imgdb.cn/item/617771622ab3f51d91a80158.jpg)

# 3. 实验分析
实验表明，**PAU**在训练速度和预测性能方面均超过固定的激活函数：

![](https://pic.imgdb.cn/item/6177728a2ab3f51d91a8c99e.jpg)

作者展示了不同网络层学习到的**PAU**的形式：

![](https://pic.imgdb.cn/item/617772152ab3f51d91a87d91.jpg)