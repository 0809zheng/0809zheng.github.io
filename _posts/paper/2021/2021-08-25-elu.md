---
layout: post
title: 'Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)'
date: 2021-08-25
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6125f25944eaada739aa5d0f.jpg'
tags: 论文阅读
---

> ELU：消除偏差偏置的指数线性单元.

- paper：Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)
- arXiv：[link](https://arxiv.org/abs/1511.07289)


早期的激活函数如**softmax**，存在较长的饱和区，容易产生梯度消失问题。

**ReLU**解决了这一问题，但是其$x<0$时取值为$0$，造成**dead ReLU**问题，即若输入为负，则不会被激活，且梯度为零，从而在网络中失去作用。此外**ReLU**的输出是非负的(均值非$0$)，会导致下一层的输入存在**bias shift**，即由于激活值不能相互抵消，会产生一个非$0$的偏差，并随着网络的深入造成偏差累积。

**LeakyReLU**等激活函数通过保留负值解决**dead ReLU**问题，但其对噪声的鲁棒性较差，在输入很小时是不连续的。

作者提出了**指数线性单元**(**exponential linear unit,ELU**)，既能够进行**bias shift correction**(将输出的均值控制在$0$附近)，又对输入噪声是鲁棒的。

![](https://pic.imgdb.cn/item/6125f4ee44eaada739b2929e.jpg)

**ELU**及其导数的表达式如下：

$$
        \text{ELU}(x) =
        \begin{cases}
        x,  & \text{if $x≥0$} \\
        α(e^x-1), & \text{if $x<0$}
        \end{cases}
$$

$$
        \text{ELU}'(x) =
        \begin{cases}
        1,  & \text{if $x≥0$} \\
        \text{ELU}(x)+\alpha, & \text{if $x<0$}
        \end{cases}
$$

其中$\alpha$控制**ELU**达到负饱和后的数值，注意到仅当$\alpha=1$时整个函数是连续的。

作者可视化了使用**ELU**和**ReLU**作为激活函数的网络每一层的激活值均值统计，使用**ELU**的网络每一层的激活值均值都非常接近$0$，这有助于网络更好的学习。

![](https://pic.imgdb.cn/item/6125fd0c44eaada739cc2200.jpg)