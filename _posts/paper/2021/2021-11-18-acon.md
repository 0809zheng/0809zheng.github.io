---
layout: post
title: 'Activate or Not: Learning Customized Activation'
date: 2021-11-18
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6194dabb2ab3f51d91609b9b.jpg'
tags: 论文阅读
---

> ACON：学习自定义的激活函数.

- paper：[Activate or Not: Learning Customized Activation](https://arxiv.org/abs/2009.04759)

作者从**Swish**是一种平滑的**ReLU**中得到启发，提出了一种激活函数**ACON**，其可以看作**maxout**的平滑形式。进一步设计了**meta-ACON**，引入可学习的参数用于控制每个神经元是否被激活，在视觉任务上提高性能。

![](https://pic.imgdb.cn/item/6195fc122ab3f51d911655c7.jpg)

最大值函数$\max(x_1,x_2,...,x_n)$的一个可微近似为：

$$ \max(x_1,x_2,...,x_n) = \frac{\sum_{i=1}^{n}x_ie^{\beta x_i}}{\sum_{i=1}^{n}e^{\beta x_i}} $$

$\beta$是开关因子，当$\beta \to ∞$上式趋近于最大值函数；当$\beta =0$上式为简单的算术平均。

当$n=2$时，记$\sigma(x) = 1/(1+e^{-x})$，则最大值函数为：

$$ \max(x_1,x_2) = \frac{x_1e^{\beta x_1}+x_2e^{\beta x_2}}{e^{\beta x_1}+e^{\beta x_2}} \\ =  \frac{x_1e^{\beta x_1}}{e^{\beta x_1}+e^{\beta x_2}} + \frac{x_2e^{\beta x_2}}{e^{\beta x_1}+e^{\beta x_2}} \\ = x_1\frac{1}{1+e^{-\beta(x_1-x_2)}}+x_2\frac{1}{1+e^{-\beta(x_2-x_1)}} \\ = x_1 \sigma(\beta(x_1-x_2))+x_2\sigma(\beta(x_2-x_1)) \\ =x_1 \sigma(\beta(x_1-x_2))+x_2[1-\sigma(\beta(x_1-x_2))] \\ = (x_1-x_2)\sigma(\beta(x_1-x_2))+x_2 $$

若自变量为任意函数形式$\eta_{\alpha}$和$\eta_{\beta}$，则**maxout**函数族$\max(\eta_{\alpha}(x),\eta_{\beta}(x))$的光滑近似表示为：

$$ \max(\eta_{\alpha}(x),\eta_{\beta}(x)) \\=  (\eta_{\alpha}(x)-\eta_{\beta}(x))\sigma(\beta(\eta_{\alpha}(x)-\eta_{\beta}(x)))+\eta_{\beta}(x) $$

通过对函数$\eta_{\alpha}$和$\eta_{\beta}$赋予不同的形式，可以得到一系列对应的光滑函数。

![](https://pic.imgdb.cn/item/6196029e2ab3f51d911bfb66.jpg)

- **ACON-A**：若令$\eta_{\alpha}=x$，$\eta_{\beta}=0$，对应**ReLU**激活函数$\max(x,0)$；此时对应形式为$x\sigma(\beta x)$，即**Swish**激活函数。
- **ACON-B**：若令$\eta_{\alpha}=x$，$\eta_{\beta}=px$，对应**PReLU**激活函数$\max(x,px)$；此时对应形式为$(1-p)x\sigma(\beta (1-p)x)+px$。
- **ACON-C**：若令$\eta_{\alpha}=p_1x$，$\eta_{\beta}=p_2x$，对应$\max(p_1x,p_2x)$；此时对应形式为$(p_1-p_2)x\sigma(\beta (p_1-p_2)x)+p_2x$。
  
本文主要讨论**ACON-C**激活函数，其函数曲线及一阶导数曲线如下。

![](https://pic.imgdb.cn/item/61960c882ab3f51d91234d53.jpg)

**Swish**激活函数具有固定的梯度上界和下界；而**ACON-C**的梯度上界和下界是可学习的。**ACON-C**的梯度计算为：

$$ \frac{d}{dx}[f_{\text{ACON-C}}(x)] = (p_1-p_2)\sigma(\beta (p_1-p_2)x)\\ + \beta(p_1-p_2)^2x\sigma(\beta (p_1-p_2)x)[1-\sigma(\beta (p_1-p_2)x)]+p_2 $$

注意到：

$$ \mathop{\lim}_{x \to ∞}\frac{d}{dx}[f_{\text{ACON-C}}(x)] = p_1 \\ \mathop{\lim}_{x \to -∞}\frac{d}{dx}[f_{\text{ACON-C}}(x)] = p_2  $$

若进一步对其求二阶导数，并令其为$0$，可得导数的极值：

$$ \max(\frac{d}{dx}[f_{\text{ACON-C}}(x)]) ≈ 1.0998p_1-0.0998p_2 \\ \min(\frac{d}{dx}[f_{\text{ACON-C}}(x)]) ≈ 1.0998p_2-0.0998p_1 $$

在**Swish**中，超参数$\beta$仅决定其梯度趋近于上界和下界的速度，梯度的上界和下界固定为$1.0998$和$-0.0998$。而**ACON-C**的梯度界则是可学习的。

开关因子$\beta$控制激活函数的非线性程度，决定是否激活。当$\beta \to ∞$时**ACON-C**趋近于$\max(p_1x,p_2x)$；当$\beta =0$时**ACON-C**为$(p_1-p_2)x/2$。

![](https://pic.imgdb.cn/item/61960d412ab3f51d9123fe0c.jpg)

作者进一步提出显式地控制$\beta$的激活函数版本**meta-ACON**，即将$\beta$表示为输入样本的函数$\beta=G(x)$。若输入为图像数据$x \in \Bbb{R}^{C\times H\times W}$，$\beta$可计算为不同的形式：
- **layer-wise**：
$$ \beta = \sigma \sum_{c=1}^{C}\sum_{h=1}^{H}\sum_{w=1}^{W}x_{c,h,w} $$
- **channel-wise**：
$$ \beta_c = \sigma W_1W_2 \sum_{h=1}^{H}\sum_{w=1}^{W}x_{c,h,w} $$
- **pixel-wise**：
$$ \beta_{c,h,w} = \sigma x_{c,h,w} $$

作者展示了使用**ACON**和**meta-ACON**激活函数的**ResNet-50**网络最后一层的神经元的$\beta$参数的分布情况。对于**ACON**，参数分布如蓝色直方图；对于**meta-ACON**，作者选择了$7$个样本，每个样本对应的分布都是不同的。$\beta$值越大表示网络引入了越多非线性。

![](https://pic.imgdb.cn/item/61960fde2ab3f51d9125ef87.jpg)

作者选用**channel-wise**版本的**meta-ACON**。实验结果如下：

![](https://pic.imgdb.cn/item/61960da22ab3f51d91244e85.jpg)