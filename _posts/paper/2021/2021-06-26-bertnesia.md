---
layout: post
title: 'BERTnesia: Investigating the capture and forgetting of knowledge in BERT'
date: 2021-06-26
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67f76ae888c538a9b5c852d3.png'
tags: 论文阅读
---

> BERTnesia：探究 BERT 中知识的捕获与遗忘.

- paper：[BERTnesia: Investigating the capture and forgetting of knowledge in BERT](https://arxiv.org/abs/2106.02902)


# 0. TL; DR

本文通过设计一种新的框架，对**BERT**的每一层进行知识探测，发现**BERT**的知识不仅存储在最后一层，中间层也贡献了大量知识（**17-60%**）。在对**BERT**进行微调时，关系知识会被遗忘，遗忘程度取决于微调目标和训练数据。研究还发现，掩码语言建模（**MLM**）在从训练数据中获取新知识方面表现最佳。这些发现有助于我们更好地理解语言模型的参数记忆，并为知识密集型任务设计更有效的训练范式。

# 1. 背景介绍

大型预训练语言模型如**BERT**在自然语言处理（**NLP**）领域取得了显著进展，不仅在传统任务如问答和机器翻译上表现出色，还在知识库补全等新领域取得了突破。**BERT**等模型通过过参数化和建模长期交互来实现对文本输入的更好理解。然而，这些模型的性能提升是以降低可解释性为代价的。

近年来，研究者们通过“探测”方法来检查**BERT**等复杂语言模型的内部工作机制，以提高其可解释性。探测方法通过构建特定的输入来测试模型是否能够解码出特定的语法或语义信息。此外，研究还发现**BERT**能够从训练过程中获取事实和关系知识，这些知识可以用于自动知识库构建等任务。

尽管如此，关于**BERT**参数记忆中的信息内容仍有许多问题未得到解答。例如，现有的探测方法主要关注**BERT**的最后一层，可能会低估较低层中的知识含量。此外，微调对关系知识的影响尚不清楚。本文旨在通过探测**BERT**的每一层来研究知识的分布和演变，并探讨微调对知识的影响。

# 2. 方法介绍

为了全面了解**BERT**的知识分布，本文提出了一个框架，通过训练一个轻量级的解码器来探测**BERT**每一层的表示。这个解码器与**BERT**的标准预训练过程相同，通过掩码语言建模（**MLM**）任务来训练。具体来说，对于每个**BERT**层训练一个独立的解码器，使其能够从该层的表示中预测被掩码的单词。通过这种方式，可以测量**BERT**每一层的知识含量。

本文使用了**BERT-base**模型（**12**层），并在**BooksCorpus**和英文维基百科上进行了预训练。为了保持比较的一致性，所有微调模型都基于这个预训练的**BERT**模型。微调任务包括命名实体识别（**NER**）、问答（**QA**）和排名（**ranking**）等。为了最小化额外参数的添加，在微调过程中尽量减少对**BERT**之外参数的修改。

![](https://pic1.imgdb.cn/item/67f76d2d88c538a9b5c854a2.png)

探测过程基于**LAMA**知识探测数据集，该数据集包含了一系列的完型填空任务，用于测试**BERT**对事实和关系知识的掌握程度。每个探测任务都是一个句子，其中某些单词被掩码，模型需要预测这些掩码单词。

![](https://pic1.imgdb.cn/item/67f76d6588c538a9b5c854ed.png)

为了测量**BERT**每一层的知识含量，将解码器输出的概率分布转换为排名，并使用排名前1（**P@1**）的精度作为主要评估指标。此外还测量了排名前10（**P@10**）和排名前100（**P@100**）的精度，以验证结果的一致性。

探测过程中，对于每个**BERT**层$l$，使用以下公式计算该层的知识含量：

$$
P@1=\max(\{P_l@1\mid ∀l∈L\})
$$

其中，$L$是所有层的集合，$P_l @1$是第$l$层的$P@1$值。这个公式允许我们综合考虑**BERT**所有层的知识，而不仅仅是最后一层。

# 3. 实验分析

实验结果表明，**BERT**的知识不仅存储在最后一层，中间层也贡献了大量知识。例如，在**T-REx**数据集上，**BERT**的最后一层忘记了**18%**的知识，而在其他数据集上，这一比例高达**33%**。这表明中间层的知识在最后一层被部分遗忘。此外还发现平均有**7%**的关系类型在中间层被更好地捕获，而在最后一层被遗忘。这些结果表明，仅关注最后一层会低估**BERT**的知识含量。

![](https://pic1.imgdb.cn/item/67f76ebf88c538a9b5c8571f.png)

通过分析**BERT**每一层的知识含量，发现关系知识随着层数的增加而逐渐增加，但在某些关系类型上，最后一层的知识含量会显著增加。例如，有**15%**的关系类型在最后一层的知识含量翻倍，而**7%**的关系类型在中间层被最好地捕获。这表明关系知识的分布并非单调增加，而是存在一定的波动。

![](https://pic1.imgdb.cn/item/67f76f5888c538a9b5c857d9.png)

微调对**BERT**的知识含量有显著影响。实验表明，所有微调模型在最后一层的知识含量都低于**BERT**。具体来说，**QA-SQUAD-1**和**QA-SQUAD-2**在微调后忘记了大量知识，分别忘记了**53%**和**35%**。相比之下，**RANK-MSMARCO**在微调后忘记了较少的知识（**35%**），并且在最后一层保留了更多的知识。这表明排名任务可能更适合保留关系知识。

![](https://pic1.imgdb.cn/item/67f7701188c538a9b5c858f9.png)

实验还探讨了不同微调目标对知识含量的影响。结果表明，掩码语言建模（**MLM**）在从训练数据中获取新知识方面表现最佳，而问答（**QA**）和排名（**ranking**）任务则在知识保留方面表现较差。具体来说，**MLM**在微调后能够更好地从训练数据中获取新知识，而**QA**和排名任务则在微调过程中丢失了更多的知识。

![](https://pic1.imgdb.cn/item/67f76fb088c538a9b5c8584b.png)
![](https://pic1.imgdb.cn/item/67f76fbd88c538a9b5c8586b.png)

实验还研究了微调数据集大小对知识含量的影响。结果表明，数据集大小对知识含量的影响并不显著。例如，**MLM-MSMARCO**在较大的**MSMarco**数据集上微调后，知识含量并没有显著增加，反而在某些情况下出现了更多的遗忘。这表明，知识含量的增加并非仅仅依赖于数据集大小，而是与训练目标和数据分布有关。

![](https://pic1.imgdb.cn/item/67f7706288c538a9b5c85969.png)