---
layout: post
title: 'Deep contextualized word representations'
date: 2021-01-01
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ebffec5132923bf8af83fa.jpg'
tags: 论文阅读
---

> ELMo：使用语言模型进行词嵌入.

- paper：Deep contextualized word representations
- arXiv：[link](https://arxiv.org/abs/1802.05365)

**Embeddings from Language Model(ELMo)**是一种上下文相关的词向量表征方法。**ELMo**首先在大规模语料库上预训练一个双向语言模型**(bidirectional language model,biLM)**
，将该模型每一层每个方向的隐状态看作对输入词的嵌入编码；对每一层的隐状态向量加权求和，作为最终的词嵌入向量。

![](https://pic.imgdb.cn/item/60ec02b65132923bf8bedb7d.jpg)

具体地，**ELMo**使用了一个双向的多层**LSTM**模型。对于一个经过词嵌入的输入句子$(e_1,e_2,...,e_n)$， 前向语言模型假设该句子出现的概率是每个词关于其前面所有词的条件概率之积：

$$ P(e_1,e_2,...,e_n) = \prod_{k=1}^{n}P(e_k|e_1,e_2,...,e_{k-1}) $$

而后向语言模型假设该句子出现的概率是每个词关于其后面所有词的条件概率之积：

$$ P(e_1,e_2,...,e_n) = \prod_{k=1}^{n}P(e_k|e_{k+1},e_{k+2},...,e_{n}) $$

双向语言模型就是上述两者的结合：

$$ P(e_1,e_2,...,e_n) = \prod_{k=1}^{n}P(e_k|e_1,e_2,...,e_{k-1})P(e_k|e_{k+1},e_{k+2},...,e_{n}) $$

双向语言模型在训练时，使得每一个待预测词都能够捕捉上下文信息。若设置$L$层，则对于输入的一个**token** $t_k$一共能够获得$2L+1$个词表示向量$R_k$(输入词嵌入以及每一层每个方向的隐状态向量):

$$ R_k = \{e_k^{LM},\overrightarrow{h}_{k,j}^{LM},\overleftarrow{h}_{k,j}^{LM} | j=1,...,L\} $$

将同一层的两个隐状态向量合并，将词表示向量$R_k$表示为$L+1$维：

$$ R_k = \{h_{k,j}^{LM}, | j=0,...,L\}, \quad h_{k,j}^{LM} = \begin{cases} e_k^{LM}, j=0 \\ [\overrightarrow{h}_{k,j}^{LM},\overleftarrow{h}_{k,j}^{LM}], j=1,...,L \end{cases} $$

根据加权平均将这些不同层得到的表征结合起来作为最终的输出表征：

$$ ELMo_k = \gamma^{task} \sum_{j=0}^{L} s_j^{task} h_{k,j}^{LM} $$

其中$s_j^{task}$是每一层的重要性权重，$\gamma^{task}$是针对下游任务的缩放系数。

**ELMo**在使用时经常和原始词向量并联使用，输入到下游模型：

$$ \{ [e_k^{LM},ELMo_k] | k=1,...,n \} $$

