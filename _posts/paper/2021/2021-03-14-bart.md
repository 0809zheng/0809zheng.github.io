---
layout: post
title: 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension'
date: 2021-03-14
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67f613a688c538a9b5c75e4e.png'
tags: 论文阅读
---

> BART: 用于自然语言生成、翻译和理解的去噪序列到序列预训练模型.

- paper：[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)

# 0. TL; DR

**BART** 是一种新型的去噪自编码器，用于预训练序列到序列模型。它通过任意的噪声函数破坏文本，然后学习重建原始文本。**BART** 使用标准的基于 **Transformer** 的神经机器翻译架构，能够广泛应用于多种自然语言处理任务。在文本生成任务中表现尤为出色，同时在理解任务中也表现良好。它在多个任务上达到了新的最佳性能，包括对话、问答和摘要任务，并在机器翻译任务上通过目标语言预训练实现了 **1.1 BLEU** 的提升。

# 1. 背景介绍

近年来，自监督学习方法在自然语言处理（**NLP**）领域取得了显著的成果。这些方法通过利用大量的无标记文本数据，学习语言的内在结构和语义信息。其中，掩码语言模型（**Masked Language Model, MLM**）是最成功的方法之一。例如，**BERT** 通过随机掩盖部分单词并预测这些单词，取得了显著的效果。然而，这些方法通常专注于特定类型的下游任务（如跨度预测、生成等），限制了它们的适用性。

**BART** 的出现旨在克服这些限制。它结合了双向编码器和自回归解码器，通过预训练一个能够处理多种下游任务的模型。**BART** 的预训练过程包括两个阶段：首先，使用任意的噪声函数破坏文本；其次，学习一个模型来重建原始文本。这种预训练方法不仅简单，而且能够泛化多种现有的预训练方案，如 **BERT** 和 **GPT**。

# 2. BART 模型

**BART** 是一个去噪自编码器，它将一个破坏的文档映射回原始文档。它使用标准的序列到序列 **Transformer** 架构，包含一个双向编码器和一个自左向右的自回归解码器。具体来说，**BART** 的架构与 **BERT** 类似，但有以下几点不同：
- 解码器的每一层都会对编码器的最后一层进行交叉注意力操作。
- **BERT** 在单词预测前使用了一个额外的前馈网络，而 **BART** 没有。

![](https://pic1.imgdb.cn/item/67f615f288c538a9b5c75fee.png)

**BART** 的预训练目标是最小化原始文档的负对数似然。在预训练阶段，**BART** 允许对文档进行任意类型的破坏，这使得它在处理不同类型的下游任务时具有很大的灵活性。

**BART** 的预训练包括两个阶段：文本破坏和重建损失优化。与传统的去噪自编码器不同，**BART** 不局限于特定的噪声方案，可以应用任何类型的文档破坏。在实验中，作者尝试了多种破坏方法，包括：
- **Token Masking**：随机选择一些单词并将其替换为 [MASK] 标记。
- **Token Deletion**：随机删除输入中的单词。
- **Text Infilling**：随机选择一些文本跨度并将其替换为一个 [MASK] 标记。
- **Sentence Permutation**：将文档中的句子随机打乱顺序。
- **Document Rotation**：随机选择一个单词，将文档旋转使其成为开头。

这些破坏方法可以组合使用，以更好地训练模型处理各种类型的输入。

![](https://pic1.imgdb.cn/item/67f6164e88c538a9b5c76033.png)

**BART** 的表示可以用于多种下游任务。具体来说，**BART** 可以用于以下几类任务：
- 序列分类任务：将相同的输入输入到编码器和解码器中，使用解码器的最后一个隐藏状态进行分类。
- **Token** 分类任务：将整个文档输入到编码器和解码器中，使用解码器的顶层隐藏状态对每个单词进行分类。
- 序列生成任务：直接对序列生成任务进行微调，如问答和摘要。编码器输入是输入序列，解码器自回归地生成输出。
- 机器翻译：将 **BART** 作为机器翻译解码器的一部分，通过添加一个新的编码器参数集来学习将外语文本映射到 **BART** 可以去噪的英文表示。

![](https://pic1.imgdb.cn/item/67f61a4788c538a9b5c76329.png)

# 3. 实验分析

作者使用了与 **RoBERTa** 相同的预训练规模，训练了一个具有 **12** 层编码器和解码器的大型 **BART** 模型。预训练数据包括新闻、书籍、故事和网络文本，总大小为 **160GB**。在预训练过程中，作者使用了文本填充和句子排列的组合，掩盖了每个文档中 **30%** 的单词，并打乱了所有句子的顺序。

**BART** 在 **SQuAD** 和 **GLUE** 等判别任务上的表现与 **RoBERTa** 相当，表明 **BART** 的单向解码器层并未降低其在判别任务上的性能。

![](https://pic1.imgdb.cn/item/67f61bc088c538a9b5c7641e.png)

**BART** 在多个文本生成任务上取得了显著的性能提升。在 **CNN/DailyMail** 摘要任务上，**BART** 超过了之前的所有工作。在 **XSum** 摘要任务上，**BART** 比之前基于 **BERT** 的最佳方法提高了约 **6** 个点。在对话任务上，**BART** 优于之前的最佳系统。在 **ELI5** 问答任务上，**BART** 也超过了之前的最佳方法。

![](https://pic1.imgdb.cn/item/67f61c1788c538a9b5c7646a.png)
![](https://pic1.imgdb.cn/item/67f61c4488c538a9b5c76498.png)

在 **WMT16** 罗马尼亚语-英语翻译任务上，**BART** 在使用回译数据的情况下，将 **BLEU** 分数从 **36.80** 提高到了 **37.96**，表明 **BART** 可以有效地作为机器翻译解码器的一部分。

![](https://pic1.imgdb.cn/item/67f61c7988c538a9b5c764c7.png)

**BART** 在摘要任务上的表现不仅在自动评估指标上有所提升，其生成的摘要也具有较高的流畅性和准确性。例如，**BART** 能够从输入文档中整合信息并生成高度抽象的摘要，同时保持事实准确性和语法正确性。这些结果表明 **BART** 的预训练方法在自然语言理解和生成方面都取得了显著的效果。

![](https://pic1.imgdb.cn/item/67f61c9b88c538a9b5c764eb.png)