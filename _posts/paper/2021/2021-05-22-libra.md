---
layout: post
title: 'Libra R-CNN: Towards Balanced Learning for Object Detection'
date: 2021-05-22
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/652de54bc458853aef2dbee8.jpg'
tags: 论文阅读
---

> Libra R-CNN: 面向目标检测中的均衡学习.

- paper：[Libra R-CNN: Towards Balanced Learning for Object Detection](https://arxiv.org/abs/1904.02701)

**Libra R-CNN**出发点为解决目标检测中的一些不均衡现象，如采样不均衡、不同阶段特征分布的不均衡、框回归过程中不均衡，提出了一些改进的过程。

![](https://pic.imgdb.cn/item/652de5b3c458853aef2e97ef.jpg)

**Libra R-CNN**的改进包括**IoU**均衡采样，对**FPN**结构的均衡以及对**L1 loss**的均衡：

![](https://pic.imgdb.cn/item/652de603c458853aef2f3da1.jpg)

## 1. IoU balanced Sampling

样本级别的随机采样会带来样本不平衡，由于负样本本身**iou**的不平衡，当采用随机采样后，会出现难负(**iou 0.5**附近)和易负(**iou**接近**0**)样本不平衡采样，导致后面性能不好。

如果是随机采样的话，随机采样到的样本超过$70\%$都是在**IoU**在**0**到**0.05**之间的易学习负样本，而实际统计得到的事实是$60\%$的**hard negative**都落在**IoU**大于**0.05**的地方。

![](https://pic.imgdb.cn/item/652de726c458853aef318341.jpg)

作者提出了**IoU-balanced Sampling**，核心操作是对负样本按照**iou**划分**k**个区间，先尝试在不同区间进行随机采样采相同多数目的样本，如果不够就全部采样；进行一轮后，如果样本数不够，再剩下的样本中均匀随机采样。保证易学习负样本和难负样本比例尽量平衡。实验表明对**K**不敏感，作者设置的是**3**。 

## 2. Balanced Feature Pyramid

![](https://pic.imgdb.cn/item/652de7dec458853aef32fb7a.jpg)

**Balanced Feature Pyramid**直接基于**FPN**来改进设计了不同**stage**特征融合的过程，包括**rescaling**、**integrating**、**refining**和**strengthening**四步，实现了在**FPN**结构下**attention map**的统一生成和使用。
- **rescaling**：将**stage** $C_2,C_3,C_4,C_5$ 通过插值或者**max pooling**缩放至相同的大小，如$C_4$的大小。
- **integrating**：接按对应元素位置求和取平均值$C = \frac{1}{L}\sum_{l=l_{\min}}^{l_{\max}}C_l$
- **refining**：采用**non-local**生成一个和**intergrating**大小相同的**attention map**，**non-local**中具体使用的方法为**embedded Gaussian**
- **strengthening**：将**attention map**通过**max pooling**或者插值恢复到各**stage**原有的尺寸，然后由原来的 $C_2,C_3,C_4,C_5$ 引出跳跃连接，二者仍然按照对应元素位置相加完成融合过程。

## 3. Balanced L1 Loss

对于定位损失中的**smooth L1 Loss**，将梯度绝对值大于$1$的样本称为**outlier**，其他的为**inlier**。**outlier**可以看做困难样本，对梯度的贡献比较大，而**inlier**这类简单样本对梯度的贡献比较小，只有$30\%$左右。

$$ L1_\text{smooth}(x) = \begin{cases} |x|-0.5, & |x| ≥ 1 \text{ (outlier)} \\ 0.5x^2, &|x| < 1 \text{ (inlier)} \end{cases} $$

$$
\frac{\partial L1_\text{smooth}}{\partial x} = \begin{cases}
x, & |x| < 1 \\
1, & x \geq 1 \\
-1, & x \leq -1
\end{cases}
$$

对于目标检测过程，期望得到更精确的定位结果，如果直接增大定位损失的权重，可能让训练变得不稳定。因此，考虑平衡定位损失中的难易样本的梯度贡献，具体来说，增加容易样本**inlier**的梯度值，以帮助网络更好地定位。最终也使得分类和回归过程更加均衡。

**Balanced L1 Loss**首先设计梯度的函数，再通过积分得到**loss**的表达式。梯度形式为：

$$
\frac{\partial L_b}{\partial x} = \begin{cases}
\alpha \ln (b|x|+1), & |x| < 1 \\
\gamma, & |x| ≥ 1
\end{cases}
$$

对于**inlier**部分使用对数函数后，显著增加了这部分的梯度。$\gamma$的取值来改变梯度的上界，而$b$则用来使梯度在$x=1$处连续。实验中设置$\alpha = 0.5, \gamma=1.5$。

在梯度表达式的基础上，积分得到**Balanced L1 Loss**的表达式：

$$
L_b(x)= \begin{cases}\frac{\alpha}{b}(b|x|+1) \ln (b|x|+1)-\alpha|x| & \text { if }|x|<1 \\ \gamma|x|+C & \text { otherwise }\end{cases}
$$

图像很直观地显示了梯度和**loss**的变化：

![](https://pic.imgdb.cn/item/652defc9c458853aef458034.jpg)