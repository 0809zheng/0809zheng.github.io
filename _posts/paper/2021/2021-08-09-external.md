---
layout: post
title: 'Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks'
date: 2021-08-09
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/611112a95132923bf84df9a1.jpg'
tags: 论文阅读
---

> External Attention: 使用两个外部记忆单元的注意力机制.

- paper：Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks
- arXiv：[link](https://arxiv.org/abs/2105.02358)

本文受自注意力机制中的线性化方法启发，设计了一种计算复杂度较小的**外部注意力机制**(**external attention**)，并在多种计算机视觉任务中取得较好的效果。

![](https://pic.imgdb.cn/item/61111a865132923bf861fde8.jpg)

标准的自注意力机制是将输入特征变换为查询矩阵$Q=W_qF_{in}$，键矩阵$K=W_kF_{in}$和值矩阵$V=W_vF_{in}$，然后做如下计算：

$$ F_{out} = \text{softmax}(Q^TK)V = \text{softmax}(F_{in}^TW_q^TW_kF_{in})W_vF_{in} $$

一种简化的自注意力运运算是取$Q=K=V=F_{in}$，此时计算为：

$$ F_{out} = \text{softmax}(Q^TK)V = \text{softmax}(F_{in}^TF_{in})F_{in} $$

作者将键矩阵和值矩阵进一步简化为固定大小的参数矩阵$M_k$和$M_v$，这两个矩阵在整个数据集上是共享的，能够隐式地学习整个数据集的通用特征，且简化了自注意力的运算量。参数矩阵$M_k$和$M_v$是通过线性层实现的：

$$ F_{out} = \text{Norm}(Q^TM_k)M_v = \text{Norm}(F_{in}^TW_q^TM_k)M_v $$

由于注意力图对特征的尺度比较敏感，因此作者没有使用**softmax**函数，而是使用如下所示的**double-normalization**：

$$ \tilde{\alpha}_{i,j} = F_{in}^TW_q^TM_k $$

$$ \tilde{\alpha}_{i,j} = \frac{\exp(\tilde{\alpha}_{i,j})}{\sum_{k}^{}\exp(\tilde{\alpha}_{k,j})} $$

$$ \alpha_{i,j} = \frac{\tilde{\alpha}_{i,j}}{\sum_{k}^{}\tilde{\alpha}_{i,k}} $$

**external attention**也可以采用**multi-head**的形式，如下所示：

![](https://pic.imgdb.cn/item/61111f695132923bf86e2497.jpg)

作者在图像分类、目标检测、实例分割、语义分割、图像生成、点云分类与分割等任务上进行了大量实验，充分证明了所提方法的优越性。