---
layout: post
title: 'Gradient Harmonized Single-stage Detector'
date: 2021-06-17
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/65372d7ac458853aef8d360e.jpg'
tags: 论文阅读
---

> 梯度均衡化单阶段检测器.

- paper：[Gradient Harmonized Single-stage Detector](https://arxiv.org/abs/1811.05181)

本文提出了**梯度均衡化机制(Gradient Harmonized Mechanism, GHM)**，用于解决目标检测任务中的正负样本不平衡和难易样本不平衡问题。**GHM**从梯度范数角度出发，对易样本进行指数降低权重，不仅仅实现了**focal loss**效果，而且还具备克服外点样本影响。

![](https://pic.imgdb.cn/item/65376bb5c458853aef388b31.jpg)

最左边图的纵坐标表示样本数目，从该图可以看出，对于一个已经收敛的模型，大部分样本都是易学习样本(并且大部分是背景样本)，但是依然有部分样本梯度范数接近$1$，这些样本极可能是外点数据即标注有错误的数据，如果训练时候强行拟合，对最终性能反而有影响。

中间图的纵坐标可以认为是梯度值。**ce loss**会导致梯度被大量易学习样本主导；**focal loss**把易学习样本的权重降低，但是没有克服外点数据的左右，反映在图中就是在梯度范数接近$1$的时候，梯度值非常大；本文提出的**GHM**对**loss**两端的梯度进行降低权重，具备了易学习样本减低权重并且外点数据梯度不会过大的效果。

## 1. GHM的分类形式

以二分类**bce loss**为例：

$$
\begin{gathered}
L_{C E}\left(p, p^*\right)= \begin{cases}-\log (p) & \text { if } p^*=1 \\
-\log (1-p) & \text { if } p^*=0\end{cases} \\
\end{gathered}
$$

$p$是预测概率值，范围是$0-1$，$p^\*$是标签，非$0$即$1$,其梯度函数为：

$$
\begin{gathered}
\frac{\partial L_{C E}}{\partial x}= \begin{cases}p-1 & \text { if } p^*=1 \\
p & \text { if } p^*=0\end{cases} \\
=p-p^* \\
\end{gathered}
$$

定义梯度范数或者梯度模长为：

$$
\begin{gathered}
g=\left|p-p^*\right|= \begin{cases}1-p & \text { if } p^*=1 \\
p & \text { if } p^*=0\end{cases} \\
\end{gathered}
$$

对于一个已经收敛的目标检测模型，大量样本所构成的梯度范数分布为：

![](https://pic.imgdb.cn/item/65376f28c458853aef43a62f.jpg)

大部分样本都是梯度范数比较小，对应易学习样本；但是依然存在很大一部分比例的梯度范数接近$1$，对应外点噪声数据。

可以对这个梯度分布图做梯度均衡化。对于分类问题**ce loss**而言，在得到梯度范数分布后，需要计算梯度密度函数：

$$
\begin{gathered}
G D(g)=\frac{1}{l_\epsilon(g)} \sum_{k=1}^N \delta_\epsilon\left(g_k, g\right) \\
\end{gathered}
$$

其中：

$$
\begin{gathered}
\delta_\epsilon(x, y)= \begin{cases}1 & \text { if } y-\frac{\epsilon}{2}<=x<y+\frac{\epsilon}{2} \\
0 & \text { otherwise }\end{cases} \\
l_\epsilon(g)=\min \left(g+\frac{\epsilon}{2}, 1\right)-\max \left(g-\frac{\epsilon}{2}, 0\right) \\
\end{gathered}
$$

其意思是设定梯度值分布间隔$\epsilon$，对梯度范数的纵坐标进行均匀切割，然后统计每个区间内的样本个数，除以横坐标区间长度即可。该密度函数可以反映出在某一梯度范数范围内的样本密度，或者说单位取值区域内分布的样本数量。

对于一个样本，若它的梯度范数为$g_k$，它的密度就定义为处于它所在的区域内的样本数量除以这个单位区域的长度$\epsilon$：

$$
\begin{gathered}
\hat{G D}(g)=\frac{R_{i n d(g)}}{\epsilon}=R_{i n d(g)} M \\
\end{gathered}
$$

这个密度分布的倒数是样本的权值，密度值越大，权重越小；由于外点数据的密度值也很大，故可以抑制外点样本影响。

**GHM**C的代码逻辑为:
- 设置间隔个数**bin=30**(论文中的$M4)，也就是把梯度范数函数值即纵坐标分成$30$个区间，区间范围是$[0,1/30),[1/30,2/30)$...
- 计算总样本数**tot=N**
- 遍历每个区间，计算梯度范数函数在指定区间范围内的样本个数**num_in_bin**；计算**N/num_in_bin**;
- 所有样本权重都除以有效**Bin**个数即有样本的区间个数，可能在某个区间范围没有任何样本，此时**weights**变量就是论文中的$\beta$
- 计算带权重的**bce loss**

$$
\begin{gathered}
\beta_i=\frac{N}{G D\left(g_i\right)} \\
\hat{L}_{G H M-C}=\frac{1}{N} \sum_{i=1}^N \hat{\beta}_i L_{C E}\left(p_i, p_i^*\right) \\
=\sum_{i=1}^N \frac{L_{C E}\left(p_i, p_i^*\right)}{G D\left(g_i\right)} \\
\end{gathered}
$$

```python
# gradient length g函数
g = torch.abs(pred.sigmoid().detach() - target)

valid = label_weight > 0
tot = max(valid.float().sum().item(), 1.0)
n = 0 # n valid bins
for i in range(self.bins):
    # 计算在指定edges范围内的样本个数
    inds = (g >= edges[i]) & (g < edges[i + 1]) & valid
    num_in_bin= inds.sum() .item()
    # 如果某个区间范围内没有样本，则直接忽略
    if num_in_bin > 0:
        if mmt > 0:
            # ema操作
            self.acc_sum[i] = mmt * self.acc_sum[i] \
                +(1 - mmt) * num_in_bin
            weights[inds] = tot / self.acc_sum[i]
        else:
            weights[inds]= tot / num_in_bin
        n += 1
if n > 0:
    # weights=论文中beta 每个样本的权重
    weights = weights / n # n是有样本的区间个数

loss = F.binary_cross_entropy_with_logits(
    pred, target, weights, reduction= 'sum') / tot
return loss * self.loss_weight
```

$M$表示对$g$进行划分区间的个数，$M$越大，划分的区间越多。实验表明该参数不能太小，否则失去了统计意义(权重都是趋于$1$，相当于没有)；$M$太大会导致切割的区间太多，密度函数统计就不准确了，效果可能会下降。

![](https://pic.imgdb.cn/item/653775b0c458853aef5587b9.jpg)

## 2. GHM的回归形式

回归常用的**loss**是**smooth l1**，其定义如下：

$$
\begin{gathered}
S L_1(d)= \begin{cases}\frac{d^2}{2 \delta} & \text { if }|d|<=\delta \\
|d|-\frac{\delta}{2} & \text { otherwise }\end{cases} \\
\end{gathered}
$$

$d$是预测值和真实值的差，对其求导数：

$$
\begin{gathered}
\frac{\partial S L_1}{\partial t_i}=\frac{\partial S L_1}{\partial d}= \begin{cases}\frac{d}{\delta} & \text { if }|d|<=\delta \\
\operatorname{sgn}(d) & \text { otherwise }\end{cases} \\
\end{gathered}
$$

$t_i$是某一个预测值，可以发现当差值大于指定范围后梯度是固定值，也就是无法反映难易程度，也就是说$d$无法作为梯度范数函数$g$，此时作者采用了等效公式**ASL**：

$$
\begin{gathered}
A S L_1(d)=\sqrt{d^2+\mu^2}-\mu \\
\end{gathered}
$$

其梯度公式为：

$$
\begin{gathered}
\frac{\partial A S L_1}{\partial d}=\frac{d}{\sqrt{d^2+\mu^2}}
\end{gathered}
$$

对应的$g$函数以及梯度均衡化效果如下：

![](https://pic.imgdb.cn/item/65377533c458853aef543b64.jpg)

因为回归分支的输入全部是正样本，即使是易学习样本也不能随意的把权重降的很低，否则对最终性能有比较大的影响。

