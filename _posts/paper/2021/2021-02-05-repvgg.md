---
layout: post
title: 'RepVGG: Making VGG-style ConvNets Great Again'
date: 2021-02-05
author: 郑之杰
cover: 'https://img.imgdb.cn/item/601d2b3c3ffa7d37b3f4adb6.jpg'
tags: 论文阅读
---

> RepVGG：使用网络结构重参数化方法改进VGGNet.

- paper：RepVGG: Making VGG-style ConvNets Great Again
- arXiv：[link](https://arxiv.org/abs/2101.03697)
- code：[github](https://github.com/megvii-model/RepVGG)

卷积神经网络的结构设计需要平衡准确率和推理速度。尽管**VGGNet**结构复杂，参数量大，但由于其是单分支网络，无需保存过多的中间结果，内存占用少；而**ResNet**网络模型准确率较高，但引入了多分支结构，需要较多内存占用。

作者提出了**RepVGG**网络，其结构简单（仅包含$3 \times 3$卷积和**ReLU**），也不需要自动结构搜索和复杂的人工结构调整。**RepVGG**分别独立地设计训练和推理的网络结构，训练时使用高精度的多分支网络学习网络参数，推理时使用低延迟的单分支网络，通过结构重参数化方法将多分支网络的权值等价为单分支网络。

![](https://img.imgdb.cn/item/601d32543ffa7d37b3f79792.jpg)

**RepVGG**沿用**VGGNet**的结构风格，将最大池化用$stride=2$的$3 \times 3$卷积代替。网络共包含$5$个阶段，每个阶段首先用$stride=2$的$3 \times 3$卷积层，再设置若干卷积层。对于分类任务，网络输出**head**使用最大池化和全连接层；对于其他视觉任务，使用对应的**head**。

每个阶段的层数设计如下表所示。除首尾两个阶段使用单层外，其余每阶段的层数逐渐增加。而每阶段的宽度则通过缩放因子$a$和$b$进行调整，通常$b ＞ a$，保证最后一个阶段能够提取更丰富的特征。为避免第一阶段卷积参数量过大，设置$min(64,64a)$。

![](https://img.imgdb.cn/item/601d33cd3ffa7d37b3f8432b.jpg)

为了进一步压缩参数，作者在特定的层加入**分组卷积**，从而达到速度和准确率之间的权衡，比如**RepVGG-A**的**3rd**, **5th**, **7th**, ..., **21st**层以及**RepVGG-B**的**23rd**, **25th**和**27th**层。值得一提的是，没有对连续的层使用分组卷积，是为了保证通道间的信息交流。

模型训练时采用多分支网络。多分支网络相当于包含大量小网络的集合，其推理速度相较于单分支网络较慢。作者在$3 \times 3$卷积的基础上添加了$1 \times 1$卷积和恒等分支(相当于单位矩阵的$1 \times 1$卷积)构成一个网络结构块，假设模型包含$n$个结构块，则可以表达$3^n$种简单网络结构。

推理结构是通过网络结构重参数化得到的。定义输入为$M^{(1)} \in \Bbb{R}^{N \times C_1 \times H_1 \times W_1}$，输出为$M^{(2)} \in \Bbb{R}^{N \times C_2 \times H_2 \times W_2}$，$*$为卷积操作，$W^{(3)} \in \Bbb{R}^{C_2 \times C_1 \times 3 \times 3}$和$W^{(1)} \in \Bbb{R}^{C_2 \times C_1 \times 1 \times 1}$分别为$3 \times 3$卷积和$1 \times 1$卷积，$\mu$、$\sigma$、$\gamma$和$\beta$是**BatchNorm**的参数。。合并并行的卷积层和**BatchNorm**，则输出计算如下：

$$ M^{(2)} = bn(M^{(1)}*W^{(3)},\mu^{(3)},\sigma^{(3)},\gamma^{(3)},\beta^{(3)}) \\ +bn(M^{(1)}*W^{(1)},\mu^{(1)},\sigma^{(1)},\gamma^{(1)},\beta^{(1)}) \\ +bn(M^{(1)},\mu^{(0)},\sigma^{(0)},\gamma^{(0)},\beta^{(0)}) $$

重参数化的核心是将**BatchNorm**及其前面的卷积层转换为单个卷积层。推理时**BatchNorm**的计算为：

$$ bn(M,\mu,\sigma,\gamma,\beta)_{:,i,:,:}=(M_{:,i,:,:}-\mu_i)\frac{\gamma_i}{\sigma_i}+\beta_i $$

由于卷积的齐次性和可加性，记：

$$ W_{i,:,:,:}'=\frac{\gamma_i}{\sigma_i}W_{i,:,:,:}, \quad b_i' = -\frac{\mu_i\gamma_i}{\sigma_i}+\beta_i $$

则可用等价的卷积操作替换之前的卷积+**BatchNorm**操作：

$$ bn(M*W,\mu,\sigma,\gamma,\beta)_{:,i,:,:}= (M*W')_{:,i,:,:}+b_i'$$

若输出维度与输入维度不相等，则去掉恒等分支。下图是一个$C_1=C_2=2$的转换过程示意图。值得一提的是，训练时为保证$3 \times 3$卷积和$1 \times 1$卷积具有相同的步长，$1 \times 1$卷积需要小一个填充像素。

![](https://img.imgdb.cn/item/601d3dd03ffa7d37b3fd4718.jpg)

实验结果显示**RepVGG**网络能够兼顾推理速度和准确率：

![](https://img.imgdb.cn/item/601d3e4d3ffa7d37b3fd7dfb.jpg)
