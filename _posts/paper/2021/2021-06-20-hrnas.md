---
layout: post
title: 'HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers'
date: 2021-06-20
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/668e4a5ed9c307b7e98c0f80.png'
tags: 论文阅读
---

> HR-NAS：通过轻量级Transformer搜索高效高分辨率网络结构.

- paper：[HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers](https://arxiv.org/abs/2106.06560)

本文设计了一个简单的轻量级视觉**Transformer**模块，然后在**HRNet**结构基础上进行神经结构搜索，将原本的3x3卷积模块扩展成了3x3,5x5,7x7,轻量级视觉**Transformer**模块等四种模块拼接的模块，然后搜索这四种模块的比例和通道数。在分类、语义分割、姿态估计、三维目标检测任务上进行了实验，论证了所提方法的有效性。

![](https://pic.imgdb.cn/item/668e4c5fd9c307b7e98eb7f3.png)

轻量级视觉**Transformer**模块将输入特征图先进行下采样投影到一个固定的小尺度，然后送进**Transformer**计算再上采样还原到输入尺度：

![](https://pic.imgdb.cn/item/668e4dfbd9c307b7e9922928.png)

本文设计了一个新的位置编码方式，这种更简单的位置编码在实验中证明超越了之前的方案：

$$
P[0,i,j] = \frac{i}{h},i\in [0, h-1] \\
P[1,i,j] = \frac{j}{w},j\in [0, w-1]
$$

之前的**NAS**主要搜索的是单分支的链式结构模型，本文由于是基于**HRNet**结构的，因此搜索的是多分支结构。将原本**HRNet**中的**Block**改为了不同尺度拼接的形式，在搜索过程中通过**mask**来屏蔽每个卷积核中一定数量的通道。

![](https://pic.imgdb.cn/item/668e4e8dd9c307b7e992fe19.png)


在不同任务上的搜索结果如下。不同颜色代表了3x3,5x5,7x7,轻量级视觉**Transformer**模块等四种模块的比例，灰色代表被屏蔽的通道，当一个模块全部被屏蔽则退化成了一个残差结构，如图可以看出原始**HRNet**确实存在很大的参数冗余。

![](https://pic.imgdb.cn/item/668e4ed2d9c307b7e9935340.png)