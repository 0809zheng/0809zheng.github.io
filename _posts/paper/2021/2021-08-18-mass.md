---
layout: post
title: 'MASS: Masked Sequence to Sequence Pre-training for Language Generation'
date: 2021-08-18
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/611e1a844907e2d39ccfcaa0.jpg'
tags: 论文阅读
---

> MASS：序列到序列的掩码语言建模.

- paper：MASS: Masked Sequence to Sequence Pre-training for Language Generation
- arXiv：[link](https://arxiv.org/abs/1905.02450)

本文提出了一种预训练方法：**Masked Sequence to Sequence**，即对输入序列随机**mask**掉连续的$k$个**token**，然后通过编码器-解码器结构预测这些**token**。

![](https://pic.imgdb.cn/item/611e1c834907e2d39cd61998.jpg)

这种预训练方法的主要优点是：
- 解码器的输入中不预测的**token**被**mask**掉，促使解码器从编码器中提取更有效的信息；
- 编码器中预测的**token**被**mask**掉，提高编码器对序列文本的理解能力；
- 预测连续的**token**，提高解码器的语言建模能力。

该方法中的超参数$k$表示**mask**掉的连续**token**长度，通过调整$k$的大小，**MASS**可以等效成其他预训练模型，即**MASS**是一种通用的预训练语言模型。
- 当$k=1$时，编码器端**mask**掉一个**token**，解码器预测一个**token**，解码器没有任何输入信息，此时等价于**BERT**中的预训练任务(自编码式的**MLM**)。

![](https://pic.imgdb.cn/item/611e1e804907e2d39cdc34dc.jpg)

- 当$k=$序列长度时，编码器端**mask**掉所有**token**，解码器预测所有**token**，编码器没有任何输入信息，此时等价于**GPT**中的预训练任务(自回归式的**LM**)。

![](https://pic.imgdb.cn/item/611e1f014907e2d39cddf3bb.jpg)

实验表明当$k$设置为序列长度的一半时能够较好的平衡编码器和解码器的预训练，防止训练过度偏向编码器$k<n/2$或解码器$k>n/2$。

![](https://pic.imgdb.cn/item/611e1f214907e2d39cde5362.jpg)