---
layout: post
title: 'Deformable DETR: Deformable Transformers for End-to-End Object Detection'
date: 2021-06-13
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6535d34ac458853aef291795.jpg'
tags: 论文阅读
---

> Deformable DETR：通过可变形Transformer实现端到端目标检测.

- paper：[Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://arxiv.org/abs/2010.04159)

**DETR**利用了**Transformer**通用以及强大的对相关性的建模能力，来取代**anchor**等一些手工设计的目标检测元素。但是依旧存在缺陷：
- 训练时间极长：相比于已有的检测器，**DETR**需要更久的训练才能达到收敛(**500 epochs**)，比**Faster R-CNN**慢了**10-20**倍。这是因为在初始化阶段，**DETR**对于特征图中的所有像素的权重是均匀的，导致要学习的注意力权重集中在稀疏的有意义的位置这一过程需要很长时间。
- 计算复杂度高：**DETR**对小目标的性能很差，现代许多种检测器通常利用多尺度特征，从高分辨率特征图中检测小物体。但是高分辨率的特征图会大大提高**DETR**复杂度，因为自注意力计算的复杂度是像素点数目的平方。

为解决**DETR**收敛速度慢和计算复杂度高的问题，本文提出了**Deformable DETR**，结合了可变形卷积(**Deformable Convolution**)的稀疏空间采样的本领，以及**Transformer**对于相关性建模的能力。

## 1. Deformable Attention Module

在自注意力机制中，每个查询向量**Query**需要与所有键向量**Key**交互以计算注意力图；**Deformable DETR**提出了可变形注意力模块(**Deformable Attention Module**)，每个查询向量**Query**的查询对象通过学习一组偏移**offset**得到，而注意力图通过线性变换得到。

![](https://pic.imgdb.cn/item/6535d73fc458853aef324f3d.jpg)

$$
\operatorname{DeformAttn}\left(\boldsymbol{z}_q, \boldsymbol{p}_q, \boldsymbol{x}\right)=\sum_{m=1}^M \boldsymbol{W}_m\left[\sum_{k=1}^K A_{m q k} \cdot \boldsymbol{W}_m^{\prime} \boldsymbol{x}\left(\boldsymbol{p}_q+\Delta \boldsymbol{p}_{m q k}\right)\right]
$$

假设输入查询**Query**的维度是$(N_q,C)$，经过线性变换后得到$\Delta_x,\Delta_y,A$，维度均为$(N_q,MK)$。其中$\Delta_x,\Delta_y$表示关注特征位置相对参考点的偏移量**offset**，$A$表示学习到的注意力图。$N_q$是查询向量的数目($N_q=HW$)，$M$是**multi-head**数量，$K$是关注特征的数量。

对输入特征$(HW,C)$做线性变换得到值矩阵$V$。根据$\Delta_x,\Delta_y$，需要为$N_q$个查询**Query**分别采样$K$个值，采样之后的值矩阵$V \in (N_q, K, C)$，$M$个**head**对应$V \in (N_q, M, K, C_M),C=MC_M$。

之后使用注意力图$A$与查询到的值矩阵$V$交互，对$K$个值进行加权平均，得到输出特征$O\in (N_q,M,C_M)$。

大多数目标检测框架受益于多尺度特征图，而**Deformable Attention Module**可以自然地扩展到多尺度特征图中。**Multi-scale Deformable Attention Module**从多尺度特征图中共采样$LK$个点，相当于对所有层均采$K$个点，融合了不同层的特征。

$$
\operatorname{MSDeformAttn}\left(\boldsymbol{z}_q, \hat{\boldsymbol{p}}_q,\left\{\boldsymbol{x}^l\right\}_{l=1}^L\right)=\sum_{m=1}^M \boldsymbol{W}_m\left[\sum_{l=1}^L \sum_{k=1}^K A_{m l q k} \cdot \boldsymbol{W}_m^{\prime} \boldsymbol{x}^l\left(\phi_l\left(\hat{\boldsymbol{p}}_q\right)+\Delta \boldsymbol{p}_{m l q k}\right)\right]
$$

## 2. Deformable DETR

![](https://pic.imgdb.cn/item/6535e122c458853aef4a9378.jpg)

**Deformable DETR**将**transformer**编码器中处理特征的部分都做替换，即所有的**self-attention**模块都使用了**Deformable Attention Module**。**Encoder**的输入输出均为多尺度**feature map**，保持相同的分辨率。

![](https://pic.imgdb.cn/item/6535e2ccc458853aef4ef146.jpg)

**Deformable DETR**使用的多尺度特征一共有$4$种尺度的特征$$\{x_l\}_{l=1}^4$$，所有的特征都是$256$通道。多尺度可变形注意力可以在多尺度特征图之间交换信息。作者还给不从尺度的特征加了尺度级的嵌入表示$$\{e_l\}_{l=1}^4$$，随机初始化并随网络一起训练。

**Deformable DETR**的**Decoder**中有交叉注意力和自注意力两种模块，它们的**Query**都来自**Object queries**，交叉注意力的 **Key**来自**Encoder**的输出，**Object queries**从**encoder**输出的特征图中提取特征。自注意力的**Key**来自**Object queries**，**Object queries**彼此交互。



