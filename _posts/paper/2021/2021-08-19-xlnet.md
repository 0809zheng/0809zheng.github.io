---
layout: post
title: 'XLNet: Generalized Autoregressive Pretraining for Language Understanding'
date: 2021-08-19
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/611e01354907e2d39c828342.jpg'
tags: 论文阅读
---

> XLNet：使用排列语言建模训练语言模型.

- paper：[XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)

预训练语言模型可以根据不同的预训练任务进行分类。两种重要的预训练任务分别是：
- 自回归式(**autoregressive**)：学习序列的条件概率分布，如**LM**(**GPT**)；即给定一个序列$x=[x_1,x_2,...,x_T]$，(以前向概率为例)最大化序列的条件概率似然。若用$h_{\theta}(\cdot)$表示神经网络，$e(\cdot)$表示词嵌入，则目标函数表示为：

$$ \mathop{\max}_{\theta} \quad \log p_{\theta}(x) =\sum_{t=1}^{T}\log p_{\theta}(x_t|x_{<t})=\sum_{t=1}^{T}\log \frac{\exp(h_{\theta}(x_{1:t-1})^Te(x_t))}{\sum_{x'}^{} \exp(h_{\theta}(x_{1:t-1})^Te(x'))} $$

- 自编码式(**autoencoding**)：学习序列的隐变量并重构，如**MLM**(**BERT**)；**BERT**中类似降噪自编码器，对输入的某些位置加上`[MASK]`噪声，并预测这些**mask**对应的**token**。若$\hat{x}$表示加上`[MASK]`后的序列，$\overline{x}$表示**mask**掉的**token**，$m_t$表示位置$t$是否被**mask**掉，则目标函数表示为：

$$ \mathop{\max}_{\theta} \quad \log p_{\theta}(\overline{x}|\hat{x}) ≈\sum_{t=1}^{T}m_t\log p_{\theta}(x_t|\hat{x})=\sum_{t=1}^{T}m_t\log \frac{\exp(h_{\theta}(\hat{x})_t^Te(x_t))}{\sum_{x'}^{} \exp(h_{\theta}(\hat{x})_t^Te(x'))} $$

这两种方法的对比如下：
- 独立性假设：自回归式假设输入序列的**token**之间是独立的，则可以用概率链式法则表示；自编码式的输入序列**token**之间不是独立的。
- 输入噪声：自编码式在训练时引入了后续任务中不会出现的噪声`[MASK]`，引入了预训练和微调之间的**discrepancy**。
- 上下文依赖：自回归式只能使用单向的序列信息，自编码式可以使用双向的上下文信息。

为了结合上述两种方式的优点，本文提出了**排列语言模型(Permutation Language Modeling,PLM)**，即给定序列$x=[x_1,x_2,...,x_T]$，首先对其进行随机地排列，共有$T!$种排列方式。然后在该排列序列$z$下进行自回归语言模型的训练:

$$ \mathop{\max}_{\theta} \Bbb{E}_{z \text{~} \mathcal{Z}_T} [\sum_{t=1}^{T}\log p_{\theta}(x_{z_t}|x_{z_{<t}})] $$

每次采样序列顺序$z$不同，则输入序列也不同，理论上模型可以学习到任意顺序。下图展示了在几种不同的序列顺序下，第$3$个**token**所获取的单向信息来自不同的**token**(相当于获取双向的上下文信息)：

![](https://pic.imgdb.cn/item/611e0d154907e2d39ca80cbb.jpg)

可以从注意力矩阵的角度理解语言模型。自编码式模型的自注意力矩阵的每一个位置都有值，位置$(i,j)$表示第$i$个**token**与第$j$个**token**的注意力得分。而自回归式模型的自注意力矩阵是一个下三角阵，即每一个输出**token**(列)只能与其前面位置的输入**token**(行)进行交互，通过对注意力矩阵进行**mask**也可以达到这种效果:

![](https://pic.imgdb.cn/item/611e0eea4907e2d39cad82cf.jpg)

如上图所示，注意力矩阵的每一行代表每一个输出，而每一列代表每一个输入，注意力矩阵的数值表示输出和输入的关联。对于自回归式模型，第一个输出$x_1$只能跟起始标记`<s>`相关，而第二个输出$x_2$只能跟起始标记`<s>`和第一个输出$x_1$相关，依此类推。

对于乱序的语言模型，相当于把上述下三角形式的注意力矩阵打乱，如对于序列`<s> → 迎 → 京 → 你 → 欢 → 北 → <e>`，其不同语言模型对应的注意力矩阵分别为：

![](https://pic.imgdb.cn/item/611e10274907e2d39cb11cb1.jpg)

对于乱序的语言模型，直接构造上述**mask**的注意力矩阵比较繁琐，有一种更简单的等效训练方法。由于注意力机制本身是一个无序的模型，序列顺序是通过位置编码引入的，因此输入不仅包括**token**，还包括**token**所在的位置**id**，如上述序列的实际输入为`{(迎, 4), (京, 2), (你, 5), (欢, 3), (北, 1)}`。若按该顺序将**token**在输入层打乱，则可以等效地构造自回归式训练：

![](https://pic.imgdb.cn/item/611e12164907e2d39cb72ff1.jpg)

作者在实现时引入了**two-stream**的自注意力结构，即把每一个输入**token**的特征分解成两部分：
- 上下文(**context**)特征$h_{\theta}(x_{z≤t})$或$h_{z_t}$: 相当于标准的自注意力计算中的隐状态；
- 查询(**query**)特征$g_{\theta}(x_{z<t},z_t)$或$g_{z_t}$: 引入位置信息。

训练中$h_{z_t}^0$初始化为词嵌入向量$e(x_{z_t})$，$g_{z_t}^0$初始化为位置编码$w$，更新如下：

$$ h_{z_t}^m ← Attention(Q=h_{z_t}^{m-1},K,V=h_{z≤t}^{m-1}) $$

$$ g_{z_t}^m ← Attention(Q=g_{z_t}^{m-1},K,V=h_{z<t}^{m-1}) $$

![](https://pic.imgdb.cn/item/611e15264907e2d39cc046f8.jpg)

由于全部的排列数量太多，因此作者设置只预测序列的最后一些**token**；即以长度$c$把序列分成两部分，只自回归地生成后面的**token**，则目标函数变为：

$$ \mathop{\max}_{\theta} \Bbb{E}_{z \text{~} \mathcal{Z}_T} [\sum_{t=c+1}^{T}\log p_{\theta}(x_{z_t}|x_{z_{<t}})] $$

此外，作者在模型中使用了一种特殊的相对位置编码。对于使用绝对位置编码的自注意力机制运算如下：

$$ \begin{aligned} q_i &= (x_i+p_i) W^Q , k_j = (x_j+p_j) W^K ,v_j = (x_j+p_j) W^V  \\ \alpha_{ij} &= \text{softmax}\{(x_i+p_i)W^Q ( (x_j+p_j)W^K)^T \} \\ &=  \text{softmax}\{ x_iW^Q (W^K)^T x_j^T+x_iW^Q (W^K)^T p_j^T+p_iW^Q (W^K)^T x_j^T+p_iW^Q (W^K)^T p_j^T \} \\ z_i &= \sum_{j=1}^{n} \alpha_{ij}(x_jW^V+p_jW^V)  \end{aligned} $$

而在**XLNet**中，作者将$x_j$的位置编码$p_j$替换为由三角函数编码表示的相对位置编码$R_{i-j}$，把$x_i$的位置编码$p_i$替换为可学习的向量$u,v$，并移除了值向量的位置编码：

$$ \begin{aligned}  \alpha_{ij} &=  \text{softmax}\{ x_iW^Q (W^K)^T x_j^T+x_iW^Q (W^K)^T R_{i-j}^T+uW^Q (W^K)^T x_j^T+vW^Q (W^K)^T R_{i-j}^T \} \\ z_i &= \sum_{j=1}^{n} \alpha_{ij}x_jW^V  \end{aligned} $$