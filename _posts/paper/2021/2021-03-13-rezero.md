---
layout: post
title: 'ReZero is All You Need: Fast Convergence at Large Depth'
date: 2021-03-13
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67e51e240ba3d5a1d7e51f9d.png'
tags: 论文阅读
---

> 你只需要ReZero：大深度的快速收敛.

- paper：[ReZero is All You Need: Fast Convergence at Large Depth](https://arxiv.org/abs/2003.04887)

# 0. TL; DR

**ReZero（Residual with Zero Initialization）**是一种简单的网络架构修改方法，通过在每个残差连接处引入一个初始化为零的可训练参数，实现了动态等距性（**dynamical isometry**），从而显著加速了深度网络的训练。**ReZero**不仅适用于全连接网络，还能在卷积网络（**ResNets**）和**Transformer**架构中实现更快的收敛速度，并且能够在不使用**LayerNorm**等归一化技术的情况下训练超过**100**层的**Transformer**模型。这一方法为深度网络的训练提供了一种高效且简单的解决方案。

# 1. 背景介绍

深度神经网络在多个领域取得了显著的成果，但随着网络深度的增加，训练难度也随之增大。主要问题包括梯度消失或爆炸，这导致训练时间延长或难以收敛。为了解决这些问题，研究者们提出了多种方法，包括精心设计的初始化方案、归一化技术（如**BatchNorm**和**LayerNorm**）以及残差连接等。残差连接通过引入跳跃路径，允许信号绕过某些层，从而改善了深层网络中的信号传播。

最近的研究表明，动态等距性是高效深度学习的关键因素。动态等距性要求网络的输入-输出雅可比矩阵的所有奇异值接近1，这意味着输入信号的所有扰动都能在网络中以相似的方式传播。然而，实现动态等距性并非易事，尤其是在使用**ReLU**激活函数或自注意力机制的网络中。**ReZero**通过一种简单的架构修改，实现了这一目标。

# 2. ReZero 方法

**ReZero**的核心思想是在每个残差连接处引入一个可训练的标量参数$α$，并将其初始化为零。具体来说，对于每个残差块，输入信号 $x_t$ 通过以下方式更新：

$$
x_{t+1} = x_t + \alpha_t \cdot F_t(x_t)
$$

其中，$F_t(x_t)$是残差块的非平凡变换，$\alpha_t$ 是初始化为零的可训练参数。在训练开始时，由于 $\alpha_t=0$，网络的输出与输入相同，即网络表示恒等函数，这自然满足动态等距性。随着训练的进行，$\alpha_t$ 会逐渐调整到合适的值，从而允许网络学习复杂的函数。

![](https://pic1.imgdb.cn/item/67e520230ba3d5a1d7e5210e.png)

**动态等距性**要求网络的输入-输出雅可比矩阵的所有奇异值接近1。**ReZero**通过将每个残差块的初始输出设置为输入本身，确保了在训练开始时网络的雅可比矩阵的奇异值为1。这使得网络在训练初期就能保持良好的梯度传播，从而加速训练过程。

![](https://pic1.imgdb.cn/item/67e530030ba3d5a1d7e52677.png)

# 3. 实验分析

作者在**CIFAR-10**数据集上对**32**层全连接网络进行了实验，比较了**ReZero**与普通全连接网络、带残差连接的全连接网络和带**LayerNorm**的全连接网络的训练速度。实验结果表明，**ReZero**网络的训练速度比其他方法快**7**到**15**倍。

![](https://pic1.imgdb.cn/item/67e52fa10ba3d5a1d7e52669.png)

作者在**CIFAR-10**数据集上对**ResNet**进行了实验，比较了**ReZero**与**Gated ResNet**、**zero γ**、**FixUp**等方法的性能。实验结果表明，**ReZero**在训练速度和测试精度上均优于其他方法。此外，**ReZero**还成功地将**18**层**ResNet**的训练时间缩短了**15%**。

![](https://pic1.imgdb.cn/item/67e52fd00ba3d5a1d7e5266e.png)

作者在语言建模任务上对**Transformer**架构进行了实验，比较了**ReZero**与**PostNorm、PreNorm**和**GPT2-Norm**等归一化方法的性能。实验结果表明，**ReZero**在训练速度上比**PostNorm**快**56%**，并且能够在不使用**LayerNorm**的情况下训练超过**100**层的**Transformer**模型。

作者进一步将**ReZero**应用于更深的**Transformer**模型，成功训练了**64**层和**128**层的**Transformer**模型。实验结果表明，**ReZero**不仅能够训练深层**Transformer**模型，而且在训练速度和测试性能上均优于普通**Transformer**模型。

![](https://pic1.imgdb.cn/item/67e5302e0ba3d5a1d7e52699.png)

