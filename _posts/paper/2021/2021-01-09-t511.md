---
layout: post
title: 'GLU Variants Improve Transformer'
date: 2021-01-09
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ed2a1c5132923bf880dc2c.jpg'
tags: 论文阅读
---

> T5.1.1：使用GLU改进预训练语言模型T5.

- paper：[GLU Variants Improve Transformer](https://arxiv.org/abs/2002.05202)

# 0. TL; DR
本文探讨了在**Transformer**模型的前馈子层中使用**Gated Linear Units（GLU）**及其变体来替代传统的**ReLU**或**GELU**激活函数。实验表明，某些**GLU**变体在预训练任务和多种下游语言理解任务中均能显著提升模型性能，降低困惑度，并在多数任务上取得更好的结果。这些改进的架构易于实现，且在计算上没有明显缺点。

# 1. 背景介绍
**Transformer**模型自2017年被提出以来，已成为自然语言处理领域的主流架构之一。它通过交替使用多头注意力机制和位置前馈网络（**FFN**）来处理序列数据。传统的**FFN**由两个线性变换组成，中间使用**ReLU**激活函数。然而，研究者们一直在探索更高效的激活函数和网络结构，以进一步提升**Transformer**的性能。

**Gated Linear Units（GLU）**是一种由**Dauphin**等人在2016年提出的神经网络层，它通过两个线性变换的逐元素乘积来实现，其中一个变换通过**sigmoid**函数激活。**GLU**的设计初衷是为了在保持线性变换的同时，引入门控机制来控制信息流动。这种设计在语言建模等任务中展现出了良好的性能。基于**GLU**的这种特性，本文提出了在**Transformer**模型中使用**GLU**及其变体来替代传统的**ReLU**激活函数，以期获得更好的性能表现。

# 2. 方法介绍

## （一）GLU及其变体
GLU的基本形式如下：

$$
\text{GLU}(x, W, V, b, c) = \sigma(xW + b) \otimes (xV + c)
$$

其中，$\sigma$ 是**sigmoid**函数，$W$ 和 $V$ 是权重矩阵，$b$ 和 $c$ 是偏置向量。**GLU**通过**sigmoid**函数对输入进行门控，然后与另一个线性变换的结果逐元素相乘。

基于**GLU**，本文提出了多种变体，分别使用不同的激活函数替代**sigmoid**函数：
- **ReGLU**：使用**ReLU**激活函数

$$
  \text{ReGLU}(x, W, V, b, c) = \max(0, xW + b) \otimes (xV + c)
$$

- **GEGLU**：使用**GELU**激活函数

$$
\text{GEGLU}(x, W, V, b, c) = \text{GELU}(xW + b) \otimes (xV + c)
$$

- **SwiGLU**：使用**Swish**激活函数

$$
\text{SwiGLU}(x, W, V, b, c, \beta) = \text{Swish}_\beta(xW + b) \otimes (xV + c)
$$

这些变体通过不同的激活函数来调整门控机制，以探索更适合**Transformer**模型的结构。

## （二）Transformer中的GLU应用
在**Transformer**模型中，传统的**FFN**层定义为：

$$
\text{FFNReLU}(x, W_1, W_2) = \max(xW_1, 0)W_2
$$

本文提出将**GLU**及其变体应用于**FFN**层，替代传统的**ReLU**激活函数。新的**FFN**层定义如下：
- **FFNGLU**：

$$
\text{FFNGLU}(x, W, V, W_2) = (\sigma(xW) \otimes xV)W_2
$$

- **FFNBilinear**：

$$
\text{FFNBilinear}(x, W, V, W_2) = (xW \otimes xV)W_2
$$

- **FFNReGLU**：

$$
\text{FFNReGLU}(x, W, V, W_2) = (\max(0, xW) \otimes xV)W_2
$$

- **FFNGEGLU**：

$$
\text{FFNGEGLU}(x, W, V, W_2) = (\text{GELU}(xW) \otimes xV)W_2
$$

- **FFNSwiGLU**：

$$
\text{FFNSwiGLU}(x, W, V, W_2) = (\text{Swish}_1(xW) \otimes xV)W_2
$$

这些新的**FFN**层结构通过引入**GLU**及其变体，增加了模型的表达能力和灵活性。同时，为了保持参数数量和计算量与原始模型一致，将隐藏单元数 $d_{ff}$ 减半。

# 3. 实验分析

## （一）预训练任务
实验使用了**Text-to-Text Transfer Transformer（T5）**的预训练任务，目标是预测文本段中的缺失部分。预训练在**C4**数据集上进行，使用**Adafactor**优化器和逆平方根学习率调度。实验结果如下表所示：

![](https://pic1.imgdb.cn/item/681dbfb258cb8da5c8e92cc4.png)

从表中可以看出，使用**GEGLU**和**SwiGLU**变体的模型在预训练任务中取得了最低的困惑度，表明这些变体在处理文本生成任务时具有更好的性能。

## （二）下游任务微调
预训练完成后，对模型在**GLUE、SuperGLUE**和**SQuAD**等下游语言理解任务上进行微调。微调使用了**131,072**步，学习率为 $10^{-3}$。实验结果如下表所示：

![](https://pic1.imgdb.cn/item/681dbfcb58cb8da5c8e92ef2.png)
![](https://pic1.imgdb.cn/item/681dbfdd58cb8da5c8e930b1.png)

从上述实验结果可以看出，使用**GEGLU**和**SwiGLU**变体的模型在多数下游任务中表现最佳，尤其是在**GLUE**和**SuperGLUE**基准测试中，这些变体在多个子任务上取得了显著的性能提升。这表明**GLU**及其变体在处理复杂的语言理解任务时具有更好的适应性和表达能力。
