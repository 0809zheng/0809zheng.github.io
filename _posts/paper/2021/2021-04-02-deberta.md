---
layout: post
title: 'DeBERTa: Decoding-enhanced BERT with Disentangled Attention'
date: 2021-04-02
author: 郑之杰
cover: 'https://img.imgdb.cn/item/606bb7638322e6675c3f2408.jpg'
tags: 论文阅读
---

> DeBERTa：使用分解注意力机制和增强型掩膜解码器改进预训练语言模型.

- paper：[DeBERTa: Decoding-enhanced BERT with Disentangled Attention](https://arxiv.org/abs/2006.03654)
- code：[github](https://github.com/microsoft/DeBERTa)

作者提出了两种新技术：**分解注意力(disentangled attention)**和**增强型掩膜解码器(enhanced mask decoder)**，用来改进预训练语言模型的性能。前者将每个单词通过词嵌入分别编码为内容向量和位置向量，根据其内容和相对位置计算单词之间的注意力权重；后者在解码层中引入绝对位置来预测被遮挡的单词。


# 1. Disentangled Attention

预训练语言模型通常用自注意力运算提取输入序列$H$的特征，计算如下：

![](https://img.imgdb.cn/item/606bc32b8322e6675c4865e6.jpg)

上述自注意力是根据上下文的**内容(content)**计算的，作者认为在计算注意力时还应该考虑其相对位置，比如**deep learning**这两个单词相邻时的依赖关系比分开时更强。因此作者引入了相对位置编码，该编码是对相对距离进行的词嵌入，**token** $ i $和**token** $ j $的相对距离计算如下，其最大距离被限定为$k$：

![](https://img.imgdb.cn/item/606bc4a78322e6675c49896b.jpg)

值得一提的是，相对位置的词嵌入$P$是在所有层共享的。在计算某一层的自注意力时，除了通常的**content**计算，还要考虑**content**和**position**之间的计算。

![](https://img.imgdb.cn/item/606bc5d68322e6675c4ac2b3.jpg)

### ⚪ 从位置编码出发理解分解注意力

通常使用绝对位置编码的自注意力机制运算如下：

$$ \begin{aligned} q_i &= (x_i+p_i) W^Q , k_j = (x_j+p_j) W^K ,v_j = (x_j+p_j) W^V  \\ \alpha_{ij} &= \text{softmax}\{(x_i+p_i)W^Q ( (x_j+p_j)W^K)^T \} \\ &=  \text{softmax}\{ x_iW^Q (W^K)^T x_j^T+x_iW^Q (W^K)^T p_j^T+p_iW^Q (W^K)^T x_j^T+p_iW^Q (W^K)^T p_j^T \} \\ z_i &= \sum_{j=1}^{n} \alpha_{ij}(x_jW^V+p_jW^V)  \end{aligned} $$

在分解注意力中，作者去掉了位置与位置之间的交互注意力$p_iW^Q (W^K)^T p_j^T$，并在输入与位置之间的交互注意力项中把位置编码$p_i,p_j$替换为相对位置编码$R_{j,i},R_{i,j}$：

$$ \alpha_{ij} =  \text{softmax}\{ x_iW^Q (W^K)^T x_j^T+x_iW^Q (W^K)^T R_{i,j}^T+R_{j,i}W^Q (W^K)^T x_j^T \}  $$


# 2. Enhanced Mask Decoder

作者指出，**NLP**的大多数任务都只需要相对位置信息，但有些场景下绝对位置信息更有帮助，于是整个模型可以看作两部分。以**Base**版的**MLM**预训练模型为例，它一共有$13$层，前$11$层使用相对位置编码，这部分称为**Encoder**；后$2$层加入绝对位置信息，这部分称为**Enhanced Mask Decoder**。


消融实验证明了这些技术的有效性：

![](https://img.imgdb.cn/item/606bc7208322e6675c4bedfc.jpg)