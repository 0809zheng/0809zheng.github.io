---
layout: post
title: 'Dynamic ReLU'
date: 2021-10-27
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/61793ad02ab3f51d91fc412e.jpg'
tags: 论文阅读
---

> DY-ReLU：动态整流线性单元.

- paper：[Dynamic ReLU](https://arxiv.org/abs/2003.10027)

作者提出了动态**ReLU**函数，将所有输入元素$x$的全局上下文信息编码到超函数$\theta(x)$中，并使用超函数决定激活函数$f_{\theta(x)}(x)$的形状，从而提高激活函数的表示能力。

![](https://pic.imgdb.cn/item/617a65492ab3f51d91e854cb.jpg)

**ReLU**激活函数表示为$y_c=\max(x_c,0)$，可推广为分段线性函数$y_c=\max_k(a_c^kx_c+b_c^k)$。作者提出的**dynamic ReLU**使用输入元素$$x=\{x_c\}$$自适应地计算参数$a_c^k$和$b_c^k$：

$$ y_c = f_{\theta(x)}(x_c) = \mathop{\max}_{1\leq k \leq K} \{a_c^k(x)x_c+b_c^k(x)\} $$

此处的超函数$\theta(x)$的输出为：

$$ [a_1^1,...,a_C^1,...,a_1^K,...,a_C^K,b_1^1,...,b_C^1,...,b_1^K,...,b_C^K]^T=\theta(x) $$

超函数$\theta(x)$的实现参考了**SENet**中的**SE**模块，即首先沿某些维度进行全局平均池化，再通过两个全连接层输出$2KC$个参数$\Delta a_{1:C}^{1:K}$和$\Delta b_{1:C}^{1:K}$。参数$a_c^k$和$b_c^k$计算为：

$$ a_c^k(x)=\alpha^k+\lambda_a \Delta a_{c}^{k}(x),\quad b_c^k(x)=\beta^k+\lambda_b \Delta b_{c}^{k}(x) $$

实验中设置$K=2$，$\alpha^1=1,\alpha^2=\beta^1=\beta^2=0$，$\lambda_1=1,\lambda_2=0.5$。

一些其他的激活函数可以看作**dynamic ReLU**的特殊情况：

![](https://pic.imgdb.cn/item/619707ca2ab3f51d919c3e19.jpg)

根据超函数输入的维度不同，作者设计了三种形式的动态**ReLU**函数：
- **DY-ReLU-A**：每一层的神经元共享激活函数
- **DY-ReLU-B**：每层神经元的每个通道共享激活函数
- **DY-ReLU-C**：每层神经元的每个元素独立使用激活函数

![](https://pic.imgdb.cn/item/617a6aef2ab3f51d91ed2dbb.jpg)

**DY-ReLU-A**引入了$2K$个参数，**DY-ReLU-B**引入了$2KC$个参数。若直接实现**DY-ReLU-C**，需要$2KCHW$个参数。为了降低参数量，作者将**DY-ReLU-C**的通道参数和空间参数分别计算，再通过相乘构造，从而把参数量降低为$2KC+HW$。

作者将**ResNet**和**MobileNet**中的**ReLU**替换为**dynamic ReLU**，实验结果如下：

![](https://pic.imgdb.cn/item/61970c562ab3f51d919ea994.jpg)

作者展示了不同输入图像在网络的不同模块上的激活函数的形状分布，图中显示激活函数对于输入是动态变化的：

![](https://pic.imgdb.cn/item/61970eb82ab3f51d91a00ecd.jpg)

对于$K=2$的情况，作者统计了每个模块中分段线性函数两段斜率的差异变化$\|a_c^1-a_c^2\|$。图中可知随着更深的网络层数，分段线性斜率差异越小，表明曲线弯曲度越低。

![](https://pic.imgdb.cn/item/61970f492ab3f51d91a06a44.jpg)