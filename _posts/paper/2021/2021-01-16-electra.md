---
layout: post
title: 'ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators'
date: 2021-01-16
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ed75ed5132923bf891e193.jpg'
tags: 论文阅读
---

> ELECTRA：判别式的预训练语言模型.

- paper：ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators
- arXiv：[link](https://arxiv.org/abs/2003.10555)

**BERT**采用掩码语言建模的方式训练了一个生成式模型，而**ELECTRA**借鉴生成对抗网络的思想，首先用掩码语言建模的方式训练一个生成器，然后对输入句子进行采样替换，将处理后的句子输入到判别器中，用于判断句子哪些部分是被替换过的。由于生成器和判别器是同步训练的，因此随着生成器的训练，判别难度会慢慢增加，使得判别器学习到更具有判别性的信息。最后将判别器作为预训练的语言模型。因此**ELECTRA**的预训练任务同时包括**掩码语言建模(mask language modeling)**和**替换词检测(Replaced Token Detection)**。

![](https://pic.imgdb.cn/item/60ed76045132923bf89250b6.jpg)

在生成对抗网络中，判别器的最优解是$D(x)=\frac{p(x)}{p(x)+q(x)}$，其中$p(x)$,$q(x)$分别是真假样本的分布。若生成器拟合能力足够强，则假样本会逼近真实样本，使得$q(x)≈p(x)$，则判别器退化为常数$D(x)=\frac{1}{2}$，失去提取判别特征的能力。因此**ELECTRA**的生成器模型不能太过复杂，其大小选在判别器的$\frac{1}{4}$-$\frac{1}{2}$之间效果较好。

由于生成对抗式的训练会使得训练过程更有针对性，所以**ELECTRA**的预训练效率更高，能用更少的时间来达到同样规格的**BERT**的效果：

![](https://pic.imgdb.cn/item/60ed78c05132923bf89f3a9a.jpg)

