---
layout: post
title: 'EfficientDet: Scalable and Efficient Object Detection'
date: 2021-06-02
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/653653d6c458853aef77a607.jpg'
tags: 论文阅读
---

> EfficientDet：可扩展的高效目标检测方法.

- paper：[EfficientDet: Scalable and Efficient Object Detection](https://arxiv.org/abs/1911.09070)


在目标检测中，为了增强网络的特征表达能力，一般都需要特征融合方法，常用的特征融合方法为**FPN**或者**PAN**，这两种方法存在一些不足之处：
1. 常见的网络特征融合部分（**FPN**和**PAN**等）通常是将低分辨率（高分辨率）的特征**resize**成高分辨率（低分辨率）与相应的高分辨率（低分辨率）特征融合，但是高层和低层特征的融合对输出特征的贡献度存在不平等；
2. 为了得到不同尺度大小的网络，通常方法是增加网络的大小和输入图像的大小，但这种方式并没有权衡网络精度和模型大小，并没有考虑到网络输入分辨率、宽度以及深度的综合影响。

针对上述不足，本文提出了**BiFPN**和联合缩放方法（**Compound Scaling**）。**BiFPN**考虑到不同特征融合对于输出特征的重要性；联合缩放方法综合考虑图像输入分辨率、网络宽度和深度这些因素，权衡了准确率和效率。

![](https://pic.imgdb.cn/item/65365524c458853aef7b88fe.jpg)

**EfficientDet**的**backbone**采用了一个高效的**EfficientNet**骨干网络，在计算资源受限的情况下，并不知道怎样调整输入分辨率、网络宽度和深度才能够达到最优，因此联合缩放提出了同时缩放这些变量；图中的**Neck**部分为替换为本文提出的**BiFPN**；图中的**Head**部分为检测网络中的预测部分，通常预测位置信息和类别信息。

## 1. BiFPN

### （1）权重特征融合

在目标检测中，常见的特征融合方式为**FPN**和**PAN**，其中**FPN**为自顶向下方式，将具有高语义信息的低分辨率特征**resize**成多细节性的高分辨率特征，再与同分辨率大小的特征融合；**PAN**在**FPN**的基础上多了自底向上的特征融合，其中**P7**特征图都是经过**Conv**和**resize**之后和**P6**相加，对于融合后的特征图而言，这两个特征图(**P7**和**P6**)的贡献相同，不能够根据现有的数据集充分利用特征的信息进行预测。为了解决这一问题，**BiFPN**在特征融合前为每一个特征设置了一个权重系数 $\phi$，根据网络损失可以动态调整不同特征的重要性，增强融合后特征的的表征能力。

![](https://pic.imgdb.cn/item/653656a4c458853aef7fac6e.jpg)

### （2）跨尺度连接

**FPN**和**PAN**都是**one-way**流的方式，为了进一步促进特征之间的融合，提高目标检测的性能，文中提出了跨层多尺度的融合方式，主要灵感来源于**NAS-FPN**，但**NAS-FPN**的训练既耗时又耗**GPU**资源，因此作者设计了**BiFPN**，在现有的基础结构上进行修改，也就是移除了网络中只有一条边输入的节点，因为如果只有一条边输入，则代表这个节点具有较少的贡献。**BiFPN**中的跨尺度连接线增强了特征之间的融合。

![](https://pic.imgdb.cn/item/6536573bc458853aef819a5e.jpg)

## 2. 联合缩放 (Compound Scaling)

为了权衡模型的精度和速度，常用的网络调整策略是通过单独调节网络的输入分辨率或者堆叠更多的**FPN**层，但在最近的图像分类中发现联合缩放网络的宽度、深度和输入分辨率具有的更好的性能。

因此本文将这种方法运用到目标检测中，用来调节特征融合网络(**Neck**)部分的宽度和深度、预测网络(**Head**)部分的宽度和深度。

特征融合网络的宽度和深度的调节：

$$
W_{BiFPN}=64*(1.35)^\phi , \quad D_{BiFPN}=3+\phi 
$$

预测网络的宽度和深度的调节：

$$
D_{Box}=D_{Class}=3+\lfloor\phi/3\rfloor 
$$

骨干网络输入图像分别率的调节：

$$
R_{Input}=512+\phi*128
$$

受这种方法的启发，本文对目标检测网络的**BackBone**的输出分辨率、宽度和深度、**BiFPN(Neck)**的深度和宽度、预测网络(**Head**)的宽度和深度同时缩放。如下表所示，根据系数的不同，系数从**0**到**7**共有**8**个**EfficientDet**版本。

![](https://pic.imgdb.cn/item/65365807c458853aef83bb4b.jpg)

## 3. 实验分析

下表是不同检测网络的精度对比。在模型参数量和浮点运算率更少的情况下，**EfficientDet**的测试精度都达到了最高，一方面依赖于通过强大的计算资源搜索出的**EfficientNet**骨干网络；另一方面是本文提出的**BiFPN**，更加灵活地运用到不同特征图的信息。

![](https://pic.imgdb.cn/item/6536584fc458853aef84764f.jpg)

下图是模型内存和速度对比。在**COCO**数据集上，相对于其他检测网络，**EfficientDet**运用更少的参数量**(4X-9X)**、**GPU**延迟率**(2X-4X)**和**CPU**延迟率**(5X-11X)**的情况下，取得了更高的**AP**。

![](https://pic.imgdb.cn/item/65365890c458853aef85233a.jpg)