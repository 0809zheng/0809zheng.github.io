---
layout: post
title: 'Online Knowledge Distillation for Efficient Pose Estimation'
date: 2021-04-26
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/64cefb9f1ddac507cc5784da.jpg'
tags: 论文阅读
---

> 高效姿态估计的在线知识蒸馏.

- paper：[Online Knowledge Distillation for Efficient Pose Estimation](https://arxiv.org/abs/2108.02092)

本文提出了一种在线知识蒸馏框架**OKDHP**，对人体姿态估计模型进行提升。**OKDHP**训练了一个多分支网络，其中每个分支都被当做独立的学生模型；教师模型不是显式存在的，而是通过加权集成多个分支的**heatmap**结果后形成的。通过优化**Pixel-wise KL Divergence**损失来优化每个学生分支模型，整个训练过程被简化到了**one-stage**，不需要额外预训练的教师模型。

![](https://pic.imgdb.cn/item/64cf03991ddac507cc6af259.jpg)

主流的**2D**姿态估计方法大多数都是基于[<font color=blue>Hourglass Network</font>](https://0809zheng.github.io/2021/04/03/hourglass.html)，其含有多个堆叠的**Hourglass**，通常有**2-stack**, **4-stack**, **8-stack**类型。后一个**Hourglass**将前一个**Hourglass**的结果作为输入，不断进行**refine**，直到结尾。**8-stack**的结果要明显好于**4-stack**，但是与之而来的问题就是计算量明显的增加。

传统的蒸馏方法通常是首先训一个**8-stack HG**作为**teacher**，选择一个**4-stack HG**作为**student**。第一步训**teacher**，第二步训**student**，整体是一个**two-stage**的过程，较为繁琐。并且如果要训练一个**8-stack HG**的**student**，就需要找到一个比**8-stack**更大的**model**去作为**teacher**。堆叠更多的层数会存在性能收益递减，并且带来计算量直线上升。

![](https://pic.imgdb.cn/item/64cf01d71ddac507cc6734ee.jpg)

本文提出了一个在线知识蒸馏的框架，即一个多分支结构。这里的**teacher**不是显式存在的，而是通过多个学生分支的结果经过了**FAU**的**ensemble**形成的。如果要得到一个**4-stack HG**的网络，直接建立一个多分支网络，在前部共享 **2**个**stack**（节约计算量），后面针对每个分支视为**student**。多个分支产生的结果经过**FAU**进行**ensemble**。整个蒸馏过程被简化到了**one-stage**，直接训练完之后，选择一个最好性能的分支，去除掉其他多余分支结构即可得到一个目标网络。

**FAU**，即**Feature Aggregation Unit**，是用来对每个分支产生的结果进行通道维度的加权集成。即将每个**heatmap**按照生成的权重进行集成。针对人体姿态估计存在着很多的尺度变化问题，采用**3x3**, **5x5**, **7x7**和**avg pool**来捕捉更大范围的信息，进而来生成对应每个分支产生的**heatmap**的**weight**。

![](https://pic.imgdb.cn/item/64cf037c1ddac507cc6a9a7f.jpg)

**OKDHP**方法不仅是对于**hourglass**类别的网络有着明显的提升，对于其他的姿态估计网络也有效果。针对其他的网络结构，如果想要应用**OKDHP**，仔细的选择网络中共享和独立的部分来设计网络。可以粗略的将网络分为**encoder（backbone）**和**decoder**部分。网络结构设计上共享整个**encoder**部分，然后建立独立的分支，每个独立的分支都对应一个完整的**decoder**部分。