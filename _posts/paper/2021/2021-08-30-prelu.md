---
layout: post
title: 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification'
date: 2021-08-30
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6129aa3e44eaada73921e6ec.jpg'
tags: 论文阅读
---

> PReLU：分类任务超越人类表现.

- paper：Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification
- arXiv：[link](https://arxiv.org/abs/1502.01852)

本文是首次公开宣布图像的识别率超越人类水平的论文，主要贡献是提出了**PReLU**激活函数和**Kaiming**初始化。

# 1. Parametric ReLU

![](https://pic.imgdb.cn/item/6129cc5a44eaada73952b71f.jpg)

一个使用**ReLU**激活函数的神经网络层计算如下：

$$ y=\max(0,Wx+b) $$

注意到$Wx+b=0$是一个超平面，上述非线性函数相当于保留空间中该超平面的一侧，而另一侧会被挤压到超平面上，以一个正交的二维空间为例：

![](https://pic.imgdb.cn/item/6129d06844eaada739599036.jpg)

注意到空间中大部分区域的信息被不同程度地压缩了。**Leaky ReLU**通过在输入为负值时进行$0<\alpha<1$倍的缩放保留了靠近超平面附近的信息：

$$ y=\max(\alpha(Wx+b),Wx+b) $$

如下图所示，若设置$\alpha=0.1$，则之前被抑制的空间区域数值缩小了$0.1$倍，但并没有被完全丢失。

![](https://pic.imgdb.cn/item/6129d0a044eaada73959f19d.jpg)

缩放倍数$\alpha$通常是人工选择的，作者认为可以将其也看作一个参数，由梯度更新训练得到。以此提出了**Parametric ReLU(PReLU)**激活函数，表达式为：

$$ \text{PReLU}(x) = \begin{cases} x, & x \geq 0 \\ \alpha x, & x < 0 \end{cases} $$

**PReLU**对$\alpha$的导数如下：

$$ \frac{d}{d\alpha}\text{PReLU}(x) = \begin{cases} 0, & x \geq 0 \\ x, & x < 0 \end{cases} $$

注意到对每一个神经元，参数$\alpha$的实际取值可能是不同的。


# 2. Kaiming Initialization
作者针对**ReLU**系列函数提出了一个新的参数初始化方法。对于网络的第$l$层，其表达式为：

$$ y_l = W_lx_l+b_l $$

其中$x_l=f(x_{l-1})$，$f$是激活函数。通常$W_l$的每个元素是从高斯分布$\mathcal{N}(0,\sigma^2)$中采样的，$b_l$初值赋$0$。若记$W_l$的维度是$n_l$，则有：

$$ \text{Var}[y_l] = n_l\text{Var}[W_lx_l] = n_l(E[(W_lx_l)^2]-E^2[W_lx_l]) $$

由于$E[W_l]=0$，因此$E^2[W_lx_l]=0$。且$\text{Var}[W_l]=E[W_l^2]-E^2[W_l]=E[W_l^2]$，因此上式可写作：

$$ \text{Var}[y_l]  = n_l(E[(W_lx_l)^2]-E^2[W_lx_l]) = n_lE[(W_lx_l)^2] \\ = n_lE[W_l^2]E[x_l^2] = n_l\text{Var}[W_l]E[x_l^2] $$

若激活函数使用**ReLU**，则有$x_l=\max(0,y_{l-1})$。注意到$E[y_{l-1}]=0$，故：

$$ E[x_l^2] = \frac{1}{2} E[y_{l-1}^2] = \frac{1}{2} (E[y_{l-1}^2]-E^2[y_{l-1}]) = \frac{1}{2} \text{Var}[y_{l-1}] $$

因此可得使用**ReLU**激活函数导致的每一层的方差变化：

$$  \text{Var}[y_l]   = \frac{1}{2}n_l\text{Var}[W_l]\text{Var}[y_{l-1}] $$

若希望每一层的输出都有相同的方差，即$\text{Var}[y_l]   =\text{Var}[y_{l-1}]$，则有：

$$  \frac{1}{2}n_l\text{Var}[W_l] = 1 $$

$$  \text{Var}[W_l] = \frac{2}{n_l} $$

因此对于使用**ReLU**激活函数的网络，$W_l$应从高斯分布$\mathcal{N}(0,\frac{2}{n_l})$中采样。

若激活函数采用**Leaky ReLU**，则有$x_l=\max(\alpha y_{l-1},y_{l-1})$，故：

$$ E[x_l^2] = \frac{1}{2} (1+\alpha) E[y_{l-1}^2] = \frac{1}{2}(1+\alpha) \text{Var}[y_{l-1}] $$

$$  \text{Var}[W_l] = \frac{2}{(1+\alpha)n_l} $$

因此对于使用**Leaky ReLU**激活函数的网络，$W_l$应从高斯分布$\mathcal{N}(0,\frac{2}{(1+\alpha)n_l})$中采样。

作者训练了一个$30$层的神经网络，使用**Xavier**初始化的网络训练困难，损失几乎不下降；而使用上述初始化的网络能够正常训练。

![](https://pic.imgdb.cn/item/6129cc4344eaada739528f3d.jpg)