---
layout: post
title: 'ALBERT: A Lite BERT for Self-supervised Learning of Language Representations'
date: 2021-01-14
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ed58545132923bf8f9380a.jpg'
tags: 论文阅读
---

> ALBERT：一种轻量型的BERT模型.

- paper：[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)

**ALBERT**是在**BERT**的基础上设计的一个轻量化预训练语言模型，相比于**BERT**，**ALBERT**做了如下改进：
- 降低特征维度：**BERT**使用的词嵌入维度是$768$，使得**Embedding**占有较大参数量；而**ALBERT**使用的词嵌入维度是$128$，输出层使用$128 \times 768$的矩阵变回$768$维，在不影响输出结果的情况下降低了参数量；
- 更改预训练任务：**BERT**使用**NSP(Next Sentence Prediction)**作为预训练任务，即预测两个句子是否具有邻接关系；而**ALBERT**使用**SOP(Sentence-Order Prediction)**作为预训练任务，即预测两个句子的先后顺序；
- 参数共享：**BERT**使用$12$层**Transformer**模块堆叠而成，表示为$y=f_n(f_{n-1}(...(f_1(x))))$；而**ALBERT**使用共享参数的**Transformer**模块，表示为$y=f(f(...(f(x))))$，如下图所示

![](https://pic.imgdb.cn/item/60ed70c35132923bf8796907.jpg)

**ALBERT**中的参数共享是一种模型正则化方法，能够有效防止模型过拟合，但也会限制模型的表达能力。参数共享使得**ALBERT**的参数量大大减小，模型训练时间和显存也会减少。由于参数共享的存在，**ALBERT**并没有使用**dropout**。

**BERT**和**ALBERT**的结构对比如下表所示：

![](https://pic.imgdb.cn/item/60ed706d5132923bf877da82.jpg)

**BERT**和**ALBERT**的实验结果对比如下表所示。在同样的规格下(比如都是**base**)，尽管**ALBERT**具有更小的参数量，但**BERT**和**ALBERT**的前向传播计算复杂度是类似的，甚至**ALBERT**还多一个矩阵运算。因此同样的模型规格下，**ALBERT**的推理时间并不占优势。当模型规模较小时，**ALBERT**的表现比**BERT**差，只有当**ALBERT**规格特别大时才能够超过**BERT**。

![](https://pic.imgdb.cn/item/60ed73115132923bf8844c32.jpg)
