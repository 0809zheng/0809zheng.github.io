---
layout: post
title: 'RoBERTa: A Robustly Optimized BERT Pretraining Approach'
date: 2021-08-16
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/611f63874907e2d39c6536ca.jpg'
tags: 论文阅读
---

> RoBERTa：鲁棒优化的BERT预训练方法.

- paper：RoBERTa: A Robustly Optimized BERT Pretraining Approach
- arXiv：[link](https://arxiv.org/abs/1907.11692)

作者针对**BERT**提出了几点改进，并提出了**RoBERTa**预训练模型。主要改进如下：

### 1. Dynamic Mask
**BERT**采用的**MLM**预训练任务会对序列中的随机**token**加上**mask**，这一步是在数据预处理的过程中实现的，这是一种**static mask**，输入网络的句子**mask**是固定的。作者将其改进为**dynamic mask**，即每次输入数据时随机生成**mask**，从而避免同一个句子每次输入时**mask**方式相同，提高了模型输入的随机性，使得模型学习到更多信息。

![](https://pic.imgdb.cn/item/611f67794907e2d39c6e0c15.jpg)

### 2. Remove NSP
**BERT**的预训练任务除了**MLM**还有**NSP**，即预测两句话是不是连续的。训练时正样本是从文章中挑选的连续两句话，占$50\%$；负样本是从不同的文章中挑选的两句话。为了判断**NSP**任务对模型表现是否有提升，作者进行了如下实验：
- **SEGMENT-PAIR+NSP**：标准的**NSP**，每一对句子总长度小于$512$。
- **SENTENCE-PAIR+NSP**：每一对句子总长度远小于$512$，采用的**batch**远大于$512$。
- **FULL-SENTENCES**：输入单个句子，句子长度可能超过输入限制，不采用**NSP**损失。
- **DOC-SENTENCES**：输入单个句子，句子长度不超过输入限制，不采用**NSP**损失，动态调整**batch**。

实验发现不采用**NSP**损失，可以轻微提高模型的效率，而且动态调整**batch**也能提高模型表现。

![](https://pic.imgdb.cn/item/611f69014907e2d39c71729f.jpg)

### 3. Larger Batch
**BERT**采用$256$的**batch**和$1$M训练轮数。作者通过实验发现，在总更新次数相同的情况下，采用更大的**batch**提高模型的训练速度和下游任务的表现。

![](https://pic.imgdb.cn/item/611f69f94907e2d39c7398b0.jpg)

### 4. More Data
**BERT**使用数据集为**BOOKCORPUS**和**English WIKIPEDIA(16G original)**，作者增加了**CC-NEWS(76G),OPEN WEB TEXT(38G),STORIES(31G)**，并设置了更长的训练时间，提高模型的表现：

![](https://pic.imgdb.cn/item/611f6ad94907e2d39c759f6f.jpg)

