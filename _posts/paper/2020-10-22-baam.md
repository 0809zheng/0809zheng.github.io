---
layout: post
title: 'BA^2M: A Batch Aware Attention Module for Image Classification'
date: 2020-10-22
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63b3a9febe43e0d30e71559e.jpg'
tags: 论文阅读
---

> BA^2M：图像分类的批量注意力模块.

- paper：[BA^2M: A Batch Aware Attention Module for Image Classification](https://arxiv.org/abs/2103.15099)

在图像分类任务中，使用**Softmax**函数构造交叉熵损失：

$$ L_i = - \log (\frac{e^{f_{y_i}}}{\sum_j^K e^{f_j}}) $$

其中$y_i$是样本$x_i$的类别，$f_i$是样本$x_i$的**logits**得分。从损失函数中可以看出，每一个图像样本对于优化过程的贡献是相等的；但是由于图像内容的复杂性不同，在计算损失的时候不同图像应该具有不同的重要性。

本文作者提出了**Batch Aware Attention Module (BA^2M)**，在批量训练中为每个样本$x_i$的损失函数赋予一个重要性权重$w_i$，从而调整其在损失计算中的重要性：

$$ \begin{aligned}  L &= -\frac{1}{N} \sum_i^N w_i\log (\frac{e^{f_{y_i}}}{\sum_j^K e^{f_j}}) \\ &= -\frac{1}{N} \sum_i^N \log (\frac{e^{w_i\cdot f_{y_i}}}{(\sum_j^K e^{f_j})^{w_i}}) \\ &\leq -\frac{1}{N} \sum_i^N \log (\frac{e^{w_i\cdot f_{y_i}}}{\sum_j^K e^{w_i\cdot f_j}}) \\ &= -\frac{1}{N} \sum_i^N \log (\frac{e^{W^T_{y_i}(w_i\cdot x_i)}}{\sum_j^K e^{W^T_j(w_i\cdot x_i)}})  \end{aligned} $$

由上式可得，对样本$x_i$的损失函数赋予权重，等价于对样本$x_i$的特征$f_i$赋予权重，并进一步近似等价于对样本$x_i$赋予权重。

![](https://pic.imgdb.cn/item/63b55597be43e0d30e2003f6.jpg)

样本权重$w_i$通过以下注意力机制生成：

![](https://pic.imgdb.cn/item/63b555d0be43e0d30e205daf.jpg)

- **Channel Attention Module** $A_C$

$$ A_C(x_i) = BN(FC_1(FC_0(GAP(x_i)))) $$

- **Local Spacial Attention Module** $A_{LS}$

$$ A_{LS}(x_i) = BN(g_2^{1\times 1}(g_1^{3\times 3}(g_0^{1\times 1}(x_i)))) $$

- **Global Spacial Attention Module** $A_{GS}$

$$ A_{GS}(x_i) = softmax(f(x_i)\times (g(x_i))^T)\times h(x_i) $$

把三个注意力分支的的注意力矩阵合并为样本$x_i$的注意力得分：

$$ A_i = mean(\max(A_C(x_i),A_{LS}(x_i),A_{GS}(x_i))) $$

最后通过**softmax**函数生成样本$x_i$的权重$w_i$：

$$ w_i = \frac{e^{A_i}}{\sum_{j=1}^Ne^{A_j}} $$

作者给出了将**BA^2M**加入其他网络的例子:

![](https://pic.imgdb.cn/item/63b559c7be43e0d30e270de8.jpg)