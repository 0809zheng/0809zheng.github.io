---
layout: post
title: 'Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer'
date: 2021-01-08
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ed21125132923bf834851a.jpg'
tags: 论文阅读
---

> T5：编码器-解码器结构的预训练语言模型.

- paper：Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
- arXiv：[link](https://arxiv.org/abs/1910.10683)

本文介绍**Google**提出的预训练语言模型**T5(text-to-text-transfer-transformer)**，该模型使用标准的编码器-解码器结构，构建了无监督+有监督的文本生成预训练任务(将预训练任务都看作**Seq2Seq**任务)，并在大量自然语言处理任务中取得最好的成绩。

**T5**的预训练包含**无监督**和**有监督**两部分。无监督部分使用**Google**构建的$800$G语料库**C4**，训练任务采用和**BERT**一样的**mask**语言建模，但将其改成了**Seq2Seq**版本，即采用编码器-解码器结构，将**masked**序列输入编码器，解码器以自回归的方式顺序生成**masked token**。

![](https://pic.imgdb.cn/item/60ed23435132923bf846986a.jpg)

有监督部分则收集了多种不同的自然语言处理监督任务数据，并也统一转化为**Seq2Seq**任务来训练：

![](https://pic.imgdb.cn/item/60ed23105132923bf844f375.jpg)

值得一提的是，微调**T5**模型时的学习率要比微调**BERT**大$10$倍以上才行(即$10^{−4}$级别，**BERT**一般是$10^{−5}$级别)，这是两者模型架构差异决定的。除了在多个自然语言处理任务中取得最好的结果，**T5**还对整个训练流程中的可调的超参数进行了讨论，比如模型架构是采用标准的编码器-解码器结构还是**UniLM**结构，无监督预训练任务是采用**mask**语言建模还是其他方式，随机**mask**的比例应该是多少等，并给出了如下表格：

![](https://pic.imgdb.cn/item/60ed23905132923bf8491a3a.jpg)

