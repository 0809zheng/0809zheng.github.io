---
layout: post
title: 'The Mathematical Relationship Between Layer Normalization and Dynamic Activation Functions'
date: 2025-03-27
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/6822b56158cb8da5c8ef8efd.png'
tags: 论文阅读
---

> 层归一化和动态激活函数之间的数学关系.

- paper：[The Mathematical Relationship Between Layer Normalization and Dynamic Activation Functions](https://arxiv.org/abs/2503.21708)

语言模型中常用的归一化函数是**LayerNorm**和**RMSNorm**。[**LayerNorm**](https://0809zheng.github.io/2020/03/04/normalization.html#3%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96-layer-normalization)针对每个训练样本计算所有特征的均值和方差，对输入归一化后进行**re-scaling**和**re-shifting**。[**RMSNorm**](https://0809zheng.github.io/2020/03/04/normalization.html#-root-mean-square-layer-normalization-rmsnorm)去掉了**LayerNorm**中的均值计算和**re-shifting**操作，进一步减少了计算负担。本文从梯度近似的角度设计了**RMSNorm**归一化的替代函数**DyISRU**，从而在网络中去掉了归一化层。

## 1. 分析RMSNorm的梯度

**RMSNorm**的公式如下：

$$
\mathbf{y} = \frac{\mathbf{x}}{||\mathbf{x}||/\sqrt{d}} = \sqrt{d}\frac{\mathbf{x}}{||\mathbf{x}||}
$$

其中$\mathbf{x} \in R^d$，$\|\mathbf{x}\|$表示$\mathbf{x}$的**L2**范数:

$$
||\mathbf{x}|| = \sqrt{\sum_{i=1}^d x_i^2}
$$

**L2**范数的导数可以通过链式法则求解：

$$
\begin{aligned}
\nabla_{\mathbf{x}} \|\mathbf{x}\| &= \nabla_{\mathbf{x}} \left(\sqrt{\mathbf{x}^\top \mathbf{x}}\right) \\
&= \frac{1}{2\sqrt{\mathbf{x}^\top \mathbf{x}}} \nabla_{\mathbf{x}} \left(\mathbf{x}^\top \mathbf{x}\right) \\
&= \frac{\mathbf{x}}{\|\mathbf{x}\|}
\end{aligned}
$$

进而有：

$$
\begin{aligned}
\nabla_{\mathbf{x}} \frac{\mathbf{x}}{||\mathbf{x}||} &= \frac{||\mathbf{x}|| - \mathbf{x} \cdot \frac{\mathbf{x}}{\|\mathbf{x}\|}}{||\mathbf{x}||^2} \\
&= \frac{I}{||\mathbf{x}||} - \frac{\mathbf{x} \mathbf{x}^\top}{||\mathbf{x}||^3}
\end{aligned}
$$

则**RMSNorm**的梯度可以表示为：

$$
\begin{aligned}
\nabla_{\mathbf{x}} \mathbf{y} &= \sqrt{d}\nabla_{\mathbf{x}} \frac{\mathbf{x}}{||\mathbf{x}||} \\
&= \sqrt{d}\left( \frac{I}{||\mathbf{x}||} - \frac{\mathbf{x} \mathbf{x}^\top}{||\mathbf{x}||^3} \right) \\
&= \frac{\sqrt{d}}{||\mathbf{x}||}\left( I - \frac{\mathbf{y} \mathbf{y}^\top}{d} \right)
\end{aligned}
$$

## 2. 寻找RMSNorm的替代

现在的目标的寻找一个函数$\mathbf{y}=f(\mathbf{x})$，若该函数能够近似**RMSNorm**的梯度，则$f$能够替代归一化层的使用，从而实现在网络中去掉归一化层的目标。

假设$\mathbf{y}=f(\mathbf{x})$是逐元素操作，即$y_i=f(x_i)$，则$f$的梯度需满足：

$$
\frac{d y_i}{d x_i} = \rho\left( 1 - \frac{y_i^2}{d} \right)
$$

其中$\rho=\sqrt{d} / \|\|\mathbf{x}\|\|$。若假设$\rho$为常数，直接求解上述微分方程得到：

$$
y_i = \sqrt{d} \tanh \left( \frac{x_i}{\rho \sqrt{d}} \right)
$$

因此取$f(x) \sim \tanh(x)$，可以实现**RMSNorm**的替代，这便是[**Dynamic Tanh**](https://0809zheng.github.io/2025/03/13/dyt.html)的结论。

## 3. DyISRU函数

注意到：

$$
\rho=\frac{\sqrt{d}}{||\mathbf{x}||}= \frac{\mathbf{y}}{\mathbf{x}} = \frac{y_i}{x_i}
$$

则$f$的梯度需满足：

$$
\frac{d y_i}{d x_i}  = \frac{y_i}{x_i} \left( 1 - \frac{y_i^2}{d} \right)
$$

求解上述微分方程得到：

$$
y_i = \frac{\sqrt{d} x_i}{\sqrt{x_i^2+C}} 
$$

其中$C$为常数。上式形如[**Inverse Square Root Unit (ISRU)**](https://0809zheng.github.io/2021/11/01/lsrlu.html)，因此被称为**DyISRU**。