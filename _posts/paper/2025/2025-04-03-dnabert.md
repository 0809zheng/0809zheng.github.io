---
layout: post
title: 'DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome'
date: 2025-04-03
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/6808ad1f58cb8da5c8c5d105.png'
tags: 论文阅读
---

> DNABERT：基因组DNA序列的预训练双向编码表示.

- paper：[DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome](https://doi.org/10.1093/bioinformatics/btab083)

# 0. TL; DR

**DNABERT**是一种基于**Transformer**架构的预训练语言模型，专门用于解析**DNA**序列中的基因调控信息。通过预训练和微调，**DNABERT**能够在多种基因组学任务中实现卓越的性能，包括预测启动子、转录因子结合位点和剪接位点等。此外，**DNABERT**还提供了可视化工具**DNABERT-viz**，用于解释模型的决策过程和识别重要的**DNA**序列模式。该模型不仅在性能上超越了现有的工具，还展示了在跨物种应用中的潜力。

# 1. 背景介绍

**DNA**序列中隐藏着丰富的基因调控信息，这些信息对于理解基因表达的调控机制至关重要。然而，由于基因调控代码的高度复杂性，尤其是存在多义性和远距离语义关系，传统的生物信息学方法往往难以有效捕捉这些信息。近年来，随着深度学习技术的发展，卷积神经网络（**CNN**）和循环神经网络（**RNN**）被广泛应用于基因组学研究中，但它们在处理长距离依赖关系和数据稀缺场景时仍存在局限性。

为了克服这些挑战，研究者们开始探索将自然语言处理（**NLP**）中的**Transformer**架构应用于**DNA**序列分析。**Transformer**模型通过自注意力机制能够全局地捕捉上下文信息，从而更好地处理**DNA**序列中的复杂语义关系。**DNABERT**正是基于这种思想开发的，它通过预训练和微调的方式，为基因组学研究提供了一种强大的工具。

# 2. DNABERT 模型

**DNABERT**模型基于**Transformer**架构，采用双向编码器表示（**BERT**）来捕捉**DNA**序列的全局上下文信息。模型的输入是一组**k-mer**标记化的**DNA**序列，每个序列被嵌入为一个数值矩阵$M$。**DNABERT**通过多头自注意力机制处理$M$，具体公式如下：

$$
\text{MultiHead}(M) = \text{Concat}(\text{head}_1, \dots, \text{head}_h)W^O
$$

其中，

$$
\text{head}_i = \text{softmax}\left(\frac{M W^Q_i (M W^K_i)^T}{\sqrt{d_k}}\right) M W^V_i
$$

这里，$W^Q_i, W^K_i, W^V_i, W^O$是可学习的线性投影参数，$d_k$是缩放因子，$h$是注意力头的数量。**DNABERT**模型包含12个**Transformer**层，每层有768个隐藏单元和12个注意力头。

![](https://pic1.imgdb.cn/item/6808ae9658cb8da5c8c5d803.png)

**DNABERT**的训练过程分为预训练和微调两个阶段。在预训练阶段，模型通过自监督学习从大量未标记的**DNA**序列中学习基本的语法和语义信息。具体来说，**DNABERT**随机掩码序列中15%的**k-mer**标记，并让模型预测这些掩码标记的内容。预训练的目标是最小化交叉熵损失：

$$
L = \sum_{i=1}^N -y_i \log(\hat{y}_i)
$$

其中，$y_i$是真实标记，$\hat{y}_i$是模型预测的概率。

在微调阶段，**DNABERT**使用特定任务的标记数据进行训练，以适应不同的下游任务，如启动子预测、转录因子结合位点预测和剪接位点预测等。微调过程中，模型的参数会根据任务的具体需求进行调整，以达到最佳性能。

![](https://pic1.imgdb.cn/item/6808af8458cb8da5c8c5dc73.png)

为了更好地捕捉**DNA**序列的上下文信息，**DNABERT**采用了**k-mer**标记化方法。具体来说，**DNA**序列被分割成长度为k的子序列（**k-mer**），每个**k-mer**被视为一个独立的标记。例如，对于k=3，**DNA**序列“**ATGGCT**”会被标记化为“**ATG, TGG, GGC, GCT**”。**DNABERT**模型支持$k=3、4、5、6$四种不同的标记化方式，每种方式都有其对应的词汇表。

为了提高模型的可解释性，**DNABERT**开发了**DNABERT-viz**工具，用于可视化模型的注意力机制和上下文关系。通过**DNABERT-viz**，研究者可以直观地观察到模型在预测过程中关注的**DNA**序列区域，以及这些区域之间的语义关系。这不仅有助于理解模型的决策过程，还可以识别出潜在的功能性**DNA**序列模式。

![](https://pic1.imgdb.cn/item/6808b1b458cb8da5c8c5f4ca.png)

# 3. 实验分析

启动子预测是基因组学中的一个重要任务，**DNABERT**在这一任务上表现出色。研究者使用**DNABERT-Prom**模型对人类**TATA**和非**TATA**启动子进行了预测，并与现有的工具如**DeePromoter、PromID、FPROM**和**FirstEF**进行了比较。实验结果表明，**DNABERT-Prom**在准确率、F1分数和马修斯相关系数（**MCC**）等指标上均优于其他工具。

![](https://pic1.imgdb.cn/item/6808b07758cb8da5c8c5e364.png)

**DNABERT-TF**模型用于预测转录因子结合位点（**TFBS**），并使用**ENCODE**数据库中的690个转录因子**ChIP-seq**数据集进行了评估。实验结果表明，**DNABERT-TF**在准确率和F1分数上均优于现有的工具，如**DeepBind、DeepSEA、Basset、DeepSite、DanQ**和**DESSO**。**DNABERT-TF**不仅在高质量数据上表现出色，即使在低质量数据上也能保持较高的召回率。

![](https://pic1.imgdb.cn/item/6808b0b758cb8da5c8c5e706.png)


剪接位点预测对于理解基因结构和可变剪接机制至关重要。**DNABERT-Splice**模型在这一任务上也取得了显著的性能提升。研究者使用包含供体、受体和非剪接位点的基准数据集进行了评估，**DNABERT-Splice**在准确率、F1分数和MCC等指标上均优于其他基线模型，如**SpliceFinder**。

![](https://pic1.imgdb.cn/item/6808b10158cb8da5c8c5eb99.png)


**DNABERT**还被应用于识别功能性遗传变异。研究者使用**dbSNP**数据库中的约7亿个短变异进行了分析，发现位于**DNABERT**预测的高注意力区域内的变异在**ClinVar、GRASP**和**NHGRI-EBI GWAS Catalog**等数据库中的比例较高。这表明**DNABERT**能够有效地识别出可能影响基因调控的变异。

![](https://pic1.imgdb.cn/item/6808b17b58cb8da5c8c5f281.png)

预训练是**DNABERT**成功的关键因素之一。研究者比较了预训练的**DNABERT**和随机初始化的**DNABERT**在启动子预测任务上的表现，发现预训练的**DNABERT**能够更快地收敛到更低的损失值。此外，预训练的**DNABERT**在跨物种应用中也表现出色，例如在小鼠**ENCODE**数据集上的表现优于其他基线模型。

![](https://pic1.imgdb.cn/item/6808b19958cb8da5c8c5f3df.png)