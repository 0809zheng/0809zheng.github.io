---
layout: post
title: 'The Curse of Depth in Large Language Models'
date: 2025-02-09
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67e50d900ba3d5a1d7e51417.png'
tags: 论文阅读
---

> 大语言模型中的深度诅咒.

- paper：[The Curse of Depth in Large Language Models](https://arxiv.org/abs/2502.05795)

# 0. TL; DR

本文提出了大语言模型（**LLMs**）中的“深度诅咒”（**Curse of Depth**）现象，即现代**LLMs**中近一半的深度层（**Transformer**块）比预期的贡献小。研究通过理论和实证分析，发现这一现象的根本原因是广泛使用的**Pre-Layer Normalization（Pre-LN）**。**Pre-LN**虽然稳定了**Transformer LLMs**的训练，但其输出方差随模型深度指数增长，导致深度**Transformer**块的导数接近单位矩阵，几乎不贡献于训练。

为解决这一训练困境，文章提出了**LayerNorm Scaling**技术，通过按深度的平方根对**Layer Normalization**的输出进行缩放，有效抑制了深度层输出方差的爆炸性增长，提高了深度层的贡献。实验结果表明，**LayerNorm Scaling**显著提升了**LLMs**的预训练性能，并且这种提升无缝地延续到监督微调阶段。

# 1. 背景介绍

近年来，大语言模型（**LLMs**）在自然语言处理领域取得了巨大成功，但研究发现这些模型的深度层往往不如预期有效。这一现象不仅为模型压缩提供了机会，更揭示了训练过程中的不足。现代**LLMs**训练资源密集，通常需要数千个**GPU**训练数月，而大量层未能有效训练意味着资源的浪费。因此，解决这一问题对于提高**LLMs**的效率和性能至关重要。

# 2. LayerNorm Scaling 技术

深度诅咒指的是**LLMs**中深度层在学习和表示上的贡献显著低于早期层的现象。这些深度层对剪枝和扰动表现出显著的鲁棒性，表明它们未能执行有意义的转换，导致资源效率低下。

![](https://pic1.imgdb.cn/item/67e510f30ba3d5a1d7e5156b.png)

**Pre-LN**在训练**Transformer LLMs**时虽然稳定，但其输出方差随模型深度指数增长。这导致深度**Pre-LN**层的导数接近单位矩阵，使得这些层在训练中几乎不贡献于表示学习。具体来说，**Pre-LN**的输出方差在深度层中显著累积，导致深度层的梯度几乎为恒等映射，限制了模型的表达能力和学习能力。

![](https://pic1.imgdb.cn/item/67e50eb60ba3d5a1d7e5146d.png)

**Pre-LN**的形式如下：

$$
x_{t+1} = x_t + F_t(\text{LayerNorm}(x_t))
$$

**Pre-LN**可以展开为：

$$
\begin{aligned}
x_{t+1} &= x_t + F_t(\text{LayerNorm}(x_t)) \\
&= x_{t-1} + F_{t-1}(\text{LayerNorm}(x_{t-1})) + F_t(\text{LayerNorm}(x_t))\\
&= \cdots \\
&= x_0 + \sum_{i=0}^t F_i(\text{LayerNorm}(x_i)) \\
\end{aligned}
$$

假设$x_0,...,x_i$的方差都为$1$，且相互独立；此时$x_l$的方差为$l$。

**LayerNorm Scaling**通过按深度的平方根对**Layer Normalization**的输出进行缩放，有效控制了深度层输出方差的增长。具体来说，对于一个具有$L$层的**Transformer**模型，**LayerNorm Scaling**将每一层$l$的输出进行缩放:

$$
x_l = \text{LayerNorm}(x_l) \cdot \frac{1}{\sqrt{l}}
$$

![](https://pic1.imgdb.cn/item/67e5104d0ba3d5a1d7e51533.png)

这种缩放方式不仅抑制了输出方差的爆炸性增长，还提高了深度层在训练中的贡献，确保了所有层都能有效地参与学习。

# 3. 实验分析

实验使用了基于**LLaMA**架构的不同大小模型（从**130M**到**1B**参数），比较了**Post-LN、DeepNorm、Pre-LN**和**LayerNorm Scaling**的性能。结果表明，**LayerNorm Scaling**在所有模型大小上均优于其他归一化技术，显著降低了困惑度，表明其在预训练阶段就展现出了更好的性能。

![](https://pic1.imgdb.cn/item/67e510760ba3d5a1d7e51544.png)

在监督微调阶段，**LayerNorm Scaling**预训练的模型在多个下游任务上的表现优于使用**Pre-LN**和**Post-LN**的模型。这表明**LayerNorm Scaling**训练的深度层能够捕捉到更丰富和多样化的特征，从而在复杂任务中实现更好的泛化能力。

![](https://pic1.imgdb.cn/item/67e510b00ba3d5a1d7e51552.png)

通过层剪枝实验，**LayerNorm Scaling**显著提高了深度层的有效性。实验表明，移除深度层会导致更大的性能下降，表明这些层在**LayerNorm Scaling**下对模型的贡献更大。

![](https://pic1.imgdb.cn/item/67e511200ba3d5a1d7e51578.png)