---
layout: post
title: 'Interventional Bag Multi-Instance Learning On Whole-Slide Pathological Images'
date: 2025-10-23
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/690306823203f7be00b4c050.png'
tags: 论文阅读
---

> 基于全切片病理图像的干预式包多实例学习.

- paper：[Interventional Bag Multi-Instance Learning On Whole-Slide Pathological Images](https://arxiv.org/abs/2303.06873)


# 0. TL; DR

这篇论文提出了一种名为**干预式包多实例学习（Interventional Bag Multi-Instance Learning, IBMIL）**的训练范式。将共享于包级别但与标签无直接因果关系的包上下文视为混杂因子$C$ **(Confounder)**。正常训练一个**MIL**模型；然后对所有训练集的包级别特征进行**K-means**聚类，每个聚类中心被视为一个具体的混杂因子状态$c_i$。通过一个注意力机制，计算当前**WSI**的包特征与所有混杂因子原型的相似度，然后将这些混杂因子加权融合到原始包特征中，再进行最终预测。

**IBMIL**作为一个通用的、正交的训练阶段，可以被无缝地应用在任何现有的**MIL**模型之后。在**Camelyon16**和**TCGA-NSCLC**两大公开数据集上，**IBMIL**为所有基线模型都带来了稳定且显著的性能提升。

# 1. 背景介绍

在利用多实例学习（**MIL**）处理全切片病理图像（**WSI**）时，**包上下文先验（bag contextual prior）**是指在一个**WSI**中普遍存在，但与关心的诊断标签没有直接因果关系的特征。比如：
-   **染色偏差**: 由于不同批次或不同医院的染色习惯，可能A医院的阳性切片普遍偏粉色，而B医院的阴性切片普遍偏紫色。
-   **组织来源**: 来自不同器官的组织，其背景结构天然不同。
-   **扫描仪伪影**: 不同的扫描设备可能引入独特的噪声或伪影模式。

当这些“上下文”特征与标签在训练数据中频繁共现时，一个强大的深度学习模型很容易学会虚假的关联**（spurious correlation）**。它可能不再费力地去寻找微小的癌细胞，而是简单地判断只要是粉色的就是阳性。一旦遇到一个不符合它所学偏见的测试样本（例如一个“紫色”的阳性切片），模型就会彻底失效。

![](https://pic1.imgdb.cn/item/690308723203f7be00b4c52b.png)


为了从根本上解决这个问题，作者引入了**结构因果模型（Structural Causal Model, SCM）**来进行分析。


$X$ (**Bag**)为 **WSI**本身。$Y$ (**Label**)是诊断标签。$C$ (**Confounder**)是包上下文先验的混杂因子。: $C$作为一个混杂因子，同时指向$X$（决定了**WSI**的外观）和$Y$（通过数据集偏差与标签产生关联）。这在$X$和$Y$之间打开了一条**后门路径（backdoor path）**$X$ <- $C$ -> $Y$。

![](https://pic1.imgdb.cn/item/690308e13203f7be00b4c5e8.png)

传统**MIL**方法学习的是关联概率 $P(Y\|X)$，它无法关闭这条后门路径，因此必然会受到混杂偏见的影响。因果推断的目标是学习真正的因果效应 $P(Y\|do(X))$。$do(X)$操作相当于在因果图上强行切断$C$指向$X$的箭头，从而消除$C$对$X$的影响，关闭后门路径。

# 2. IBMIL 模型

**IBMIL**在**MIL**的“特征提取+聚合器训练”两阶段之后加入了第三阶段：因果干预训练。

![](https://pic1.imgdb.cn/item/690309d33203f7be00b4c757.png)

阶段一+阶段二与现有的主流方法完全一致：使用自监督或有监督的方式，训练一个强大的**CNN**或**Transformer**来提取每个**patch**的特征。固定特征提取器，训练一个**MIL**聚合器（如**ABMIL, TransMIL**等），使其能够根据实例特征聚合出包特征，并进行分类。

因果干预训练的目标是利用**后门调整（backdoor adjustment）**公式，来近似计算因果效应$P(Y\|do(X))$。后门调整的理论公式为：

$$ P(Y | do(X)) = \sum_c P(Y | X, C=c) P(c) $$

这个公式的直观解释是：要想知道$X$对$Y$的真实因果效应，需要穷尽所有可能的混杂因子状态$c$，在每个状态$c$下观察$P(Y\|X)$，然后根据$c$的先验概率$P(c)$进行加权平均。

要在实践中实现它，需要解决两个问题：如何**得到混杂因子$C$的各种状态**？以及如何**高效地进行求和**？

**IBMIL**通过无监督聚类近似混杂因子$C$。对于训练好的**MIL**模型，其输出的包级别特征$B$已经隐式地编码了各种“包上下文”信息。使用阶段二训练好的模型，计算出整个训练集中所有**WSI**的包特征。对这些包特征进行**K-means**聚类得到$K$个聚类中心。这$K$个聚类中心被用作对离散化的混杂因子状态${c_1, c_2, ..., c_K}$的近似。每一个$c_i$可以被理解为一个“原型上下文”，比如“深染风格”、“浅染风格”、“多淋巴细胞背景”等。所有这些$c_i$构成了**混杂因子字典（Confounder Dictionary）**$C$。这种方法是无监督的，不依赖于类别标签，因此可以利用海量的无标签数据来构建更丰富的混杂因子字典。

计算$P(Y\|X, C=c)$表示在给定包$X$和特定混杂$c$的条件下，$Y$的概率。**IBMIL**将其建模为：

$$ P(Y | X, C=c) \approx P(Y | B \oplus h(X, c)) $$

其中，$B$是$X$的原始包特征，$h(X,c)$是通过一个注意力机制计算出的、$c$对$X$的修正特征，$⊕$表示向量拼接。

直接对所有$c$进行求和计算量巨大。作者采用了一种归一化加权几何平均**（Normalized Weighted Geometric Mean）**的技巧，将求和操作移入**softmax**内部，从而将多次前向传播简化为一次。最终后门调整公式被近似为：

$$ P(Y | do(X)) \approx P\left( Y \bigg| B \oplus \sum_{i=1}^K \alpha_i c_i P(c_i) \right) $$

其中，$α_i$是通过一个注意力网络计算出的、包$B$与每个混杂因子$c_i$的相似度权重。

通过上述步骤，**IBMIL**构建了一个新的、可端到端训练的“干预网络”，其输出近似于真实的因果效应$P(Y\|do(X))$，从而能够在预测时消除混杂偏见。

# 3， 实验分析

**IBMIL**为所有测试的**SOTA MIL**方法（**ABMIL, DSMIL, TransMIL, DTFD-MIL**），在所有测试的特征提取器（**ResNet-18, ViT, CTransPath**）上，都在两个数据集（**Camelyon16, TCGA-NSCLC**）上带来了稳定且显著的性能提升。**IBMIL**的成功不依赖于特定的聚合器或特征提取器，证明了它是一个正交的、即插即用的通用增强模块。

在使用较弱的**ImageNet**预训练**ResNet-18**时，**IBMIL**带来的性能提升最大（平均**AUC**提升高达5.4%）。这说明，当特征本身包含较多与数据集相关的偏见时，**IBMIL**的去混杂能力能发挥最大作用。在阳性实例稀疏的**Camelyon16**上，**IBMIL**的提升效果比在相对均衡的**TCGA-NSCLC**上更显著。这说明，当模型更难学习到真正的因果信号时，它就更容易依赖虚假关联，此时因果干预的价值就更大。

![](https://pic1.imgdb.cn/item/690329543203f7be00b4fd35.png)

实验表明，模型性能对混杂因子字典大小$K$和特征空间的维度的取值不敏感，在一个很宽的范围内都能带来稳定的性能提升。这降低了调参的难度。即使省略掉“训练聚合器”这一步，直接用简单的**max/mean-pooling**来生成包特征并构建混杂因子字典，**IBMIL**依然能为基线模型带来显著提升，甚至效果与三阶段版本相当。这表明**IBMIL**可以被简化为一个更高效的框架，而无需预先训练一个复杂的聚合器。

![](https://pic1.imgdb.cn/item/69032adb3203f7be00b4ff8a.png)

