---
layout: post
title: 'Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning'
date: 2025-10-09
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/68f0a14bc5157e1a8878d41b.png'
tags: 论文阅读
---

> 具有自监督对比学习的双流多实例学习网络用于全切片图像分类.

- paper：[Dual-stream Multiple Instance Learning Network for Whole Slide Image Classification with Self-supervised Contrastive Learning](https://arxiv.org/abs/2011.08939)


# 0. TL; DR

这篇论文提出了**双流多实例学习网络（Dual-stream Multiple Instance Learning Network, DSMIL）**。**DSMIL**采用双流并行的聚合器：一个流通过**max-pooling**寻找关键实例；另一个流通过自注意力机制进行加权聚合。模型采用自监督对比学习训练，并引入了金字塔式的特征融合策略。

在**Camelyon16**和**TCGA**肺癌两大**WSI**数据集上，**DSMIL**全面超越了所有基于**MIL**的先前方法。在仅使用弱监督**（slide-level）**标签的情况下，**DSMIL**的分类性能与使用全监督**（pixel-level）**标签训练的方法**差距不足2%**。在肿瘤定位任务上，**DSMIL**同样表现卓越。

# 1. 背景介绍

数字病理学中的**WSI**具有超高分辨率 (**Gigapixel**，10万x10万像素)，通用的处理范式是“切片-分类”（**patch-based**），即将**WSI**分割成成千上万个小图块（**patches**）。通常只能获取到切片级别的标签，即整张切片是“阳性”还是“阴性”。此外在一张阳性**WSI**中，真正的癌变区域可能只占整个组织的不到10%。这意味着，一个“阳性包”里，绝大多数的“实例”（**patches**）其实是阴性的。

**max-pooling**是**多实例学习（MIL）**中最常用的聚合器，它只关注那个得分最高的实例。在**WSI**中，这会导致模型只学习如何识别“最典型”的癌细胞，而忽略了肿瘤区域内其他同样重要但形态各异的细胞，以及周围的微环境信息。这可能导致决策边界的偏移，降低泛化能力。本文只在设计一个既能捕捉实例间复杂关系、又能高效学习高质量**WSI**特征的**MIL**模型。

![](https://pic1.imgdb.cn/item/68f0a46ec5157e1a8878d679.png)

# 2. DSMIL 模型

**DSMIL**建立在三个相互关联的核心组件之上：双流**MIL**聚合器、自监督对比学习策略，以及多尺度融合机制。

![](https://pic1.imgdb.cn/item/68f0a591c5157e1a8878db75.png)

## 2.1 双流MIL聚合器

假设一个包$B$包含$n$个实例，每个实例的嵌入表示为$h_i$。**DSMIL**采用**双流（Dual-stream）**架构。

![](https://pic1.imgdb.cn/item/68f0a6aac5157e1a8878e18e.png)

### 流一：关键实例流 (Critical Instance Stream)

这条路径能够快速、直接地定位出包内得分最高的“关键实例”。一个实例分类器（线性层$W_0$）为每个实例$h_i$计算一个分数。对所有实例分数进行**max-pooling**，得到这个流的包分数$c_m(B)$，以及得分最高的那个关键实例的嵌入$h_m$。

$$ c_m(B) = \max_i \{ W_0 h_i \} $$

### 流二：非局部上下文流 (Non-local Context Stream)

这条路径能够建模所有实例与“关键实例”之间的关系，捕捉上下文信息。将每个实例$h_i$（包括关键实例$h_m$）分别通过两个线性层$W_q$和$W_v$，得到查询向量$q_i$和信息向量$v_i$。

$$ q_i = W_q h_i, \quad v_i = W_v h_i $$

计算每个实例的查询向量$q_i$与关键实例的查询向量$q_m$之间的内积相似度，并通过**softmax**归一化，得到注意力权重$U(h_i, h_m)$。

$$ U(h_i, h_m) = \frac{\exp(\langle q_i, q_m \rangle)}{\sum_{k=0}^{N-1} \exp(\langle q_k, q_m \rangle)} $$

使用计算出的注意力权重，对所有实例的信息向量$v_i$进行加权求和，得到最终的包嵌入$b$。

$$ b = \sum_{i=0}^{N-1} U(h_i, h_m) v_i $$

用一个线性层$W_b$对包嵌入$b$进行分类，得到这个流的包分数$c_b(B)$。

最终两个流的输出被简单地平均，得到最终的包预测分数：

$$ c(B) = \frac{1}{2} (c_m(B) + c_b(B)) $$

## 2.2 自监督对比学习

**DSMIL**使用了**自监督对比学习**。将训练集中的所有**WSI**切片成大量无标签的**patches**。从一个**patch**中随机裁剪出两个视图（**view**），并进行数据增强（如旋转、色彩抖动）。模型的目标是在潜在空间中，拉近来自同一个**patch**的两个视图的距离（正样本对），同时推远来自不同**patch**的视图的距离（负样本对）。训练收敛后，丢弃最后的投影头，保留**CNN**主干网络作为固定的特征提取器。


## 2.3 金字塔式多尺度融合

**DSMIL**设计了一种金字塔式的**多尺度特征融合机制**。比如有10x和5x两种放大倍率的特征，一个5x的**patch**在空间上会包含若干个10x的**patches**。将这个5x **patch**的特征向量，复制并拼接到它包含的每一个10x **patch**的特征向量上。这样每个高倍镜的实例特征，就同时携带了自身的细节信息（来自10x）和其所处的更宏观的上下文信息（来自5x）。

![](https://pic1.imgdb.cn/item/68f0a983c5157e1a8878ec12.png)

这种做法为模型提供了更丰富的、跨尺度的信息；并且在注意力计算时，由于同一低倍镜区域下的高倍镜实例共享了相同的前半部分特征，它们的注意力权重会天然地受到一种局部约束，使得注意力分配更加合理和平滑。

# 3. 实验分析

**Camelyon16**是一个正负实例极不平衡的乳腺癌转移检测数据集。在该数据集上，**DSMIL-LC**（多尺度版本）的分类准确率（89.92%）和AUC（91.65%）显著优于所有其他**MIL**方法，与全监督方法的差距极小。这证明了在极不平衡的数据上，**DSMIL**的双流设计能够有效避免只关注少数几个实例而导致的性能下降，且自监督预训练弥补了弱监督信号的不足。

![](https://pic1.imgdb.cn/item/68f0aa61c5157e1a8878ec56.png)

在肿瘤定位任务中，**DSMIL**的**FROC**分数也优于其他**MIL**方法，其生成的热力图与真实肿瘤区域高度吻合。

![](https://pic1.imgdb.cn/item/68f0aa88c5157e1a8878ec5a.png)

**TCGA**是一个更均衡的肺癌数据集，包含两种肺癌亚型。在该数据集上，**DSMIL-LC**同样取得了最佳的分类性能（准确率95.71%，AUC 98.15%），超越了其他方法。这说明即使在数据相对均衡的情况下，**DSMIL**对实例间关系的精细建模依然能带来性能优势。

![](https://pic1.imgdb.cn/item/68f0aaeec5157e1a8878ec73.png)

作者进行了一系列消融研究：
- 对比学习的贡献: 在**Camelyon16**上，使用自监督对比学习特征的模型，准确率比使用**ImageNet**预训练特征的模型高出**超过24%**（86.82% vs 62.02%），比用**max-pooling**端到端训练的特征高出约16%。这说明自监督对比学习是获得高质量**WSI**特征的关键。
- 多尺度融合的贡献: **DSMIL**的金字塔式融合方法，准确率比简单的拼接或混合多尺度特征的方法高出至少1.5%。这说明精心设计的多尺度融合策略能有效提升性能。

![](https://pic1.imgdb.cn/item/68f0ab4ac5157e1a8878ec95.png)

在与**WSI**无关的经典**MIL**基准数据集（**Musk, Fox**等）上，**DSMIL**聚合器也取得了全面的**SOTA**性能，平均准确率比**GNN-MIL、DP-MINN**等近期方法高出约3%。说明**DSMIL**聚合器本身就是一个通用且强大的**MIL**解决方案。

![](https://pic1.imgdb.cn/item/68f0ab76c5157e1a8878eca5.png)