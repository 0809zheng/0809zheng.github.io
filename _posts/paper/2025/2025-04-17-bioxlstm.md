---
layout: post
title: 'Bio-xLSTM: Generative modeling, representation and in-context learning of biological and chemical sequences'
date: 2025-04-17
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/6819c20158cb8da5c8dfdb6a.png'
tags: 论文阅读
---

> Bio-xLSTM: 生物和化学序列的生成式建模、表示与上下文学习.

- paper：[Bio-xLSTM: Generative modeling, representation and in-context learning of biological and chemical sequences](https://arxiv.org/abs/2411.04165)

# 0. TL; DR
本文提出了 **Bio-xLSTM**，这是一个基于 **xLSTM** 架构的模型套件，专门用于生物和化学序列的建模。**Bio-xLSTM** 在 **DNA**、蛋白质和化学序列的生成建模、表示学习以及上下文学习方面表现出色，能够处理长序列并捕捉长程依赖关系，为药物发现、蛋白质工程和精准医学等领域提供了强大的工具。

# 1. 背景介绍

生物序列（如 **DNA**、**RNA** 和蛋白质）以及化学序列（如小分子的 **SMILES** 表示）是分子生物学、基因组学和药物发现的核心。准确的计算模型能够将这些序列数据转化为可操作的见解，例如预测基因变异的表型、设计新型蛋白质以及发现潜在的药物分子。近年来，随着大规模数据收集项目的推进（如人类基因组计划和蛋白质结构数据库 **UniProt**），如何从这些数据中提取有用信息成为关键问题。

目前，生物和化学序列的语言模型大多基于 **Transformer** 架构。尽管 **Transformer** 在自然语言处理中取得了巨大成功，但其自注意力机制的时间复杂度为序列长度的二次方，这使得其在处理长序列（如人类基因组）时面临计算瓶颈。此外，生物和化学序列通常需要长程上下文信息来准确建模，例如蛋白质的三维折叠和基因调控。因此，开发能够高效处理长序列的模型是当前研究的热点。

**xLSTM** 是一种新型的循环神经网络架构，具有线性的时间复杂度和常数级的内存需求，能够有效处理长序列。它通过引入增强的记忆结构和指数门控机制，在自然语言处理任务中表现优异。本文将 **xLSTM** 架构扩展到生物和化学领域，提出了 **Bio-xLSTM** 模型套件，包括针对 **DNA**、蛋白质和化学序列的特定变体。

# 2. Bio-xLSTM 模型

**xLSTM** 架构由 **sLSTM** 和 **mLSTM** 两种类型的层组成，它们被集成到更大的残差网络中。**sLSTM** 和 **mLSTM** 都是循环神经网络，通过输入、输出和遗忘门控制信息流动，并利用细胞状态和归一化状态来维护长期记忆。

**sLSTM** 的前向传播公式如下：

![](https://pic1.imgdb.cn/item/6819c49358cb8da5c8dfdeae.png)

**mLSTM** 的前向传播公式如下：

![](https://pic1.imgdb.cn/item/6819c4a358cb8da5c8dfdeb3.png)


**Bio-xLSTM** 包括针对不同生物和化学序列的特定变体：
- **DNA-xLSTM** 专门用于 **DNA** 序列建模，引入了反向互补（**RC**）等变性块，能够处理长上下文。其配置包括：
1. DNA-xLSTM-500k：500k 参数，128 维嵌入，5 个 sLSTM 块，上下文长度为 1,024。
2. DNA-xLSTM-2M：2M 参数，256 维嵌入，6 个 sLSTM 块，上下文长度为 1,024。
3. DNA-xLSTM-4M：4M 参数，256 维嵌入，9 个 mLSTM 块，上下文长度为 32,768。
- **Prot-xLSTM** 用于蛋白质序列建模，支持同源性感知的蛋白质语言建模和上下文学习。其配置包括：
1. Prot-xLSTM-26M：26M 参数，512 维嵌入，16 个 mLSTM 块。
2. Prot-xLSTM-102M：102M 参数，1,024 维嵌入，16 个 mLSTM 块。
- **Chem-xLSTM** 用于化学序列建模，能够生成小分子的 **SMILES** 表示，并支持上下文学习。其配置包括：
1. Chem-xLSTM-15M：15M 参数，512 维嵌入，9 个 mLSTM 块，上下文长度为 100。
2. Chem-xLSTM-15M-icl：15M 参数，512 维嵌入，9 个 mLSTM 块，上下文长度为 4,096。

![](https://pic1.imgdb.cn/item/6819c56958cb8da5c8dfdf12.png)

**Bio-xLSTM** 支持多种建模方法，包括因果语言建模（**CLM**）、掩码语言建模（**MLM**）和上下文学习（**ICL**）。这些方法能够捕捉序列的长程依赖关系，并利用上下文信息进行生成和预测。

![](https://pic1.imgdb.cn/item/6819c57a58cb8da5c8dfdf29.png)

# 3. 实验分析

## 3.1 DNA 序列实验

**DNA-xLSTM** 在人类基因组（**GRCh38**）上进行了预训练，使用因果语言建模（**CLM**）和掩码语言建模（**MLM**）任务。实验结果表明，**DNA-xLSTM** 在两种任务中均优于 基线模型，表明其在重建 **DNA** 序列方面表现优异。

![](https://pic1.imgdb.cn/item/6819c62e58cb8da5c8dfdf65.png)

在基因组分类任务中，**DNA-xLSTM** 在 18 个数据集中的 12 个上表现最佳，甚至在某些任务上超过了拥有 500M 参数的 **Nucleotide Transformer** 模型。这表明 **DNA-xLSTM** 能够学习到丰富的 **DNA** 序列表示。

![](https://pic1.imgdb.cn/item/6819c65358cb8da5c8dfdf71.png)

## 3.2 蛋白质序列实验

**Prot-xLSTM** 在 **OpenProteinSet** 数据集上进行了同源性感知训练，使用填充中间（**FIM**）策略。实验结果表明，**Prot-xLSTM** 在生成蛋白质序列和预测突变体适应性方面优于 **ProtMamba** 和 **Transformer** 模型。例如，在蛋白质生成任务中，**Prot-xLSTM-102M** 的困惑度最低，表明其生成的蛋白质序列与真实序列最为接近。

![](https://pic1.imgdb.cn/item/6819c67d58cb8da5c8dfdf7f.png)

## 3.3 化学序列实验

**Chem-xLSTM** 在 **ChEMBL** 数据集上进行了无条件分子生成任务。实验结果表明，**Chem-xLSTM** 在生成小分子的 **Fréchet ChemNet Distance（FCD）**指标上表现最佳，生成的分子与目标分布最为接近。

![](https://pic1.imgdb.cn/item/6819c73958cb8da5c8dfdfa1.png)

**Chem-xLSTM** 在分子域条件生成任务中表现出色，能够根据提供的上下文生成特定领域的分子。例如，在条件生成任务中，**Chem-xLSTM-15M-icl** 的困惑度最低，表明其能够生成与目标分子域最为接近的分子。

![](https://pic1.imgdb.cn/item/6819c75258cb8da5c8dfdfa3.png)