---
layout: post
title: 'Adaptive Pooling in Multi-Instance Learning for Web Video Annotation'
date: 2025-10-19
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/68ff25843203f7be00a6833d.png'
tags: 论文阅读
---

> 网页视频标注的多实例学习中的自适应池化.

- paper：[Adaptive Pooling in Multi-Instance Learning for Web Video Annotation](https://openaccess.thecvf.com/content_ICCV_2017_workshops/w5/html/Zhou_Adaptive_Pooling_in_ICCV_2017_paper.html)


# 0. TL; DR

这篇论文提出了一种**可学习自适应池化（Learnable Adaptive Pooling）**函数，并配合一个包+实例联合损失函数（**Bag+Instance Loss**）解决多实例学习（**MIL**）。池化函数基于**广义均值（Generalized Mean）**，其指数$r$是一个可学习的参数，可以为每个视频类别独立学习，并在训练过程中动态调整。

在两个大规模网络视频数据集（**FCVID, CCV**）上，搭载了可学习池化和联合损失的模型，其视频标注准确率（**mAP**）全面超越了当时的**SOTA**方法。

# 1. 背景介绍：

在**YouTube**等平台上，每天都有海量的视频被上传。这些视频内容丰富，但其标签（**tags**）通常是弱（**weakly**）的：用户可能只给视频打上几个笼统的标签，如“美食”、“旅行”；一个标签（如“狗”）可能只对应视频中很短的一个片段，并不知道它具体出现在何时何地。

这种只有视频级标签，没有帧级时间戳的特性，使得网络视频标注成为一个典型的**多实例学习（MIL）**问题：**包 (Bag)**是一整个视频。**实例 (Instance)**是视频中的每一帧（或关键帧）。标签仅在包（视频）级别提供。

在基于深度学习的**MIL**视频标注框架中，一个标准的流程是：
1.  用一个**CNN**模型独立地处理视频中的每一帧，得到每一帧对于所有可能标签的预测分数（或概率）。
2.  通过一个**池化函数（Pooling Function）**，将所有帧的预测分数“聚合”成一个视频级别的预测分数。
3.  用视频级别的预测分数和真实的视频标签计算损失，并进行反向传播。

池化函数不仅决定了如何从成百上千帧的信息中提炼出视频的最终标签，更关键的是，它决定了在反向传播时梯度如何从视频级别的损失分配回每一帧，从而指导**CNN**模型的学习。本文的出发点是能否设计一种可学习的、动态的池化函数，让模型自己学会如何在不同的视频类别和不同的训练阶段，采用最合适的聚合策略。

# 2. 自适应池化与联合损失

作者为**MIL**提出了一套完整的解决方案，包含一个新颖的池化函数和一个为之量身定制的损失函数。

## 2.1 可学习自适应池化 (Learnable Adaptive Pooling)

作者利用了**广义均值（Generalized Mean）**或**Lp范数**来构建池化函数。

对于一个视频$i$和一个类别$c$，所有帧的预测概率为$\{q_{ij}^c\}_{j=1}^{N_i}$。视频级别的预测概率$p_i^c$由以下公式计算：

$$ p_i^c = \left( \frac{1}{N_i} \sum_{j=1}^{N_i} (q_{ij}^c)^{r_c^{(t)}} \right)^{\frac{1}{r_c^{(t)}}} $$

指数$r$是一个可学习的参数。$r$是为每个类别$c$独立学习的，即$r_c$。$r$还是训练阶段$t$的函数，即$r_c^{(t)}$，它会随着训练的进行而变化。

当$r$很小 (如$r=1$)，退化为**算术平均值（Average Pooling）**。在训练早期，**CNN**的预测能力很弱，输出近乎随机。此时采用平均池化，可以有效地抑制随机噪声，让模型稳定学习。当$r$很大 (如$r -> ∞$)，趋近于**最大值（Max Pooling）**。在训练后期，**CNN**已经具备了较强的判别能力，能够准确地为关键帧打出高分。此时采用最大池化，可以让模型更多地关注那些高置信度的关键帧。

通过将$r$设为可学习参数，模型就获得了在**average**和**max**之间进行平滑、自适应过渡的能力。它会自动为不同特性的视频类别，以及在训练的不同阶段，找到最合适的聚合策略。

![](https://pic1.imgdb.cn/item/68ff28393203f7be00a687bf.png)

## 2.2 “包+实例”联合损失函数 (Bag+Instance Loss)

如果只用传统的视频级（包级）损失，监督信号可能太弱，不足以指导$r$的精细调整。为此作者设计了一个包+实例”联合损失函数。

包级别损失 ($ℓ_{ci}$)是标准的**MIL**损失，例如交叉熵，用于衡量视频级预测$p_{ci}$与真实标签$y_{ci}$之间的差距。

$$ \ell_{ci} = -y_{ci} \log(p_{ci}) - (1-y_{ci}) \log(1-p_{ci}) $$

“伪”实例级别损失 ($R^c_{ij}$): 由于没有真实的帧级别标签，作者提出用模型自身的预测来生成一个伪标签。

$$ R_{ij}^c = -u_{ij}^c \log(q_{ij}^c) - (1-u_{ij}^c) \log(1-q_{ij}^c) $$

其中，伪标签$u_{ij}^c$由一个简单的阈值法得到：$u_{ij}^c = \mathbb{I}(q_{ij}^c \ge 0.5)$。这个$R_{ij}^c$衡量了模型在实例级别预测的不确定性或置信度。

联合损失 ($L_{ci}$)不仅要最小化包级别损失，还要最小化包级别损失与平均实例级别损失之间的差距。

$$ L_i^c = \ell_{ci} + \lambda \left( \ell_{ci} - \frac{1}{N_i} \sum_{j=1}^{N_i} R_{ij}^c \right)^2 $$

通过要求“包级误差”和“平均实例级误差”尽可能接近，这个损失函数施加了一个强大的约束：化函数必须能够高效地将实例级别的信息（和不确定性）传递到包级别。这为学习最优的池化参数$r$提供了更直接、更丰富的监督信号，从而使得$r$的调整与**CNN**分类器的学习状态更加同步和协调。

如果一个视频被标记为负包（$y_{ci} = 0$），那么确切地知道其中所有帧都是负的。此时模型只使用实例级别的损失，直接优化所有$q_{ij}^c$趋近于0。

# 3. 实验分析

作者在**FCVID、CCV**和**UCF101**三个大规模视频数据集上对所提出的方法进行了验证。

在**FCVID**数据集上，仅将池化函数从固定的**AveP**或**MaxP**换成可学习的**LearnP**，**mAP**就能带来6-9个百分点的巨大提升（在**BL**损失下）。仅将损失函数从传统的包级损失**BL**换成联合损失**BIL**，**mAP**也能带来约5个百分点的提升（在使用**AveP**时）。当**LearnP**和**BIL**一起使用时，模型达到了最佳性能**78.5% mAP**。

![](https://pic1.imgdb.cn/item/68ff29f63203f7be00a68e3b.png)

学习曲线表明**LearnP + BIL**组合的收敛速度最快，性能也最高。**MaxP**在早期表现最差，说明在模型不确定时，**max-pooling**的“武断”决策是有害的。**AveP**在早期表现尚可，但后期乏力，说明其抑制噪声的能力在后期不再是主要矛盾。这直观地展示了自适应池化在不同阶段扮演不同角色的优势。

学习到的指数$r$分析，对于所有视频类别，学习到的$r$值都随着训练的进行而**单调递增**。这印证了作者的设计初衷：从早期的**average-like**平滑过渡到后期的**max-like**。

![](https://pic1.imgdb.cn/item/68ff2a6b3203f7be00a6903a.png)

作者进一步分析了最终学到的$r_c$值与每个类别视频内容特性的关系。他们发现，$r_c$与该类别视频帧间预测分数的平均标准差$ν_c$呈现出强正相关。如果一个视频类别的帧内容变化很大，那么$ν_c$就大，模型会为其学习一个较大的$r_c$，以便更关注那些关键帧。反之，如果一个类别的帧内容很相似，$ν_c$就小，模型会学习一个较小的$r_c$，倾向于平均所有帧的信息。这证明了模型确实在根据数据特性，自适应地学习最佳的池化策略。

![](https://pic1.imgdb.cn/item/68ff2aab3203f7be00a6918e.png)

训练好的模型可以输出每一帧对于特定标签的预测分数。通过可视化这些分数，可以清晰地看到，模型为那些与标签语义最相关的关键帧分配了最高的分数。

![](https://pic1.imgdb.cn/item/68ff2bfd3203f7be00a69894.png)