---
layout: post
title: 'SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features'
date: 2025-02-20
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67c00510d0e0a243d406cef1.png'
tags: 论文阅读
---

> SigLIP 2：使用改进的语义理解、定位和密集特征的多模态视觉语言编码器.

- paper：[SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features](https://arxiv.org/abs/2502.14786)

# 0. TL; DR

**SigLIP 2** 是一种新型多模态视觉-语言编码器，继承了 [<font color=blue>SigLIP</font>](https://0809zheng.github.io/2024/02/29/siglip.html) 的架构并引入了多种改进技术，包括基于描述的预训练、自监督损失（如自蒸馏和掩码预测）以及在线数据整理。这些改进使得 **SigLIP 2** 在零样本分类、图像-文本检索和视觉-语言模型（**VLMs**）的特征提取等核心能力上超越了其前身 **SigLIP**。此外，**SigLIP 2** 在多语言理解、定位任务和密集特征提取方面表现出色，并支持多种分辨率和保留输入的原始宽高比。模型提供四种尺寸：**ViT-B (86M), L (303M), So400m (400M), and g (1B)**，以满足不同推理成本和性能需求。

# 1. 背景介绍

近年来，对比学习方法（如 **CLIP** 和 **ALIGN**）在大规模图像-文本数据集上取得了显著进展，能够实现细粒度的零样本分类和高效的跨模态检索。这些模型通过将图像和文本嵌入到同一语义空间，为视觉和语言的结合提供了强大的基础。然而，这些模型在多语言支持、密集特征提取和定位任务上仍存在不足。

**SigLIP 2** 的出现旨在解决这些问题。它继承了 **SigLIP** 的成功经验，并引入了多种改进技术，如基于描述的预训练、自监督损失和在线数据整理。这些改进不仅提升了模型在核心任务上的性能，还在多语言理解、密集特征提取和定位任务上取得了显著进展。此外，**SigLIP 2** 还通过支持多种分辨率和保留输入的原始宽高比，进一步扩展了其应用场景。

![](https://pic1.imgdb.cn/item/67c6e8e1d0e0a243d40bde53.png)

# 2. SigLIP 2 模型

**SigLIP 2** 基于 **SigLIP** 的架构设计。模型使用标准的 **Vision Transformer（ViT）**架构，图像和文本编码器共享相同的架构，但视觉编码器的大小可以根据需要进行调整。图像和文本表示通过注意力池化（**MAP head**）进行聚合，文本长度设置为 **64**，并使用多语言 **Gemma** 分词器进行处理。

**SigLIP 2** 的训练目标结合了多种技术，以提升模型的语义理解能力和多语言支持能力。包括：
- **Sigmoid** 损失：与**SigLIP**相同，使用 **Sigmoid** 损失，通过将图像嵌入与文本嵌入组合成二分类问题，训练模型区分匹配和不匹配的图像-文本对。该损失增强了语义理解任务的性能。
- 基于描述的预训练：引入了 **LocCa** 的描述预训练方法，通过在图像编码器上附加一个标准的 **Transformer** 解码器，模型能够学习生成图像描述、自动指代表达预测和基于区域的描述生成。这些任务不仅提升了模型的 **OCR** 能力，还显著增强了定位任务的性能。
- 自蒸馏和掩码预测：自蒸馏通过让学生网络匹配教师网络（学生网络的滑动平均）的全局表示，而掩码预测则通过让学生网络预测被掩码的图像块的特征。这些技术显著提升了模型在密集预测任务（如分割和深度估计）中的性能。

为了支持多种分辨率并保留输入图像的原始宽高比，**SigLIP 2** 引入了 **NaFlex** 变体。**NaFlex** 结合了 **FlexiViT** 和 **NaViT** 的思想，能够处理不同类型的图像，并在适当分辨率下进行处理。例如，文档图像可以使用更高的分辨率进行处理，同时最小化宽高比失真对某些推理任务（如 **OCR**）的影响。

为了提升小模型（如 **ViT-B/16** 和 **ViT-B/32**）的性能，**SigLIP 2** 使用了在线数据整理技术。通过从教师模型中选择“可学习”的样本，小模型能够在有限的计算资源下获得更好的性能。

# 3. 实验分析

**SigLIP 2** 在多个基准数据集上进行了评估，包括零样本分类（**ImageNet、ObjectNet、ImageNetv2、ImageNet ReaL**）、图像-文本检索（**COCO、Flickr、XM3600**）以及密集预测任务（如分割、深度估计和法线估计）。

**SigLIP 2** 在零样本分类和图像-文本检索任务中表现出色，全面超越了 **SigLIP** 和其他基线模型。例如，在 **ImageNet** 零样本分类任务中，**SigLIP 2** 的 **L/16** 模型达到了 **82.5%** 的准确率，显著高于 **SigLIP** 的 **80.5%**。在多语言检索任务（**XM3600**）中，**SigLIP 2** 的召回率也大幅超过 **SigLIP**，仅略低于专门针对多语言数据训练的 **mSigLIP**。

![](https://pic1.imgdb.cn/item/67c6ea33d0e0a243d40bde8f.png)

**NaFlex** 变体在处理文档和描述相关任务时表现出色。例如，在 **TextCaps** 和 **Screen2Words** 等数据集上，**NaFlex** 的性能优于标准 **SigLIP 2** 变体。这表明 **NaFlex** 在处理宽高比敏感的应用时具有显著优势。

![](https://pic1.imgdb.cn/item/67c6ea67d0e0a243d40bde96.png)

**SigLIP 2** 作为视觉编码器，与 **Gemma 2 LLM** 结合后，在多个视觉-语言任务中表现出色。例如，在 **COCO-35L** 数据集上，**SigLIP 2** 的 **L/16** 模型达到了 **114.8** 的平均分数，显著高于 **SigLIP** 的 **111.9**。这表明 **SigLIP 2** 在提取视觉特征方面具有更强的能力。

![](https://pic1.imgdb.cn/item/67c6eae2d0e0a243d40bdea4.png)

**SigLIP 2** 在密集预测任务中也取得了显著进展。例如，在 **PASCAL VOC** 分割任务中，**SigLIP 2** 的 **So/14** 模型达到了 **78.1%** 的 **mIoU**，显著高于 **SigLIP** 的 **73.8%**。此外，在深度估计和法线估计任务中，**SigLIP 2** 也表现出色，进一步证明了其在密集特征提取方面的优势。

![](https://pic1.imgdb.cn/item/67c6eb11d0e0a243d40bdeaa.png)

