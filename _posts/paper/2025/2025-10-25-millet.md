---
layout: post
title: 'Inherently Interpretable Time Series Classification via Multiple Instance Learning'
date: 2025-10-25
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/69293fbeb4e23f08080a9196.png'
tags: 论文阅读
---

> 通过多实例学习实现内在可解释性时序分类.

- paper：[Inherently Interpretable Time Series Classification via Multiple Instance Learning](https://arxiv.org/abs/2311.10049)


# 0. TL; DR

这篇论文将多实例学习**（Multiple Instance Learning, MIL）**的思想引入时序分类领域，设计了一个名为**MILLET (Multiple Instance Learning for Locally Explainable Time series classification)**的框架。该框架将**TSC**问题重新定义为**MIL**问题，并用**MIL**池化方法替换传统**TSC**模型中的全局平均池化层。

在包含85个数据集的**UCR**时间序列分类基准库上，**MILLET**框架不仅提供了内在的可解释性，而且在预测性能上优于原始的黑箱模型。在**WebTraffic**数据集上，**MILLET**生成的可解释性热力图，其质量显著优于经典的**CAM**和昂贵的**SHAP**等方法。

# 1. 背景介绍

近年来，以**FCN、ResNet、InceptionTime**为代表的深度学习（**DL**）模型在时间序列分类（**Time Series Classification, TSC**）领域取得了巨大成功。然而，这些**DL**模型普遍存在两个致命的缺陷：
1.  黑箱问题: 它们能给出“是什么”的预测（例如，“这段心电图是心力衰竭”），但无法告诉“为什么”（例如，“因为在XX秒到XX秒之间出现了这个异常的波形”）。这种缺乏可解释性的特性，极大地限制了它们在医疗、金融等高风险领域的实际应用和信任建立。
2.  信息聚合粗糙: 这些模型通常在最后阶段使用全局平均池化**（Global Average Pooling, GAP）**，将所有时间点的特征表示成一个向量。这种做法对所有时间点一视同仁，无法区分哪些是决定性的“信号”，哪些是无关紧要的“噪声”，从而限制了模型的预测性能上限。

![](https://pic1.imgdb.cn/item/692941e27f8e1c737821ee5f.png)

为了解决这些问题，一些研究者尝试使用后处理（**post-hoc**）的可解释性方法，如**LIME、SHAP**等。但这些方法往往计算成本极高（需要多次模型前向传播）。作者通过引入多实例学习**（Multiple Instance Learning, MIL）**，让**TSC**模型自身就具备可解释性，即**内在可解释性（inherently interpretable）**，并且这种方法还能改善其粗糙的信息聚合方式。

# 2. MILLET框架

将**TSC**问题重新框定为**MIL**问题是**MILLET**框架的理论基础：将一整条时间序列视为一个**MIL**的**包 (bag)**，将序列中的每个时间点 **(time point)**视为一个实例 **(instance)**。模型必须能够生成每个时间点的预测，这是实现内在可解释性的前提。并且必须尊重时间点的顺序，这与经典**MIL**的**i.i.d.**假设不同。

**MILLET**框架保留了传统**DL-TSC**模型中的特征提取器和分类器，只将中间的**GAP**替换为更先进的**MIL**池化算子。这是一个**即插即用（plug-and-play）**的改造。作者探索了四种**MIL**池化方法：

- **Attention Pooling**: 通过一个注意力头为每个时间点嵌入$z_j$学习一个权重$a_j$，然后进行加权平均。
    
    $$ \hat{Y}_i = \psi_{CLF}\left( \frac{1}{t} \sum_{j=1}^t a_{ji} z_i^j \right) $$

- **Instance Pooling**: 先分类，后聚合。直接用分类器$ψ_{CLF}$对每个时间点嵌入$z_j$进行预测，得到时间点预测$ŷ_j$，然后再对所有时间点预测进行平均。
    
    $$ \hat{y}_i^j = \psi_{CLF}(z_i^j); \quad \hat{Y}_i = \frac{1}{t} \sum_{j=1}^t (\hat{y}_i^j) $$

- **Additive Pooling**: 这是**Attention**和**Instance**的串联。先用注意力权重$a_j$加权时间点嵌入$z_j$，然后再送入分类器。
    
    $$ \hat{y}_i^j = \psi_{CLF}(a_{ji} z_i^j); \quad \hat{Y}_i = \frac{1}{t} \sum_{j=1}^t (\hat{y}_i^j) $$

- **Conjunctive Pooling**: 这是**Attention**和**Instance**的并行组合。注意力头$ψ_{ATTN}$和分类器$ψ_{CLF}$独立地作用于原始的时间点嵌入$z_j$，分别得到权重$a_j$和预测$ŷ_j$。最后，用权重$a_j$来加权预测$ŷ_j$。
    
    $$ a_{ji} = \psi_{ATTN}(z_i^j); \quad \hat{y}_i^j = \psi_{CLF}(z_i^j); \quad \hat{Y} = \frac{1}{t} \sum_{j=1}^t (a_{ji} \hat{y}_i^j) $$


![](https://pic1.imgdb.cn/item/6929428c7f8e1c737821ef7c.png)

联合池化意在强调一个时间点要想对最终结果有重要贡献，必须同时被注意力头和分类器都认为是重要的。通过上述改造，**MILLET**模型自然地获得了内在可解释性：
-   对于**Instance, Additive, Conjunctive**这三种池化，它们直接生成了每个时间点的类别预测$ŷ_j$，这本身就是一张类别相关的贡献度图。
-   对于**Attention**池化，其注意力权重$a_j$可以被用作每个时间点重要性的度量。


为了进一步提升性能，作者还为**MILLET**模型引入了三项增强：
-   位置编码 **(Positional Encoding)**: 在特征提取后，为每个时间点嵌入加入位置编码，以增强模型对时序的感知。
-   复制填充 **(Replicate Padding)**: 将卷积层中的零填充改为复制边界值填充，以消除在序列首尾产生的虚假信号。
-   **Dropout**: 在池化层前加入**Dropout**，以防止过拟合。

# 3. 实验分析

### 3.1 WebTraffic合成数据集

作者构建了一个名为**WebTraffic**的合成数据集，其中包含10类具有不同“信号模式”（如尖峰、截断、偏移等）的时间序列，这些信号模式的位置是已知的。

![](https://pic1.imgdb.cn/item/69294465b4e23f08080a93f9.png)

在预测准确率上，**MILLET**模型（平均0.874）优于原始的**GAP**模型（平均0.850）。其中，**Conjunctive InceptionTime**模型达到了最高的**0.940**。使用**AOPCR**和**NDCG@n**两个指标，将**MILLET**的内在解释与**CAM**和昂贵的**SHAP**进行对比。结果显示，**MILLET**的解释质量全面优于**CAM**和**SHAP**。特别是**SHAP**，由于时间序列过长，其采样近似方法几乎失效。

![](https://pic1.imgdb.cn/item/692945867f8e1c737821f456.png)

可视化结果显示，**MILLET**能够准确地定位出不同类别信号模式所在的区域。有趣的是，它并非简单地高亮整个区域，而是捕捉到了模式的关键部分，如“截断”模式的起止点，这为理解模型的决策提供了更深层次的洞察。

![](https://pic1.imgdb.cn/item/692945bc7f8e1c737821f4c1.png)

### 3.2 UCR基准库

在包含**85**个不同领域数据集的**UCR**时间序列分类基准库上进行大规模评测。在所有**MIL**池化方法中，**Conjunctive Pooling**表现最佳。在与七大类**TSC SOTA**方法的对比中，**Conjunctive InceptionTime**的性能与最顶尖的、极其复杂的集成方法**HIVE-COTE 2 (HC2)**和混合方法**Hydra-MR**相当，甚至在平衡准确率上略微胜出。

![](https://pic1.imgdb.cn/item/6929460fb4e23f08080a95bf.png)

实验发现了一个有趣的预测性能-可解释性权衡：模型越复杂（**InceptionTime > ResNet > FCN**），预测性能越强，但可解释性（**AOPCR**）越差。**MILLET**的引入，整体上抬高了这条权衡曲线。

![](https://pic1.imgdb.cn/item/6929464cb4e23f08080a9638.png)

在**LargeKitchenAppliances**数据集上，**MILLET**能够清晰地识别出代表“洗衣机”、“烘干机”、“洗碗机”的不同用电模式，其解释比**CAM**更稀疏、聚焦，比**SHAP**更清晰、稳定。

![](https://pic1.imgdb.cn/item/69294670b4e23f08080a9663.png)
