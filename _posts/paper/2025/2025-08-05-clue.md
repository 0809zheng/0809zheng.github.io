---
layout: post
title: 'Cross-Linked Unified Embedding for cross-modality representation learning'
date: 2025-08-05
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/68f994113203f7be00901699.png'
tags: 论文阅读
---

> 跨模态表示学习的交叉链接统一嵌入.

- paper：[Cross-Linked Unified Embedding for cross-modality representation learning](https://openreview.net/forum?id=Tfb73TeKnJ-)

# 0. TL; DR
**CLUE (Cross-Linked Unified Embedding)**是一个半监督的深度生成模型，它扩展多模态变分自编码器（**Multi-modal VAE**），学习一个既能保留共享信息，又能捕捉模态特异性信息的统一嵌入空间。

**CLUE**除了用自编码器编码每个模态的特征，还用交叉编码器进行跨模态的特征编码。通过将全局潜在空间分解为多个模态特异的子空间，能够同时保留共享信息和特异信息，获得更全面的细胞表征。

**CLUE**在**NeurIPS 2021**多模态单细胞数据整合竞赛中斩获第一，在后续的基准测试中性能也全面超越了**Seurat v4、MultiVI**等一系列**SOTA**方法。


# 1. 背景介绍

细胞是一个复杂的生命系统，其功能由**DNA、RNA**、蛋白质等多个分子层面协同调控。单细胞多组学技术（如**SHARE-seq, CITE-seq**）能够在同一个细胞内窥探多个分子层面的奥秘，为理解基因调控提供了直接的证据。

然而多组学数据虽然信息价值极高，但实验成本高、技术难度大、通量相对较低，通常只有几千到几万个细胞。单组学数据则技术成熟、成本低、通量极高，动辄可以获得数百万甚至上千万个细胞。能否将在小规模配对数据上学到的跨模态关联知识，迁移应用到大规模的单组学数据上，从而实现模态不完备（**modality-incomplete**）数据的整合问题。

许多基于**VAE**的半监督方法存在一个共同的局限性：它们倾向于将所有模态都投影到一个单一的、共享的潜在空间。为了实现“共享”，模型往往会优先学习模态间的共同信息，而忽略或丢弃了每个模态独有的特异性信息。例如，某些非编码区的**ATAC peak**可能不直接调控邻近基因，但在细胞身份定义中扮演重要角色，这些信息在追求“共享”的过程中很容易被丢失。

**CLUE**的目标是构建一个更全面的嵌入空间，既能整合共享信息，又能保留每个模态的特异性，从而获得更精准、更丰富的细胞表征。

![](https://pic1.imgdb.cn/item/68f99a163203f7be00904896.png)

# 2. CLUE 模型

CLUE是一个基于深度生成模型的框架，通过引入交叉编码器和多重对齐损失扩展了标准的多模态**VAE**架构，实现了对模态不完备数据的强大整合能力。

假设有 $K$ 种模态，一个细胞 $c$ 的数据可以表示为 $x_{K_c}^{(c)}$，其中 $K_c$ 是该细胞被观测到的模态集合。希望为每个细胞学习一个全局的潜在表征 $z$，它由各个模态的特异性潜在变量 $z_k$ 拼接而成，即 $z = (z_1, z_2, ..., z_K)$。

在变分自编码器（**VAE**）的框架下，通过最大化证据下界（**ELBO**）来优化模型。对于一个细胞 $c$，其**ELBO**可以写成：

$$
\begin{aligned}
\text{ELBO}(x_{K_c}^{(c)}; \theta, \phi) &= \underbrace{\sum_{k \in K} \left[ \mathbb{E}_{q(z_k|x_{K_c}^{(c)};\phi)} \ln p(x_k^{(c)}|z_k; \theta) \right]}_{\text{重构项}} 
\\ &- \underbrace{\sum_{k \in K} \text{KL}\left( q(z_k|x_{K_c}^{(c)};\phi) \parallel p(z_k) \right)}_{\text{正则化项}}
\end{aligned}
$$

后验分布 $q(z_k\|x_{K_c}^{(c)};\phi)$ 的含义是利用细胞$c$所有观测到的模态数据 $x_{K_c}^{(c)}$，来推断其在第$k$个模态下的潜在状态 $z_k$。传统方法假设推断 $z_k$ 只需要第 $k$ 种模态自身的数据，即 $q(z_k\|x_{K_c}^{(c)}) \approx q(z_k\|x_k^{(c)})$，这导致模型退化为$K$个独立的**VAE**，无法处理模态缺失的情况。

**CLUE**直接对 $q(z_k\|x_{K_c}^{(c)})$ 进行建模。它假设对于第 $k$ 个模态的潜在状态 $z_k$ 的推断，是来自所有已观测模态 $k' \in K_c$ 的信息推断的乘积：

$$
q(z_k|x_{K_c}^{(c)};\phi) = \prod_{k' \in K_c} q(z_k|x_{k'}^{(c)};\phi)
$$

**CLUE**不仅需要学习自身编码器 **(self-encoder)** $q(z_k\|x_k^{(c)})$（例如，用**RNA**数据编码**RNA**潜在态），还需要学习一系列交叉编码器 **(cross-encoder)** $q(z_k\|x_{k'}^{(c)})$ (其中 $k \neq k'$)（例如，用**RNA**数据 $x_1$ 去编码**ATAC**的潜在态 $z_2$）。

![](https://pic1.imgdb.cn/item/68f99a473203f7be00904a99.png)

为了让不同模态的潜在空间能够对齐，**CLUE**在**VAE**的**ELBO**损失之外，额外引入了两种对齐损失。
- **均方误差损失 (MSE Loss, $L_{MSE}$):** 这是一个局部对齐的损失。对于同一个配对细胞，其通过不同模态推断出的潜在表征 $z_k^{(c)}$ 应该彼此靠近。最小化每个细胞$c$的各个模态潜在表征与其平均值$\bar{z}^{(c)}$之间的均方误差。
    
$$
L_{MSE}(\phi) = \mathbb{E}_c \left[ \frac{1}{|K_c|} \sum_{k \in K_c} ||z_k^{(c)} - \bar{z}^{(c)}||^2 \right]
$$

- **对抗性损失 (Adversarial Loss, $L_D$):**这是一个**全局对齐**的损失。引入一个**模态判别器 (modality discriminator)** $q(k\|z; \psi)$，它的任务是判断一个给定的潜在表征 $z$ 是从哪种模态的数据编码而来的。编码器则要进行对抗训练，目标是让判别器完全无法分辨其来源。
    
$$
L_D(\phi, \psi) = \mathbb{E}_c \left[ \frac{1}{|K_c|} \sum_{k \in K_c} \mathbb{E}_{q(z|x_k; \phi)} \ln q(k|z; \psi) \right]
$$
    
最终的优化目标是一个**min-max**博弈：编码器（参数$\phi$）试图最小化判别器的分类准确率，而判别器（参数$\psi$）则试图最大化它。通过这种对抗，模型被强迫学习到一个真正与模态无关的、高度融合的潜在空间。

$$
\begin{cases}
\max_{\psi} & \lambda_D L_D(\phi, \psi) \\
\min_{\theta, \phi} & \lambda_{self} L_{self}(\theta, \phi) + \lambda_{cross} L_{cross}(\theta, \phi) + \lambda_{MSE} L_{MSE}(\phi) + \lambda_D L_D(\phi, \psi)
\end{cases}
$$

其中，$L_{self}$ 和 $L_{cross}$ 分别是基于自身编码器和交叉编码器的**VAE**损失，$\lambda$ 们是各项损失的权重。

# 3. 实验分析

作者在**NeurIPS 2021**多模态单细胞数据整合竞赛的数据集以及其他公开数据集上，对**CLUE**的性能进行了全面评估。

## 实验一：NeurIPS 2021 竞赛

这场竞赛吸引了**280**支队伍参与，任务是在**Multiome（RNA+ATAC）**和**CITE-seq（RNA+Protein）**两个任务上，对齐和匹配不同模态的细胞。

**CLUE**在所有四个子任务（**GEX-ATAC, ATAC-GEX, GEX-ADT, ADT-GEX**）以及总分上均获得了第一名，且领先优势巨大。其总分（0.0539）比第二名高出 **26%**，比第三名高出 **158%**。

![](https://pic1.imgdb.cn/item/68f99c0a3203f7be00905c4f.png)

**UMAP**可视化结果显示，在从未见过的测试数据上，**CLUE**能够将两种模态的数据（图中左侧，不同颜色）完美地混合在一起，同时还能清晰地保留不同细胞类型的生物学结构（图中右侧，相同颜色聚类）。

![](https://pic1.imgdb.cn/item/68f99be63203f7be00905ada.png)

## 实验二：与SOTA方法的基准比较

除了竞赛队伍，作者还与该领域其他几个公开发表的**SOTA**方法（**Seurat v4**的**Bridge-integration, MultiVI, Cobolt**）进行了正面比较。

无论是在竞赛数据集还是额外的**SHARE-seq**数据集上，**CLUE**在**Matching Score**（越高越好）和**FOSCTTM**（越低越好）两个核心指标上都全面超越了所有对比方法。

![](https://pic1.imgdb.cn/item/68f99c523203f7be00905eee.png)
![](https://pic1.imgdb.cn/item/68f99c6e3203f7be00906035.png)

## 实验三：消融研究

作者进行了消融实验，即分别去掉交叉损失($L_{cross}$)、**MSE**损失($L_{MSE}$)和对抗性损失($L_D$)，观察性能变化。

实验结果表明去掉任何一个损失项都会导致性能下降。其中对抗性损失($L_D$)的贡献最大，去掉它会导致匹配得分骤降47.3%；这说明通过对抗训练实现的全局对齐，对于学习高质量的统一嵌入空间至关重要。去掉MSE损失和交叉损失也会分别导致16.5%和17.6%的性能下降，证明了局部对齐和跨模态翻译模块的必要性。

![](https://pic1.imgdb.cn/item/68f99ca43203f7be009063e8.png)