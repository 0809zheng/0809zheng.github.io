---
layout: post
title: 'DNA language model GROVER learns sequence context in the human genome'
date: 2025-04-12
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/680aedef58cb8da5c8c98f9f.png'
tags: 论文阅读
---

> DNA语言模型GROVER学习人类基因组中的序列上下文.

- paper：[DNA language model GROVER learns sequence context in the human genome](https://www.nature.com/articles/s42256-024-00872-0)

# 0. TL; DR

本文介绍了一种名为 **GROVER（Genome Rules Obtained Via Extracted Representations）**的 **DNA** 语言模型，旨在通过自然语言处理技术解析人类基因组中的序列上下文。**GROVER** 基于 **BERT** 架构，通过字节对编码（**BPE**）生成频率平衡的词汇表，并在掩码标记预测任务上进行预训练。实验表明，**GROVER** 在预测下一个核苷酸序列（**next-k-mer** 预测）和多个基因组生物学任务上优于其他模型，能够学习到基因组的语法结构和功能注释信息。

# 1. 背景介绍

人类基因组的初稿已经发布超过 **20** 年，但基因组中复杂的“遗传密码”仍未被完全解读。虽然基因中的 **DNA** 三联体编码氨基酸，但基因组中还有更多层次的编码信息，例如基因调控、转录本结构与稳定性、基因组复制以及基因组的稳定性与功能的平衡。这些信息都隐藏在基因组序列中，需要复杂的算法来解析。近年来，自然语言处理（**NLP**）中的大型语言模型（**LLMs**）为这一问题提供了新的思路。基于 **Transformer** 架构的模型在文本数据处理上表现出色，也被尝试用于基因组学研究。

然而，基因组与自然语言存在显著差异。基因组没有明确的方向和“单词”定义，传统的语言模型难以直接应用于 **DNA** 序列。为此，研究者们开发了多种 **DNA** 语言模型（**DLM**），例如 **LOGO、DNABERT** 和 **Nucleotide Transformer** 等。这些模型通过不同的策略生成词汇表，并在特定任务上取得了进展。但这些模型往往依赖于额外的信息或复杂的架构，难以解释模型的学习内容。因此，本文提出了一种新的方法：使用字节对编码（**BPE**）生成频率平衡的词汇表，并结合 **BERT** 架构训练 **GROVER** 模型，以更好地学习基因组的序列上下文。

# 2. GROVER 模型

![](https://pic1.imgdb.cn/item/680aee7d58cb8da5c8c993c9.png)

为了克服基因组序列中核苷酸频率不均衡的问题，本文采用了字节对编码（**BPE**）方法。**BPE** 通过逐步合并最频繁的核苷酸对来生成新的标记（**tokens**），从而生成频率平衡的词汇表。从最初的四个核苷酸（**A、C、G、T**）开始，经过多轮合并，生成包含不同长度核苷酸序列的词汇表。最终，通过 **600** 轮 **BPE** 生成的词汇表被选为 **GROVER** 的词汇表，该词汇表包含 **601** 个标记，涵盖了从单核苷酸到 **16** 核苷酸的序列。

**GROVER** 基于 **BERT** 架构，包含 12 个 **Transformer** 编码器块。每个块包含多头注意力机制和前馈层，通过掩码标记预测任务进行预训练。具体来说，模型输入为长度不超过 **510** 个标记的序列，其中 2.2% 的标记被随机选择并替换为特殊标记 **MASK**。模型的目标是预测这些被掩盖的标记。通过交叉熵损失函数优化模型参数，训练过程中模型不断更新标记的嵌入表示。掩码标记预测损失函数如下：

$$
  \mathcal{L} = -\sum_{i \in \text{masked tokens}} \log P(y_i | x_{\text{context}})
$$

其中 $y_i$ 是被掩盖的真实标记，$x_{\text{context}}$ 是上下文中的其他标记，$P(y_i | x_{\text{context}})$ 是模型预测标记 $y_i$ 的概率。

# 3. 实验分析

### 3.1 词汇表优化与性能评估
为了验证 **GROVER** 的性能，研究者使用了 **next-k-mer** 预测任务来选择最优的词汇表。该任务要求模型预测序列中的下一个核苷酸序列（长度为 k）。实验结果表明，**GROVER** 在 600 轮 **BPE** 生成的词汇表上表现最佳，远高于其他固定长度的 **k-mer** 模型。

![](https://pic1.imgdb.cn/item/680aefcb58cb8da5c8c9a350.png)

### 3.2 词汇表的频率平衡与学习内容
**GROVER** 的词汇表通过 **BPE** 生成，具有频率平衡的特点。该词汇表中大部分标记为 **4-mer** 和 **5-mer**，且标记长度与标记频率呈正相关。通过分析训练后的标记嵌入，**GROVER** 学习到了标记的频率、**GC** 含量、**AG** 含量等特征，这些特征与基因组的功能注释密切相关。例如，标记频率最高的 **4-mers** 和 **5-mers** 形成明显的聚类，而重复序列（如 **LINE、SINE**）则表现出方向性差异。

![](https://pic1.imgdb.cn/item/680af15e58cb8da5c8c9b1d9.png)

### 3.3 GROVER 的上下文学习能力
**GROVER** 不仅学习了标记本身的特征，还能够识别序列上下文。通过计算标记在不同上下文中的自相似性（**cosine similarity**），发现 **GROVER** 的标记嵌入在不同 **Transformer** 层中表现出低自相似性，表明模型能够识别上下文差异。此外，**GROVER** 还能够区分基因组中的不同功能区域，如染色质状态、复制时间等，即使这些信息未直接提供给模型。

![](https://pic1.imgdb.cn/item/680af10458cb8da5c8c9adcf.png)
![](https://pic1.imgdb.cn/item/680af11358cb8da5c8c9ae41.png)

### 3.4 细化任务的性能验证
为了验证 **GROVER** 在基因组生物学任务中的适用性，研究者选择了三个代表性任务：**Prom300**（启动子分类）、**PromScan**（启动子扫描）和 **CTCF** 结合位点预测。

实验结果表明，**GROVER** 在这些任务上均优于其他模型。例如，在 **Prom300** 任务中，**GROVER** 的 **Matthew** 相关系数（**MCC**）达到 99.6%，远高于其他模型；在 **PromScan** 任务中，**GROVER** 的 **MCC** 为 63%，优于 **NT** 模型（52%）；在 **CTCF** 结合位点预测任务中，**GROVER** 的 **MCC** 为 60%，与 **DNABERT-2** 模型相当。

![](https://pic1.imgdb.cn/item/680af0c758cb8da5c8c9abd9.png)