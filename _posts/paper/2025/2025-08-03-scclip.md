---
layout: post
title: 'scCLIP: Multi-modal Single-cell Contrastive Learning Integration Pre-training'
date: 2025-08-03
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/68f746d73203f7be008780af.png'
tags: 论文阅读
---

> scCLIP：多模态单细胞对比学习整合预训练.

- paper：[scCLIP: Multi-modal Single-cell Contrastive Learning Integration Pre-training](https://openreview.net/forum?id=KMtM5ZHxct)

# 0. TL; DR

**scCLIP**是一个基于 **Transformer** 和 **对比学习** 的单细胞多组学整合预训练框架，通过基因组 **patching** 策略将染色质开放区域（**peaks**）按基因组坐标排序并分块（**patch**），每一块作为一个**token**；通过对比学习联合训练**scRNA-seq**和**scATAC-seq**两个模态的**Transformer**编码器，学习一个统一的嵌入空间。

在大规模图谱数据集的基准测试中，**scCLIP**在跨模态细胞匹配任务上显著优于现有SOTA方法。**scCLIP**学习到的特征具有零样本迁移能力，即在一个数据集上训练好的模型，无需重新训练，可以直接用于分析一个全新的、未见过的数据集，并取得优异的对齐效果。

# 1. 背景介绍

单细胞测序技术，特别是**scRNA-seq**和**scATAC-seq**，已经成为揭示细胞异质性和功能的利器。前者告诉我们“哪些基因在表达”，后者则揭示了“哪些基因可能被表达”（通过染色质开放状态）。将两者结合，就能在单细胞层面建立从“表观调控”到“基因表达”的直接联系。

整合这两种模态的数据面临三大挑战：
1.  维度诅咒与数据质量： **scATAC-seq**的特征维度极高（通常是几十万到上百万个**peaks**），数据极其稀疏，信噪比低，给建模带来了巨大困难。
2.  模态间的不平衡： 许多方法（如**WNN、totalVI**）倾向于生成一个统一的细胞表征，但这往往会导致信息量更丰富的**scRNA-seq**数据“主导”最终的嵌入，削弱了**scATAC-seq**的贡献。
3.  可扩展性与泛化性差： 现有的一些匹配方法（如**CLUE、MatchCLOT**）虽然在特定任务上表现不错，但通常难以扩展到百万细胞级别的图谱数据，并且模型不具备泛化能力，无法应用于新的、未见过的数据集。

基于**Transformer**的多模态模型**CLIP**通过对比学习，成功地将图像和文本两种完全不同的模态对齐到了一个共享的嵌入空间，展现了零样本学习能力。能否借鉴**CLIP**的思想，构建一个能够处理并对齐**scRNA-seq**和**scATAC-seq**数据的**Transformer**模型？

# 2. scCLIP 模型

**scCLIP**主要由两部分构成：为两个模态定制的 **Tokenization（分词）** 模块，以及基于对比学习的 **Multi-modal Embedding Learning（多模态嵌入学习）** 模块。

![](https://pic1.imgdb.cn/item/68f83f8d3203f7be008a43f8.png)

### 2.1 Tokenization

**Transformer**的输入是一系列的“**token**（词元）”。如何将一个包含数十万特征的细胞谱图向量转换成一个合理长度的**token**序列，是**scCLIP**的关键。

对于**scATAC-seq**：
1.  特征排序：首先将所有的**peaks**（特征）按照它们在参考基因组上的物理坐标（染色体号和起止位置）进行排序。
2.  分块（**Patching**）： 将这个排好序的、超长的细胞特征向量 $x ∈ R^F$ (F可达百万) 切割成 $C$ 个连续的、不重叠的**patch**。每个**patch**的长度为 $P$，即 $F = C × P$。
3.  **Token**化： 每一块**patch**被视为一个**token**，它使得每个**token**都具有了明确的生物学意义：代表了一段连续基因组区域的染色质开放状态。
4.  全局表征：在**token**序列的开头加入一个特殊的可学习**token**（$[CLS]$），其在经过**Transformer**编码后的最终输出将作为整个细胞的全局表征。
5.  位置编码：为了保留每个**patch**在基因组上的相对位置信息，为每个**token**加入了标准的一维位置编码。

对于**scRNA-seq**：采用类似的方法，将基因按其在基因组上的坐标排序后进行分块和**token**化。

通过基因组分块策略，成功地将一个难以处理的超高维向量，转换成了**Transformer**可以高效处理的、具有生物学意义的**token**序列。

### 2.2 多模态嵌入学习

有了两个模态的**Transformer**编码器（一个用于**RNA**，一个用于**ATAC**）后，目标是学习一个共享的嵌入空间，使得来自同一个细胞的**RNA**和**ATAC**能够在这个空间里“相认”。采用**对比学习**来实现这一目标。

假设在一个**batch**中有 $N$ 个配对的 (**ATAC, RNA**) 样本。**正样本对 (Positive Pair)**是来自同一个细胞的**RNA**嵌入和**ATAC**嵌入，整个**batch**中有 $N$ 对。**负样本对 (Negative Pairs)**是来自不同细胞的**RNA**和**ATAC**嵌入组合，整个**batch**中有 $N² - N$ 对。

目标是最大化 $N$ 个正样本对的余弦相似度，同时最小化 $N² - N$ 个负样本对的余弦相似度。这可以被构建成一个分类问题：对于一个**RNA**嵌入，需要在 $N$ 个**ATAC**嵌入中正确地把它分类到与之配对的那个。反之亦然。这个过程通过对称的**交叉熵损失 (Cross-Entropy Loss)** 来实现：
    
$$
L = \frac{1}{2} \times (\text{CE}(f, y_{\text{ATAC}}) + \text{CE}(g, y_{\text{RNA}}))
$$
    
其中 $f$ 和 $g$ 分别是来自**RNA**和**ATAC**编码器的细胞嵌入向量。$y_{\text{ATAC}}$ 和 $y_{\text{RNA}}$ 是它们对应的正确配对标签。$\text{CE}(\cdot, \cdot)$ 是标准的交叉熵函数，它作用于一个 $N \times N$ 的相似度矩阵（矩阵的每个元素是**RNA**嵌入和**ATAC**嵌入的余弦相似度）。

通过最小化这个损失函数，**scCLIP**学习到一个能够区分正确配对和错误配对的共享嵌入空间，从而实现了两个模态的深度对齐。

# 3. 实验分析

作者通过一系列实验，展示了**scCLIP**在多个维度上的优越性。

## 3.1 实验一：大规模数据整合与对齐

作者在一个包含 **37.7万个细胞** 的伪配对人类胎儿图谱数据集**Fetal Atlas**上验证了**scCLIP**的可扩展性和整合能力。

**UMAP**可视化结果显示，**scCLIP**生成的联合嵌入空间中，不同模态得到了很好的混合，同时不同细胞类型（右图）又能被清晰地分离开。

![](https://pic1.imgdb.cn/item/68f843563203f7be008a455b.png)

在量化评估中，**scCLIP**取得了最高的 **Silhouette score (0.486)**，表明其保留生物学差异（区分细胞类型）的能力最强。同时，其 **Batch entropy mixing score (0.578)** 与**SOTA**方法**CLUE**持平，表明其对齐不同模态的能力也十分优秀。

![](https://pic1.imgdb.cn/item/68f843733203f7be008a4564.png)

## 3.2 实验二：跨模态匹配

跨模态匹配是指给定一个细胞的**RNA**谱，在数万个**ATAC**谱中找到属于同一个细胞的那个。使用 **Matching Score** (越高越好) 和 **FOSCTTM** (越低越好) 两个核心指标进行评估。

结果显示，在不同抽样数量下，**scCLIP**的性能都超越了之前的**SOTA**方法**CLUE**和**MatchCLOT**，证明了**scCLIP**在单细胞级别精准匹配上的巨大优势。

![](https://pic1.imgdb.cn/item/68f843ca3203f7be008a4572.png)

## 3.3 实验三：零样本迁移学习

作者在一个老年痴呆症（**AD**）人脑数据集上训练**scCLIP**模型。然后不经过任何重新训练或微调，直接使用训练好的**RNA**和**ATAC**编码器，为一个完全不同的、来自**10x Genomics**官网的 **human_brain_3k** 数据集生成细胞嵌入。

训练所用的**AD**数据集本身实现了很好的整合，**human_brain_3k**不仅其自身的**RNA**和**ATAC**模态对齐得很好，而且其细胞嵌入与**AD**数据集中对应的细胞类型（如兴奋性神经元、抑制性神经元等）重叠在了一起。这证明了**scCLIP**学习到的不是特定于某个数据集特征，而是跨数据集通用的、与细胞身份相关的特征。

![](https://pic1.imgdb.cn/item/68f844253203f7be008a4599.png)