---
layout: post
title: 'Learning Activation Functions to Improve Deep Neural Networks'
date: 2021-10-26
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6177560e2ab3f51d9196efa4.jpg'
tags: 论文阅读
---

> APL：自适应分段线性单元.

- paper：[Learning Activation Functions to Improve Deep Neural Networks](https://arxiv.org/abs/1412.6830)

通常的激活函数在每个神经元上都是固定的，作者设计了一种分段线性激活函数，通过梯度下降法为每个神经元独立学习不同的激活函数；该激活函数被称为**自适应分段线性单元**(**adaptive piecewise linear unit, APL**)。

**APL**的表达式如下，是由一系列形如**Hinge**函数组成的：

$$ h(x) = \max(0,x)+\sum_{s=1}^{S}a^s\max (0,-x+b^s) $$

对于每个神经元，额外引入$2S$个可学习的参数，通过梯度下降更新这些参数。下图展示了$S=1$时的一些情况，相比于**maxout**激活函数，该函数可以表示非凸函数，且只需要一个输入。

![](https://pic.imgdb.cn/item/61775ac52ab3f51d9199e0af.jpg)

通过应用**APL**激活函数，提升了分类网络的性能。

![](https://pic.imgdb.cn/item/617761c42ab3f51d919eb4f8.jpg)

作者展示了不同层的不同神经元的激活函数，虚线表示激活函数的初始化结果，实线为最终学习得到的函数。

![](https://pic.imgdb.cn/item/617760f32ab3f51d919e2a7c.jpg)