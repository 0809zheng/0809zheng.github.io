---
layout: post
title: 'Rethinking ImageNet Pre-training'
date: 2020-09-14
author: 郑之杰
cover: 'https://pic.downk.cc/item/5f618e8f160a154a67641b9c.jpg'
tags: 论文阅读
---

> 对计算机视觉任务中预训练的一些讨论.

- paper：Rethinking ImageNet Pre-training
- arXiv：[link](https://arxiv.org/abs/1811.08883)

在目标检测，语义分割等视觉任务中通常使用在**ImageNet**数据集上预训练的模型作为**backbone**，通过微调来提升模型性能。作者认为，预训练的模型在**ImageNet**之类的大规模数据上确实提取到一些通用特征，但使用预训练模型并不是必须的。

虽然在目标检测领域，也有一些论文（如**ICCV2017**的**DSOD**、**ECCV2018**的**DetNet**和**CornerNet**）提出不使用预训练模型，但它们都设计了一些特殊的结构加上各种**tricks**使得可以训练。作者从更广泛的角度讨论了这个问题。

# 实验设置
**Batch Norm(BN)**可以帮助模型更好的优化，但是依赖于**batch**的大小。目标检测任务的输入图像分辨率通常比分类任务大，导致**batch size**不能设置太大，而较小的**batch size**又会引入噪声，使训练不稳定。通常引入预训练模型时选择冻结**BN**的统计量，微调时直接使用其值。而重新训练的模型有两种引入**Norm**的方式：
1. 使用**Group Normalization(GN)**。
2. 使用**synchronized batch normalization(SyncBN)**。设置单卡的**batch size**后，传统的**BN**按照单卡样本计算统计值，而**SyncBN**按照多卡样本（单卡样本$×$**GPU**数）计算统计值。

实验时每个模型都训练到收敛。

# 实验分析
下图表明随着训练轮数的增加，预训练和重新训练的模型都达到了相同的表现，但使用预训练模型所需的轮数更少。**AP**突变的地方是使用了学习率衰减。

![](https://pic.downk.cc/item/5f61d1f8160a154a6773cf92.jpg)

在基础模型上，作者引入了新的技巧（使用数据增强、使用**Cascade R-CNN**模型），试验结果表明两种方式得到的最终结果仍然几乎一致。

![](https://pic.downk.cc/item/5f61d29f160a154a6773fc5e.jpg)

对于位置敏感的任务（人体关键点检测），重新训练的模型在很少的迭代次数下便达到了与预训练模型相当的结果。作者认为关键点检测对精细的空间位置有更高的要求，在这种情况下预训练模型的作用大打折扣，它并不能够提供丰富的定位信息来辅助关键点检测。

![](https://pic.downk.cc/item/5f61d329160a154a6774287f.jpg)

之前的实验使用了**COCO**的全部数据进行训练。作者通过实验表明，即使使用更少的数据训练，重新训练的模型仍然可以达到不低于预训练模型的效果：

![](https://pic.downk.cc/item/5f61d360160a154a67743e22.jpg)

通过实验作者得到如下结论：
1. 针对目标任务重新训练网络模型是可行的；
2. 重新训练网络模型需要更多的训练轮数才能收敛；
3. 经过足够多的训练轮数，重新训练并不会比使用**ImageNet**预训练的模型表现差，即使训练集更小（**10k COCO**）；
4. 使用**ImageNet**预训练的模型能够加速训练在目标任务上的收敛；
5. 使用**ImageNet**预训练的模型并不能解决过拟合问题，除非数据集非常小；
6. 如果目标任务相比于分类对位置更敏感，则使用**ImageNet**预训练的模型帮助非常小。

