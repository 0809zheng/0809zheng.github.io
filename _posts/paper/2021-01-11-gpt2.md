---
layout: post
title: 'Language Models are Unsupervised Multitask Learners'
date: 2021-01-11
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ed3fc15132923bf843d532.jpg'
tags: 论文阅读
---

> GPT2：语言模型是无监督的多任务模型.

- paper：[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)

**GPT2**模型是对预训练语言模型**GPT**的改进，模型更大，训练数据更多。它在大规模无监督语料库上进行语言模型学习，可以微调到不同的下游自然语言处理任务中。

**GPT2**模型采用语言建模作为无监督训练任务，即建立输入序列的条件概率模型$p(x)=\prod_{i=1}^{n}p(s_i\|s_1,s_2,...,s_{i-1})$。作者认为这种建模可以自然地适配自然语言处理中的各种任务，因为这些下游无监督任务可以被表示成$p(\text{output}\|\text{input},\text{task})$(本质还是序列到序列问题)。而**BERT**那种双向建模模型$p(x)=\prod_{i=1}^{n}p(s_i\|s_1,...,s_{i-1},s_{i+1},...,s_{n})$，则无法直接解决上述问题。

**GPT2**模型采用**Transformer**的解码器，在结构上把**LayerNorm**提前到自注意力运算和全连接层之前：

![](https://pic.imgdb.cn/item/60ed45375132923bf8751d1c.jpg)
