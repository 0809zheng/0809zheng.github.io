---
layout: post
title: 'AdaX: Adaptive Gradient Descent with Exponential Long Term Memory'
date: 2020-12-21
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/62148f172ab3f51d91b542c2.jpg'
tags: 论文阅读
---

> AdaX：基于指数长期记忆的自适应梯度下降.

- paper：[AdaX: Adaptive Gradient Descent with Exponential Long Term Memory](https://arxiv.org/abs/2004.09740)

# 1. Adam的衰减策略

**Adam**优化器的更新过程如下：

$$ \begin{align} m_t &= β_1m_{t-1} + (1-β_1)g_t \\ v_t &= β_2v_{t-1} + (1-β_2)g_t^2 \\\hat{m}_t &= \frac{m_t}{1-β_1^t} \\ \hat{v}_t &= \frac{v_t}{1-β_2^t} \\ θ_t&=θ_{t-1}-\gamma \frac{\hat{m}_t}{\sqrt{\hat{v}_t}+ε} \end{align} $$

在**Adam**优化器中，计算梯度$g_t$的二阶矩$v_t$使用指数滑动平均的形式，表示为：

$$ \hat{v}_t = \frac{v_t}{1-β_2^t} = \frac{\beta_2 v_{t-1}+(1-\beta_2)g_t^2}{1-β_2^t} \\ = \frac{\beta_2 \hat{v}_{t-1}(1-β_2^{t-1})+(1-\beta_2)g_t^2}{1-β_2^t} \\ = \beta_2\frac{1-β_2^{t-1}}{1-β_2^t}\hat{v}_{t-1} + \frac{1-\beta_2}{1-β_2^t}g_t^2 \\ = \beta_2\frac{1-β_2^{t-1}}{1-β_2^t}\hat{v}_{t-1} +(1-\beta_2\frac{1-β_2^{t-1}}{1-β_2^t})g_t^2 $$

若记$\hat{\beta}_{2,t}=\beta_2\frac{1-β_2^{t-1}}{1-β_2^t}$，则$\hat{v}_t$的更新过程为：

$$ \hat{v}_t = \hat{\beta}_{2,t}\hat{v}_{t-1} +(1-\hat{\beta}_{2,t})g_t^2 $$

当$t=1$时，$$\hat{\beta}_{2,t}=0$$，此时$$\hat{v}_t=g_t^2$$，使用实时梯度校正学习率；当$$t \to ∞$$时，$$\hat{\beta}_{2,t}=\beta_2$$，由于训练后期梯度变小，仍然校正学习率可能会导致梯度方向的改变，从而导致训练不稳定。因此希望训练后期算法退化为**SGD**，即$$\hat{\beta}_{2,t}\to 1$$。

# 2. AdaX的衰减策略

作者提出了**AdaX**。**AdaX**对**Adam**有两个改进，其一是去除了动量的偏差修正($\hat{m}_t = \frac{m_t}{1-β_1^t}$)；其二是引入了指数长期记忆，即修改了二阶矩的累积形式：

$$ v_t = (1+\beta_2)v_{t-1}+\beta_2 g_t^2 \\ \hat{v}_t = \frac{v_t}{(1+\beta_2)^t-1} $$

通常取$\beta_2=0.0001$，这种形式使得历史二阶矩的累积比重越来越大，即实现对二阶矩的长期记忆。上式也可写作：

$$ \hat{v}_t = \frac{v_t}{(1+\beta_2)^t-1}  = \frac{(1+\beta_2)v_{t-1}+\beta_2 g_t^2}{(1+\beta_2)^t-1} \\ = \frac{(1+\beta_2)((1+\beta_2)^{t-1}-1)\hat{v}_{t-1}+\beta_2 g_t^2}{(1+\beta_2)^t-1} \\ =  \frac{(1+\beta_2)^t-1-\beta_2}{(1+\beta_2)^t-1}\hat{v}_{t-1}+ \frac{\beta_2}{(1+\beta_2)^t-1}g_t^2 \\ = (1- \frac{\beta_2}{(1+\beta_2)^t-1})\hat{v}_{t-1}+ \frac{\beta_2}{(1+\beta_2)^t-1}g_t^2 $$

若记$\hat{\beta}_{2,t}=1- \frac{\beta_2}{(1+\beta_2)^t-1}$，则$\hat{v}_t$的更新过程为：

$$ \hat{v}_t = \hat{\beta}_{2,t}\hat{v}_{t-1} +(1-\hat{\beta}_{2,t})g_t^2 $$

当$t=1$时，$$\hat{\beta}_{2,t}=0$$；当$$t \to ∞$$时，$$\hat{\beta}_{2,t}=1$$。

**AdaX**的完整更新过程为：

$$ \begin{align} m_t &= β_1m_{t-1} + (1-β_1)g_t \\ v_t &= (1+\beta_2)v_{t-1}+\beta_2 g_t^2 \\ \hat{v}_t &= \frac{v_t}{(1+\beta_2)^t-1} \\ θ_t&=θ_{t-1}-\gamma \frac{m_t}{\sqrt{\hat{v}_t}+ε} \end{align} $$

# 3. 实验分析

试验结果表明，由于不稳定的二阶矩，**Adam**在实验早期收敛速度较快，但最终性能不如**SGD**，可能陷入局部极小值。**AdaX**消除了二阶矩累积的不稳定性，且仍具有较高的性能表现。

![](https://pic.imgdb.cn/item/62148efe2ab3f51d91b4fe20.jpg)