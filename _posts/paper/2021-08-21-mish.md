---
layout: post
title: 'Mish: A Self Regularized Non-Monotonic Activation Function'
date: 2021-08-21
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6120b7e74907e2d39c91ecb8.jpg'
tags: 论文阅读
---

> Mish：一种自正则化的非单调激活函数.

- paper：Mish: A Self Regularized Non-Monotonic Activation Function
- arXiv：[link](https://arxiv.org/abs/1908.08681)
- code: [github](https://github.com/digantamisra98/Mish)

# 1. 动机
**Swish**激活函数是通过神经结构搜索构造的，使用**ResNet-20**在**CIFAR-10**分类数据集上搜索了$10K$步，获得以下结果：

$$ \text{Swish}(x) = x\cdot \text{sigmoid}(x) $$

作者进一步设计了一些与**Swish**相似的激活函数，通过实验验证它们的表现：

![](https://pic.imgdb.cn/item/6120be604907e2d39c9feb91.jpg)

作者选出了表现最好、实验最稳定的一个激活函数，将其称为**Mish**激活函数：

$$ \text{Mish}(x) = x\cdot \text{tanh}(\text{softplus}(x)) \\ =x\cdot \text{tanh}(\ln(1+e^x)) $$

# 2. Mish的特性
**Mish**激活函数主要有以下几个特点
- **连续可微**(**continuously differentiable**)：避免了梯度优化时由于奇点的存在而引入的副作用；
- **无上界**(**unbounded above**)：避免饱和使得接近$0$的梯度减缓训练过程；
- **有下界**(**bounded below**)：下界约为$-0.30884$，具有正则化的作用，；
- **非单调**(**non-monotonic**)：保存了一定程度的负值信息，增强了表现力和信息流动。

此外，**Mish**使得**输出平面**(**output landscape**)和**损失平面**(**loss landscape**)更平滑。

输出平面是指随机初始化一个网络，将可视化空间的坐标输入网络，输出相应的标量。作者展示了使用**ReLU**和**Mish**激活函数对应的输出平面，后者更加平滑：

![](https://pic.imgdb.cn/item/6120c1eb4907e2d39ca744cf.jpg)

更平滑的输出平面会产生更平滑的损失平面，有助于更容易的优化和更好的泛化。作者展示了使用**ReLU**，**Mish**和**Swish**激活函数对应的损失平面。**Mish**对应的损失平面更加平滑，具有更宽的最小区域，取得的损失值也更小；而其余两个损失平面具有多个局部极小值。

![](https://pic.imgdb.cn/item/6120c2d24907e2d39ca934c9.jpg)

# 3. Mish与Swish的联系
**Mish**的一阶导数可以与**Swish**建立联系：

$$ f(x) = x\cdot \text{tanh}(\text{softplus}(x)) \\ f'(x) =  \text{tanh}(\text{softplus}(x)) +x\cdot (\text{sech}^2(\text{softplus}(x)))\cdot \text{sigmoid}(x) \\ = (\text{sech}^2(\text{softplus}(x)))\cdot(x\cdot \text{sigmoid}(x)) + \text{tanh}(\text{softplus}(x)) \\ = \Delta(x)\text{Swish}(x)+\frac{f(x)}{x} $$

上式中$\Delta(x)$相当于一个预条件算子(**preconditioner**)，使得梯度更加平滑，且提供了更强的正则化效果，使得**Mish**比**Swish**在更深更复杂的网络中表现更好。