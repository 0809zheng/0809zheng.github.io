---
layout: post
title: 'Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks'
date: 2020-12-19
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/621339102ab3f51d919b9eb4.jpg'
tags: 论文阅读
---

> NovoGrad：使用层级自适应二阶矩进行梯度归一化.

- paper：[Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks](https://arxiv.org/abs/1905.11286)

作者提出了**NovoGrad**，一种大批量训练时的自适应随机梯度下降方法。该方法的特点是逐层梯度归一化和解耦权重衰减，对学习率和初始化权重的选择具有鲁棒性，且内存占用量只有**Adam**的一半。

## 1. Adam算法的缺点

**Adam**在更新过程中累积低阶矩（忽略偏差修正）：

$$ m_t = \beta_1 \cdot m_{t-1} + (1-\beta_1) \cdot g_t \\ v_t = \beta_2 \cdot v_{t-1} + (1-\beta_2) \cdot g_t^2 \\ \theta_t = \theta_{t-1} - \alpha \cdot \frac{m_t}{\sqrt{v_t}+\epsilon} $$

在实践中，二阶矩的累积可能会消失或爆炸，尤其是在训练的初始阶段。此外，**Adam**的训练准确性通常低于动量**SGD**算法。

作者将**Adam**中二阶矩的计算替换为逐层计算，并使用经过二阶矩归一化的梯度来计算一阶矩，从归一化梯度中解耦权重衰减因子，从而提出了**NovoGrad**算法。

## 2. NovoGrad

**NovoGrad**对网络的每一层分别计算二阶矩：

$$ v_t^l = \beta_2 \cdot v_{t-1}^l + (1-\beta_2) \cdot ||g_t^l||^2  $$

使用上述二阶矩进行梯度归一化，计算一阶矩：

$$ m_t^l = \beta_1 \cdot m_{t-1}^l + \frac{g_t^l}{\sqrt{v_t^l}+\epsilon}  $$

进一步将权重衰减增加到一阶矩的计算中：

$$ m_t^l = \beta_1 \cdot m_{t-1}^l + (\frac{g_t^l}{\sqrt{v_t^l}+\epsilon}+\lambda w_t^l)  $$

参数更新公式如下：

$$ w_{t+1} = w_t-\eta m_t $$

使用下列初始化消除偏差：

$$ v_1^l=||g_1^l||^2, m_1^l=\frac{g_1^l}{\sqrt{v_1^l}}+\lambda w_1^l $$

**NovoGrad**在一阶矩的计算中使用梯度归一化，而不是在权值更新中对步长归一化，以提高算法对非常大的梯度“异常值”的鲁棒性。 

![](https://pic.imgdb.cn/item/621353d22ab3f51d91f4e5cf.jpg)

## 3. 实验分析

作者训练了四种模型：**ResNet-50**用于图像分类，**Transformer-big**用于机器翻译，**Jasper**用于语音识别，**Transformer-XL**用于语言模型。

实验结果表明，**NovoGrad**的表现与**Adam**类似，但只需要大约一半的内存。

