---
layout: post
title: 'Empirical Evaluation of Rectified Activations in Convolutional Network'
date: 2021-08-31
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/612c78c844eaada739a65e35.jpg'
tags: 论文阅读
---

> RReLU：受限激活函数的经验验证.

- paper：Empirical Evaluation of Rectified Activations in Convolutional Network
- arXiv：[link](https://arxiv.org/abs/1505.00853)

使用**ReLU**等非饱和的激活函数能够缓解梯度消失/爆炸问题，并加速收敛。有些研究认为**ReLU**的表现得益于其输出是稀疏的，即大约有一半的输出为$0$(对应输入为负值)。
作者在数据集**CIFAR-10/CIFAR100**上对**ReLU**族激活函数进行实验，实验结果表明稀疏性并不是关键，通过对负半轴增加较小的负值能够提高表现。


![](https://pic.imgdb.cn/item/612c792e44eaada739a74171.jpg)


**RReLU**全称是**Randomized Leaky ReLU**，是在**kaggle**竞赛[ National Data Science Bowl (NDSB)](https://www.kaggle.com/c/datasciencebowl) 上被提出的。其表达式如下：

$$
        \text{RReLU}(x) =
        \begin{cases}
        x,  & x≥0 \\
        \alpha x, & x<0
        \end{cases}
$$

- 训练时参数$\alpha$从均匀分布$U(l,u)$中抽样得到，$0≤l<u<1$。
- 测试时参数$\alpha$是固定的：$\alpha=\frac{l+u}{2}$

注意到上述训练和测试采用不同设置的方式类似于**Dropout**，通过引入随机性增加了正则化的效果。

实验结果如下，通过实验作者得出以下几个结论：
- 负半轴斜率不为$0$的**leaky**系列**ReLU**比标准的**ReLU**表现好，说明稀疏性并不重要；
- 在小数据集上，**PReLU**训练误差最小，容易过拟合；
- **RReLU**的测试误差最小，说明随机化的引入具有正则化的效果。

![](https://pic.imgdb.cn/item/612c80aa44eaada739b8d0e3.jpg)