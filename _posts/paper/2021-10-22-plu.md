---
layout: post
title: 'Learning specialized activation functions with the Piecewise Linear Unit'
date: 2021-10-22
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/61729bee2ab3f51d91e00048.jpg'
tags: 论文阅读
---

> PWLU：使用分段线性单元学习激活函数.

- paper：[Learning specialized activation functions with the Piecewise Linear Unit](https://arxiv.org/abs/2104.03693)

最近对激活函数进行自动搜索的方法(如**Swish**)逐渐取代了手工设计的激活函数(如**ReLU**族)。然而这种自动设计的方法具有一些局限性：
- 自动搜索的空间通常是受限和离散的，激活函数组合的微小变化可能导致完全不同的结果，增加了搜索困难。
- 搜索方法通常需要评估大量不同的候选激活函数，计算成本较高。
- 搜索时通常在给定数据集和网络结构上进行，针对不同数据集或网络结构设计不同的激活函数是不现实的。

也有一些工作使用通用近似器作为激活函数(如**APL**和**PAU**)。这些通用近似器的参数是连续的，可以通过梯度更新。但这些方法给出的近似器公式复杂，可能导致学习不稳定或推理效率低。

作者提出了一种**分段线性单元**(**piecewise linear unit, PWLU**)，包括一种基于分段线性公式的近似器和一种基于梯度的学习方法。分段线性公式实现简单，覆盖了任意标量函数，可以通过梯度学习参数，且推理效率高。**PWLU**可以实现在不同数据集和网络结构上学习不同的激活函数。

![](https://pic.imgdb.cn/item/61736d9c2ab3f51d91a2d1c2.jpg)

**PWLU**的函数形式如上图所示。给定区间数量$N$，$N$越大函数拟合能力越强；左边界$B_L$和右边界$B_R$，定义函数关注的有效区域；$N+1$个分界点的函数值$Y_P$，控制函数的形状；最左端斜率$K_L$和最右端斜率$K_R$，控制边界外的区域。

**PWLU**的形式如下：

$$ \text{PWLU}_N(x,B_L,B_R,Y_P,K_L,K_R) = \\ \begin{cases}  (x-B_L)*K_L+Y_P^0, & x<B_L \\ (x-B_R)*K_R+Y_P^N, & x\geq B_R \\ (x-B_{idx})*K_{idx}+Y_P^{idx}, & B_L\leq x<B_R \end{cases} $$

其中$idx$为$x$所属区间的索引号，$B_{idx}$为该区间的左端点，$K_{idx}$为该区间的斜率。区间$[B_L,B_R]$被均分为$N$份，则每个子区间的宽度为$d=\frac{B_R-B_L}{N}$，所属区间计算为：

$$ idx=\lfloor \frac{x-B_L}{d} \rfloor $$

$$ B_{idx} = B_L+idx*d $$

$$ K_{idx}=\frac{Y_P^{idx+1}-Y_P^{idx}}{d} $$

**PWLU**具有以下性质：
- **PWLU**是一种通用近似器，可以近似任意连续有界标量函数。
- **PWLU**随其参数连续变化，可以通过梯度优化。
- **PWLU**的自由度在一个有界区间内，可以充分利用可学习参数。
- 由于等分区间，**PWLU**在计算方面非常高效。

**PWLU**的梯度计算如下：

![](https://pic.imgdb.cn/item/617372bb2ab3f51d91a77a74.jpg)

**PWLU**随网络训练一起更新。通常把**PWLU**初始化为**ReLU**(设置$N$为偶数)，此时$K_L=0,K_R=1,B_L=-B_R,Y_P^{idx}=\text{ReLU}(B_{idx})$。

**PWLU**在训练时可能会出现**输入边界不对齐**(**Input-boundary misalignment**)。**PWLU**的主要自由度分布在区间$[B_L,B_R]$内，如果输入数据的分布没有和该区域对其，则会降低有效自由度。如下图所示，输入分布主要在左端，与区间$[B_L,B_R]$只有很少的交集，则**PWLU**学习到的大部分参数对网络的贡献很小，从而影响网络的性能。

![](https://pic.imgdb.cn/item/617375142ab3f51d91a94574.jpg)

作者提出了一种基于统计的对齐方法，如下图所示。首先将所有**PWLU**初始化为**ReLU**。该对齐方法在整个$T$轮训练中分成两个阶段（实验中$T'=5$,$N=16$）。

![](https://pic.imgdb.cn/item/617375f02ab3f51d91a9ec68.jpg)

第一阶段($0$~$T'-1$轮)不更新**PWLU**的参数(保持为**ReLU**)，每轮统计输入数据的均值$\mu$和标准差$\sigma$，并滑动更新参数：

$$ \mu=\mu*0.9+\text{mean}(x)*0.1 $$

$$ \sigma=\sigma*0.9+\text{std}(x)*0.1 $$

第二阶段($T'$~$T$轮)将每个**PWLU**的参数重置如下：

$$ B_L=\mu-3*\sigma, B_R=\mu+3*\sigma $$

$$ K_L=0,K_R=1 $$

$$ Y_P^{idx}=\text{ReLU}(B_{idx}),idx \in \{0,1,2,...,N\} $$

参数重置后**PWLU**会与输入数据对齐，此时再通过梯度更新**PWLU**的参数。

下图展示了不同激活函数在不同网络结构上的性能表现，以及相对于**ReLU**激活函数的改进。同一个激活函数在不同的网络上的改进是不一致的。**PWLU**在不同结构上均有较大的性能提升。

![](https://pic.imgdb.cn/item/617399d22ab3f51d91c80e78.jpg)

作者进一步可视化不同层学习到的**PWLU**函数。结果表明，浅层激活函数类似于线性函数，而深层激活函数呈“V”型。

![](https://pic.imgdb.cn/item/61739b6f2ab3f51d91c9f772.jpg)