---
layout: post
title: 'GLU Variants Improve Transformer'
date: 2021-01-09
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ed2a1c5132923bf880dc2c.jpg'
tags: 论文阅读
---

> T5.1.1：使用GLU改进预训练语言模型T5.

- paper：GLU Variants Improve Transformer
- arXiv：[link](https://arxiv.org/abs/2002.05202)

本文中作者借助**门控线性单元(gated linear unit,GLU)**对预训练语言模型**T5**进行了一次升级。将之前的预训练语言模型**T5**记为**T5.1.0**，而使用**GLU**后的预训练语言模型记为**T5.1.1**。**T5.1.1**相对于**T5.1.0**的主要区别如下：
1. **T5.1.0**中全连接层的激活函数使用**ReLU**：$$\text{FFN}_{\text{ReLU}}(x,W_1,W_2)=\text{max}(xW_1,0)W_2$$；而**T5.1.1**中激活函数使用**GELU**激活的门控线性单元：$$\text{FFN}_{\text{GEGLU}}(x,W,V,W_2)=(\text{GELU}(xW) \otimes xV)W_2$$。尽管这样增加了一半参数，但效果明显提升。
2. **T5.1.0**中编码器输入端的**Embedding**,解码器输入端的**Embedding**和解码器输出端的**Embedding**是共享参数的；而**T5.1.1**中只让编码器输入端的**Embedding**和解码器输入端的**Embedding**共享参数，额外训练解码器输出端的**Embedding**。尽管这样大大增加了参数量，但效果会更好。
3. **T5.1.1**在预训练阶段只做无监督预训练，且去掉了**Dropout**；在下游任务的微调阶段使用**Dropout**。

门控线性单元其实是一类方法的总称。对于具有门控特性的激活函数$\sigma$，可以构造门控线性单元：

$$ GLU(x,W,V) = \sigma(xW) \otimes xV $$

实验证明门控线性单元能够有效地提升**T5**模型的性能：

![](https://pic.imgdb.cn/item/60ed2e2d5132923bf8a466b4.jpg)
