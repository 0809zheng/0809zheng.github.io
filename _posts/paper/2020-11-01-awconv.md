---
layout: post
title: 'An Attention Module for Convolutional Neural Networks'
date: 2020-11-01
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63b936b2be43e0d30ee93161.jpg'
tags: 论文阅读
---

> AW-conv：一个卷积神经网络的注意力模块.

- paper：[An Attention Module for Convolutional Neural Networks](https://arxiv.org/abs/2108.08205v1)

当前注意力机制存在两个问题：
- 近似问题(**the approximation problem**)：注意力机制真正达到自适应校准需要生成和特征图尺寸一样大的注意力图，而现有注意力机制为降低计算量都采用沿某些维度的注意力。
- 容量不足问题(**the insufficient capacity problem**)：注意力机制不能改变特征的尺寸，从而限制了网络容量。

**AW-conv**通过生成与卷积核尺寸相同的注意力图并作用于卷积核，实现了多通道、多区域的注意力机制。记卷积核$$K \in \Bbb{R}^{C_2\times C_1 \times h \times w}$$，生成的注意力图为$$A \in \Bbb{R}^{C_2\times C_1 \times h \times w}$$，则先对特征应用注意力机制，再应用卷积核得到的输出特征为：

$$ O_{[c_2,c_1,m,n]} = Convolution(I \otimes A, K) \\ = \sum_{c_1}^{C_1} \sum_{i}^{h} \sum_{j}^{w} (I_{[c_1,m+i,n+j]}\times A_{[c_2,c_1,i,j]}) \times K_{[c_2,c_1,i,j]} \\ = \sum_{c_1}^{C_1} \sum_{i}^{h} \sum_{j}^{w}I_{[c_1,m+i,n+j]}\times ( A_{[c_2,c_1,i,j]}\times K_{[c_2,c_1,i,j]})  \\ = Convolution(I, A \otimes K) $$

因此等价于先把注意力图与卷积核相乘，再作用于输入特征。

![](https://pic.imgdb.cn/item/63b93ac7be43e0d30eef26ff.jpg)

注意力图$A$的生成过程采用如下网络，通过逐点卷积实现：

![](https://pic.imgdb.cn/item/63b93ad8be43e0d30eef38d9.jpg)