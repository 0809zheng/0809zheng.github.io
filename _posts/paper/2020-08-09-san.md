---
layout: post
title: 'Second-order Attention Network for Single Image Super-Resolution'
date: 2020-08-09
author: 郑之杰
cover: 'https://pic.downk.cc/item/5f73e3d4160a154a67af857e.jpg'
tags: 论文阅读
---

> SAN：超分辨率二阶注意力网络.

- paper：Second-order Attention Network for Single Image Super-Resolution
- CVPR：[link](https://openaccess.thecvf.com/content_CVPR_2019/html/Dai_Second-Order_Attention_Network_for_Single_Image_Super-Resolution_CVPR_2019_paper.html)
- code：[github](https://github.com/daitao/SAN)

作者认为，现存的超分辨率模型仍然存在一些限制：
1. 大多基于卷积神经网络的方法没有充分利用原始**LR**图像的信息；
2. 这些方法主要专注于设计更深或更宽的网络，却很少发掘层间特征的内在相关性。

作者提出了**SAN**以获得更好的特征表示和特征相关性学习。作者提出了二阶通道注意力机制进行特征的相关性学习，提出了非局部增强残差组来捕捉长距离的空间信息。网络整体结构如下：

![](https://pic.downk.cc/item/5f73e513160a154a67afcda7.jpg)

**SAN**主要由四个部分组成：
1. 一层卷积层提取浅层特征；
2. 非局部增强残差组提取深层特征；
3. 上采样层构造高分辨率特征；
4. 一层卷积层重建高分辨率图像。

## 非局部增强残差组 Non-locally Enhanced Residual Group (NLRG)
**NLRG**是由首尾的**NL-RL**模块和中间的**SSRG**模块构成的。

同源残差组**Share-source Residual Group(SSRG)**模块是由$G$个局部**LSRAG**模块和一个同源残差连接**SSC**构成的。同源残差连接将浅层特征加到每一组的输入中，从而传递丰富的低频信息。

**NL-RL**模块将特征按空间划分为$k \times k$，在每个区域中进行**non-local**操作。这样避免了对整体操作引入的大量计算量。

**LSRAG**模块引入了局部残差连接，并使用二阶通道注意力机制。

## 二阶通道注意力 Second-order Channel Attention (SOCA)
通道注意力是指对特征的每一个通道提取一个统计量，根据该统计量对特征的每一个通道重新赋予权重。通常使用一阶统计量（如均值，最大值），二阶通道注意力机制使用了二阶统计量。

作者通过**协方差归一化**计算每个通道的二阶统计量。先计算特征映射的协方差矩阵，然后将协方差矩阵进行特征值分解，最后将协方差归一化，转化为特征值的$\alpha$(文中取$\frac{1}{2}$)次幂。这样便得到了归一化的协方差矩阵，代表了不同通道特征的相关性。

记特征表示为$X$，计算其协方差矩阵：

$$ \Sigma = X \overline{I} X^T $$

其中$\overline{I} = \frac{1}{s} (I - \frac{1}{s}1)$，$I$表示$s \times s$的单位矩阵，$1$表示全$1$矩阵。

对上述协方差矩阵进行奇异值分解：

$$ \Sigma = U \Lambda U^T $$

将协方差归一化转化为特征值的幂：

$$ \hat{Y} = \Sigma^\alpha = U \Lambda^\alpha U^T $$

其中$\alpha$为正实数。$\alpha = 1$表示无归一化。

对协方差矩阵进行归一化后，可计算统计量：

$$ z_c = H_{GCP}(y_c) = \frac{1}{C} \sum_{i}^{C} {y_c(i)} $$