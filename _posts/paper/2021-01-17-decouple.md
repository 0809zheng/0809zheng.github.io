---
layout: post
title: 'Decoupling Representation and Classifier for Long-Tailed Recognition'
date: 2021-01-17
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ee78ad5132923bf865396a.jpg'
tags: 论文阅读
---

> 将长尾分布的图像分类问题解耦为表示学习和分类.

- paper：Decoupling Representation and Classifier for Long-Tailed Recognition
- arXiv：[link](https://arxiv.org/abs/1910.09217)

在本文中，作者提出了一种解决长尾分布数据集上的图像分类问题的新思路：将模型**解耦(decouple)**为**表示学习(representation learning)**和**分类(classification)**两个部分。如对常用的**ResNeXt**分类模型，可以看作一个**backbone**和一个**classifier**(通常是最后一层全连接层)。**backbone**从图像中提取特征；**classifier**给每个特征不同类别的得分。由于后者对于全连接层是很直观的，因此对模型的这种拆分应该是合理的。

本文的实验过程如下，首先在长尾分布数据集上训练模型，然后通过重采样方法构造一个类别平衡数据集，将模型在数据集上微调并测试性能。通过实验得到以下结论：
1. 数据不平衡问题并不影响模型(在长尾分布数据集上)学习到高质量的特征表示；
2. 在类别重采样的数据上进行微调后，模型能获得较好的长尾分布识别能力。

### 1. Sampling Strategies
首先介绍几种使用重采样方法构造类别平衡样本的采样策略。若数据集共有$C$个类别，第$j$个类别的样本总数为$n_j$，数据集总样本数为$N$；则重采样时采样到第$j$个类别的样本的概率是：

$$ p_j = \frac{n_j^q}{\sum_{i=1}^{C}n_i^q} $$

其中$q \in \[0,1\]$控制重采样的比例。其取值不同对应不同的重采样策略：
- **Instance-balanced sampling**：取$q=1$，即每个样本被采样的概率相同，样本数越多的类别被采样到的概率越大：

$$ p_j^{IB} = \frac{n_j}{\sum_{i=1}^{C}n_i} = \frac{n_j}{N} $$

- **Class-balanced sampling**：取$q=0$，即每个类别被采样的概率相同：

$$ p_j^{CB} = \frac{1}{\sum_{i=1}^{C}1} = \frac{1}{C} $$

- **Square-root sampling**：取$q=\frac{1}{2}$：

$$ p_j^{SR} = \frac{n_j^\frac{1}{2}}{\sum_{i=1}^{C}n_i^\frac{1}{2}} $$

- **Progressively-balanced sampling**：实例平衡和类别平衡的混合，训练前期偏向实例平衡，训练后期偏向类别平衡：

$$ p_j^{PB}(t) = (1-\frac{t}{T})p_j^{IB} +\frac{t}{T}p_j^{CB} $$


### 2. Fine Tuning
作者使用**两阶段(two-stage)**的训练方法。首先在原始数据集(类别不平衡数据集)上训练模型(这一步被看作特征学习阶段)，然后构造类别平衡数据集，在其上进行微调。下面介绍几种微调方法：
- **Joint**：特征学习和微调同时进行，即先构造类别平衡数据集，再进行训练(同时训练**backbone**和**classifier**)，这也是之前常用的方法。
- **Classifier Re-training (cRT)**：在原始数据集上训练后，固定模型的**backbone**，只微调**classifier**。
- **Nearest Class Mean classifier (NCM)**：(不需要微调)首先计算训练集上每一个类别的平均特征表示，再用最近邻方法判断新数据的类别。
- **$\tau$-normalized classifier ($\tau$-normalized)**：实验经验表明不平衡数据集上训练的模型的**classifier**，其不同类别之间的权重范数差别较大；平衡数据集上训练的模型的**classifier**，其不同类别之间的权重范数比较接近。引入超参数$\tau$对类别$i$的权重$w_i$进行缩放：$$\tilde{w}_i = \frac{w_i}{\|w_i\|^{\tau}} $$
- **Learnable weight scaling (LWS)**：思路同上，其缩放系数$f_i$是学习得到的：$$\tilde{w}_i = f_i*w_i $$


### 3. 实验分析

作者分析了在不同的重采样策略下、使用不同的微调方法获得的模型性能。即使是最简单的采样策略(**IB**)和不进行微调的方法(**NCM**)，模型也能获得不错的性能，这说明即使在长尾分布数据集上，模型学习到的特征也具有较高的质量。

![](https://pic.imgdb.cn/item/60ee847f5132923bf8d590d9.jpg)

下图表示微调不同的结果获得的分类性能。只需要微调**classifier**便能获得最好的性能。

![](https://pic.imgdb.cn/item/60ee84c35132923bf8d8073e.jpg)

作者分析了不同类别对应的**classifier**权重范数(下图左)，表明范数大小与类别数具有比较明显的相关性，而**$\tau$-normalized**能够捕捉这种关系；下图右对**$\tau$-normalized**的超参数$\tau$进行了消融：

![](https://pic.imgdb.cn/item/60ee850f5132923bf8dad6ed.jpg)