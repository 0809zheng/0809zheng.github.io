---
layout: post
title: 'Incorporating Nesterov Momentum into Adam'
date: 2020-12-12
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/620089762ab3f51d917ee290.jpg'
tags: 论文阅读
---

> Nadam：将Nesterov动量引入Adam算法.

- paper：[Incorporating Nesterov Momentum into Adam](https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ)

## 1. 相关工作：将动量引入梯度下降

更新深度学习模型参数$\theta$常用梯度下降法：

$$ g_t = \nabla_{\theta_{t-1}} f_t(\theta_{t-1})  \\ \theta_t = \theta_{t-1} - \alpha_t g_t $$

将动量引入梯度下降是最常用的改进：

$$ m_t = \mu_t m_{t-1} + \alpha_t g_t  \\ \theta_t = \theta_{t-1} - m_t $$

通过引入动量，算法能够在更新步长始终较小但方向相同的特征维度上加速移动，而在更新方向显著振荡的特征维度上移动更慢。

动量更新相当于在前一个动量方向和当前梯度方向各迈出一步：

$$ \theta_t = \theta_{t-1} - (\mu_t m_{t-1} + \alpha_t g_t) $$

注意到前一个动量$m_{t-1}$和当前梯度$g_t$是不相关的，为了获得更高质量的梯度方向，可以在计算梯度之前先用动量更新一步参数，即**Nesterov**动量方法：

$$ g_t = \nabla_{\theta_{t-1}} f_t(\theta_{t-1}-\mu m_{t-1})  \\ m_t = \mu_t m_{t-1} + \alpha_t g_t  \\ \theta_t = \theta_{t-1} - m_t $$

[<font color=Blue>Nesterov Momentum</font>](https://0809zheng.github.io/2020/12/08/nesterov.html)在实现时会修改成如下形式：

$$ g_t = \nabla_{\theta_{t-1}} f_t(\theta_{t-1})   \\ \theta_t = \theta_{t-1} - (\mu_{t+1} m_{t} + \alpha_t g_t) $$

对比动量和**Nesterov**动量，其不同在于更新时动量沿着前一个动量$\mu_t m_{t-1}$的方向，而**Nesterov**动量沿着当前动量$\mu_{t+1} m_{t}$的方向。

**Adam**算法(不考虑自适应学习率部分)动量累积从衰减和的形式替换为指数滑动平均的形式，并引入偏差修正：

$$ m_t = \mu m_{t-1} + (1-\mu) g_t  \\ \theta_t = \theta_{t-1} - \alpha_t \frac{m_t}{1-\mu^t} $$

## 2. Nadam：将Nesterov动量引入Adam算法

**Adam**算法的更新步骤也可以写成：

$$ \theta_t = \theta_{t-1} - \alpha_t (\frac{\mu m_{t-1}}{1-\mu}+\frac{(1-\mu_t) g_t}{1-\mu}) $$

随着更新轮数变化而逐渐改变衰减率$\mu$通常是有帮助的，因此设置动态衰减率$$\{\mu_1,...,\mu_T\}$$。在实现时采用以下公式(衰减率$\beta_1=0.9,\psi=0.004$)：

$$ \mu_t = \beta_1(1-\frac{1}{2}0.96^{t\psi}) $$

则更新公式表示为：

$$ \theta_t = \theta_{t-1} - \alpha_t (\frac{\mu_t m_{t-1}}{1-\prod_{i=1}^{t}\mu_i}+\frac{(1-\mu_t) g_t}{1-\prod_{i=1}^{t}\mu_i}) $$

对照动量和**Nesterov**动量，则将**Nesterov**动量引入**Adam**算法只需要把动量更新方向$\mu_t m_{t-1}$替换为$\mu_{t+1} m_{t}$：

$$ \theta_t = \theta_{t-1} - \alpha_t (\frac{\mu_{t+1} m_{t}}{1-\prod_{i=1}^{t}\mu_i}+\frac{(1-\mu_t) g_t}{1-\prod_{i=1}^{t}\mu_i}) $$

完整的**Nadam**算法流程如下：

![](https://pic.imgdb.cn/item/6203416d2ab3f51d91ddacbc.jpg)

## 3. 实验结果

作者使用不同的基于动量的优化方法训练了一个生成**MNIST**数据集的卷积自编码器。试验结果表明，即使需要比较多的超参数，**Nadam**算法在没有微调学习率的情况下也获得了最好的表现。

![](https://pic.imgdb.cn/item/6203428d2ab3f51d91de97e8.jpg)