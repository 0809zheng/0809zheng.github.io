---
layout: post
title: 'Sluice networks: Learning what to share between loosely related tasks'
date: 2021-09-29
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/62dd34caf54cd3f9373cb422.jpg'
tags: 论文阅读
---

> 水闸网络：学习松散相关任务之间的共享表示.

- paper：[Sluice networks: Learning what to share between loosely related tasks](https://arxiv.org/abs/1705.08142v1)

在多任务学习范式中，如果任务之间的关系比较松散，很难估计学习任务共享的特征是否能够提高性能。本文作者设计了**水闸网络(Sluice Network)**，这是一种多任务学习的通用框架，通过可训练参数实现了子空间、层和跳跃连接等所有组合的硬共享或软共享。通过在自然语言七个不同的领域中三个任务上的实验，表明该网络具有更好的性能并能够适应噪声。

# 1. 水闸网络 Sluice Network

![](https://pic.imgdb.cn/item/62de4c82f54cd3f937b9f90f.jpg)

作者设计的水闸网络如图所示。网络的每一层都划分成一系列子空间，比如网络$A$的第一层划分子空间$G_{A,1,1},G_{A,1,2}$，这种划分允许网络同时学习任务特定和共享的表示。然后网络每个子空间的输出通过可学习参数$\alpha$进行加权组合，并被传递到下一层。此外每一层的加权结果还通过可学习参数$\beta$组合成最后的输出特征。因此水闸网络能够学习共享哪些层和子空间，以及网络在哪些层学习到输入数据的最佳表示。

将多任务特征学习建模为一个矩阵正则化问题。设共有$M$个松散的相关任务，为每个任务训练一个$K$层网络，网络总参数量为$D$。则总参数矩阵表示为$$W \in \Bbb{R}^{M \times D}$$，其中每一行表示每一个任务训练的网络。

水闸网络不仅学习每个任务的参数，还学习参数的正则化项$\Omega(W)$：

$$ \lambda_1 \mathcal{L}_1(f(x;\theta_1),y_1)+\cdots + \lambda_M \mathcal{L}_M(f(x;\theta_M),y_M) + \Omega(W) $$

正则化项$\Omega(W)$是通过可学习参数$\alpha$对特征子空间进行加权组合而构造的。假设第$k$层共拆分成$N$个子空间，则不同子空间之间的交互为：

$$ \begin{bmatrix} \tilde{x}_{A,1} \\ \vdots \\ \tilde{x}_{B,N} \end{bmatrix} =  \begin{bmatrix} \alpha_{A1A1} & \cdots & \alpha_{A1BN} \\ \vdots & \vdots & \vdots  \\ \alpha_{BNA1} & \cdots & \alpha_{BNBN} \end{bmatrix}  \begin{bmatrix} x_{A,1} \\ \vdots \\ x_{B,N} \end{bmatrix} $$

作者增加了一个惩罚来阻止共享子空间和任务特定子空间之间的冗余，这是通过每个模型的逐层子空间之间的正交约束建模的（以两个子空间为例）：

$$ \mathcal{L}_c = \sum_{m=1}^{M} \sum_{k=1}^K ||G_{m,k,1}^TG_{m,k,2}||_F^2 $$

每个网络是输出是每一层线性组合子空间的进一步线性组合，即通过不同的层来学习层次关系：

$$ \tilde{x}_{A} = \begin{bmatrix} \beta_{A,1} & \cdots & \beta_{A,K} \end{bmatrix} \begin{bmatrix} \tilde{x}_{A,1} \\ \vdots \\ \tilde{x}_{A,K} \end{bmatrix} $$

对于具有$M$个任务、每个任务使用$K$层网络以及每层拆分$N$个子空间的水闸网络，共引入了$KM^2N^2$个$α$参数和$KM$个$β$参数。

# 2. 实验分析

作者在多个松散相关的自然语言处理任务上进行实验，选择**OntoNotes 5.0**数据集，该数据集提供跨不同语言和领域的一系列标注数据。

![](https://pic.imgdb.cn/item/62de5f9bf54cd3f9371ba3dd.jpg)

作者选取**组块（chunking, CHUNK）**、**命名实体识别（named entity recognition, NER）**和**语义角色标记（semantic role labeling, SRL）**作为主任务，并将它们分别与**词性标记（part-of-speech tagging, POS）**配对作为辅助任务。不同任务的标注样例如下：

![](https://pic.imgdb.cn/item/62de6061f54cd3f9371f49a5.jpg)

作者给出了不同任务组合下的模型性能表现：

![](https://pic.imgdb.cn/item/62de6140f54cd3f93723864a.jpg)

作者对不同的线性组合情况进行消融实验：

![](https://pic.imgdb.cn/item/62de61c7f54cd3f937262868.jpg)

作者对不同任务中不同层学习到的$\alpha$进行可视化。结果表明，更复杂的目标任务往往会共享更多信息：

![](https://pic.imgdb.cn/item/62de6269f54cd3f937291c87.jpg)


