---
layout: post
title: 'Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination'
date: 2022-10-19
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63da2181ac6ef86016045219.jpg'
tags: 论文阅读
---

> 通过非参数化实例级判别实现无监督特征学习.

- paper：[Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination](https://arxiv.org/abs/1805.01978v1)

本文作者设计了无监督学习的**个体判别(Instance-level Discrimination)**任务，即把每一个数据样本看作一个类别，类别总数等于训练集的总样本数$N$。

![](https://pic.imgdb.cn/item/63da233dac6ef86016074b0e.jpg)

该方法首先对输入图像$x$进行特征提取、低维特征映射和**L2**归一化，从而把样本特征$v$映射到单位球面上，进一步通过一个非参数化分类器计算该特征属于类别$i$的概率：

$$ P(C=i|v) = \frac{\exp(v_i^Tv/\tau)}{\sum_{j=1}^N \exp(v_j^Tv/\tau)} $$

![](https://pic.imgdb.cn/item/63da237bac6ef8601607d72a.jpg)

为了避免每次都重复计算所有样本的特征表示，把样本在之前迭代中的特征向量$$V=\{v_i\}$$存储在内存中，用于计算成对相似度。

在实践中由于样本数量$N$较大，在计算非参数化概率时使用蒙特卡洛近似采样一个样本子集$$\{j_k\}_{k=1}^M$$：

$$ P(C=i|v) = \frac{\exp(v_i^Tv/\tau)}{\sum_{j=1}^N \exp(v_j^Tv/\tau)} \approx \frac{\exp(v_i^Tv/\tau)}{\frac{N}{M}\sum_{k=1}^M \exp(v_{j_k}^Tv/\tau)} $$

损失函数采用[**噪声对比估计** (NCE)](http://proceedings.mlr.press/v9/gutmann10a.html)，即运行[逻辑回归](https://0809zheng.github.io/2020/03/13/logistic-regression.html)来区分目标样本和噪声。假设噪声服从均匀分布$P_N=1/N$，则样本$i$属于目标样本的后验分布为：

$$ h(i,v) = \frac{P(i|v)}{P(i|v)+MP_N(i)} $$

进而建立二元交叉熵损失：

$$ \mathcal{L}_{InstDisc} = - \Bbb{E}_{p_{\text{data}}} [\log h(i,v_i^{(t-1)})]  - M \cdot \Bbb{E}_{P_N} [\log(1-h(i,v'^{(t-1)}))] $$

由于每个类别只有一个样本，因此训练过程不稳定，容易出现波动；为了改进训练的平滑性，在损失函数中额外引入近端优化(**proximal optimization**)项：

$$ \begin{aligned} \mathcal{L}_{InstDisc} = &- \Bbb{E}_{p_{\text{data}}} [\log h(i,v_i^{(t-1)})-\lambda ||v_i^{(t)}-v_i^{(t-1)}||_2^2]  \\ & - M \cdot \Bbb{E}_{P_N} [\log(1-h(i,v'^{(t-1)}))] \end{aligned} $$