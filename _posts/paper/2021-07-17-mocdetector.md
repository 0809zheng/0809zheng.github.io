---
layout: post
title: 'Actions as Moving Points'
date: 2021-07-17
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ef91265132923bf871a9e7.jpg'
tags: 论文阅读
---

> MOC-detector：一种anchor-free的时空动作检测模型.

- paper：Actions as Moving Points
- arXiv：[link](https://arxiv.org/abs/2001.04608)
- code：[GitHub](https://deeperaction.github.io/multisports/)

# 1. 问题分析
目前的时空动作检测方法主要使用基于**anchor-based**的目标检测方法(如**Faster RCNN**,**SSD**)，这些方法会预先设计大量的**anchor**，当扩展到视频检测领域需要设计**3D anchor**时，随着输入视频帧长度增加，可能的**anchor**数目会急剧增加。作者提出了一种**anchor-free**的时空动作检测方法：**MOC(MovingCenter)-detector**。

作者认为，合理的使用运动信息可以有效提高动作识别精度。
将动作实例建模为**每一帧动作中心点沿时序的运动轨迹**，依照这种对动作的简化建模方式，思路流程如下：首先在一段动作的视频帧中预测该动作实例的**关键帧中心点**及其动作**类别**，如下图红色点所示。其次预测每一帧上的**动作中心点**距离**关键帧中心点**的运动矢量，如下图黄色箭头所示。然后在每一帧中移动**关键帧中心点**，得到该帧上的**动作中心点**，如下图绿色点所示。最后根据每一帧的**动作中心点**预测一个检测框，如下图深灰色矩形所示。

![](https://pic.imgdb.cn/item/60efb69c5132923bf8414224.jpg)

# 2. 模型介绍

![](https://pic.imgdb.cn/item/60efb4c75132923bf8380225.jpg)

**MOC-detector**的网络结构如上图所示。首先将连续$K$帧图像分别输入共享权重的**2D Backbone**提取每一帧的高层语义特征，获得$K$张特征图；然后由三个分支共同协作来完成时空动作检测任务。三个分支分别介绍如下：
1. **中心点预测分支(Center Branch)**：用于检测**关键帧中心点**的空间位置和所属**类别**。将$K$张特征图沿通道拼接，经过一个$3×3$卷积得到时空融合特征，再通过一个$1×1$卷积得到$K$帧图像序列的**关键帧中心点**的分布热图，其中响应最高的点对应**关键帧中心点**的位置和类别。
2. **运动估计分支(Movement Branch)**：用于估计每一帧上的**动作中心点**距离**关键帧中心点**的运动矢量，形成中心点运动轨迹。将**关键帧中心点**移动到当前帧的对应中心点，至此可以形成连续$K$帧内每个**动作中心点**的运动轨迹。
3. **包围框回归分支(Box Branch)**：逐帧处理，输入单帧特征图，在每一帧的**动作中心点**直接回归**bbox**大小来得到动作实例在空间上的包围框。

# 3. 实验分析
### 与其他模型的性能对比

作者在**UCF101-24**和**JHMDB**两个数据集上对比了不同模型的结果。**MOC-detector**可以比**anchor-based**的方法更精确地从短时序视频片段中定位动作。该方法和基于**3D backbone**的方法效果差不多，但能够提供一个更简单的框架。

![](https://pic.imgdb.cn/item/60efbcd25132923bf8655467.jpg)

### 运行时间分析
随着输入帧数增加，模型准确率提高，但运行时间延长。**MOC-detector**在输入帧数$K=7$的情况下可以达到$25$**fps**。

![](https://pic.imgdb.cn/item/60efbfe75132923bf87a093f.jpg)

![](https://pic.imgdb.cn/item/60efc0225132923bf87b9d82.jpg)

### 运动估计分支(Movement Branch)的有效性
模型的假设是将动作实例建模为**每一帧动作中心点沿时序的运动轨迹**，因此需要评估其有效性。具体地，对模型中的运动估计分支进行评估，运动估计分支用于将**关键帧中心点**移动到其他帧上来定位检测框的动作中心点位置，这一步被称作**Move Center**。之后包围框回归分支直接在每一帧的动作中心点位置回归检测框大小，这一步被称作**Bbox Align**。实验设置了三种策略：
1. **No Movement**：所有帧都使用**关键帧中心点**作为当前帧的动作中心点；
2. **Semi Movement**：在**关键帧中心点**处生成检测框，再移动到当前帧的动作中心点处；
3. **Full Movement**：在当前帧的动作中心点处生成检测框。

![](https://pic.imgdb.cn/item/60efc0365132923bf87c2a8d.jpg)
![](https://pic.imgdb.cn/item/60efc0425132923bf87c7a2b.jpg)

### 在线视频流处理
**MOC-detector**也可以用于处理在线视频流。对于视频流，只使用之前的$K-1$帧和当前帧作为输入，由于每帧的特征只需提取一次，可以将之前$K-1$帧的特征保存在**缓冲区**中。当一个新的帧到来时，通过模型提取出它的特征，使用$K$帧的特征产生$K$帧的**tubelets**，**link**算法将这些**tubelets**和之前的结果连接得到视频级别的检测结果。

![](https://pic.imgdb.cn/item/60efbdc95132923bf86ba9ec.jpg)