---
layout: post
title: 'Deep Bayesian Active Learning with Image Data'
date: 2022-08-03
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63183c9716f2c2beb1cae8b7.jpg'
tags: 论文阅读
---

> BALD：贝叶斯不一致主动学习.

- paper：[Deep Bayesian Active Learning with Image Data](https://arxiv.org/abs/1703.02910)

# 1. Deep Bayesian Active Learning

**深度贝叶斯主动学习(deep bayesian active learning)**的出发点是使用**贝叶斯神经网络(bayesian neural network)**估计预测结果的不确定性。

贝叶斯神经网络的网络参数并不是确定性的数值，而是从分布中采样的结果。在进行推断时，需要对所有可能的参数情况进行积分，使用无穷多的神经网络的集成进行输出预测。

即使单个网络的不确定性估计能力较差(深度网络经常对输出过度自信)，所有网络的集成结果能够改善不确定性的估计。直观来说，如果大多数网络都认为某张图像属于同一个类别，则集成网络具有较高的置信度，对应的不确定性较小。

在实践中，对分布中的所有可能的参数值进行积分是不可解的，因此采用蒙特卡洛积分进行近似。

# 2. Monte Carlo dropout

**Monte Carlo dropout**的基本思路是对每一个神经元设置**Bernoulli**先验，即参数以概率$p$被设置为$0$，这可以通过**dropout**实现。在测试时通过多次应用**dropout**并将结果取平均便可以实现蒙特卡洛积分近似。具体步骤如下：

① 贝叶斯神经网络接受输入$x$，为计算输出类别$c$，遍历所有可能的权重设置，按其概率进行加权：

$$ p(y=c|x) = \int p(y=c|x,w)p(w)dw $$

② 由于参数的实际分布未知，通过**Bernoulli**分布$q^{\*}(w)$进行近似，使用**dropout**模拟**Bernoulli**分布：

$$ p(y=c|x) ≈ \int p(y=c|x,w)* q^{*}(w)dw $$

③ 采用蒙特卡洛积分进行进一步近似，多次运行带**dropout**的网络，并将结果进行平均：

$$ p(y=c|x) ≈ \frac{1}{T} \sum_t p(y=c|x,w_t) = \frac{1}{T}\sum_t p_c^t $$

# 3. MC Dropout as Entropy Acquisition Function

基于熵的不确定性**获取函数(acquisition function)**定义为：

$$ H = -\sum_c p(y=c|x)\log p(y=c|x) $$

若使用**MC dropout**的预测结果，获取函数为：

$$ H = -\sum_c (\frac{1}{T}\sum_t p_c^t)\log (\frac{1}{T}\sum_t p_c^t) $$

# 4. Bayesian Active Learning by Disagreement (BALD)

有时数据的标签是未知的，因此无法通过熵来估计不确定性。**BALD**的出发点是找出多数采样网络都对自己的预测结果自信，但是这些结果的不一致性较大的样本。

**BALD**的目标是找到从模型参数中能够获得的信息增益最大的样本，这等价于模型输出和模型参数的互信息最大：

$$ I(y;w | x,D_{train}) = H(y|x,D_{train}) - E_{p(w|D_{train})}[H(y|x,w,D_{train})] $$

第一项寻找平均输出结果的熵较大的样本，第二项寻找在给定参数信息的情况下输出结果熵较小的样本；两者之差即为模型参数为预测结果提供的信息量。

采用蒙特卡洛积分进行近似，则目标函数为：

$$ I(y;w | x,D_{train}) ≈ -\sum_c (\frac{1}{T}\sum_t p_c^t)\log (\frac{1}{T}\sum_t p_c^t) +\frac{1}{T}\sum_{t,c} p_c^t\log p_c^t $$

为计算第一项，运行多次网络，将输出结果平均后计算熵，等价于$f(E[x])$。为计算第二项，运行多次网络，计算每一次运行结果的熵，并取平均，等价于$E[F(x)]$。由于熵函数$f$为凹函数，因此由[琴生不等式](https://0809zheng.github.io/2022/07/20/jenson.html) $f(E[x]) \geq E[F(x)]$上式恒为非负值，可用于估计模型的认知不确定度。



# 5. 实验结果

实验结果表明，**BALD**的表现显著优于随机采样和基于平均标准偏差的采样，与基于熵和基于变差比的方法表现类似。

![](https://pic.imgdb.cn/item/63185dc916f2c2beb1f33281.jpg)