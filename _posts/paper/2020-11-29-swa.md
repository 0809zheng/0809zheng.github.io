---
layout: post
title: 'Averaging Weights Leads to Wider Optima and Better Generalization'
date: 2020-11-29
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6238597d27f86abb2ad1ff8b.jpg'
tags: 论文阅读
---

> SWA：通过随机权重平均寻找更宽的极小值.

- paper：[Averaging Weights Leads to Wider Optima and Better Generalization](https://arxiv.org/abs/1803.05407)

深度神经网络通常是通过随机梯度下降算法进行优化的，且优化过程中会设置学习率衰减。作者发现使用周期或恒定学习率，并沿着参数的优化轨迹平均多个点，能够提高泛化能力，且几乎没有计算开销。这种方法称为**随机权重平均(stochastic weight averaging, SWA)**。

为了增加权重空间的探索范围，以恒定或周期学习率运行梯度下降算法。有研究表明以恒定学习率运行**SGD**相当于以损失最小值为中心的高斯分布中采样，对于高维空间则相当于在高维高斯分布的球体。通过对多次采样结果进行平均能够获得球体内部的解。

![](https://pic.imgdb.cn/item/623a7ccf27f86abb2af7ca81.jpg)


## 1. 分析SGD轨迹

采用周期性学习率，假设学习率变化周期为$c$，每个周期中学习率从$\alpha_1$线性下降为$\alpha_2$。则第$i$次更新中的学习率为：

$$ t(i) = \frac{1}{c} (\text{mod}(i-1,c)+1 ) \\ \alpha(i) = (1-t(i))\alpha_1 + t(i) \alpha_2 $$

下图展示了循环学习率的设置与相应的泛化误差变化：

![](https://pic.imgdb.cn/item/623a83a127f86abb2a158ff6.jpg)

作者在**CIFAR-100**数据集上训练了**ResNet-164**模型，并绘制了优化轨迹的初始点、中间点与终点(图中十字点)，其余点是这些点的仿射组合：

![](https://pic.imgdb.cn/item/623a84f227f86abb2a1b44a9.jpg)

左边两图是使用周期性学习率的结果；右边两图是使用恒定学习率$t(i) = \alpha_1$的结果。结果表明周期性学习率在探索参数空间时更准确。从图中也能看出训练损失和测试误差没有完全对齐，通过平均优化轨迹能够获得泛化性能更好的点。

## 2. 随机权重平均 SWA

若学习率变化周期为$c$(对于恒定学习率$c=1$)，当训练轮数$i$每完成一个周期时($\text{mod}(i,c)=0$)，计算已累积的模型数量：

$$ n_{\text{model}} = \frac{i}{c} $$

并累积平均权重$w_{\text{SWA}}$：

$$ w_{\text{SWA}} = \frac{n_{\text{model}} \cdot w_{\text{SWA}}+w_i}{n_{\text{model}}+1} $$

如果网络存在**BatchNorm**，则应在训练结束后使用平均权重$w_{\text{SWA}}$对数据额外进行一次前向传播，从而计算每一层神经元的相关统计量。

![](https://pic.imgdb.cn/item/623a898127f86abb2a3008ca.jpg)

## 3. 极小值的宽度

网络收敛到局部极小值有两种情况，平坦极小值和尖锐极小值。由于训练损失和测试误差不完全对齐，只有当收敛到较宽的平坦极小值时，才能在小扰动下仍然保持近似最优。

![](https://pic.imgdb.cn/item/6238440f27f86abb2a6acd01.jpg)

作者以极小值$w_{\text{SWA}}$和$w_{\text{SGD}}$为中心，沿不同距离采样了参数点，并计算这些参数点对应的训练损失和测试误差。结果表明随着采样距离的增加，**SWA**方法始终保持较低的误差，即收敛到更平坦的极小值。

![](https://pic.imgdb.cn/item/623ac7c427f86abb2a293ad1.jpg)

## 4. 实验分析

作者给出了不同方法对应的实验结果：

![](https://pic.imgdb.cn/item/623acb0727f86abb2a35dc78.jpg)