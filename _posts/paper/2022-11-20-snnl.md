---
layout: post
title: 'Analyzing and Improving Representations with the Soft Nearest Neighbor Loss'
date: 2022-11-20
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63d386f7face21e9efe2f364.jpg'
tags: 论文阅读
---

> 通过软最近邻损失分析和改进表示学习.

- paper：[Analyzing and Improving Representations with the Soft Nearest Neighbor Loss](https://arxiv.org/abs/1902.01889)

**软最近邻损失(Soft Nearest Neighbor Loss)**用于在表征空间中度量不同类别数据的纠缠度。给定数据集$$\{x_i,y_i\}_{i=1}^N$$，该损失定义为：

$$ -\frac{1}{N} \sum_{i=1}^N \log \frac{\sum_{i\neq j,y_i=y_j,j=1,...,N} \exp(-f(x_i,x_j)/ \tau)}{\sum_{i\neq k,k=1,...,N} \exp(-f(x_i,x_k)/ \tau)} $$

其中温度$\tau$用于调整特征在表示空间中的聚集程度，温度越大则特征距离越大，聚集程度越低。通过该损失可以测量网络学习到特征的纠缠度。

### ⚪ 判别模型的特征纠缠

一般认为不同类别的表征具有明显的区分度（即纠缠度较低）则模型具有较好的性能。然而作者在**CIFAR-10**数据集上进行了实验，对不同网络模块的特征纠缠度进行测量，得到结果如下图所示：

![](https://pic.imgdb.cn/item/63d388dbface21e9efe7d899.jpg)

结果表明除最后一层，其它层的特征纠缠度在学习过程中先快速下降再上升；因此低层表征往往学到的是类别无关的特征。

### ⚪ 生成模型的特征纠缠

本文也利用了纠缠度来观察生成对抗模型的生成数据与真实数据的差异，其结果如下：

![](https://pic.imgdb.cn/item/63d38a05face21e9efeb34ce.jpg)

结果反映了生成对抗网络的训练过程中，生成数据和真实数据越来越接近，即纠缠度越来越高，这和生成对抗网络的目标是一致的。

### ⚪ 正则项

软最近邻损失可以被用作正则项，在训练过程中调整特征之间的纠缠度。论文中采用了**CNN**模型在**MNIST**，**Fashion-MNIST**和**SVHN**三个图像分类数据集上进行测试，采用**ResNet**模型在**CIFAR10**数据集上进行测试，其结果如下：

![](https://pic.imgdb.cn/item/63d38a87face21e9efecd412.jpg)

