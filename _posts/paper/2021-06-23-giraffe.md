---
layout: post
title: 'GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields'
date: 2021-06-23
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60d2d225844ef46bb2e43728.jpg'
tags: 论文阅读
---

> GIRAFFE：将场景表示为组合的生成式神经特征场.

- paper：GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields
- award：CVPR2021 Best Paper
- arXiv：[link](https://arxiv.org/abs/2011.12100)
- code：[github](https://github.com/autonomousvision/giraffe)
- website：[link](https://autonomousvision.github.io/giraffe/)

目前图像生成模型已经能够生成具有高分辨率的现实图像。**GIRAFFE**是一个可控的图像生成模型，它能够控制生成图像的内容(如背景,图像内的物体数量,每个物体的位置和姿态)。作者将组合的三维场景表示(**compositional 3D scene representation**)引入生成模型，即将场景表示为生成的神经特征场(**neural feature field**)的组合。

![](https://pic.imgdb.cn/item/60d2d84d844ef46bb2179664.jpg)

**GIRAFFE**的整体流程如上图所示。首先从代表形状和外观(**shape and appearance**)的随机噪声中采样生成背景和单个物体，并在代表姿态(**pose**)的随机噪声中采样调整它们的三维姿态，将其表示为特征场。将这些物体通过组合生成**3D**场景表示。再从代表姿态的随机噪声中采样控制相机的位置，经过体积渲染(**volume rendering**)和神经渲染(**neural rendering**)后生成目标图像。

![](https://pic.imgdb.cn/item/60d2e10f844ef46bb255df3d.jpg)

**GIRAFFE**的实现细节如上图所示(其中橙色部分是可学习的网络，蓝色部分是固定操作)。生成器$G_{\theta}$接收相机姿态$ξ$和$N$个形状和外观编码$z_s^i$,$z_a^i$及仿射变换$T_i$，生成包括$N-1$个目标和背景的合成图像。判别器$D_{\phi}$提供对抗损失。

传统的辐射场(**radiance field**)是指将一个**3D**点$x \in \Bbb{R}^3$和观测方向$d \in \Bbb{S}^2$映射为一个体积密度$\sigma \in \Bbb{R}^+$和**RGB**值$c \in \Bbb{R}^3$的连续函数。作者将**RGB**值替换为一个维度为$M_f$的特征，并提出了神经特征场(**neural feature field**)的概念，用一个多层感知机实现，并引入形状和外观的条件编码$z_s,z_a \text{~} \mathcal{N}(0,I)$：

$$ h_{\theta}：\Bbb{R}^{L_x} \times \Bbb{R}^{L_d} \times \Bbb{R}^{M_s} \times \Bbb{R}^{M_a} → \Bbb{R}^{+} \times \Bbb{R}^{M_f} \\ (\gamma(x),\gamma(d),z_s,z_a) → (\sigma,f) $$

由于输入$x,d$的维度远低于输出特征，因此先对输入进行位置编码：

$$ \gamma(t,L)=(sin(2^0t \pi),cos(2^0t \pi),...,sin(2^Lt \pi),cos(2^Lt \pi)) $$

其中$t$是输入标量(比如$x$的一个元素)，$L$是特征维度扩张的程度。每一个神经特征场相当于在给定相机位置时，对某个目标的三维场景表示。形状和外观的条件编码控制该目标的外貌，引入放射变换$$T=\{s,t,R\}$$控制其姿态。若$s,t \in \Bbb{R}^3$表示缩放和平移参数，$R$表示旋转矩阵，则$x$调整为：

$$ k(x) = R \cdot diag(s_1,s_2,s_3) \cdot x + t $$

每个目标由其在三维场景中的神经特征场$h_{\theta}=(\sigma,f)$表示，通过组合操作(**composition operator**)将其加权合成为一个整体神经特征场：

$$ C(x,d) = (\sigma,\frac{1}{\sigma}\sum_{i=1}^{N}\sigma_i f_i) $$

对整体特征场进行**3D**体积渲染(**volume rendering**)。选择$N_s$个采样点$$\{x_j\}_{j=1}^{N_s}$$，对生成特征场中的每一个单位体积，进行如下变换：

$$ \pi_{vol}：(\Bbb{R}^{+} \times \Bbb{R}^{M_f})^{N_s} \times → \Bbb{R}^{M_f} , \quad \{\sigma_j,f_j\}_{j=1}^{N_s} → f $$

之后进行**2D**神经渲染(**neural rendering**)，将上述渲染特征进一步映射为生成图像：

$$ \pi_{\theta}^{neural}：\Bbb{R}^{H_V \times W_V \times M_f} \times \Bbb{R}^{H \times W \times 3} $$

神经渲染是通过下面的神经网络实现的：

![](https://pic.imgdb.cn/item/60d2ece6844ef46bb2a1768e.jpg)

实验结果如下，该模型可以控制生成图像中任意目标的形状和位置，还可以增减额外的目标：

![](https://pic.imgdb.cn/item/60d2ed84844ef46bb2a55ec7.jpg)
