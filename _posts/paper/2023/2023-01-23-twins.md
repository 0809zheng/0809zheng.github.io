---
layout: post
title: 'Twins: Revisiting the Design of Spatial Attention in Vision Transformers'
date: 2023-01-23
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/643b693a0d2dde5777518446.jpg'
tags: 论文阅读
---

> Twins：重新思考视觉Transformer中的空间注意力设计.

- paper：[Twins: Revisiting the Design of Spatial Attention in Vision Transformers](https://arxiv.org/abs/2104.13840)

相较于**CNN**来说，**Transformer**由于其能高效地捕获远距离依赖的特性，近期在计算机视觉领域也引领了一波潮流。**Transformer**主要是依靠**Self-Attention**去捕获各个**token**之间的关系，但是这种**Global Self-Attention**的计算复杂度太高，不利于在**token**数目较多的密集检测任务（分割、检测）中使用。

基于以上考虑，目前主流有两种应对方法：
1. 一种是以**SwinTransformer**为代表的**Locally-Grouped Self-Attention**。其在不重叠的窗口内计算**Self-Attention**，当窗口大小固定时，整体的计算复杂度将下降，然后再通过其他方法去实现窗口间的互动，例如**SwinTransformer**中的**Shift-Window**方法。但这种方法的缺点在于窗口的大小会不一致，不利于现代深度学习框架的优化和加速。
2. 一种是以**PVT**为代表的**Sub-Sampled Version Self-Attention**。其在计算**Self-Attention**前，会先对**QKV Token**进行下采样，从而降低计算复杂度。

本文整体思路可以认为是**PVT+SwinTransformer**的结合：在局部窗口内部计算**Self-Attention（SwinTransformer）**，同时对每个窗口内部的特征进行压缩，然后再使用一个全局**Attention**机制去捕获各个窗口的关系（**PVT**）。

## ⚪ Twins-PCPVT

**PVT**中的**Global Sub-Sample Attention**是十分高效的，当配合上合适的**Positional Encodings（Conditional Positional Encoding）**时，其能取得媲美甚至超过目前**SOTA**的**Transformer**结构。

**PVT**通过逐步融合各个**Patch**的方式，形成了一种多尺度的结构，使得其更适合用于密集预测任务例如目标检测或者是语义分割，其继承了**ViT**和**DeiT**的**Learnable Positional Encoding**的设计，所有的**Layer**均直接使用**Global Attention**机制，并通过**Spatial Reduction**的方式去降低计算复杂度。 

作者通过实验发现，**PVT**与**SwinTransformer**的性能差异主要来自于**PVT**没有采用一个合适的**Positional Encoding**方式，通过采用**Conditional Positional Encoding（CPE）**去替换**PVT**中的**PE**，**PVT**即可获得与当前最好的**SwinTransformer**相近的性能。

![](https://pic.imgdb.cn/item/643b6a240d2dde57775243a1.jpg)

## ⚪ Twins-SVT

更进一步，基于**Separable Depthwise Convolution**的思想，本文提出了一个**Spatially Separable Self-Attention（SSSA）**。该模块仅包含矩阵乘法，在现代深度学习框架下能够得到优化和加速。通过提出的**Spatially Separable Self-Attention（SSSA）**去缓解**Self-Attention**的计算复杂度过高的问题。**SSSA**由两个部分组成：**Locally-Grouped Self-Attention（LSA）**和**Global Sub-Sampled Attention（GSA）**。

![](https://pic.imgdb.cn/item/643b6b250d2dde5777530ef2.jpg)

### (1) Locally-Grouped Self-Attention（LSA）

首先将**2D feature map**划分为多个**Sub-Windows**，并仅在**Window**内部进行**Self-Attention**计算，计算量会大大减少，由$O(H^2W^2d)$下降至$O(k_1k_2HWd)$，其中$k_1=\frac{H}{m},k_2=\frac{W}{n}$，当$k_1,k_2$固定时，计算复杂度将仅与$HW$呈线性关系。

```python
class LocalAttention(nn.Module):
    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0., patch_size = 7):
        super().__init__()
        inner_dim = dim_head *  heads
        self.patch_size = patch_size
        self.heads = heads
        self.scale = dim_head ** -0.5

        self.to_q = nn.Conv2d(dim, inner_dim, 1, bias = False)
        self.to_kv = nn.Conv2d(dim, inner_dim * 2, 1, bias = False)

        self.to_out = nn.Sequential(
            nn.Conv2d(inner_dim, dim, 1),
            nn.Dropout(dropout)
        )

    def forward(self, fmap):
        shape, p = fmap.shape, self.patch_size
        b, n, x, y, h = *shape, self.heads
        x, y = map(lambda t: t // p, (x, y))

        fmap = rearrange(fmap, 'b c (x p1) (y p2) -> (b x y) c p1 p2', p1 = p, p2 = p)

        q, k, v = (self.to_q(fmap), *self.to_kv(fmap).chunk(2, dim = 1))
        q, k, v = map(lambda t: rearrange(t, 'b (h d) p1 p2 -> (b h) (p1 p2) d', h = h), (q, k, v))

        dots = einsum('b i d, b j d -> b i j', q, k) * self.scale

        attn = dots.softmax(dim = - 1)

        out = einsum('b i j, b j d -> b i d', attn, v)
        out = rearrange(out, '(b x y h) (p1 p2) d -> b (h d) (x p1) (y p2)', h = h, x = x, y = y, p1 = p, p2 = p)
        return self.to_out(out)
```

### (2)  Global Sub-Sampled Attention（GSA）

**LSA**缺乏各个**Window**之间的信息交互，比较简单的一个方法是，在**LSA**后面再接一个**Global Self-Attention Layer**，这种方法在实验中被证明也是有效的，但是其计算复杂度会较高。

另一个思路是，将每个**Window**提取一个维度较低的特征作为各个**window**的表征，然后基于这个表征再去与各个**window**进行交互，相当于**Self-Attention**中的**Key**的作用，这样一来，计算复杂度会下降至：$O(mnHWd)=O(\frac{H^2W^2d}{k_1k_2})$。

这种方法实际上相当于对**feature map**进行下采样，因此被命名为**Global Sub-Sampled Attention**。 综合使用**LSA**和**GSA**，可以取得类似于**Separable Convolution（Depth-wise+Point-wise）**的效果，整体的计算复杂度为：$O(\frac{H^2W^2d}{k_1k_2}+k_1k_2HWd)$。同时有：$\frac{H^2W^2d}{k_1k_2}+k_1k_2HWd \geq 2HWd\sqrt{HW}$，当且仅当$k_1k_2 = \sqrt{HW}$。

考虑到分类任务中，$H=W=224$是比较常规的设置，同时使用方形框，则有$k_1=k_2$，第一个**stage**的**feature map**大小为$56$，可得$k_1=k_2=\sqrt{56}=7$。 当然可以针对各个**Stage**去设定其窗口大小，不过为了简单性，所有的$k$均设置为$7$。

```python
class GlobalAttention(nn.Module):
    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0., k = 7):
        super().__init__()
        inner_dim = dim_head *  heads
        self.heads = heads
        self.scale = dim_head ** -0.5

        self.to_q = nn.Conv2d(dim, inner_dim, 1, bias = False)
        self.to_kv = nn.Conv2d(dim, inner_dim * 2, k, stride = k, bias = False)

        self.dropout = nn.Dropout(dropout)

        self.to_out = nn.Sequential(
            nn.Conv2d(inner_dim, dim, 1),
            nn.Dropout(dropout)
        )

    def forward(self, x):
        shape = x.shape
        b, n, _, y, h = *shape, self.heads
        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim = 1))

        q, k, v = map(lambda t: rearrange(t, 'b (h d) x y -> (b h) (x y) d', h = h), (q, k, v))

        dots = einsum('b i d, b j d -> b i j', q, k) * self.scale

        attn = dots.softmax(dim = -1)
        attn = self.dropout(attn)

        out = einsum('b i j, b j d -> b i d', attn, v)
        out = rearrange(out, '(b h) (x y) d -> b (h d) x y', h = h, y = y)
        return self.to_out(out)
```

**Twins-SVT**的完整实现可参考[vit-pytorch](https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/twins_svt.py)。