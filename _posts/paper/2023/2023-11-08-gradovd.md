---
layout: post
title: 'Open Vocabulary Object Detection with Pseudo Bounding-Box Labels'
date: 2023-11-08
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/65825df9c458853aefb579da.jpg'
tags: 论文阅读
---

> 通过伪边界框标签实现开放词汇目标检测.

- paper：[Open Vocabulary Object Detection with Pseudo Bounding-Box Labels](https://arxiv.org/abs/2111.09452)

大多数目标检测方法依赖于以实例级边界框标签的形式进行监督学习，因此需要非常昂贵的人工标注工作来构建训练数据集。此外当需要检测一个新的目标类别时，必须进一步为这个新的类别在图像中标注大量的边界框。

本文设计了一种伪边界框标签生成策略**Grad-OVD**，用于从现有图像描述数据集中自动获取不同目标的伪边界框标签。具体来说，给定一个预训练的视觉语言模型和一个图像-描述样本对，在图像中计算**Grad-CAM**激活图，对应于描述中感兴趣的目标。然后将激活映射转换为对应类别的伪标签框。开放词汇检测器可以直接在这些伪标签的监督下进行训练。该方法可以大大增加训练数据的大小和训练类别的数量。

![](https://pic.imgdb.cn/item/6585330dc458853aef21b989.jpg)

预训练的视觉语言模型接收图像$I$和描述$X=[x_1,...,x_{N_T}]$，使用图像编码器提取图像特征$V$，使用文本编码器获取文本表示$T$；其中$N_T$是描述中的单词数(包括**[CLS]**和**[SEP]**)。然后采用由交叉注意力层组成的的多模态编码器来融合图像和文本编码器的信息。

在交叉注意力层$l$中，描述中感兴趣的目标$x_t$与图像区域的交互作用如下式所示，其中$A^l_t$表示在第$l$个交叉注意力层对应的视觉注意力得分。$H^{l−1}_t$表示从前$(l−1)$交叉注意层获得的隐藏表示，$H^0_t$表示来自文本编码器的$x_t$。

$$
\begin{aligned}
& \mathbf{A}_{t}^{l}=\text{Softmax}\left(\frac{\mathbf{h}_{t}^{l-1}\mathbf{V}^T}{\sqrt{d}} \right) \\
& \mathbf{h}_{t}^{l}=\mathbf{A}_{t}^{l} \cdot \mathbf{V}
\end{aligned}
$$

交叉注意力层测量视觉区域表示与输入描述文本的相关性，因此视觉注意力分数可以直接反映不同视觉区域对文本的重要性。使用**Grad-CAM**基于注意力得分可视化激活图，以通过描述中给定物体的名称来定位图像中的物体。**Grad-CAM**从多模态编码器获取最终输出$s$，并计算其相对于注意力分数的梯度。给定目标名称$x_t$的图像的最终激活映射$Φ_t$计算为:

$$
Φ_t = \mathbf{A}_{t}^{l} \cdot \max (\frac{\partial s}{\partial \mathbf{A}_{t}^{l}}, 0)
$$

获得描述中感兴趣目标的激活映射后，绘制一个覆盖激活区域的边界框作为类别的伪标签。采用现有的候选边界框生成器生成边界框候选项$B = [b_1, b_2, ..., b_K]$，并选择与$Φ_t$重叠最多的一个:

$$
\hat{b}=\arg\max_i \frac{\sum_{b_i}\Phi_{t}(b_i)}{\sqrt{|b_i|}}
$$

在训练期间，维护感兴趣的目标列表(称为目标词汇表)，并获得训练词汇表中所有目标的伪边界框标签。结果显示被激活的区域与相关区域对应良好。生成的边界框质量很好。当它们直接用于训练开放词汇目标检测器时，目标检测器显著优于当前的**SOTA**开放词汇目标检测器。

![](https://pic.imgdb.cn/item/65854235c458853aef5579d0.jpg)

实验从现有的图像标题数据集(包括**COCO Caption**、**Visual-Genome**和**SBU Caption**)的组合中生成不同对目标的伪边界框标签。用于伪标签生成和检测器训练的最终数据集包含大约一百万张图像。默认对象词汇表是由**COCO**、**PASCAL VOC**、**Objects365**和**LVIS**中所有目标名称的联合构建的，从而产生$1582$个类别。

作者针对边界框生成方法进行了消融实验。默认的提议生成器是用**COCO**检测类别训练的**Mask-RCNN**，还使用无监督提案生成器**Selective Search**进行实验。结果表明即使使用无监督提案生成器，方法也是有效的。

![](https://pic.imgdb.cn/item/6585461fc458853aef645dcb.jpg)

作者针对预训练视觉语言模型进行了消融实验。结果表明与使用**ALBEF**的方法相比，使用**LXMERT**的方法性能略差。这可能是由于**LXMERT**使用较少的图像描述数据进行训练。

![](https://pic.imgdb.cn/item/65854697c458853aef66570b.jpg)

作者针对目标词汇表大小进行了消融实验。结果表明在预训练期间在**COCO**类别之外添加额外的目标类别将有利于模型性能。此外使用更大的词汇表可以提高模型对包含大量目标类别的数据集的泛化能力。

![](https://pic.imgdb.cn/item/65854712c458853aef68734a.jpg)

作者针对生成伪标签的数据量进行了消融实验。用于伪标签生成的数据越多，用于训练开放词汇检测器的不同目标的数量就越多。结果表明方法可以从更大的数据集中获益。当使用更多数据时，性能在目标集上提高了约$2\%$。

![](https://pic.imgdb.cn/item/65854781c458853aef6a4585.jpg)

作者针对文本编码器进行了消融实验。除了默认选择的文本编码器(**CLIP**)之外，还使用另一种编码器**Bert (base)**。结果表明与使用**CLIP**文本编码器的方法相比，使用**Bert**编码器的方法在**COCO**目标集上的性能略差。这可能是由于**CLIP**文本编码器使用图像描述对进行训练，从而在图像相关任务中具有更好的泛化性能。

![](https://pic.imgdb.cn/item/65854813c458853aef6ca658.jpg)

