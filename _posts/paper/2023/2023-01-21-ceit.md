---
layout: post
title: 'Incorporating Convolution Designs into Visual Transformers'
date: 2023-01-21
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6427ddfba682492fcc6c47ad.jpg'
tags: 论文阅读
---

> CeiT：将卷积设计整合到视觉Transformers中.

- paper：[Incorporating Convolution Designs into Visual Transformers](https://arxiv.org/abs/2103.11816)

**CeiT**想借助**CNN**来提升**Transformer**的性能，作者认为**CNN**最重要的特征是不变性 (**invariance**) 和局部性 (**locality**)。不变性是指卷积的权重共享机制，使得卷积能够捕获一个相邻区域的特征且具有平移不变性；局部性是指在视觉任务中，相邻的**pixel**之间往往是相互关联的。

但是**Transformer**很难利用好这些特性，即很难高效地提取**low-level**的特征，**self-attention**模块的长处是提炼**token**之间的**long-range**的信息之间的关系，往往会忽略空间信息。基于此，作者想把**CNN**的特性融入进来以解决这些问题，使模型既具备**CNN**的提取**low-level**特征的能力，强化局部特征的提取，也具有**Transformer**的提炼**token**之间的 **long-range** 的信息之间的关系的能力。

为了有效地提取**low-level feature**，作者通过 **Image-to-tokens** 先使用卷积+**Flatten**操作把图片变为**tokens**，而不是通过直接分**patch**的方法。为了强化局部特征的提取，作者把**MLP**层的**Feed-Forwardnetwork**换成了 **Locally-enhanced Feed-Forward layer**，在空间维度上促进相邻**token**之间的相关性。除此之外，在**Transformer**顶部使用 **Layer-wise Class token Attention** 进一步提升性能。

## 1. Image-to-tokens

**ViT**采用的是直接把一张$H\times W$的图片分成$N$个**patch**，每个**patch**的大小是$P \times P$的，所以**patch**的数量$N=HW/P^2$。但这么做会很难捕捉到**low-level**的信息，比如图片的边和角的信息。而且**self-attention**建模的是全局的信息，所以相当于是使用了很大的**kernel**，这样的**kernel**由于参数量过多导致很难训练，需要大量的数据。

鉴于此作者提出了**Image-to-tokens**模块，如图所示是一个轻量化的模块，由一个卷积操作加上一个**Batch Normalization** + **Max-pooling**构成：

$$
x' = I2T(x) = MaxPool(BN(Conv(x)))
$$

其中$x' \in R^{H/S \times W/S \times D}$，$S$是卷积操作的**stride**值，$D$是做完卷积操作以后的**channel**数。这步卷积得到的$x'$会再分成**patch**，为了保持与**ViT**的**patch**数量的一致性，此时的**patch**的大小将有原来的$P \times P$变为$P/S \times P/S$，其中的$S=4$。**I2T**模块充分利用了卷积在提取**low-level**的特征方面的优势，通过缩小**patch**的大小来降低嵌入的训练难度。

![](https://pic.imgdb.cn/item/6427e0cfa682492fcc7247b2.jpg)

## 2. Locally-enhanced Feed-Forward layer

为了结合**CNN**提取局部信息的优势和**Transformer**建立远程依赖关系的能力，强化局部特征的提取，作者把**MLP**层的**Feed-Forwardnetwork** 换成了 **Locally-enhanced Feed-Forward layer**，在空间维度上促进相邻**token**之间的相关性。

具体的做法是：保持**MSA**模块不变，保留捕获**token**之间全局相似性的能力。相反，原来的前馈网络层被**Locally-enhanced Feed-Forward layer (LeFF)**取代。结构如图所示。

![](https://pic.imgdb.cn/item/6427e138a682492fcc72d524.jpg)

**LeFF**的具体流程是：首先输入的**token** $x_t^h \in R^{(N+1)\times C}$由前面的**MSA**模块得到，然后分成**2**部分，第**1**部分是把**class token** $x_c^h \in R^{C}$单独拿出来，剩下的第**2**部分是$x_p^h \in R^{N\times C}$。接着把第**2**部分通过**linear projection**拓展到高维的$x_p^{l_1} \in R^{N\times (eC)}$，其中$e$代表**expand ratio**。接着将其还原成**2D**的图片$x_p^{s} \in R^{\sqrt{N}\times \sqrt{N}\times (eC)}$，再通过**Depth-wise convolution**得到$x_p^{d} \in R^{\sqrt{N}\times \sqrt{N}\times (eC)}$，再**Flatten**成$x_p^{f} \in R^{N\times (eC)}$的张量。最后通过**Linear Projection**映射回原来的维度$x_p^{l_2} \in R^{N\times C}$，并与一开始的**class token concat**起来得到$x_{t+1}^h \in R^{(N+1)\times C}$。每个**Linear Projection**和**convolution**之后都会加上**BatchNorm** 和 **GELU** 操作。总的流程可以写成下式：

$$
\begin{aligned}
\mathbf{x}_c^h, \mathbf{x}_p^h & =\operatorname{Split}\left(\mathbf{x}_t^h\right) \\
\mathbf{x}_p^{l_1} & =\operatorname{GEL}\left(\operatorname{BN}\left(\operatorname{Linear} 1\left(\mathbf{x}_p^h\right)\right)\right) \\
\mathbf{x}_p^s & =\operatorname{SpatialRestore}\left(\mathbf{x}_p^{l_1}\right) \\
\mathbf{x}_p^d & =\operatorname{GELU}\left(\operatorname{BN}\left(\operatorname{DWConv}\left(\mathbf{x}_p^s\right)\right)\right) \\
\mathbf{x}_p^f & =\operatorname{Flatten}\left(\mathbf{x}_p^d\right) \\
\mathbf{x}_p^{l_2} & =\operatorname{GELU}\left(\operatorname{BN}\left(\operatorname{Linear} 2\left(\mathbf{x}_p^f\right)\right)\right) \\
\mathbf{x}_t^{h+1} & =\operatorname{Concat}\left(\mathbf{x}_c^h, \mathbf{x}_p^{l_2}\right)
\end{aligned}
$$

## 3. Layer-wise Class token Attention

在**CNN**中，随着网络层数的加深，感受野在不断地变大。所以不同**layer**的**feature representation**是不同的。为了综合不同**layer**的信息，作者提出了**Layer-wise Class token Attention**模块，它把$L$个**layer**的**class token**都输入进去，并经过一次**Multi-head Self-attention**模块和一个**FFN**网络，得到最终的**output**，如下图所示。它计算的是这$L$个**layer**的**class token**的相互关系。

![](https://pic.imgdb.cn/item/6427e330a682492fcc756055.jpg)