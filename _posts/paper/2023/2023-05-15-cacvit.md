---
layout: post
title: 'Vision Transformer Off-the-Shelf: A Surprising Baseline for Few-Shot Class-Agnostic Counting'
date: 2023-05-15
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/667e6f17d9c307b7e9618eef.png'
tags: 论文阅读
---

> 现成的视觉Transformer：令人惊讶的少样本类别无关计数基准.

- paper：[Vision Transformer Off-the-Shelf: A Surprising Baseline for Few-Shot Class-Agnostic Counting](https://arxiv.org/abs/2303.02001)

类别无关计数问题可以表述为模板匹配问题，即分别提取查询图像和示例的特征，然后匹配它们的特征相似性，从而形成提取-匹配流程。这项工作指出，通过视觉Transformer中的自注意力机制能够同时实现特征提取与匹配过程：**ViT**中的自注意力过程分为自注意力组和交叉注意力组，前者关注的是查询图像与样本的特征提取，后者关注的是查询图像与样本的匹配过程。

![](https://pic.imgdb.cn/item/667e708cd9c307b7e9644c02.png)

基于此作者设计了**Class-Agnostic Counting Vision Transformer (CACViT)**。查询图像和相应的示例被切分成图像**patch**并嵌入为**token**，然后通过自注意力层进行处理。最后将查询图像的输出特征与最后一个注意力层的相似度图进行连接，并预测最终的密度图。

![](https://pic.imgdb.cn/item/667e7205d9c307b7e966f3cd.png)

注意到**CACViT**中的自注意力图可以被划分为四部分。
- $A_{query}$：查询图像的特征提取；
- $A_{class}$：浅层关注前景，深层关注背景；
- $A_{match}$：查询图像和样例之间的特征匹配；
- $A_{exp}$：样例图像的特征提取。

![](https://pic.imgdb.cn/item/667e73f8d9c307b7e96b658d.png)

虽然**ViT**中的自注意机制适合类别无关计数任务，但该结构中的某些限制或功能可能导致信息丢失。固定尺寸的输入会丢失尺度信息（左图），用于标准化注意力图的**softmax**会削弱表达目标数量的能力（右图）。

![](https://pic.imgdb.cn/item/667e74abd9c307b7e96cdb7b.png)

为了引入尺度信息，作者提出了考虑宽高比的尺度嵌入。假设一个样例的原始尺寸为$W_k\times H_k$，输入时将其尺寸调整为$W_z\times H_z$，则把$W_k, H_k$离散化为$W_z,H_z$个中间值并广播为$W_z\times H_z$的嵌入。将两者求和后连接到样例。

![](https://pic.imgdb.cn/item/667e7656d9c307b7e96fd5ab.png)

为了引入数量级信息，数量级信息可以用图像尺寸与样本尺寸的比值粗略表示。假设一个样例的原始尺寸为$W_k\times H_k$，图像**patch**的尺寸为$W_p\times H_p$，则图像**patch**能够描述样例的最大容量为：

$$
ME_k = \frac{W_p\times H_p}{W_k\times H_k}
$$

如果有K个样本，可以从样本中计算嵌入**ME**的平均值，并与注意力图的相似度值相乘，得到最终的相似度图。

在**FSC-147**数据集上，**CACViT**在**1-shot**和**3-shot**设置上都明显优于所有比较的方法。

![](https://pic.imgdb.cn/item/667e775cd9c307b7e97198c7.png)