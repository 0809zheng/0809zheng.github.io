---
layout: post
title: 'Towards a Unified View of Parameter-Efficient Transfer Learning'
date: 2023-02-13
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/648eaec71ddac507cc7a386f.jpg'
tags: 论文阅读
---

> 参数高效迁移学习的统一视角.

- paper：[Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2110.04366)

近年来提出了多种参数高效的迁移学习方法，这些方法仅微调少量（额外）参数即可获得强大的性能。虽然有效，但人们对为什么有效的关键要素以及各种高效微调方法之间的联系知之甚少。

下图展示了不同的微调方法在**Xsum**数据集上做英文文本摘要任务的效果（**ROUGE-2**是该任务的评价指标，越大越好）以及高效微调方法参数量相对于全参数微调参数量的百分比。从图中发现**Adapter**，**Prefix Tuning**和**LoRA**都是性能比较好的方法。

![](https://pic.imgdb.cn/item/648eaf551ddac507cc7aefd8.jpg)

作者分解了当下最先进的参数高效迁移学习方法（**Adapter, Prefix Tuning**和**LoRA**）的设计，并提出了一种在它们之间建立联系的统一框架**MAM Adapter**。具体来说，将它们重新构建为对预训练模型中特定隐状态的修改，并定义一组设计维度，不同的方法对这些维度做相应的变化。

![](https://pic.imgdb.cn/item/648eafca1ddac507cc7b8466.jpg)

作者分析了不同微调方法的内部结构和结构插入形式的相似之处。具体分析点包括新增可训练参数结构形式（**functional form**）、结构插入形式（**Insertion form**）、新增结构修改的具体位置（**modified representation**）、新增结构组合函数（**composition function**）。

![](https://pic.imgdb.cn/item/648eb09f1ddac507cc7caea5.jpg)

![](https://pic.imgdb.cn/item/648eb0e41ddac507cc7d0458.jpg)

作者得出如下结论：
- 并行放置的**Adapter**优于顺序放置的**Adapter**，并且与 **FFN** 并行放置的**Adapter**优于多头注意力（**MHA**）并行放置的**Adapter**。
- **soft prompt**可以通过仅更改 $0.1\%$ 的参数来有效地修改注意力。

![](https://pic.imgdb.cn/item/648eb1621ddac507cc7db291.jpg)

通过上述分析，作者提出了**mix-and-match（MAM）**，**MAM Adapter**是用 **FFN** 层的并行**Adapter**和**soft prompt**的组合。通过最终的实验结果，可以看到 **MAM Adapter** 在仅用了$6.7\%$参数量（相比全量微调）的情况下，在**Xsum**和**MT**这两个任务上达到了和全量微调相近的效果。

![](https://pic.imgdb.cn/item/648eb1f91ddac507cc7e78c6.jpg)
