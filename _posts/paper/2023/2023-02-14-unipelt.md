---
layout: post
title: 'UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning'
date: 2023-02-14
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/648eb3fb1ddac507cc81a73b.jpg'
tags: 论文阅读
---

> UniPELT：参数高效的语言模型微调的统一框架.

- paper：[UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning](https://arxiv.org/abs/2110.07577)

近年来，涌现出了许多针对语言模型的参数高效微调（**PELT**）方法，在模型训练参数极大减少的情况下，模型效果与全量微调相当。但是不同的**PELT**方法在同一个任务上表现差异可能都非常大，这让针对特定任务选择合适的方法非常繁琐。基于此，作者提出了**UniPELT**方法，将不同的**PELT**方法作为子模块，并通过门控机制学习激活最适合当前数据或任务的方法。

**UniPELT**是**LoRA**、**Prefix Tuning**和**Adapter**的门控组合。**LoRA**用于$W_Q$和$W_V$注意力矩阵，**Prefix Tuning**应用于**Transformer**层的**key**和**value**，**Adapter**应用于**Transformer**块的**feed-forward**子层之后。 

对于每个模块，通过线性层实现门控，通过$G_P$参数控制**Prefix-tuning**方法的开关，$G_L$控制**LoRA**方法的开关，$G_A$控制**Adapter**方法的开关。可训练参数包括 **LoRA** 矩阵$W_{down},W_{up}$、**Prefix-tuning**参数$P_K,P_V$、**Adapter**参数和门控函数权重。即图中蓝颜色的参数为可学习的参数。

![](https://pic.imgdb.cn/item/648eb58d1ddac507cc83cea1.jpg)

**UniPELT** 仅用 **100** 个样本就在低数据场景中展示了相对于**LoRA**、**Prefix Tuning**和**Adapter**方法的显著改进。在更多数据的场景中，**UniPELT** 的性能与这些方法相当或更好。

![](https://pic.imgdb.cn/item/648eb5dd1ddac507cc843afb.jpg)

实验还对不同 **PELT** 方法训练时间和推理时间进行了分析。
- 从训练速度来看，**UniPELT**比之前微调的方法多一些，但是还在能接受的范围，
- 从推理时间来看，**BitFit**方法增加的最少，**UniPELT**方法时间增加了$27\%$。
- 从训练参数量来看，**LoRA**，**BitFit**，**Prefix-tuning**都比较小，**UniPELT**参数量相对会多一些。

![](https://pic.imgdb.cn/item/648eb62d1ddac507cc84b755.jpg)

**UniPELT**方法始终优于常规的全量微调以及它在不同设置下包含的子模块，通常超过在每个任务中单独使用每个子模块的最佳性能的上限；并且通过研究结果表明，多种 **UniPELT** 方法的混合可能对模型有效性和鲁棒性都有好处。