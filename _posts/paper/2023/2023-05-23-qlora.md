---
layout: post
title: 'QLoRA: Efficient Finetuning of Quantized LLMs'
date: 2023-05-23
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/648e96141ddac507cc561090.jpg'
tags: 论文阅读
---

> QLoRA：量化大型语言模型的高效微调.

- paper：[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)

微调大型语言模型 (**LLM**) 是提高其性能以及添加所需或删除不需要的行为的一种非常有效的方法。然而微调非常大的模型非常昂贵；以 **LLaMA 65B** 参数模型为例，常规的 **16 bit**微调需要超过 **780 GB** 的 **GPU** 内存。虽然最近的量化方法可以减少 **LLM** 的内存占用，但此类技术仅适用于推理场景。基于此作者提出了**QLoRA**，并首次证明了可以在不降低任何性能的情况下微调量化为 **4 bit**的模型。

**QLoRA**使用一种新颖的高精度技术将预训练模型量化为 **4 bit**，然后添加一小组可学习的低秩适配器权重，这些权重通过量化权重的反向传播梯度进行微调。**QLORA** 有一种低精度存储数据类型（**4 bit**），还有一种计算数据类型（**BFloat16**）。实际上无论何时使用 **QLoRA** 权重张量，都会将张量反量化为 **BFloat16**，然后执行 **16** 位矩阵乘法。

**QLoRA**提出了两种技术实现高保真 **4 bit**微调——**4 bit NormalFloat(NF4)** 量化和双量化。此外，还引入了分页优化器，以防止梯度检查点期间的内存峰值，从而导致内存不足的错误，这些错误在过去使得大型模型难以在单台机器上进行微调。具体说明如下：
- **4bit NormalFloat（NF4）**：对于正态分布权重而言，一种信息理论上最优的新数据类型，该数据类型对正态分布数据产生比 **4 bit**整数和 **4bit** 浮点数更好的实证结果。
- 双量化（**Double Quantization**）：对第一次量化后的那些常量再进行一次量化，减少存储空间。
- 分页优化器：使用**NVIDIA**统一内存特性，该特性可以在**GPU**偶尔**OOM**的情况下，进行**CPU**和**GPU**之间自动分页到分页的传输，以实现无错误的 **GPU** 处理。该功能的工作方式类似于 **CPU** 内存和磁盘之间的常规内存分页。使用此功能为优化器状态（**Optimizer**）分配分页内存，然后在 **GPU** 内存不足时将其自动卸载到 **CPU** 内存，并在优化器更新步骤需要时将其加载回 **GPU** 内存。

![](https://pic.imgdb.cn/item/648e97f41ddac507cc58b740.jpg)

实验证明，无论是使用**16bit**、**8bit**还是**4bit**的适配器方法，都能够复制**16bit**全参数微调的基准性能。这说明，尽管量化过程中会存在性能损失，但通过适配器微调，完全可以恢复这些性能。

![](https://pic.imgdb.cn/item/648e98821ddac507cc5970fc.jpg)

实验还比较了不同的**4bit**数据类型对效果（**zero-shot**均值）的影响，其中，**NFloat** 显著优于**Float**，而**NFloat + DQ**略微优于**NFloat**；虽然**DQ**对精度提升不大，但是对于内存控制效果更好。

![](https://pic.imgdb.cn/item/648e99201ddac507cc5a4c70.jpg)

除此之外，论文中还对不同大小模型、不同数据类型、在 **MMLU**数据集上的微调效果进行了对比。使用**QLoRA（NFloat4 + DQ）**可以和**Lora(BFloat16)**持平，同时使用**QLORA（ FP4）**的模型效果落后于前两者一个百分点。

![](https://pic.imgdb.cn/item/648e99661ddac507cc5aa003.jpg)

作者在实验中也发现了一些有趣的点，比如：指令调优虽然效果比较好，但只适用于指令相关的任务，在聊天机器人上效果并不佳，而聊天机器人更适合用**Open Assistant**数据集去进行微调。通过指令类数据集的调优更像是提升大模型的推理能力，并不是为聊天而生的。

