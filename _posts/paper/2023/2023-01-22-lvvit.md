---
layout: post
title: 'All Tokens Matter: Token Labeling for Training Better Vision Transformers'
date: 2023-01-22
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/643bbb8f0d2dde5777b4fd6b.jpg'
tags: 论文阅读
---

> LV-ViT：使用标志标签更好地训练视觉Transformers.

- paper：[All Tokens Matter: Token Labeling for Training Better Vision Transformers](https://arxiv.org/abs/2104.10858)

本文提出了**Token Labeling**更好地训练视觉**Transformers**，**Token Labeling**是一个新的训练目标，借助它可以使得参数量只有**26M/56M/150M**的视觉**Transformer**模型在**ImageNet**上的精度达到$84.4\%/85.4\%/86.2\%$。

# 1. 训练视觉Transformer的技巧

作者首先总结和列举了之前有过的几种不同的**Training Techniques**，看看它们对视觉**Transformer**模型的效果到底如何。

## （1）增加模型的深度

原始的视觉**Transformer**是**12**个**block**，简单地多加几个**block**对模型性能没什么改变。

## （2）引入卷积

在视觉**Transformer**中加入一些卷积操作，能够更容易地提取到**low-level**的信息，这是因为卷积操作具有**inductive bias**的能力，使得模型在浅层能很好地学习到**low-level**的信息，这些信息使得输入给 **Transformer Block** 的 **patch embedding** 更有意义。本文沿用了这一操作，在模型中加入了卷积。同时为了使得卷积的感受野更大，本文使得卷积操作的**stride**更小，这样一来就能够提供更多的**overlapped**的信息。

## （3）调整残差连接

原始**Transformer**模型的前向过程是这样的：

$$
X \leftarrow X +SA(LN(X)) \\
X \leftarrow X +FF(LN(X))
$$

这样的过程可以视为从输入信号$X$开始，不断地在它上面叠加一些其他的信息。在残差连接中引入**scale factor**，在叠加之前要先乘上一个系数，而且不同的**dimension**要乘的系数也不同。

## （4）Re-labeling技术

在训练阶段总是会采用一些数据增强的方法，比较常用的是**random crop**方法，但是在**random crop**以后，标签并不总是准确的，因为**ImageNet**中的许多图像包括多个**object**，并且具有**Ground Truth**标签的对象可能不会保留在**random crop**之后的图像中。这个问题，当**random crop**的图像越小时就会越严重，因为**crop**出来的图像越小，可能越不会包含**Ground Truth**的**label**信息。在使用单标签标注时，图像随机剪裁可能包含与真值完全不同的对象，为训练带来噪声甚至不准确的监督信号。

**Re-labeling**技术的具体做法是使用多标签对 **ImageNet** 训练集进行重新标注。作者首先在一个巨大的数据集上训练了一个强大的图像分类器模型，这个数据集是**JFT-300M**或者**InstagramNet-1B**，并把这个模型在**ImageNet**上进行**fine-tune**。

把一张图片输入这个强分类模型，取它**Global Average Pooling**之前的输出特征$L \in R^{H\times W\times C}$，把它叫做**Label map**。原来的分类器得到$1 \times 1\times C$维的输出结果，而现在只需要$H\times W\times C$维的**Label map**。

假设**random crop**的区域是$[c_x,c_y,c_w,g_h]$，在训练时首先把这个区域的图片给**resize**成**224×224**的大小作为输入图片，再把特征**Label map**位于$[c_x,c_y,c_w,g_h]$的区域进行**ROI Align**操作，得到$H \times W\times C$维的输出结果。最后对这个结果进行**softmax**操作得到最终的$1 \times 1\times C$维的输出结果，作为这个区域图像的**label**进行训练。

![](https://pic.imgdb.cn/item/643bafd60d2dde5777a57e40.jpg)

**Re-labeling**的伪代码如下：

```python
Algorithm A1 ReLabel Pseudo-code
for each training iteration do
   # Load image data and label maps (assume the minibatch size is 1 for simplicity)
   input, label map = get minibatch(dataset)
   # Random crop augmentation
   [cx,cy,cw,ch] = get crop region(size(input))
   input = random crop(input, [cx,cy,cw,ch])
   input = resize(input, [224; 224])
   # LabelPooling process
   target = RoIAlign(label map, coords=[cx,cy,cw,ch], output size=(1; 1))
   target = softmax(target)
   # Update model
   output = model forward(input)
   loss = cross entropy loss(output, target)
   model update(loss)
end for
```

对于**Random crop**的每一个**crop**的区域，在参与训练时，其使用的标签都不再是原图的标签了，而是由这个强分类器得到的软标签。而且，使用强分类器生成软标签，解决了标注成本问题；在最终池化层之前使用像素级多标签预测，以充分利用额外的位置特定监督信号。

![](https://pic.imgdb.cn/item/643bb18a0d2dde5777a7982d.jpg)

# 2. Token-labeling

本文作者把**Re-labeling**技术用在了视觉**Transformer**中，设计了**Token-labeling**方法。视觉**Transformer**模型的输出，即$[X^{cls},X^1,...,X^N]$，现在只利用了$X^{cls}$，而剩下的$N$个**token**其实是可以结合上面的**Re-labeling**技术加以利用。

一张图片分了**patch**以后，每个**patch**都会转化成**token**，这些**token**其实也是有软标签的。只需要给一个预训练的强分类模型输入一下这个图片，就能够使用**Re-labeling**技术得到所有这些**token**的软标签$[y^1,...,y^N]$，这里的每一个$y$都是一个$C$维向量，它也叫作**token label**。

![](https://pic.imgdb.cn/item/643bb4910d2dde5777acd9bc.jpg)

定义一个 **token labeling loss**：

$$
L_{aux} = \frac{1}{N} \sum_{i=1}^N H(X^i,y^i)
$$

则总损失为：

$$
L_{total} = H(X^{cls},y^{cls}) + \beta L_{aux}
$$

同时在训练模型时使用了**CutMix**技术，它能提高模型的性能和鲁棒性。但如果直接在原始图像上应用**CutMix**，产生的一些补丁可能包含来自两个图像的内容，导致一个小**patch**内的混合区域，如左图所示。这样的话，很难为每个输出**token**分配一个干净、正确的软标签。考虑到这种情况，作者重新考虑了**CutMix**的增强方法，并给出了**MixToken**，它可以被看作是在**Patch embedding**后对令牌进行操作的**CutMix**的修改版本，如图右侧所示。

![](https://pic.imgdb.cn/item/643bb5a70d2dde5777aebc95.jpg)

具体来说，对于两个图像$I_1,I_2$及其对应的**token label** $Y_1=[y_1^1,...,y_1^N]$以及$Y_2=[y_2^1,...,y_2^N]$，首先把这2个图像变为**tokens**：$T_1=[t_1^1,...,t_1^N],T_2=[t_2^1,...,t_2^N]$。然后通过一个**mask** $M$得到一个新的**tokens**：

$$ \hat{T} = T_1 \odot M + T_2 \odot (1-M) $$

同时也把这2张图片对应的软标签使用这个**mask** 进行结合：

$$ \hat{Y} = Y_1 \odot M + Y_2 \odot (1-M) $$

**class token**的标签可以写为：

$$ \hat{y}^{cls} = \overline{M} y_1^{cls}+(1-\overline{M}) y_2^{cls} $$