---
layout: post
title: 'Parameter-Efficient Transfer Learning for NLP'
date: 2023-02-01
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/648d64d21ddac507ccaabefd.jpg'
tags: 论文阅读
---

> 自然语言处理中的参数高效的迁移学习.

- paper：[Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)

目前在大规模预训练模型上进行微调是**NLP**中一种高效的迁移学习方法，但是对于众多的下游任务而言，微调是一种低效的参数更新方式：对于每一个下游任务，都需要去更新语言模型的全部参数，这需要庞大的训练资源。进而，人们会尝试固定预训练模型的大部分参数，针对下游任务只更新一部分参数（大部分情况下都是只更新模型最后几层的参数），但是由于语言模型的不同位置的网络聚焦于不同的特征，针对具体任务中只更新高层网络参数的方式在不少情形遭遇到精度的急剧下降。

本文提出了**Adapter**，会针对每个下游任务在语言模型的每层**Transformer**中新增$2$个带有少量参数的**adapter**模块，针对下游任务训练时只更新**adapter**模块参数，而冻结原有语言模型的参数，从而实现将强大的大规模语言模型的能力高效迁移到诸多下游任务中去，同时保证模型在下游任务的性能。**Adapter**通过引入$0.5\%～5\%$的模型参数可以达到不落后全量微调模型$1\%$的性能。

**Transformer**的每层网络包含两个主要的子模块，一个**attention**多头注意力层跟一个**feedforward**层，这两个子模块后续都紧随一个**projection**操作，将特征大小映射回原本的输入的维度，然后连同**skip connection**的结果一同输入**layer normalization**层。而**adapter**直接应用到这两个子模块的输出经过**projection**操作后，并在**skip-connection**操作之前，进而可以将**adapter**的输入跟输出保持同样的维度，所以输出结果直接传递到后续的网络层，不需要做更多的修改。每层**Transformer**都会被插入两个**adapter**模块。

![](https://pic.imgdb.cn/item/648d68991ddac507ccb24dbb.jpg)

**adapter**的具体结构如图所示。每个 **Adapter** 模块主要由两个前馈（**Feedforward**）子层组成，第一个前馈子层（**down-project**）将**Transformer**块的输出作为输入，将原始输入维度$d$（高维特征）投影到$m$（低维特征），通过控制$m$的大小来限制**Adapter**模块的参数量，通常情况下，$m< <d$。然后，中间通过一个非线形层。在输出阶段，通过第二个前馈子层（**up-project**）还原输入维度，将$m$（低维特征）重新映射回$d$（原来的高维特征），作为**Adapter**模块的输出。同时，通过一个**skip connection**来将**Adapter**的输入重新加到最终的输出中去，这样可以保证即便 **Adapter** 一开始的参数初始化接近**0**，也由于**skip connection**的设置而接近于一个恒等映射，从而确保训练的有效性。

$$
h \leftarrow h + f(hW_{down})W_{up}
$$

通过实验发现，只训练少量参数的**Adapter**方法的效果可以媲美全量微调，这也验证了**Adapter**是一种高效的参数训练方法，可以快速将语言模型的能力迁移到下游任务中去。

![](https://pic.imgdb.cn/item/648e5e0d1ddac507cc0aece3.jpg)

**Adapter** 最佳的中间层特征维度$m$视数据集的大小而异，如：**MINI**数据集为**256**，最小的**RTE**数据集为**8**。如果始终将维度限制在**64**，将导致平均准确率略微下降。

