---
layout: post
title: 'Learning Object-Language Alignments for Open-Vocabulary Object Detection'
date: 2023-11-13
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/658b90eac458853aef07d3b5.jpg'
tags: 论文阅读
---

> 为开放词汇目标检测学习目标-语言对齐.

- paper：[Learning Object-Language Alignments for Open-Vocabulary Object Detection](https://arxiv.org/abs/2211.14843)

最近的一些**OVD**工作通过裁剪图像从面向分类的模型中提取视觉区域特征。这类模型被训练为区域-单词（**region-word pairs**）匹配，它们的性能受到预训练模型的限制。因此本文提出新的方法直接完成全局图像-文本匹配（**image-text pairs**）。

![](https://pic.imgdb.cn/item/658b93edc458853aef11a927.jpg)

**VLDet**直接从**image-text pairs**训练目标检测器，而不依赖昂贵的基础注释或提取面向分类的视觉模型。论文的主要出发点是从**image-text pairs**中提取**region-word pairs**可以表述为两个集合的元素匹配问题，该问题可以通过找到区域和单词之间具有最小全局匹配成本的二分匹配来有效解决。

具体来说，将图像区域特征视为一个集合，将词嵌入视为另一个集合，并将内积相似度作为区域特征与词嵌入对齐分数。为了找到最低成本，最优二分匹配将强制每个图像区域在**image-text pairs**的全局监督下与其对应的词对齐。通过用最佳区域词对齐损失代替目标检测中的分类损失，论文提出的方法可以帮助将每个图像区域与相应的词匹配并完成目标检测任务。

![](https://pic.imgdb.cn/item/658b9483c458853aef13b6c9.jpg)

从**image-text pairs**学习目标和语言的对齐。由于图像文本对数据的低成本，假设图像-文本数据集涵盖了更多种类的目标，则能够增加检测器的词汇量。然而目标检测需要更细粒度的**region-word pairs**进行训练。关键的挑战是如何找到区域集和单词集之间的对应关系。

**VLDet**不再为每张图像生成伪边界框标签，而是提出将区域-词对齐问题表述为最优二分匹配问题。将图像-文本对视为特殊的区域-单词对。通过将整个图像视为一个特殊区域并将来自文本编码器的整个**captioning**特征视为一个特殊词来提取图像的 **RoI** 特征。对于图像，将其**captioning**视为正样本，将同一小**batch**中的其他**captioning**视为负样本。同样的，由于采用了匹配的模式，整个框架的**loss**同样采用了**BCE loss**：

$$
L_{r e g i o n-w o r d}=\sum_{i=1}^{|W|}-\left[\log\sigma(s_{i k})+\sum_{j\in W^{\prime}}\log(1-\sigma(s_{j k}))\right]
$$

下表展示了针对**open vocabulary COCO**数据集的不同方法的性能。可以看出，**VLDet**在新类上表现最好，表明使用图像文本对的二分匹配损失的优越性。**Base-only** 方法表示使用完全监督的 **COCO base-category** 检测数据训练的 **Faster R-CNN**，**CLIP** 的**embeddings**作为分类器头。虽然 **CLIP** 具有对新类的泛化能力，但它只达到了 **1.3 mAP**。尽管 **ViLD** 和 **RegionCLIP** 使用 **CLIP** 提取区域**proposal**特征，但它们在新类别上的表现不如**VLDet**。这些蒸馏方法需要来自预训练 **CLIP** 模型的图像编码器和文本编码器来学习图像区域和词汇表之间的匹配。因此它们在新类别上的表现受到预训练模型的限制。

![](https://pic.imgdb.cn/item/658b96e5c458853aef1bb8ed.jpg)