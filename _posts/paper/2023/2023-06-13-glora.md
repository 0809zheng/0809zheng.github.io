---
layout: post
title: 'One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning'
date: 2023-06-13
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/648ebfb11ddac507cc93bcd7.jpg'
tags: 论文阅读
---

> GLoRA：参数高效微调的广义LoRA方法.

- paper：[One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2305.14314)

近年来，大规模深度神经网络在各种领域和任务中表现出前所未有的性能。这些大模型通常具有数百万甚至数十亿参数，在通用的大规模数据集上进行预训练，然后通过迁移学习的微调适应下游目标任务。

考虑到对大模型进行全量微调（即微调大模型的所有参数）需要的庞大的计算资源，大模型通常采用参数高效的微调方法。比如[<font color=blue>Prompt Tuning</font>](https://0809zheng.github.io/2023/02/05/prompttuning.html)在**Transformer**的输入中引入了少量的可学习参数；[<font color=blue>Adapter</font>](https://0809zheng.github.io/2023/02/01/adapter.html)在**Transformer**模块中引入了包含少量参数的网络；[<font color=blue>LoRA</font>](https://0809zheng.github.io/2023/02/10/lora.html)通过学习低秩分解矩阵对来减少可训练参数的数量。

然而不同的下游任务数据集在分布和组成上存在显著差异。上述微调策略可能无法充分解释这些差异，从而阻碍其适应不同数据集的能力。本文提出了一种新的微调方法：广义**LoRA**（**GLoRA**），**GLoRA**通过一个针对参数高效微调的统一公式构建，同时考虑了权重、特征和**token**维度，通过采用可扩展、模块化、逐层的结构搜索来学习网络每层的微调参数。由于微调参数采用重参数化形式，因此不会引入额外的推理成本。

对于模型微调，本文提出了一个统一的公式，该公式包含所有可调维度（包括但不限于权重空间和特征空间）。此外，在推理阶段采用了重参数化策略，将辅助参数纳入对应的预训练参数中，从而不额外增加推理成本。

$$
\begin{aligned}
f(x) &= (W_0 + \underbrace{W_0A + B}_{\text{weight space}} )x + \underbrace{CW_0+Db_0+E}_{\text{feature space}}+b_0 \\
&= W_{uni}x+b_{uni}
\end{aligned}
$$

![](https://pic.imgdb.cn/item/648ec9d31ddac507cca27daa.jpg)

其中$A,B,C,D,E$是下游任务中可训练的张量，$W_0,b_0$是预训练模型的参数（微调时被冻结）。其中$A$用于缩放权重参数，$B$用于缩放输入和偏移权重参数，$C$用于补充层级可训练**prompt**（与**prompt tuning**相似），$D,E$用于缩放和偏移偏置参数。

$A,B,C,D,E$可以在网络的不同层中设置为不同的形式，可取标量、向量、低秩分解(如**LoRA**)或不采用的形式，具体的搜索空间为：

$$
\begin{aligned}
A &= \{ \text{LoRA},\text{vector},\text{scalar},\text{none} \} \\
B &= \{ \text{LoRA},\text{vector},\text{scalar},\text{none} \} \\
C &= \{ \text{LoRA},\text{vector},\text{none} \} \\
D &= \{ \text{vector},\text{scalar},\text{none} \} \\
E &= \{ \text{vector},\text{scalar},\text{none} \} \\
\end{aligned}
$$

为了确定每一层中$A,B,C,D,E$的最佳配置，采用了进化搜索方法。进化搜索相比于广泛的超参数搜索不需要额外的超参数调整，能够实现效率与有效性的平衡，但仍然会增加训练时间。在搜索过程中采用权重共享策略，即为每个张量定义一个矩阵，根据分量（**LoRA**、向量、标量或无），对子矩阵进行索引。

下图展示了在$72$次不同的搜索中，网络不同层中可训练参数的设置分布情况。结果表明参数$D,E$在不同的调整中表现出明显的无适应性，而参数$A,B$表现出较高的适应性。

![](https://pic.imgdb.cn/item/648ed0d61ddac507ccac20e1.jpg)

实验结果表明，在**VTAB-1k**基准测试集（包含$19$个不同的视觉图像集）上，**GLoRA**的平均精度超过以往所有最先进的参数高效微调方法。

![](https://pic.imgdb.cn/item/648ecd781ddac507cca79ddb.jpg)

此外，在一些细粒度视觉识别数据集上，**GLoRA**进行少样本学习的性能也超过现有方法：

![](https://pic.imgdb.cn/item/648ecde41ddac507cca8263a.jpg)

下图显示了在不同任务中网络不同层对应可学习参数的分布。结果表明多头自注意力模块的投影层具有的可训练参数最少，而**MLP**中的全连接层需要的可训练参数最多。并且结构化（**structured**）任务需要的参数量最大，因为这些数据集相比于预训练的自然数据集具有明显的语义鸿沟。

![](https://pic.imgdb.cn/item/648ed2421ddac507ccadff28.jpg)