---
layout: post
title: 'SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis'
date: 2023-07-04
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/6672b769d9c307b7e9a0af2f.png'
tags: 论文阅读
---

> SDXL：改进高分辨率图像合成的隐扩散模型.

- paper：[SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis](https://arxiv.org/abs/2307.01952)

**SDXL**模型是对[<font color=Blue>Stable Diffusion</font>](https://0809zheng.github.io/2022/06/18/ldm.html)模型的官方改进，相比于**SDv1.5**模型的**0.98B**参数，**SDXL**模型的参数提高到**6.6B**。**SDXL**模型的改进主要在以下几个方面：
- 文本编码器结合了**OpenClip ViT-G/14**与**CLIP ViT-L**；
- 引入图像尺寸条件，允许尺寸小于$256\times 256$的图像参与训练；
- **UNet**模型的大小提高3倍；
- 默认图像尺寸调整为$1024\times 1024$。

![](https://pic.imgdb.cn/item/6672b92ed9c307b7e9a39be9.png)

**SDXL**模型是一个二阶段的级联扩散模型，包括**Base**模型和**Refiner**模型。其中**Base**模型是基本的文生图模型；在**Base**模型之后级联的**Refiner**模型是图生图模型，用于对**Base**模型生成的图像隐特征进行精细化。

![](https://pic.imgdb.cn/item/6672baa1d9c307b7e9a72c4a.png)

**Base**模型由**U-Net**、**VAE**和两个**CLIP Text Encoder**组成。**Refiner**模型采用独立的**U-Net**，而**VAE**和一个**CLIP Text Encoder**与**Base**模型共用（**Refiner**模型的**Text Encoder**只使用了**OpenCLIP ViT-bigG**）。

**SDXL**模型输入的最大**Token**数是77，当输入文本的**Token**数量超过77后，将通过**Clip**操作拉回77；如果Token数不足77则会**padding**操作。

**SDXL**模型分别提取两个**CLIP Text Encoder**的倒数第二层特征，并进行连接操作作为文本条件。其中**OpenCLIP ViT-bigG**的特征维度为77x1280，而**CLIP ViT-L**的特征维度是77x768，所以输入总的特征维度是77x2048（77是最大的**token**数）。

### ⚪ 训练技巧：图像尺寸条件化

**Stable Diffusion**的训练过程主要分成两个阶段，一个是在256x256的图像尺寸上进行预训练，然后在512x512的图像尺寸上继续训练。这两个阶段的训练过程都要对最小图像尺寸进行约束。第一阶段会将尺寸小于256x256的图像舍弃；第二阶段会将尺寸小于512x512的图像舍弃。这样的约束会导致训练数据中的大量数据被丢弃。

下图展示了如果将尺寸小于256x256的图像筛除，整个数据集将减少39\%的数据。如果加上尺寸小于512x512的图像，未利用数据占整个数据集的百分比将更大。

![](https://pic.imgdb.cn/item/6672bd69d9c307b7e9ac4ad9.png)

**SDXL**模型为了在解决数据集利用率问题的同时不引入噪声伪影，将**U-Net（Base）**模型与原始图像分辨率相关联，核心思想是将输入图像的原始高度和宽度作为额外的条件嵌入**U-Net**模型中。图像高度和宽度都使用傅里叶特征编码进行独立嵌入，然后将特征连接到时序特征后。

结果表明，引入图像尺寸条件后，模型在训练过程中能够学习到图像的原始分辨率信息，从而在推理生成阶段更好地适应不同尺寸的图像生成，而不会产生噪声伪影的问题；在不断增大分辨率条件时，生成的图像质量不断提升。

![](https://pic.imgdb.cn/item/6672be15d9c307b7e9ade778.png)


### ⚪ 训练技巧：图像裁剪参数条件化

由于需要输入固定的图像尺寸用作训练，很多数据在预处理阶段会被裁剪。典型预处理方式是先调整图像尺寸，使得最短边与目标尺寸匹配，然后再沿较长边对图像进行随机裁剪或者中心裁剪。虽然裁剪是一种数据增强方法，但是训练中对图像裁剪导致的图像特征丢失，可能会导致模型在图像生成阶段出现不符合训练数据分布的特征。

**SDXL**模型引入了图像裁剪参数条件化策略。其主要思想是在加载数据时，将左上角的裁剪坐标通过傅里叶编码并嵌入**U-Net（Base）**模型中，并与原始图像尺寸一起作为额外的条件嵌入**U-Net**模型，从而在训练过程中让模型学习到对“图像裁剪”的认识。

![](https://pic.imgdb.cn/item/6672bee6d9c307b7e9afd798.png)

### ⚪ 训练技巧：多尺度训练

**SDXL**模型首先在256x256和512x512的图像尺寸上分别预训练600000步和200000步（**batch size** = 2048），总的数据量 600000 x 2000000 x 2048 约等于16亿。

接着在1024x1024的图像尺寸上采用多尺度方案来进行微调，并将数据分成不同纵横比的**bucket**，并且尽可能保持每个**bucket**的像素数接近1024×1024，同时相邻的**bucket**之间高度或宽度一般相差64像素左右：

![](https://pic.imgdb.cn/item/6672bf8ad9c307b7e9b15a0b.png)

在训练过程中，一个**Batch**从一个**bucket**里的图像采样，并且在每个训练步骤中在不同的**bucket**大小之间交替切换。除此之外，**aspect ratio**也会作为条件嵌入到**U-Net（Base）**模型中，让模型能够更好地学习到“多尺度特征”。