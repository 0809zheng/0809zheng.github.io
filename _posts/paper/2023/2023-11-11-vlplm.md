---
layout: post
title: 'Exploiting Unlabeled Data with Vision and Language Models for Object Detection'
date: 2023-11-11
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/658a8d37c458853aef1f9dc8.jpg'
tags: 论文阅读
---

> 通过视觉和语言模型探索目标检测中的无标签数据.

- paper：[Exploiting Unlabeled Data with Vision and Language Models for Object Detection](https://arxiv.org/abs/2207.08954)

构建通用目标检测框架需要扩展到更大的标签和训练数据集，然而大规模获取类别标注的成本过高。作者利用最近视觉和语言模型中丰富的语义来定位和分类未标记图像中的目标，有效地生成用于目标检测的伪标签。从通用的和类无关的**region proposal**机制开始，作者使用视觉和语言模型将图像的每个区域分类为下游任务所需的任何目标类别。

该方法利用最近提出的视觉和语言模型**CLIP**来生成用于目标检测的伪标签。首先使用两阶段**proposal**生成器预测区域。对于每个区域使用预训练的**V&L**模型获得预测类别的概率分布。为了改进定位质量，作者融合两阶段**proposal**生成器的**CLIP**分数和目标分数，并通过在**proposal**生成器中重复应用定位头来移除冗余提**proposal**。最后将生成的伪标签与原始**ground truth**相结合，训练最终检测器。

![](https://pic.imgdb.cn/item/658a9195c458853aef2c5521.jpg)

从未标记数据中学习的方法是通过伪标签。首先在有限的**Ground Truth**数据上训练教师模型，然后为未标记的数据生成伪标签，最后训练学生模型。作者提出了一种用于目标检测的通用训练策略，以处理不同形式的未标记数据。定义一个目标检测器的通用损失函数：

$$
\mathcal{L}(\theta,\mathcal{I})=\frac{1}{N_{\mathcal{I}}}\sum_{i=1}^{N_{\mathcal{I}}}[I_{i}\in \mathcal{I}_{L}] l_{s}(\theta,I_{i})+\alpha[I_{i}\in \mathcal{I}_{U}] l_{u}(\theta,I_{i})
$$

监督损失包含分类的标准交叉熵损失和回归的**L1**损失:

$$
l_{s}(\theta,I)=\frac{1}{N^{s}}\sum_{i}l_{c l s}\left(C_{i}^{\theta}(I),c_{\sigma(i)}^{s}\right)+[\sigma(i)\neq nil] l_{r e g}\left(T_{i}^{\theta}(I),\mathrm{t}_{\sigma(i)}^{s}\right)
$$

无监督损失使用具有高置信度的伪标签作为监督信号：

$$
l_{u}(\theta,I)=\frac{1}{N^{u}}\sum_{i}[\max(p_{\sigma(i)}^u)\geq \tau]\cdot l_{c l s}\left(C_{i}^{\theta}(I),c_{\sigma(i)}^{u}\right)+[\sigma(i)\neq nil] l_{r e g}\left(T_{i}^{\theta}(I),\mathrm{t}_{\sigma(i)}^{u}\right)
$$

通过使用网络抓取的数据 (图像和相应的文本)，**VL**语言模型可以在没有昂贵人工注释的情况下在大规模图像-文本对数据集上进行训练，覆盖不同的图像域和自然文本中丰富的语义。**VL**模型是为任意类别生成伪标签的理想外部知识来源，可用于下游任务，例如开放词汇或半监督目标检测。

使用最近的**V & L**模型**CLIP**生成伪标签的整体流水线如图所示。首先将一个未标记的图像输入两阶段检测器以获得区域**proposal**。然后根据这些区域裁剪图像块，并将其输入图像编码器，以嵌入**CLIP**视觉空间中。使用相应的**CLIP**文本编码器为特定任务所需的类别名称生成嵌入。对于每个区域，通过点积计算区域嵌入和文本嵌入之间的相似性，并使用**softmax**获得类别上的分布。

![](https://pic.imgdb.cn/item/658a947bc458853aef34c850.jpg)

上述框架面临两个关键挑战:
1. 为开放词汇检测所需的新类别生成可靠的区域**proposal**；
2. 克服原始**CLIP**模型的定位质量较差的特点。

为了利用未标记数据进行开放式词汇检测等任务，**proposal**生成器不仅应该能够定位训练期间看到的类别，还应该能够定位新类别。两级检测器的区域建议网络（**RPN**）对于新类别具有良好的泛化能力。因此作者训练了一个标准的两阶段检测器**Faster R-CNN**作为**proposal**生成器。

在裁剪区域**proposal**上直接应用**CLIP**会产生较低的定位质量。作者发现**RPN**分数是衡量区域**proposal**定位质量的一个良好指标。利用这一观察结果，将**RPN**分数与**CLIP**预测值进行平均。其次作者去除**proposal**生成器的阈值和**NMS**，并将**proposal**框多次馈送到**RoI**头。通过重复**RoI**头将冗余框推近彼此。这样可以产生位置更好的边界框，并提供更好的伪标签。

![](https://pic.imgdb.cn/item/658a971cc458853aef3cbf42.jpg)

作者演示了生成伪标签在两个特定任务中的价值：开放词汇检测，其中模型需要推广到看不见的目标类别；半监督目标检测，其中可以使用额外的未标记图像来改进模型。本文的实证评估显示了伪标签在这两项任务中的有效性，在这两项任务中，本文的表现优于竞争基线，并实现了开放词汇表目标检测的**SOTA**。

![](https://pic.imgdb.cn/item/658a973cc458853aef3d17a2.jpg)
