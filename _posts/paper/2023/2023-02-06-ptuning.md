---
layout: post
title: 'GPT Understands, Too'
date: 2023-02-06
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/648d8d2d1ddac507cc026be7.jpg'
tags: 论文阅读
---

> P-Tuning：GPT也能够理解.

- paper：[GPT Understands, Too](https://arxiv.org/abs/2103.10385)

大模型的**Prompt**构造方式严重影响下游任务的效果。比如：**GPT-3**采用人工构造的模版来做上下文学习，但人工设计的模版的变化特别敏感，加一个词或者少一个词，或者变动位置都会造成比较大的变化。

![](https://pic.imgdb.cn/item/648d8d781ddac507cc02ca9b.jpg)

同时，近来的自动化搜索模版工作成本也比较高，以前这种离散化的**token**的搜索出来的结果可能并不是最优的，导致性能不稳定。

基于此作者提出了**P-Tuning**，设计了一种连续可微的**virtual token**。

![](https://pic.imgdb.cn/item/648d8de91ddac507cc0369e5.jpg)


**P-Tuning**方法把传统人工设计模版中的真实**token**替换成可微的**virtual token**，并用提示编码器(**MLP+LSTM**)进行一层处理。**P-Tuning**加入的可微**virtual token**仅限于输入层；**virtual token**的位置也不一定是前缀，插入的位置是可选的。


![](https://pic.imgdb.cn/item/648d8fa31ddac507cc059627.jpg)

![](https://pic.imgdb.cn/item/648d8fcd1ddac507cc05ca16.jpg)

经过预训练的**LM**的词嵌入已经变得高度离散，如果随机初始化**virtual token**，容易优化到局部最优值，而这些**virtual token**理论是应该有相关关联的。因此，作者通过实验发现用一个**prompt encoder**来编码会收敛更快，效果更好。即用一个**LSTM+MLP**去编码这些**virtual token**以后，再输入到模型。

从对比实验证实看出，**P-Tuning**获得了与全参数一致的效果。甚至在某些任务上优于全参数微调。

![](https://pic.imgdb.cn/item/648d90271ddac507cc063f52.jpg)

在实验中还发现，相同参数规模，如果进行全参数微调，**Bert**在自然语言理解**NLU**任务上的效果超过**GPT**很多；但是在**P-Tuning**下，**GPT**可以取得超越**Bert**的效果。

![](https://pic.imgdb.cn/item/648d90d41ddac507cc0717b6.jpg)
