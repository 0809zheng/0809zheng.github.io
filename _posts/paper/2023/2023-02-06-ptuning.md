---
layout: post
title: 'GPT Understands, Too'
date: 2023-02-06
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/648d8d2d1ddac507cc026be7.jpg'
tags: 论文阅读
---

> P-Tuning：GPT也能够擅长神经语言理解任务.

- paper：[GPT Understands, Too](https://arxiv.org/abs/2103.10385)

**Prompt**是由自然语言构成的前缀/后缀，通过**Prompt**使得下游任务也转换成跟预训练任务格式一致的完形填空问题，能更加充分地利用原始预训练模型，从而实现零样本、小样本学习。比如：**GPT-3**采用人工构造的模版来做上下文学习。由于**GPT-3**模型是从左往右解码的，因此预测部分通常放在句末：

![](https://pic.imgdb.cn/item/648e592f1ddac507cc04fc19.jpg)

大模型的**Prompt**构造方式严重影响下游任务的效果。人工设计的模版的变化特别敏感，加一个词或者少一个词，或者变动位置都会造成比较大的变化。同时，近来的自动化搜索模版工作成本也比较高，以前这种离散化的**token**的搜索出来的结果可能并不是最优的，导致性能不稳定。

![](https://pic.imgdb.cn/item/648d8d781ddac507cc02ca9b.jpg)

基于此作者提出了**P-Tuning**，放弃了“模版由自然语言构成”这一常规要求，从而将模版的构建转化为连续参数优化问题，设计了一种连续可微的**virtual token**。

**P-Tuning**考虑如下形式的**Prompt**：

![](https://pic.imgdb.cn/item/648e59f41ddac507cc0637b4.jpg)

其中**[u1]～[u6]**代表**BERT**词表里边的**[unused1]～[unused6]**，也就是用几个从未见过的**token**来构成模板，这里的**token**数目是一个超参数，放在前面还是后面也可以调整。可以用标注数据来求出这个模板。
- 当标注数据比较少时，固定整个模型的权重，只优化**[unused1]～[unused6]**这几个**token**的**Embedding**；
- 当标注数据充足时，可以放开所有权重微调，由于跟预训练任务更一致，因此效果会比直接加个全连接层微调更好。

**P-Tuning**并不是随机初始化几个新**token**然后直接训练的，而是通过一个**prompt encoder**（由一个双向的**LSTM**+两层**MLP**组成）把这几个**Embedding**算出来，并且将这个**LSTM**模型设为可学习的。**LSTM**出现的**token**表示相关性更强，某种程度上来说更像“自然语言”，此外还能防止局部最优。


![](https://pic.imgdb.cn/item/648d8fa31ddac507cc059627.jpg)

![](https://pic.imgdb.cn/item/648d8fcd1ddac507cc05ca16.jpg)

经过预训练的**LM**的词嵌入已经变得高度离散，如果随机初始化**virtual token**，容易优化到局部最优值，而这些**virtual token**理论是应该有相关关联的。因此，作者通过实验发现用一个**prompt encoder**来编码会收敛更快，效果更好。即用一个**LSTM+MLP**去编码这些**virtual token**以后，再输入到模型。

从对比实验证实看出，**P-Tuning**获得了与全参数一致的效果。甚至在某些任务上优于全参数微调。

![](https://pic.imgdb.cn/item/648d90271ddac507cc063f52.jpg)

在实验中还发现，相同参数规模，如果进行全参数微调，**Bert**在自然语言理解**NLU**任务上的效果超过**GPT**很多；但是在**P-Tuning**下，**GPT**在**SuperGLUE**上的成绩首次超过了同等级别的**BERT**模型，这颠覆了一直以来“**GPT**不擅长**NLU**”的结论。

![](https://pic.imgdb.cn/item/648d90d41ddac507cc0717b6.jpg)
