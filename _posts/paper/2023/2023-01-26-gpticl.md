---
layout: post
title: 'Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers'
date: 2023-01-26
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67fe1fa288c538a9b5d19e03.png'
tags: 论文阅读
---

> 为什么GPT可以上下文学习？语言模型隐式地作为元优化器执行梯度下降.

- paper：[Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers](https://arxiv.org/abs/2212.10559)

# 0. TL; DR

本文探讨了大型预训练语言模型（如**GPT**）在上下文学习（**ICL**）中的工作机制。研究发现，**GPT**可以通过隐式优化（即不更新参数）来实现类似微调（**finetuning**）的效果。具体来说，**GPT**通过演示示例生成“元梯度”（**meta-gradients**），并通过注意力机制将这些元梯度应用于原始模型，从而构建一个上下文学习模型。实验结果表明，上下文学习的行为与显式微调非常相似。此外，本文还提出了一种基于动量的注意力机制，进一步验证了对上下文学习的理解，并展示了其在模型设计中的潜力。

# 1. 背景介绍

近年来，大型预训练语言模型（如**GPT**）在自然语言处理（**NLP**）领域取得了显著进展。这些模型通过上下文学习（**ICL**）在新任务上表现出色，即通过推理而非微调来完成任务。与微调需要额外的参数更新不同，**ICL**只需要几个演示示例，模型就可以预测未见过的输入的标签。尽管**ICL**在性能上取得了巨大成功，但其工作机制仍是一个未解之谜。

本文旨在解释**GPT**如何实现上下文学习，并将其视为隐式优化过程。具体来说，本文将**ICL**视为隐式微调，并通过理论分析和实验验证了这一观点。研究发现，**Transformer**模型中的注意力机制与梯度下降具有对偶形式，这为理解**ICL**提供了新的视角。

# 2. 方法介绍

本文探讨了**Transformer**模型中的注意力机制与梯度下降之间的对偶关系。具体来说，注意力机制可以被视为一种隐式的优化过程，其中注意力值被视为“元梯度”，用于更新模型的参数。

![](https://pic1.imgdb.cn/item/67fe248f88c538a9b5d1a763.png)

梯度下降在进行优化时可以表示为：

$$
F(x) = (W_0 + \Delta W) x
$$

其中$W_0$ 是初始化的参数矩阵，$\Delta W$ 是通过梯度下降更新的参数矩阵，$x$ 是输入表示。

在反向传播中，参数更新 $\Delta W$ 是通过历史输入表示 $x'_i$ 和对应输出的误差信号 $e_i$ 的外积累加得到的：

$$
\Delta W = \sum_i e_i \otimes x'_i
$$

结合上述两个公式，可以得到梯度下降的输出：

$$
\begin{aligned}
F(x) &= W_0 x + \Delta W x \\
&= W_0 x + \sum_i e_i \otimes x'_i x \\
&= W_0 x + \sum_i e_i ({x'_i}^\top x) \\
&= W_0 x + \text{LinearAttn}(E, X', x)
\end{aligned}
$$

其中$E$ 是历史输出误差信号，作为值（**values**）；$X'$ 是历史输入，作为键（**keys**）；$x$ 是当前输入，作为查询（**query**）。

在上下文学习（**ICL**）中，**Transformer**的注意力机制可以被视为一种隐式的优化过程。具体来说，给定一个查询输入 $x$，其注意力查询向量为 $q = W_Q x$，注意力结果可以表示为：

$$
\begin{aligned}
F_{\text{ICL}}(q) &= \text{Attn}(V, K, q) \\
&= W_V [X'; X] \text{softmax}\left(\frac{(W_K [X'; X])^\top q}{\sqrt{d}}\right)
\end{aligned}
$$

为了简化分析，作者将标准注意力近似为线性注意力，去除**softmax**操作和缩放因子：

$$
\begin{aligned}
F_{\text{ICL}}(q) &\approx W_V [X'; X] (W_K [X'; X])^\top q \\
&= W_V X (W_K X)^\top q + W_V X' (W_K X')^\top q
\end{aligned}
$$

定义 $W_{\text{ZSL}} = W_V X (W_K X)^\top$ 作为零样本学习（**ZSL**）中初始化的参数，因为 $W_{\text{ZSL}} q$ 是没有演示示例时的注意力结果。根据线性层优化的对偶形式，可以推导出**Transformer**注意力的对偶形式：

$$
\begin{aligned}
F_{\text{ICL}}(q) &= W_{\text{ZSL}} q + W_V X' (W_K X')^\top q \\
&= W_{\text{ZSL}} q + \text{LinearAttn}(W_V X', W_K X', q) \\
&= W_{\text{ZSL}} q + \sum_i W_V x'_i (W_K x'_i)^\top q \\
&= W_{\text{ZSL}} q + \Delta W_{\text{ICL}} q \\
&= (W_{\text{ZSL}} + \Delta W_{\text{ICL}}) q
\end{aligned}
$$

其中$\Delta W_{\text{ICL}}$ 是通过演示示例计算的参数更新，类似于梯度下降中的 $\Delta W$；$W_V X'$ 被视为元梯度（**meta-gradients**），用于计算更新矩阵 $\Delta W_{\text{ICL}}$。

基于上述对**Transformer**注意力的分析，作者进一步比较了上下文学习（**ICL**）和显式微调（**finetuning**）之间的关系。显式微调的注意力结果可以表示为：

$$
\begin{aligned}
F_{\text{FT}}(q) &= (W_V + \Delta W_V) X X^\top (W_K + \Delta W_K)^\top q \\
&= (W_{\text{ZSL}} + \Delta W_{\text{FT}}) q
\end{aligned}
$$

其中$\Delta W_K$ 和 $\Delta W_V$ 是通过反向传播从任务特定的训练目标中获得的参数更新；$\Delta W_{\text{FT}}$ 是微调引入的对 $W_{\text{ZSL}}$ 的更新。

通过比较上下文学习和显式微调，作者发现它们在以下方面具有相似性：
1. **梯度下降**：两者都通过隐式或显式的梯度下降更新 $W_{\text{ZSL}}$。
2. **相同的训练信息**：ICL的元梯度和显式微调的梯度都来源于相同的训练示例。
3. **相同的因果顺序**：ICL和显式微调都遵循相同的训练示例顺序。
4. **目标相同**：两者都直接影响注意力键和值的计算。


# 3. 实验分析

本文使用两个预训练的**GPT**模型（**1.3**亿和**2.7**亿参数）进行实验。对于每个任务，使用相同的模板来格式化零样本学习（**ZSL**）、显式微调（**FT**）和上下文学习（**ICL**）的示例。实验中，**ICL**固定演示示例的数量为**32**，显式微调使用与**ICL**相同的演示示例作为训练示例，并使用**SGD**作为优化器。


## 3.1 ICL与FT的实验结果

### ⚪ ICL覆盖显式微调的正确预测

下表展示了六个分类数据集上的验证准确率。结果表明，**ICL**和显式微调都能显著提高性能，表明它们的优化对下游任务都有帮助。

![](https://pic1.imgdb.cn/item/67fe233088c538a9b5d1a17d.png)

为了比较**ICL**和显式微调的模型预测，本文定义了一个召回率指标（**Rec2FTP**），用于衡量**ICL**能够覆盖多少显式微调的正确预测：

$$
\text{Rec2FTP} = \frac{N(\text{FT} > \text{ZSL}) \cap (\text{ICL} > \text{ZSL})}{N(\text{FT} > \text{ZSL})}
$$

其中，$N(\text{FT} > \text{ZSL})$ 是显式微调能够正确预测但零样本学习（**ZSL**）不能的查询示例数量，$N(\text{FT} > \text{ZSL}) \cap (\text{ICL} > \text{ZSL})$ 是**ICL**也能够正确预测的示例数量。

下表展示了两个**GPT**模型在六个数据集上的**Rec2FTP**分数。结果显示，**ICL**能够覆盖显式微调超过85%的正确预测。这表明从模型预测的角度来看，**ICL**可以覆盖显式微调的大部分正确行为。

![](https://pic1.imgdb.cn/item/67fe235f88c538a9b5d1a1df.png)

### ⚪ ICL倾向于与显式微调相同方向更新注意力输出

为了比较**ICL**和显式微调对注意力输出的影响，本文定义了一个相似度指标（**SimAOU**），用于衡量**ICL**和显式微调对注意力输出的更新是否相似：

$$
\text{SimAOU}(\Delta \text{FT}) = \cos(h^{(\text{ICL})} - h^{(\text{ZSL})}, h^{(\text{FT})} - h^{(\text{ZSL})})
$$

其中，$h^{(\text{ICL})}$ 和 $h^{(\text{FT})}$ 分别是ICL和显式微调的注意力输出，$h^{(\text{ZSL})}$ 是零样本学习的注意力输出。

下表展示了两个**GPT**模型在六个数据集上的**SimAOU**分数。结果显示，**ICL**的更新与显式微调的更新相似，而与随机更新的相似度接近零。这表明从表示的角度来看，**ICL**倾向于与显式微调相同方向更新注意力输出。

![](https://pic1.imgdb.cn/item/67fe237488c538a9b5d1a230.png)

### ⚪ ICL倾向于生成与显式微调相似的注意力权重

下表展示了两个**GPT**模型在六个数据集上的**SimAM**分数。结果显示，与显式微调前的注意力权重相比，**ICL**更倾向于生成与显式微调后的注意力权重相似的权重。这表明从注意力行为的角度来看，**ICL**与显式微调相似。

![](https://pic1.imgdb.cn/item/67fe238c88c538a9b5d1a28b.png)

### ⚪ ICL和显式微调倾向于对训练示例分配相似的注意力

为了比较**ICL**和显式微调对训练示例的注意力权重，本文使用**Kendall**秩相关系数来衡量它们的相似性：

$$
\text{Kendall}(\text{ICL}, \text{FT}) = \frac{P_c - P_d}{N(N-1)/2}
$$

其中，$P_c$ 是一致对的数量，$P_d$ 是不一致对的数量，$N$ 是训练示例的数量。

下表展示了两个**GPT**模型在六个数据集上的**Kendall**秩相关系数。结果显示，**ICL**和显式微调对训练示例的注意力权重的顺序相似，而与随机注意力权重的相似度接近零。这表明**ICL**和显式微调倾向于对训练示例分配相似的注意力。

![](https://pic1.imgdb.cn/item/67fe23a488c538a9b5d1a303.png)

## 3.2 动量注意力机制的实验验证

受**Transformer**注意力与梯度下降的对偶形式启发，本文提出了一种基于动量的注意力机制。具体来说，动量注意力机制通过指数移动平均（**EMA**）来平均注意力值，从而引入动量机制：

$$
\text{MoAttn}(V, K, q_t) = \text{Attn}(V, K, q_t) + \text{EMA}(V)
$$

其中，$V$ 是值，$K$ 是键，$q_t$ 是查询，$\text{EMA}(V)$ 是动量项。

下表展示了两个**GPT**模型在训练集和不同输入长度的验证集上的困惑度。结果表明，应用动量注意力的模型在所有验证集上都取得了比普通**Transformer**更低的困惑度。

![](https://pic1.imgdb.cn/item/67fe241588c538a9b5d1a4d8.png)

下表展示了两个**GPT**模型在六个上下文学习数据集上的准确率。结果表明，应用动量注意力的模型在所有数据集上都取得了比普通**Transformer**更高的准确率。这表明引入动量机制可以提高**Transformer**注意力的性能。

![](https://pic1.imgdb.cn/item/67fe242a88c538a9b5d1a54b.png)
