---
layout: post
title: 'DetCLIP: Dictionary-Enriched Visual-Concept Paralleled Pre-training for Open-world Detection'
date: 2023-11-14
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/658b9cfac458853aef30f798.jpg'
tags: 论文阅读
---

> DetCLIP：用于开放世界检测的字典增强视觉概念并行预训练.

- paper：[DetCLIP: Dictionary-Enriched Visual-Concept Paralleled Pre-training for Open-world Detection](https://arxiv.org/abs/2209.09407)

开放世界目标检测的目标是识别和定位任意类别的目标。最近的工作 **GLIP** 将其构建成了一个 **phrase grounding** 任务，将所有待检测的类别整合成一个句子，然后送入 **text encoder** 进行编码并和图像特征进行对齐。在 **text encoder** 中，会学习所有类别之间的 **attention**，这其实是没有必要且低效的，尤其是当类别数量增加时，效率更低。

基于此，作者提出了 **DetCLIP**，设计了一个并行的结构来提高效率，不会将整个 **prompt text** 送入 **text encoder**，而是将每个 **concept** 都作为一个独立的个体，将所有的 **concept** 分别送入不同的 **text encoder**。这样能够避免模型受到不相关类别无效关联，并且能给每个 **concept** 都产生一个长描述。

![](https://pic.imgdb.cn/item/658bc582c458853aefb9137d.jpg)

另外预训练时使用的数据集一般域间差别比较大且命名也有较大的不同，比如一个相同类别的目标在不同数据集中的命名可能是不同的。这样就很难直接从命名上来获得这些类别之间的关联。所以作者重新构建了一个 **concept dictionary**，丰富联合不同数据集进行 **pre-training** 的 **prompt text**。首先从现有数据集中组合了一个 **dictionary**，然后基于上面的 **dictionary**，**DetCLIP** 能够自动丰富概念和描述。

![](https://pic.imgdb.cn/item/658bc5cfc458853aefba17de.jpg)

## 1. 并行概念表述 Paralleled Concept Formulation

**DetCLIP**引入了并行结构，会将每个类别名称单独送入 **text encoder** 来得到对应的编码结果，模型是从单独的 **concept** 中学习其语言特征的，可以提高学习效率；此外并行结构可以根据类别数量来很容易的扩展。

![](https://pic.imgdb.cn/item/658bc6d1c458853aefbe06f7.jpg)

不同数据如何适应这个并行结构：
- 检测数据：假设图中有 $k$ 个 **positive category**，首先将类别数量扩展到 $N$ （随机抽取负类别），$N$ 是预定义好的数量，用于构造 **alignment loss**，然后将 $N$ 个类别名称作为独立的句子送入 **text encoder**，并且使用 **[end of sentence] token** 的编码作为每个类别的 **text embedding**，最后，将所有 $N$ 个 **text embedding** 连接起来和 **gt** 去计算 **alignment loss**，**coco** 示例如下：**P=[''person'', ''bicycle'', ''car'', ..., ''toothbrush'']**
- **grounding** 数据：从 **grounding** 标注的 **caption** 中抽取 **positive phrase**，然后同样扩展到长度 $N$，一个例子如下：**P=[''a woman'', ''a herding dog'', ''three cattle'', ''neg_1'', ..., ''neg_m'']**
- **Image-text pair** 数据：只有图像和对应的描述，没有标注框。为了获得目标框，首先使用 **RPN** 来生成与类别无关的 **proposal**，然后使用预训练好的 **CLIP** 为这些 **proposal** 生成伪标签，然后和前面的处理方法一样。

## 2. 概念字典 Concept Dictionary

由于现有的 **detection/grounding/image-textpair** 这些数据集有较大的 **domain gap** 和不同的 **labeling space**。这些概念也会有包含或层级的关系，这些语义的关系可能会促进预训练，但仅从词汇名称中很难发现他们直接的关系。

所以作者构建了一个大规模的词汇字典，来将不同数据源的词汇统一到一个词汇空间，并且能够通过描述来提供不同词汇之间的关联。比如：
-  **car** 的描述为：**a motor vehicle with four wheels usually propelled by an internal combustion engine**
-  **motorcycle** 的描述为：**a motor vehicle with two wheels and a strong frame**

作者首先从多个源头收集 **concept**：**image-text pair** 数据集（**YFCC100m**）、检测数据集中的类别（**Object365**、**OpenImage**）、物体数据集中的物体名称（**Tings** 数据集）。然后会先去重然后放入词汇字典中，得到了包含约 **14k** 词汇和对应定义的字典。对于输入的 **concept**，如果该 **concept** 在字典里，则会使用该 **concept** 对应的描述；如果不在字典里，会通过计算相似性矩阵，来找出与其最接近的 **concept**，并且找到对应的描述。

## 3. DetCLIP

**DetCLIP** 包含：
- 一个 **image encoder** 来对图像进行编码
- 一个 **text encoder** 来对 **concept** 进行编码
- 一个对齐计算模块来用于计算所有 **region-word pairs** 的对齐得分

![](https://pic.imgdb.cn/item/658bcb3cc458853aefce62d3.jpg)

作者分别对并行设置**PF**、概念增强**CE**、负样本**NS**和标签补充**LC**进行消融：

![](https://pic.imgdb.cn/item/658bcb9ac458853aefd054d7.jpg)


