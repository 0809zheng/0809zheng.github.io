---
layout: post
title: 'Do We Really Need Explicit Position Encodings for Vision Transformers?'
date: 2023-01-11
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63fda35df144a01007ac3b6c.jpg'
tags: 论文阅读
---

> 视觉Transformer真的需要显式位置编码吗？

- paper：[Do We Really Need Explicit Position Encodings for Vision Transformers?](https://arxiv.org/abs/2102.10882)

**self-attention**结构的特点是可以建模一整个输入序列的信息，并能根据图片的内容来动态调整感受野，但是**self-attention**具有**排列不变性 (permutation-invariant)**，即无法建模输入序列的顺序信息，输入这个序列顺序的调整是不会影响输出结果的。**Transformer**引入了位置编码机制。位置编码在图像识别任务中的作用是保持像素间的空间位置关系，建模像素点前后左右的位置信息。位置编码可以设置为可学习的，也可以设置为不可学习的正弦函数。

位置编码的缺点是长度往往是固定的。比如输入图片的大小是**224×224**的，分成大小为**16×16**的**patch**，那么序列长度是**196**。所以训练时把位置编码的长度也设置为**196**。但是后续进行迁移学习时输入图片是**384×384**的，分成大小为**16×16**的**patch**，那么序列长度是**576**。此时长度**196**的位置编码就不够了。如果直接去掉位置编码会严重地影响分类性能，因为输入序列的位置信息丢失了。

本文提出了一种新的位置编码策略**CPVT**，既能解决传统位置编码不可变长度的局限性，又能起到位置编码的作用。**CPVT**能自动生成一种包含位置信息的编码**PEG**，编码过程是即时的 (**on-the-fly**)，能够灵活地把位置信息引入**Transformer**中。

![](https://pic.imgdb.cn/item/63fda563f144a01007aee896.jpg)

**CPVT**生成编码是通过**Positional Encoding Generator (PEG)**实现的。首先把输入$$X \in \mathbb{R}^{B \times N \times C}$$ **reshape**回**3D**的张量$$X^{\prime} \in \mathbb{R}^{B \times C \times H \times W}$$，然后通过深度卷积处理该张量，再把输出变为$$X^{\prime \prime} \in \mathbb{R}^{B \times N \times C}$$，与输入$X$连接后作为输出。整个过程中**class token** $$Y \in \mathbb{R}^{B \times C}$$保持不变。

![](https://pic.imgdb.cn/item/63fda737f144a01007b1d329.jpg)

卷积操作的卷积核大小$k \geq 3$，零填充$p=\frac{k-1}{2}$。**PEG**的卷积部分以**zero-padding**作为参考点，以卷积操作提取相对位置信息，借助卷积得到适用于**Transformer**的可变长度的位置编码。可视化使用**CPVT**以后**的attention maps**，最左上方的格子关注的点在左上，而最左下方的格子关注的点在左下，以此类推，所以**CPVT**依然能够学习到**local information**。

![](https://pic.imgdb.cn/item/63fda84bf144a01007b354f4.jpg)

```python
class PEG(nn.Module):
    def __init__(self, dim=256, k=3):
    self.proj = nn.Conv2d(dim, dim, k, 1, k//2, groups=dim)

    def forward(self, x, H, W):
    B, N, C = x.shape
    cls_token, feat_token = x[:, 0], x[:, 1:]
    cnn_feat = feat_token.transpose(1, 2).view(B, C, H, W)
    x = self.proj(cnn_feat) + cnn_feat
    x = x.flatten(2).transpose(1, 2)
    x = torch.cat((cls_token.unsqueeze(1), x), dim=1)
    return x
```