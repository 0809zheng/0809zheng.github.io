---
layout: post
title: 'P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks'
date: 2023-02-07
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/648d92521ddac507cc0992a6.jpg'
tags: 论文阅读
---

> P-Tuning v2：提示微调可与跨规模和任务的通用微调相媲美.

- paper：[P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/abs/2110.07602)

[<font color=blue>P-Tuning</font>](https://0809zheng.github.io/2023/02/06/ptuning.html)等微调方法存在两个主要的问题：
- 第一，缺乏模型参数规模和任务通用性。
1. 缺乏规模通用性：之前的方法当模型规模超过**100**亿参数时，可以与全量微调相媲美。但是对于那些较小的模型（从**100M**到**1B**），这些方法和全量微调的表现有很大差异，这大大限制了微调方法的适用性。
2. 缺乏任务普遍性：尽管**P-tuning**在一些 **NLU** 基准测试中表现出优势，但对硬序列标记任务（即序列标注）的有效性尚未得到验证。
- 第二，缺少深度提示优化，在**P-tuning**中，连续提示只被插入**transformer**第一层的输入**embedding**序列中，在接下来的**transformer**层中，插入连续提示的位置的**embedding**是由之前的**transformer**层计算出来的，这可能导致两个可能的优化挑战。
1. 由于序列长度的限制，可调参数的数量是有限的。
2. 输入**embedding**对模型预测只有相对间接的影响。

考虑到这些问题，作者提出了**P-Tuning v2**，它利用深度提示优化对**P-Tuning**进行改进，作为一个跨规模和**NLU**任务的通用解决方案。

**P-Tuning v2**在**Transformer**网络的每一层都加入了可学习的**Prompts tokens**作为输入，这带来两个方面的好处：
- 更多可学习的参数（从**P-tuning**的$0.01\%$增加到$0.1\%-3\%$），同时也足够参数高效。
- 加入到更深层结构中的**Prompt**能给模型预测带来更直接的影响。

![](https://pic.imgdb.cn/item/648d92c61ddac507cc0a44b0.jpg)

**P-Tuning v2**的一些改进：
- **移除重参数化的编码器**。以前的方法利用重参数化功能来提高训练速度和鲁棒性（如**P-Tuning**中的**LSTM**）。在**P-tuning v2**中，作者发现重参数化的改进很小，尤其是对于较小的模型，同时还会影响模型的表现。
- **针对不同任务采用不同的提示长度**。提示长度在提示优化方法的超参数搜索中起着核心作用。实验发现不同的理解任务可能有不同的最佳提示长度。
- **引入多任务学习**。先在多任务的**Prompt**上进行预训练，然后再适配下游任务。多任务学习可能是相当有帮助的：一方面，连续提示的随机惯性给优化带来了困难，这可以通过更多的训练数据或与任务相关的无监督预训练来缓解；另一方面，连续提示是跨任务和数据集的特定任务知识的完美载体。实验表明在一些困难的序列任务中，多任务学习可以作为**P-tuning v2**的有益补充。
- **回归传统的分类标签范式**。标签词映射器（**Label Word Verbalizer**）一直是提示优化的核心组成部分，它将**one-hot**类标签变成有意义的词，以利用预训练语言模型。尽管它在**few-shot**设置中具有潜在的必要性，但在全数据监督设置中，映射器并不是必须的。**P-Tuning v2**回归传统的**CLS**标签分类范式，采用随机初始化的分类头（**Classification Head**）应用于**tokens**之上，以增强通用性，可以适配到序列标注任务。

论文展示了**P-tuning v2**在不同模型规模下的表现。对于简单的**NLU**任务，如**SST-2**（单句分类），**P-Tuning**在较小的规模下没有显示出明显的劣势。但是当涉及到复杂的挑战时，如自然语言推理（**RTE**）和多选题回答（**BoolQ**），**P-Tuning**的性能会非常差。相反，**P-Tuning v2**在较小规模的所有任务中都与微调的性能相匹配。并且**P-tuning v2**在**RTE**中的表现明显优于微调，特别是在**BERT**模型中。

![](https://pic.imgdb.cn/item/648d95f41ddac507cc0f1370.jpg)

**P-Tuning v2**在**GLUE**和**SuperGLUE**等相对简单的**NLU**问题上可以与微调相媲美。为了评估**P-Tuning v2**在一些困难的**NLU**挑战中的能力，作者选择了三个典型的序列标注任务（名称实体识别（**NER**）、抽取式问答（**QA**）和语义角色标签（**SRL**）），共八个数据集。观察到**P-Tuning v2**在所有任务上都能与全量微调相媲美。

![](https://pic.imgdb.cn/item/648d96571ddac507cc0fa209.jpg)

论文还通过消融实验研究了不同任务上**Prompt Length**的影响：
- 针对简单任务：如情感分析，较短的**Prompt（~20）**即可取得不错的效果。
- 针对复杂任务：如阅读理解，需要更长的**Prompt（~100）**。

![](https://pic.imgdb.cn/item/648d96881ddac507cc0fdf0b.jpg)

**P-Tuning v2**是一种在不同规模和任务中都可与微调相媲美的提示方法。**P-Tuning v2**对从**330M**到**10B**的模型显示出一致的改进，并在序列标注等困难的序列任务上以很大的幅度超过了**P-Tuning**。
