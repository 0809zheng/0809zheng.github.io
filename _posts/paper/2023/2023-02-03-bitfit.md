---
layout: post
title: 'BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models'
date: 2023-02-03
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/648d74131ddac507cccc8353.jpg'
tags: 论文阅读
---

> BitFit：基于Transformer的掩码语言模型的简单参数高效微调.

- paper：[BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199)

对于预训练语言模型，虽然对每个任务进行全量微调（微调所有参数）非常有效，但它也会为每个预训练任务生成一个独特的大型模型，特别是随着任务数量的增加，部署和维护的成本也会增大。

理想状况的高效微调方法应满足：
- 到达能够匹配全量微调的效果。
- 仅更改一小部分模型参数。
- 使数据可以通过流的方式到达，而不是同时到达，便于高效的硬件部署。
- 改变的参数集合在不同下游任务中是一致的。

本文作者提出了一种参数量更小的稀疏的微调方法**BitFit (Bias-terms Fine-tuning)**，它训练时只更新**bias**参数与特定任务的输出层参数。

对于**Transformer**模型而言，涉及到的**bias**参数有**attention**模块中计算**query,key,value**跟合并多个**attention**结果时涉及到的**bias**、**MLP**层中的**bias**、**LayerNorm**层的**bias**参数。

![](https://pic.imgdb.cn/item/648d76471ddac507ccd1f4fd.jpg)

![](https://pic.imgdb.cn/item/648d75441ddac507cccf5c89.jpg)

在**Bert-Base/Bert-Large**这种模型里，**bias**参数仅占模型全部参数量的$0.08\%～0.09\%$。但是通过在**Bert-Large**模型上基于**GLUE**数据集进行了 **BitFit**、**Adapter**和**Diff-Pruning**的效果对比发现，**BitFit**在参数量远小于**Adapter**、**Diff-Pruning**的情况下，效果与**Adapter**、**Diff-Pruning**相当，甚至在某些任务更优。

![](https://pic.imgdb.cn/item/648d75e31ddac507ccd0df86.jpg)

同时，通过实验结果还可以看出，**BitFit**微调结果相对全量参数微调而言, 只更新极少量参数的情况下，在多个数据集上都达到了不错的效果。虽不及全量参数微调，但是远超固定全部模型参数的**Frozen**方式。

同时，通过对比**BitFit**训练前后的参数，发现很多**bias**参数并没有太多变化（例如计算**key**所涉及到的**bias**参数）。发现计算**query**和将特征维度从**N**放大到**4N**的**FFN**层（**intermediate**）的**bias**参数变化最为明显，只更新这两类**bias**参数也能达到不错的效果，反之，固定其中任何一者，模型的效果都有较大损失。

![](https://pic.imgdb.cn/item/648d76961ddac507ccd2eb76.jpg)
