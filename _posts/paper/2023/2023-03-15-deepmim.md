---
layout: post
title: 'DeepMIM: Deep Supervision for Masked Image Modeling'
date: 2023-03-15
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/655aab67c458853aef1d1c18.jpg'
tags: 论文阅读
---

> DeepMIM：掩码图像建模中的深度监督.

- paper：[DeepMIM: Deep Supervision for Masked Image Modeling](https://arxiv.org/abs/2303.08817)

**DeepMIM**在 **Masked Image Modeling** 训练过程中加上 **Deep Supervision**，可以促进浅层学习更有意义的表示，加快模型收敛速度并扩大注意力的多样性。

![](https://pic.imgdb.cn/item/655aaba0c458853aef1d8ea4.jpg)

**DeepMIM** 采用编码器-多解码器架构进行 **ViT** 预训练的掩模和预测任务。具有**12**个**Transformer** 块的 **ViT-B** 作为编码器，**4**个独立的具有 **4** 层 **Transformer** 块的解码器分别置于编码器的第 **6**、**8**、**10**、**12** 个 **Transformer** 块后。

在 **Masked Image Modeling** 上构建这种架构的难度主要在于，如何从原始输入中提取监督信号来指导中间层的学习。浅层 **ViT** 产生的特征区别较差，这些中间特征可能没有能力重构过于复杂的目标。

对此，**DeepMIM** 提出可选择的 **Hybrid Target Generator** 模块，将预训练好的 **MAE** 产生的模糊重建结果与原始像素按比例混合后作为中间层的监督信号。

$$
t = \alpha x + (1-\alpha) \hat{x}
$$

实验中建议设置 $α_1= 0,\alpha_2=1/3,\alpha_3= 2/3$。尽管使用 **Hybrid Target** 作为中间层监督信号可以提高微调性能，但存在额外的计算开销。所以作者建议，只有在有一个现成预训练过的 **MIM** 模型时才使用 **Hybrid Target**。 

总损失是由 $N$ 个额外解码器和 $1$ 个主解码器产生的 $N+1$ 个**L2**重建损失之和：

$$
L = \sum_{i=1}^N ||M(t_i)-p_i||^2+||M(x)-p||^2
$$

**DeepMIM** 使用训练集和验证集损失曲线，证明在下游任务的微调期间的表现要优于 **MAE**。

![](https://pic.imgdb.cn/item/655ab2bcc458853aef2c2af5.jpg)

**DeepMIM** 使用 **centered kernel alignment（CKA）**来识别最后一层产生的特征和中间层产生的特征之间的对应关系。从第一层到倒数第二层，**DeepMIM-MAE** 的 **CKA** 得分总是超过 **MAE**，说明**DeepMIM-MAE** 的中间层的特征更具鉴别性。

![](https://pic.imgdb.cn/item/655ab357c458853aef2d56d9.jpg)

**DeepMIM** 使用 **CKA** 来计算来自 **MAE**（或**DeepMIM-MAE**） 第 **8** 层的特征与来自 **DeepMIM-MAE**（或**MAE**） 所有层的特征之间的相似性，**MAE** 的中间层（第**8**层）特征与 **DeepMIM-MAE** 的较浅的层特征（第**3**层和第**4**层）具有最大的对齐。相比之下，**DeepMIM-MAE** 的中间（第**8**层）特征与 **MAE** 的更深块（第**9**层和第**10**层）的特征更加紧密。本研究表明，**DeepMIM**显著增强了特征对浅层的鉴别能力。

![](https://pic.imgdb.cn/item/655ab3ebc458853aef2e8a52.jpg)

**DeepMIM** 计算不同 **attention heads** 之间的余弦相似性来探索 **head** 的多样性。更多样化的 **head** 表明有更强的代表能力。研究表明，与**MAE**相比，**DeepMIM** 产生的 **head** 更多样化。

![](https://pic.imgdb.cn/item/655ab42dc458853aef2f127a.jpg)

为了进一步评估来自浅块的特征的质量，**DeepMIM** 冻结了浅层的一个子集，并对其余的层进行微调。当可训练块的数量从 **1** 个（只有最后一个层是可训练的）变化到**12**个（所有层都是可训练的）时，**DeepMIM-MAE** 的性能始终显著优于 **MAE**。

![](https://pic.imgdb.cn/item/655ab474c458853aef2fafad.jpg)

**DeepMIM** 随机初始化预训练的 **ViT-B** 的最后 **K** 个层，然后以端到端的方式在 **ImageNet** 上对 **ViT-B** 进行微调。**DeepMIM-MAE** 在每种情况下都始终优于 **MAE**，这表明浅层的良好表示有利于更深层的学习，尤其是当它们被随机初始化时。

![](https://pic.imgdb.cn/item/655ab4b7c458853aef3048f8.jpg)