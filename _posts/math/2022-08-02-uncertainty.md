---
layout: post
title: '深度学习中的不确定性(Uncertainty)'
date: 2022-08-02
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/631563d816f2c2beb10dbce6.jpg'
tags: 数学
---

> 使用贝叶斯深度学习建模深度学习中的不确定性.

- paper：[What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?](https://arxiv.org/abs/1703.04977)

现有的深度学习方法大多只能给出特定的预测结果，而不能给出结果的不确定性程度。
深度学习中输出结果的不确定性主要有两种：**偶然不确定性**是由数据中的固有噪声导致的，**认知不确定性**是由模型对数据缺乏足够的认知导致的。贝叶斯深度学习框架可以用于建模这两种不确定性。

在贝叶斯建模中，有两种主要类型的不确定性可以建模。
- **偶然不确定性(aleatoric uncertainty)**是由观测数据中的固有噪声导致的，比如传感器噪声或运动噪声，即使收集更多数据，也无法减少这种不确定性。
- **认知不确定性(epistemic uncertainty)**也称为**模型不确定性(model uncertainty)**，即模型参数的不确定性，表示由于缺乏足够的训练数据导致模型认知不足。这种不确定性可以通过增加训练数据来降低。

偶然不确定性可进一步分为**同方差不确定性**和**异方差不确定性**。
- **同方差不确定性(homoscedastic uncertainty)**又叫**任务依赖(task-dependent)**的不确定性，是指不依赖于输入数据而是依赖于任务的不确定性，该不确定性对所有输入数据保持不变，但在不同任务之间变化。
- **异方差不确定性(heteroscedastic uncertainty)**又叫**数据依赖(data-dependent)**的不确定性，是指依赖于输入数据的不确定性，表现在模型的输出上，比如一些样本可能比其他样本具有更多的噪声输出。

下图给出了在**CamVid**数据集上进行语义分割任务时模型输出的偶然不确定性和认知不确定性之间的差异。偶然不确定性捕获了观测中固有的噪声，比如在物体的边界或远离相机的物体上的偶然不确定性会增加。认知不确定性是由对数据认知的缺乏导致的，比如在语义和视觉上更具挑战性的像素的认知不确定性较大。

![](https://pic.imgdb.cn/item/6315a90c16f2c2beb1545080.jpg)

## (1) 建模认知不确定性

建模认知不确定性的通常方法是为模型权重设置先验分布，在给定数据集上估计模型权重的变化情况。

### ⚪ 估计方法：贝叶斯神经网络与MC dropout

若模型权重不是确定性的数值，而是概率分布的形式，则这种模型称为**贝叶斯神经网络(bayesian neural network)**。

贝叶斯神经网络并不直接优化模型权重，而是对所有可能的权重进行平均(称为**边缘化 marginalisation**)。比如为权重指定高斯先验分布$W$~$$\mathcal{N}(0,I)$$，贝叶斯网络的随机输出值为$f_W(x)$，则模型似然为$p(y\|f_W(x))$。

给定数据集$$X=\{x_1,...,x_N\},Y=\{y_1,...,y_N\}$$，贝叶斯推理用于计算权重的后验分布$p(W\|X,Y)$，当给定数据时，该后验函数可以捕获一组合理的模型权重。然而贝叶斯推理在实践中难以执行，因为后验分布$p(W\|X,Y)$的计算比较困难。

**Monte Carlo dropout**给出了一种对神经网络进行贝叶斯推理的近似方法，其基本思路是对每一个神经元设置**Bernoulli**先验，即参数以概率$p$被设置为$0$，这可以通过**dropout**实现。

**MC dropout**在测试阶段多次应用**dropout**，每次测试都会得到一个权重的近似后验分布，并把所有结果取平均。通过计算这些近似后验结果的方差(回归问题)或熵(分类问题)，可以给出认知不确定性的估计值。


### ⚪ 估计回归问题中的认知不确定性

对于回归任务，模型的认知不确定性可以通过多次**MC dropout**预测结果的方差来近似：

$$ \text{Var}(y) ≈ \frac{1}{T}\sum_{t=1}^{T} f_{\hat{W}_t}(x)^Tf_{\hat{W}_t}(x) - E(y)^TE(y) $$

其中模型的平均预测通过所有预测结果的平均值近似：

$$ E(y) ≈ \frac{1}{T}\sum_{t=1}^{T} f_{\hat{W}_t}(x) $$

### ⚪ 估计分类问题中的认知不确定性

对于分类任务，概率向量$p$可以通过对多次**MC dropout**的结果进行平均得到：

$$ p(y=c|x) ≈ \frac{1}{T} \sum_t p(y=c|x,w_t) = \frac{1}{T}\sum_t p_c^t $$

则结果的认知不确定性可以通过熵来估计：

$$ \begin{aligned} H(p) &= -\sum_c p(y=c|x)\log p(y=c|x) \\ & ≈ -\sum_c (\frac{1}{T}\sum_t p_c^t)\log (\frac{1}{T}\sum_t p_c^t) \end{aligned} $$


## (2) 建模异方差偶然不确定性

偶然不确定性是通过为模型输出设置先验分布来估计的。同方差不确定性假设每个输入$x$的观测噪声$σ$恒定；异方差不确定性假设观测噪声$\sigma(x)$随输入$x$变化。

对于回归任务，模型似然定义为具有观测噪声$\sigma$的高斯分布$p(y\|f^W(x))=$ $$\mathcal{N}(f^W(x),\sigma^2)$$。则负对数似然简化为：

$$ -\log p(y_i|f_{\hat{W}_i}(x_i)) ∝ \frac{1}{2\sigma^2} ||y_i-f_{\hat{W}_i}(x_i)||^2 + \frac{1}{2} \log \sigma^2 $$

其中$\sigma$是模型的观测噪声参数，捕捉输出中存在多少噪声。该观测噪声可以从数据中学习得到：

$$ \mathcal{L}_{NN}(\theta) = \frac{1}{N}\sum_{i=1}^N \frac{1}{2\sigma(x_i)^2} ||y_i-f(x_i)||^2 + \frac{1}{2} \log \sigma(x_i)^2 $$

## (3) 结合偶然和认知不确定性

仍以回归问题为例，为了同时建模认知不确定性和异方差偶然不确定性，采用贝叶斯神经网络将输入数据$x$同时映射到输出$\hat{y}$和偶然不确定性$\sigma^2$。采用**MC dropout**构造模型参数的后验采样$\hat{W}$，则预测过程为：

$$ [\hat{y},\hat{\sigma}^2] = f_{\hat{W}}(x) $$

目标函数设置为负对数似然：

$$ \mathcal{L}_{BNN}(\theta) = \frac{1}{D} \sum_i \frac{1}{2}\hat{\sigma}_i^{-2} ||y_i-\hat{y}_i||^2 + \frac{1}{2} \log \hat{\sigma}_i^2 $$

在实践中，为了进行无范围限制的稳定回归，不直接预测方差$\hat{\sigma}_i^2$，而是预测对数方差$s_i = \log \hat{\sigma}_i^2$:

$$ \mathcal{L}_{BNN}(\theta) = \frac{1}{D} \sum_i \frac{1}{2}\exp(-s_i) ||y_i-\hat{y}_i||^2 + \frac{1}{2} s_i $$

则输出$y$的预测不确定性近似为：

$$ \text{Var}(y) ≈ \frac{1}{T}\sum_{t=1}^{T}\hat{y}_t^2 - (\frac{1}{T}\sum_{t=1}^{T}\hat{y}_t)^2 + \frac{1}{T}\sum_{t=1}^{T}\hat{\sigma}_t^2  $$

## (4) 应用场合

根据上述分析，偶然不确定性不能通过使用更多数据而降低；对于域外的新数据样本，引入的偶然不确定性不变，但认知不确定性增大。

需要考虑偶然不确定性的场合：
- 较多的训练样本：此时认知不确定性能被很好的解释
- 实时应用场合：认知不确定性需要进行蒙特卡洛采样，而偶然不确定性计算成本较低

需要考虑认知不确定性的场合：
- 安全性考虑的场合：认知不确定性需要理解与训练数据不同的样本
- 小数据集：训练数据是稀疏的

