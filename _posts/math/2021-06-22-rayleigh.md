---
layout: post
title: '瑞利商(Rayleigh Quotient)和广义(Generalized)瑞利商'
date: 2021-06-22
author: 郑之杰
cover: ''
tags: 数学
---

> Rayleigh Quotient and Generalized Rayleigh Quotient.

1. 瑞利商的定义
2. 瑞利商的性质
3. 广义瑞利商
4. 瑞利商在机器学习中的应用

# 1. 瑞利商的定义
对于一个**Hermitan**矩阵$A$(复域的共轭对称矩阵,满足$A^H=A$)及非零向量$\textbf{x}=(x_1,...,x_n)^T$，**瑞利商(Rayleigh Quotient)**定义为：

$$ R(A,x) = \frac{x^HAx}{x^Hx} $$

其中$x^H$是$x$的共轭转置向量。在实数域中瑞利商可以表示为：

$$ R(A,x) = \frac{x^TAx}{x^Tx} $$

# 2. 瑞利商的性质
设矩阵$A$的特征值与特征向量分别为$\lambda_1,...,\lambda_n,v_1,...,v_n$，且满足：

$$ \lambda_{min}=\lambda_1≤\lambda_2≤...≤\lambda_n=\lambda_{max} $$

则瑞利商的取值范围是：

$$ \lambda_{min}≤\frac{x^HAx}{x^Hx}≤\lambda_{max} $$

### ⚪ 证明方法 1
由于$A$是**Hermitan**矩阵，则存在酉矩阵$U$(复正交矩阵)对$A$进行相似对角化：

$$ A = U \Lambda U^H $$

其中对角矩阵$\Lambda=diag(\lambda_1,\lambda_2,...,\lambda_n)$。将上式代入瑞利商：

$$ R(A,x) = \frac{x^HAx}{x^Hx} = \frac{x^HU \Lambda U^Hx}{x^Hx} = \frac{(U^Hx)^H \Lambda (U^Hx)}{x^Hx} $$

记向量$p=U^Hx=(p_1,...,p_n)^T$，则：

$$ R(A,x) = \frac{p^H \Lambda p}{x^Hx} = \frac{\sum_{i=1}^{n} \lambda_i |p_i|^2}{\sum_{i=1}^{n}|x_i|^2} $$

根据特征值的大小关系，可得不等式：

$$ \lambda_1 \sum_{i=1}^{n} |p_i|^2≤\sum_{i=1}^{n} \lambda_i |p_i|^2≤\lambda_n \sum_{i=1}^{n} |p_i|^2 $$

因此可得：

$$ \frac{\lambda_1 \sum_{i=1}^{n} |p_i|^2}{\sum_{i=1}^{n}|x_i|^2}≤R(A,x)≤\frac{\lambda_n \sum_{i=1}^{n} |p_i|^2}{\sum_{i=1}^{n}|x_i|^2} $$

记$u_{ij}$为酉矩阵$U$的第$i$行第$j$列元素，则：

$$ p_i=\sum_{j=1}^{n}{u^H}_{ij}x_j=\sum_{j=1}^{n}u_{ji}x_j, \quad p_i^T=\sum_{j=1}^{n}x_ju_{ij} $$

$$ |p_i|^2 = p_i^Tp_i = \sum_{j=1}^{n}x_ju_{ij}\sum_{k=1}^{n}u_{ki}x_k = \sum_{j=1}^{n}\sum_{k=1}^{n}x_ju_{ij}u_{ki}x_k $$

进一步有：

$$ \sum_{i=1}^{n} |p_i|^2 = \sum_{i=1}^{n} \sum_{j=1}^{n}\sum_{k=1}^{n}x_ju_{ij}u_{ki}x_k = \sum_{j=1}^{n}\sum_{k=1}^{n}x_jx_k (\sum_{i=1}^{n}u_{ij}u_{ki}) $$

由于$U$是酉矩阵，因此$U^HU=I$，写作$I_{jk}=\sum_{i=1}^{n}u_{ji}u_{ik}$。当$j≠k$时$I_{jk}=0$，当$j=k$时$I_{jk}=1$。因此上式进一步化简为：

$$ \sum_{i=1}^{n} |p_i|^2 = \sum_{j=k}^{}
x_jx_k = \sum_{i=1}^{n} |x_i|^2 $$

代入原不等式，得：

$$ \lambda_1≤R(A,x)≤\lambda_n $$

### ⚪ 证明方法 2
注意到对于任意非零实数$c$，使用$x'=cx$取代$x$，计算瑞利商：

$$ R(A,x') = \frac{x'^TAx'}{x'^Tx'} = \frac{cx^TAcx}{cx^Tcx} = \frac{x^TAx}{x^Tx} = R(A,x) $$

因此对向量$x$**等比例缩放不影响瑞利商的值**。不妨取$x^Tx=1$，此时对瑞利商求极值，就是在约束$x^Tx=1$下，求$R(A,x) = x^TAx$的极值。

采用拉格朗日乘子法，定义拉格朗日函数：

$$ L(x,\alpha)= x^TAx-\alpha(x^Tx-1)$$

上式对$x$求梯度：

$$ \frac{\partial L(x,\alpha)}{\partial x} = 2Ax-2\alpha x $$

令梯度为$0$，对应极小值或极大值的情况，此时有：

$$ Ax=\alpha x $$

即$A$的特征值$\alpha$使得瑞利商取得极值，且极值为$R(A,x) = x^TAx =\alpha x^Tx = \alpha$。

# 3. 广义瑞利商
对于一个**Hermitan**矩阵$A$,$B$及非零向量$x=(x_1,...,x_n)^T$，其中$B$是正定矩阵，则其**广义瑞利商(Generalized Rayleigh Quotient)**定义为：

$$ R(A,B,x) = \frac{x^HAx}{x^HBx} $$

### ⚪ 化简方法 1
记$x = B^{-\frac{1}{2}}x'$，则上式表示为：

$$ R(A,B,x) = \frac{x^HAx}{x^HBx} = \frac{(B^{-\frac{1}{2}}x')^HAB^{-\frac{1}{2}}x'}{(B^{-\frac{1}{2}}x')^HBB^{-\frac{1}{2}}x'} \\ = \frac{ {x'}^H {B^{-\frac{1}{2}}} AB^{-\frac{1}{2}}x' }{ {x'}^H {B^{-\frac{1}{2}}} BB^{-\frac{1}{2}}x' } = \frac{ {x'}^H B^{-\frac{1}{2}} AB^{-\frac{1}{2}}x' }{ {x'}^Hx' } = R(B^{-\frac{1}{2}} AB^{-\frac{1}{2}},x') $$

因此$R(A,B,x)$的最大值和最小值分别为矩阵$B^{-\frac{1}{2}} AB^{-\frac{1}{2}}$的最大特征值和最小特征值。由于矩阵$B^{-\frac{1}{2}} AB^{-\frac{1}{2}}$与矩阵$B^{-1} A$具有相同的特征值，因此$R(A,B,x)$的最大值和最小值分别为矩阵$B^{-1} A$的最大特征值和最小特征值。

### ⚪ 化简方法 2
由于对向量$x$**等比例缩放不影响广义瑞利商的值**，不妨取$x^TBx=1$，此时对广义瑞利商求极值，就是在约束$x^TBx=1$下，求$R(A,B,x) = x^TAx$的极值。

采用拉格朗日乘子法，定义拉格朗日函数：

$$ L(x,\alpha)= x^TAx-\alpha(x^TBx-1)$$

上式对$x$求梯度：

$$ \frac{\partial L(x,\alpha)}{\partial x} = 2Ax-2\alpha Bx $$

令梯度为$0$，对应极小值或极大值的情况，此时有：

$$ B^{-1}Ax=\alpha x $$

即$B^{-1}A$的特征值$\alpha$使得广义瑞利商取得极值，且极值为$R(A,B,x) = x^TB^{-1}Ax =\alpha x^Tx = \alpha$。

# 4. 瑞利商在机器学习中的应用

### ⚪ 计算谱范数

- 应用：[谱正则化](https://0809zheng.github.io/2020/09/19/snr.html)、[谱归一化](https://0809zheng.github.io/2022/02/08/sngan.html)

参数矩阵的**谱范数(spectral norm)**定义为：

$$ ||W||_2 = \mathop{\max}_{x \neq 0} \frac{||Wx||}{||x||} $$

谱范数是一种由向量范数诱导出来的矩阵范数，作用相当于向量的模长。注意到谱范数$\|\|W\|\|_2$的平方为：

$$ ||W||_2^2 = \mathop{\max}_{x \neq 0} \frac{x^TW^TWx}{x^Tx} $$

上式右端为瑞利商，因此谱范数$\|\|W\|\|_2$的平方的取值为$W^TW$的最大特征值。

**谱正则化(Spectral Norm Regularization)**是指把谱范数的平方作为正则项，从而增强网络的泛化性：

$$ \mathcal{L}(x,y;W) + \lambda ||W||_2^2 $$

**谱归一化(Spectral Normalization)**是指使用谱范数对网络参数进行归一化，从而精确地使网络满足**Lipschiitz**连续性：

$$ W \leftarrow \frac{W}{||W||_2^2} $$

