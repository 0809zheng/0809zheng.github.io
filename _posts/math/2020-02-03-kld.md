---
layout: post
title: '概率分布之间的散度(Divergence)'
date: 2020-02-03
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/62ac690b0947543129746277.jpg'
tags: 数学
---

> Divergence between distributions.

本文介绍几种概率分布之间的**散度(divergence)**，它们是用来衡量同一个**随机变量(random variable)**的两个**概率分布(distribution)**的“距离”的分布距离度量指标。需要注意的是这些散度并不是标准的距离指标，因为它们不满足可交换性。

- KL散度 (Kullback-Leibler Divergence)
1. KL散度的定义和性质
2. 前向KL散度与反向KL散度
- JS散度 (Jenson-Shannon Divergence)
1. JS散度的定义和性质
2. JS散度的缺点
- f散度 (f Divergence)
1. f散度的定义和性质
2. f散度的局部变分估算
- W散度 (Wasserstein Divergence)

# ⚪ KL散度 (Kullback-Leibler Divergence)

## 1. KL散度的定义和性质

记随机变量$z$的两个概率分布$p$和$q$，对于**离散型(discrete)**随机变量，**KL**散度形式如下：

$$ D_{KL}[q||p] = \sum_{z}^{} {q(z) \log \frac{q(z)}{p(z)}} = -\sum_{z}^{} {q(z) \log \frac{p(z)}{q(z)}} $$

对于**连续型(continuous)**随机变量，**KL**散度形式如下：

$$ D_{KL}[q||p] = \int_{}^{} {q(z) \log \frac{q(z)}{p(z)} dz} = -\int_{}^{} {q(z) \log \frac{p(z)}{q(z)} dz} $$

总的来说，上述各种形式可以统一写作**期望(expectation)**的形式：

$$ D_{KL}[q||p] = \mathbb{E}_{q(z)} \log \frac{q(z)}{p(z)} = -\mathbb{E}_{q(z)} \log \frac{p(z)}{q(z)} $$

**KL**散度的性质：
- 不对称性(**asymmetric**)：$D_{KL}[q\|\|p] ≠ D_{KL}[p\|\|q]$
- 非负性：$D_{KL}[q\|\|p] ≥ 0 \quad \forall q,p$

## 2. 前向KL散度与反向KL散度

实践中，常用一个**近似(approximate)**概率分布$q$去估计一个**理论(theoretic)**但是**不可解(intractable)**的概率分布$p$。通常假设近似分布$q$形式比理论分布$p$简单，比如$q$是**单模态(unimodal)**的，$p$是**双模态(bimodal)**的。

![](https://pic.imgdb.cn/item/62ac6803094754312972d57e.jpg)

**前向KL散度(Forward KL)**形式如下：

$$ D_{KL}[p||q] = \sum_{z}^{} {p(z) \log \frac{p(z)}{q(z)}} $$

从上述公式和图像中可以看出，在$p(z)>0$而$q(z)=0$的位置将有较大的惩罚，由于$\mathop{\lim}_{q(z)→0} \log \frac{p(z)}{q(z)} → ∞$，这促使学习到的分布$q$将会覆盖分布$p$存在的任何区域，造成**zero avoiding**现象。

**反向KL散度(Reverse KL)**形式如下：

$$ D_{KL}[q||p] = \sum_{z}^{} {q(z) \log \frac{q(z)}{p(z)}} $$

从上述公式和图像中可以看出，在$p(z)=0$的位置只有$q(z)=0$才能使其值不趋近于$∞$，这促使学习到的分布$q$在$p(z)=0$的位置也为$0$，造成**zero forcing**现象。

# ⚪ JS散度 (Jenson-Shannon Divergence)

## 1. JS散度的定义和性质

**JS散度(Jenson-Shannon Divergence)**也用来衡量同一个随机变量的两个概率分布$p,q$的相似程度。与**KL**散度不同，**JS**散度是**对称**(**symmetic**)的，并且更加平滑。**JS**散度形式如下：

$$ D_{JS}[p||q] =  \frac{1}{2} D_{KL}[p||\frac{p+q}{2}] + \frac{1}{2} D_{KL}[q||\frac{p+q}{2}]  $$

**JS**散度的取值范围是$[0,1]$。

## 2. JS散度的缺点

假设具有以下概率分布：

$$ \begin{aligned} &\forall (x,y) \in P, x=0,y\text{ ~ } U(0,1) \\ &\forall (x,y) \in Q, x=\theta, 0 \leq \theta \leq 1,y\text{ ~ } U(0,1)  \end{aligned} $$

![](https://pic1.imgdb.cn/item/634226fe16f2c2beb1e49d0d.jpg)

当$\theta \neq 0$时分布$P,Q$没有重叠。此时计算两个分布的**KL**散度和**JS**散度：

$$ \begin{aligned} D_{KL}[P || Q] &= \sum P \log \frac{P}{Q}  = \sum_{x=0,y\text{ ~ } U(0,1)} 1 \cdot \log \frac{1}{0} = + \infty  \\  D_{KL}[Q || P] &= \sum Q \log \frac{Q}{P}  = \sum_{x=\theta,y\text{ ~ } U(0,1)} 1 \cdot \log \frac{1}{0} = + \infty \\ D_{JS}[P || Q] &= \frac{1}{2}D_{KL}[P || \frac{P+Q}{2}] + \frac{1}{2}D_{KL}[Q || \frac{P+Q}{2}] \\ & =  \frac{1}{2}\sum_{x=0,y\text{ ~ } U(0,1)} 1 \cdot \log \frac{1}{1/2} +  \frac{1}{2} \sum_{x=\theta,y\text{ ~ } U(0,1)} 1 \cdot \log \frac{1}{1/2} \\ & =  \log 2 \end{aligned} $$

当$\theta = 0$时分布$P,Q$完全重叠，此时有：

$$ D_{KL}[P || Q] = D_{KL}[Q || P] = D_{JS}[P || Q] = 0  $$

根据上述结论，当$P,Q$没有重叠时**KL**散度变为无穷大，而**JS**散度始终为常数；当参数$\theta$变化时**JS**散度的取值发生跳变。因此**JS**散度作为分布差异衡量指标时不能始终保持平滑。


# ⚪ f散度 (f Divergence)

- paper；[<font color=Blue>f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization</font>](https://0809zheng.github.io/2022/02/07/fgan.html)

## 1. f散度的定义和性质

一般地，$p(x)$和$q(x)$之间的**f散度**定义为：

$$ D_f[p || q] = \int_{x}^{} {q(x)f(\frac{p(x)}{q(x)})dx} $$

其中函数$f(\cdot)$的性质：
- $f(\cdot)$是非负实数到实数的映射$$\Bbb{R}^{*} \to \Bbb{R}$$；
- $f(1)=0$；对应$p(x)=q(x)$时散度为$0$;
- $f(\cdot)$是凸函数：该性质使**f**散度恒大于等于零(根据[Jenson不等式](https://0809zheng.github.io/2022/07/20/jenson.html))：

$$ D_f[P || Q] = \int_{x}^{} {q(x)f(\frac{p(x)}{q(x)})dx} ≥ f(\int_{x}^{} {q(x)\frac{p(x)}{q(x)}dx}) = f(1) = 0 $$

当函数$f(\cdot)$选择不同时，**f**散度对应到不同的散度：
- **KL散度**：$f(x) = x \log x$

$$ D_f[P || Q] = \int_{x}^{} {q(x) \frac{p(x)}{q(x)} \log(\frac{p(x)}{q(x)})dx} = \int_{x}^{} {p(x) \log(\frac{p(x)}{q(x)})dx} $$

- **Reverse KL散度**：$f(x) = -\log x$

$$ D_f[P || Q] = \int_{x}^{} {q(x) (-\log(\frac{p(x)}{q(x)}))dx} = \int_{x}^{} {q(x) \log(\frac{q(x)}{p(x)})dx} $$

- $\chi^2$**散度**：$f(x) = (x-1)^2$

$$ D_f[P || Q] = \int_{x}^{} {q(x) (\frac{p(x)}{q(x)}-1)^2dx} = \int_{x}^{} {\frac{(p(x)-q(x))^2}{q(x)}dx} $$


下表给出了不同的散度对应的凸函数$f(x)$：

![](https://pic1.imgdb.cn/item/63451cc916f2c2beb1167c6e.jpg)

## 2. f散度的局部变分估算

注意到只有$p(x)$和$q(x)$的解析形式均已知时**f**散度才能求解。在实际应用中，有些概率分布形式未知，只能通过采样获得有限的样本；此时可以通过共轭函数估算**f**散度。

### (1) 凸函数的共轭函数

对于凸函数$f(x)$，选择任意一个点$\xi$，计算$y=f(x)$在$x=\xi$处的切线：

$$ y = f(\xi) + f'(\xi)(x-\xi) $$

凸函数的函数总在其切线上方，因此有：

$$ \begin{aligned} f(x) & \geq f(\xi) + f'(\xi)(x-\xi) \\ & = f(\xi) - f'(\xi)\xi  + f'(\xi)x \end{aligned} $$

对于定义域内的所有点$$\xi \in \Bbb{D}$$，$f(x)$可以用其所有切线簇表示：

$$ f(x) = \mathop{\max}_{\xi \in \Bbb{D}} \{ f(\xi) - f'(\xi)\xi  + f'(\xi)x \} $$

记$t=f'(\xi)$，$f^{\*}(t) = -f(\xi) + f'(\xi)\xi$，则有：

$$ f(x) = \mathop{\max}_{t \in f'(\Bbb{D})} \{ xt - f^{*}(t) \} $$

上式的几何意义为，对于凸函数$f(x)$，任意一点$x$处的取值为其所有切线簇在该点取值的最大值。

![](https://pic.downk.cc/item/5ebcfd25c2a9a83be542cee6.jpg)

式中$f^{\*}(t)$称为凸函数$f(x)$的**共轭函数（conjugate function）** ，具有如下性质：
- $f^*$也是凸函数；
- $(f^{\*})^{\*}=f$，因此有：

$$ f^{*}(t) = \mathop{\max}_{x \in \Bbb{D}} \{ xt - f(x) \} $$

直观上，通过共轭函数$f^*(t)$给出了凸函数$f(x)$的线性近似。对于任意$x$，通过共轭函数给出$f(x)$的一个下界$xt-f^{\*}(t)$，并且该下界关于$x$是线性的；通过在每一个$x$处最大化$t$才能取得$f(x)$的结果，因此该方法称为**局部变分**方法，$t$可以看作$x$的函数$t(x)$：

$$ f(x) = \mathop{\max}_{t \in f'(\Bbb{D})} \{ xt(x) - f^{*}(t(x)) \} $$

### (2) f散度的估算公式

根据共轭函数给出的凸函数$f(x)$表达式：

$$ f(x) = \mathop{\max}_{t \in f'(\Bbb{D})} \{ xt(x) - f^{*}(t(x)) \} $$

**f**散度可以表示为：

$$ \begin{aligned} D_f[p || q]& = \int_{x}^{} {q(x)f(\frac{p(x)}{q(x)})dx} \\ &= \int_{x}^{} {q(x)(\mathop{\max}_{t \in f'(\Bbb{D})} \{ \frac{p(x)}{q(x)} t(x) - f^*(t(x)) \})dx} \\ &= \mathop{\max}_{t \in f'(\Bbb{D})}\int_{x}^{} {q(x)( \frac{p(x)}{q(x)} t(x) - f^*(t(x)) )dx} \\ &= \mathop{\max}_{t \in f'(\Bbb{D})} \int_{x}^{} p(x)t(x)- {q(x)f^*(t(x))dx} \\&= \mathop{\max}_{t \in f'(\Bbb{D})} \{ \Bbb{E}_{x\text{~}p(x)}[t(x)]- \Bbb{E}_{x\text{~}q(x)}[f^*(t(x))] \} \end{aligned} $$

其中$t(x)$可以用神经网络拟合。注意到选定凸函数$f(x)$后，$t(x)$的值域是有限制的。在实践时可以通过施加激活函数约束$t(x)$的输出范围。激活函数的选择应满足以下几点：
1. 激活函数的定义域为$$\Bbb{R}$$，值域为$f'(x)$的值域；
2. 最好选择全局光滑的函数；如值域要求为$$\Bbb{R}^+$$，则优先考虑$e^x$而不是$$\text{ReLU}(x)$$；
3. 选择激活函数时，应使其与$f^*(\cdot)$的复合运算比较简单。

下面列出一些凸函数对应的共轭函数及其激活函数选择：

![](https://pic1.imgdb.cn/item/6345194e16f2c2beb11096c4.jpg)

# ⚪ W散度 (Wasserstein Divergence)

- paper；[<font color=Blue>f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization</font>](https://0809zheng.github.io/2022/02/07/fgan.html)

## 1. W散度的定义和性质

$p(x)$和$q(x)$之间的**Wasserstein**散度定义为：

$$ D_{W_{k,p}}[p || q] = \mathop{ \max}_{f} \int_x p(x)f(x)dx - \int_x q(x)f(x)dx - k\int_x r(x) || \nabla_xf(x) ||^p dx $$

或写作采样形式：

$$ D_{W_{k,p}}[p || q] = \mathop{ \max}_{f} \Bbb{E}_{x \text{~} p(x)}[f(x)] - \Bbb{E}_{x \text{~} q(x)}[f(x)] - k\Bbb{E}_{x \text{~} r(x)}[ || \nabla_xf(x) ||^p ] $$

其中$f(x)$是任意函数，$r(x)$是一个样本空间跟$p(x)$和$q(x)$一样的分布，$k>0, p > 1$。

**Wasserstein**散度具有以下性质：
- **Wasserstein**散度是一个对称的散度，即$D_{W_{k,p}}[p \| q]=D_{W_{k,p}}[q \| p]$。
- **Wasserstein**散度的最优解跟[<font color=blue>Wasserstein距离</font>](https://0809zheng.github.io/2022/05/16/Wasserstein.html)具有类似的性质。