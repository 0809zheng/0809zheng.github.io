---
layout: post
title: '概率分布之间的散度(Divergence)'
date: 2020-02-03
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/62ac690b0947543129746277.jpg'
tags: 数学
---

> Divergence between distributions.

本文介绍几种概率分布之间的**散度(divergence)**，它们是用来衡量同一个随机变量**(random variable)**的两个概率分布**(distribution)**的“距离”的度量指标。

一般地，散度$D[p,q]$是关于概率分布$p(x)$和$q(x)$的标量函数，并且满足：
- 非负性：$D[p,q]\geq 0$恒成立；
- $D[p,q]=0 \leftrightarrow p=q$



需要注意的是这些散度并不是标准的距离指标，因为它们不满足对称性和三角不等式；然而散度可以被用作分布距离度量指标。

常见的散度包括：

- 总变差 (Total Variation)
- KL散度 (Kullback-Leibler Divergence)
1. KL散度的定义和性质
2. 前向KL散度与反向KL散度
- JS散度 (Jenson-Shannon Divergence)
1. JS散度的定义和性质
2. JS散度的缺点
- f散度 (f Divergence)
1. f散度的定义和性质
2. f散度的局部变分估算
- W散度 (Wasserstein Divergence)
1. W散度的定义和性质
2. W距离也是一种散度
- 平方势散度 (Quadratic Potential Divergence)
1. 平方势散度的定义和性质

# ⚪ 总变差 (Total Variation)

总变差既是一种概率散度，又是一种[积分概率度量](https://0809zheng.github.io/2022/12/06/ipm.html)。一般地，$p(x)$和$q(x)$之间的**总变差**定义为：

$$ D_{TV}[p||q] = \int |p(x)-q(x)|dx $$

总变差的对偶形式如下：

$$ \begin{aligned}  D_{TV}[p||q] &= \int |p(x)-q(x)|dx \\ &= \int \mathop{\max}_{f(x) \in [-1,1]} p(x)f(x)-q(x)f(x)dx  \\ &=\mathop{\max}_{f(x) \in [-1,1]} \mathbb{E}_{x \text{~}p(x)}[f(x)] -\mathbb{E}_{x \text{~}q(x)}[f(x)] \end{aligned} $$


# ⚪ KL散度 (Kullback-Leibler Divergence)

## 1. KL散度的定义和性质

记随机变量$z$的两个概率分布$p$和$q$，对于**离散型(discrete)**随机变量，**KL**散度形式如下：

$$ D_{KL}[q||p] = \sum_{z}^{} {q(z) \log \frac{q(z)}{p(z)}} = -\sum_{z}^{} {q(z) \log \frac{p(z)}{q(z)}} $$

对于**连续型(continuous)**随机变量，**KL**散度形式如下：

$$ D_{KL}[q||p] = \int_{}^{} {q(z) \log \frac{q(z)}{p(z)} dz} = -\int_{}^{} {q(z) \log \frac{p(z)}{q(z)} dz} $$

总的来说，上述各种形式可以统一写作**期望(expectation)**的形式：

$$ D_{KL}[q||p] = \mathbb{E}_{q(z)} \log \frac{q(z)}{p(z)} = -\mathbb{E}_{q(z)} \log \frac{p(z)}{q(z)} $$

**KL**散度的性质：
- 不对称性(**asymmetric**)：$D_{KL}[q\|\|p] ≠ D_{KL}[p\|\|q]$
- 非负性：$D_{KL}[q\|\|p] ≥ 0 \quad \forall q,p$

## 2. 前向KL散度与反向KL散度

实践中，常用一个**近似(approximate)**概率分布$q$去估计一个**理论(theoretic)**但是**不可解(intractable)**的概率分布$p$。通常假设近似分布$q$形式比理论分布$p$简单，比如$q$是**单模态(unimodal)**的，$p$是**双模态(bimodal)**的。

![](https://pic.imgdb.cn/item/62ac6803094754312972d57e.jpg)

**前向KL散度(Forward KL)**形式如下：

$$ D_{KL}[p||q] = \sum_{z}^{} {p(z) \log \frac{p(z)}{q(z)}} $$

从上述公式和图像中可以看出，在$p(z)>0$而$q(z)=0$的位置将有较大的惩罚，由于$\mathop{\lim}_{q(z)→0} \log \frac{p(z)}{q(z)} → ∞$，这促使学习到的分布$q$将会覆盖分布$p$存在的任何区域，造成**zero avoiding**现象。

**反向KL散度(Reverse KL)**形式如下：

$$ D_{KL}[q||p] = \sum_{z}^{} {q(z) \log \frac{q(z)}{p(z)}} $$

从上述公式和图像中可以看出，在$p(z)=0$的位置只有$q(z)=0$才能使其值不趋近于$∞$，这促使学习到的分布$q$在$p(z)=0$的位置也为$0$，造成**zero forcing**现象。

# ⚪ JS散度 (Jenson-Shannon Divergence)

## 1. JS散度的定义和性质

**JS散度(Jenson-Shannon Divergence)**也用来衡量同一个随机变量的两个概率分布$p,q$的相似程度。与**KL**散度不同，**JS**散度是**对称**(**symmetic**)的，并且更加平滑。**JS**散度形式如下：

$$ D_{JS}[p||q] =  \frac{1}{2} D_{KL}[p||\frac{p+q}{2}] + \frac{1}{2} D_{KL}[q||\frac{p+q}{2}]  $$

**JS**散度的取值范围是$[0,1]$。

## 2. JS散度的缺点

假设具有以下概率分布：

$$ \begin{aligned} &\forall (x,y) \in P, x=0,y\text{ ~ } U(0,1) \\ &\forall (x,y) \in Q, x=\theta, 0 \leq \theta \leq 1,y\text{ ~ } U(0,1)  \end{aligned} $$

![](https://pic1.imgdb.cn/item/634226fe16f2c2beb1e49d0d.jpg)

当$\theta \neq 0$时分布$P,Q$没有重叠。此时计算两个分布的**KL**散度和**JS**散度：

$$ \begin{aligned} D_{KL}[P || Q] &= \sum P \log \frac{P}{Q}  = \sum_{x=0,y\text{ ~ } U(0,1)} 1 \cdot \log \frac{1}{0} = + \infty  \\  D_{KL}[Q || P] &= \sum Q \log \frac{Q}{P}  = \sum_{x=\theta,y\text{ ~ } U(0,1)} 1 \cdot \log \frac{1}{0} = + \infty \\ D_{JS}[P || Q] &= \frac{1}{2}D_{KL}[P || \frac{P+Q}{2}] + \frac{1}{2}D_{KL}[Q || \frac{P+Q}{2}] \\ & =  \frac{1}{2}\sum_{x=0,y\text{ ~ } U(0,1)} 1 \cdot \log \frac{1}{1/2} +  \frac{1}{2} \sum_{x=\theta,y\text{ ~ } U(0,1)} 1 \cdot \log \frac{1}{1/2} \\ & =  \log 2 \end{aligned} $$

当$\theta = 0$时分布$P,Q$完全重叠，此时有：

$$ D_{KL}[P || Q] = D_{KL}[Q || P] = D_{JS}[P || Q] = 0  $$

根据上述结论，当$P,Q$没有重叠时**KL**散度变为无穷大，而**JS**散度始终为常数；因此这两种分布作为分布差异衡量指标时不能始终保持平滑。


# ⚪ f散度 (f Divergence)

- paper：[<font color=Blue>f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization</font>](https://0809zheng.github.io/2022/02/07/fgan.html)

## 1. f散度的定义和性质

一般地，$p(x)$和$q(x)$之间的**f散度**定义为：

$$ D_f[p || q] = \int_{x}^{} {q(x)f(\frac{p(x)}{q(x)})dx} $$

其中函数$f(\cdot)$的性质：
- $f(\cdot)$是非负实数到实数的映射$$\Bbb{R}^{*} \to \Bbb{R}$$；
- $f(1)=0$；对应$p(x)=q(x)$时散度为$0$;
- $f(\cdot)$是凸函数：该性质使**f**散度恒大于等于零(根据[Jenson不等式](https://0809zheng.github.io/2022/07/20/jenson.html))：

$$ D_f[P || Q] = \int_{x}^{} {q(x)f(\frac{p(x)}{q(x)})dx} ≥ f(\int_{x}^{} {q(x)\frac{p(x)}{q(x)}dx}) = f(1) = 0 $$

当函数$f(\cdot)$选择不同时，**f**散度对应到不同的散度：
- **KL散度**：$f(x) = x \log x$

$$ D_f[P || Q] = \int_{x}^{} {q(x) \frac{p(x)}{q(x)} \log(\frac{p(x)}{q(x)})dx} = \int_{x}^{} {p(x) \log(\frac{p(x)}{q(x)})dx} $$

- **Reverse KL散度**：$f(x) = -\log x$

$$ D_f[P || Q] = \int_{x}^{} {q(x) (-\log(\frac{p(x)}{q(x)}))dx} = \int_{x}^{} {q(x) \log(\frac{q(x)}{p(x)})dx} $$

- $\chi^2$**散度**：$f(x) = (x-1)^2$

$$ D_f[P || Q] = \int_{x}^{} {q(x) (\frac{p(x)}{q(x)}-1)^2dx} = \int_{x}^{} {\frac{(p(x)-q(x))^2}{q(x)}dx} $$

**f**散度在当$P,Q$没有重叠时也始终为常数(或无穷大)。

下表给出了不同的散度对应的凸函数$f(x)$：

![](https://pic1.imgdb.cn/item/63451cc916f2c2beb1167c6e.jpg)

## 2. f散度的局部变分估算

注意到只有$p(x)$和$q(x)$的解析形式均已知时**f**散度才能求解。在实际应用中，有些概率分布形式未知，只能通过采样获得有限的样本；此时可以通过共轭函数估算**f**散度。

### (1) 凸函数的共轭函数

对于凸函数$f(x)$，选择任意一个点$\xi$，计算$y=f(x)$在$x=\xi$处的切线：

$$ y = f(\xi) + f'(\xi)(x-\xi) $$

凸函数的函数总在其切线上方，因此有：

$$ \begin{aligned} f(x) & \geq f(\xi) + f'(\xi)(x-\xi) \\ & = f(\xi) - f'(\xi)\xi  + f'(\xi)x \end{aligned} $$

对于定义域内的所有点$$\xi \in \Bbb{D}$$，$f(x)$可以用其所有切线簇表示：

$$ f(x) = \mathop{\max}_{\xi \in \Bbb{D}} \{ f(\xi) - f'(\xi)\xi  + f'(\xi)x \} $$

记$t=f'(\xi)$，$f^{\*}(t) = -f(\xi) + f'(\xi)\xi$，则有：

$$ f(x) = \mathop{\max}_{t \in f'(\Bbb{D})} \{ xt - f^{*}(t) \} $$

上式的几何意义为，对于凸函数$f(x)$，任意一点$x$处的取值为其所有切线簇在该点取值的最大值。

![](https://pic.downk.cc/item/5ebcfd25c2a9a83be542cee6.jpg)

式中$f^{\*}(t)$称为凸函数$f(x)$的**共轭函数（conjugate function）** ，具有如下性质：
- $f^*$也是凸函数；
- $(f^{\*})^{\*}=f$，因此有：

$$ f^{*}(t) = \mathop{\max}_{x \in \Bbb{D}} \{ xt - f(x) \} $$

直观上，通过共轭函数$f^*(t)$给出了凸函数$f(x)$的线性近似。对于任意$x$，通过共轭函数给出$f(x)$的一个下界$xt-f^{\*}(t)$，并且该下界关于$x$是线性的；通过在每一个$x$处最大化$t$才能取得$f(x)$的结果，因此该方法称为**局部变分**方法，$t$可以看作$x$的函数$t(x)$：

$$ f(x) = \mathop{\max}_{t \in f'(\Bbb{D})} \{ xt(x) - f^{*}(t(x)) \} $$

### (2) f散度的估算公式

根据共轭函数给出的凸函数$f(x)$表达式：

$$ f(x) = \mathop{\max}_{t \in f'(\Bbb{D})} \{ xt(x) - f^{*}(t(x)) \} $$

**f**散度可以表示为：

$$ \begin{aligned} D_f[p || q]& = \int_{x}^{} {q(x)f(\frac{p(x)}{q(x)})dx} \\ &= \int_{x}^{} {q(x)(\mathop{\max}_{t \in f'(\Bbb{D})} \{ \frac{p(x)}{q(x)} t(x) - f^*(t(x)) \})dx} \\ &= \mathop{\max}_{t \in f'(\Bbb{D})}\int_{x}^{} {q(x)( \frac{p(x)}{q(x)} t(x) - f^*(t(x)) )dx} \\ &= \mathop{\max}_{t \in f'(\Bbb{D})} \int_{x}^{} p(x)t(x)- {q(x)f^*(t(x))dx} \\&= \mathop{\max}_{t \in f'(\Bbb{D})} \{ \Bbb{E}_{x\text{~}p(x)}[t(x)]- \Bbb{E}_{x\text{~}q(x)}[f^*(t(x))] \} \end{aligned} $$

其中$t(x)$可以用神经网络拟合。注意到选定凸函数$f(x)$后，$t(x)$的值域是有限制的。在实践时可以通过施加激活函数约束$t(x)$的输出范围。激活函数的选择应满足以下几点：
1. 激活函数的定义域为$$\Bbb{R}$$，值域为$f'(x)$的值域；
2. 最好选择全局光滑的函数；如值域要求为$$\Bbb{R}^+$$，则优先考虑$e^x$而不是$$\text{ReLU}(x)$$；
3. 选择激活函数时，应使其与$f^*(\cdot)$的复合运算比较简单。

下面列出一些凸函数对应的共轭函数及其激活函数选择：

![](https://pic1.imgdb.cn/item/6345194e16f2c2beb11096c4.jpg)

# ⚪ W散度 (Wasserstein Divergence)

- paper：[<font color=Blue>f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization</font>](https://0809zheng.github.io/2022/02/07/fgan.html)

## 1. W散度的定义和性质

$p(x)$和$q(x)$之间的**Wasserstein**散度定义为：

$$ D_{W_{k,p}}[p || q] = \mathop{ \max}_{f} \int_x p(x)f(x)dx - \int_x q(x)f(x)dx - k\int_x r(x) || \nabla_xf(x) ||^p dx $$

或写作采样形式：

$$ D_{W_{k,p}}[p || q] = \mathop{ \max}_{f} \Bbb{E}_{x \text{~} p(x)}[f(x)] - \Bbb{E}_{x \text{~} q(x)}[f(x)] - k\Bbb{E}_{x \text{~} r(x)}[ || \nabla_xf(x) ||^p ] $$

其中$f(x)$是任意函数，$r(x)$是一个样本空间跟$p(x)$和$q(x)$一样的分布，$k>0, p > 1$。

**Wasserstein**散度具有以下性质：
- **Wasserstein**散度是一个对称的散度，即$D_{W_{k,p}}[p \| q]=D_{W_{k,p}}[q \| p]$。
- **Wasserstein**散度的最优解跟[<font color=blue>Wasserstein距离</font>](https://0809zheng.github.io/2022/05/16/Wasserstein.html)具有类似的性质。

## 2. W距离也是一种散度

[<font color=blue>Wasserstein距离</font>](https://0809zheng.github.io/2022/05/16/Wasserstein.html)的对偶形式定义如下：

$$    \mathcal{W}[p(x),q(x)]  = \mathop{\sup}_{f, ||f||_L \leq 1} \{ \Bbb{E}_{x \text{~} p(x)}[f(x)]-\Bbb{E}_{x \text{~} q(x)}[f(x)] \} $$

$$\|f\|_L \leq 1$$是指**Lipschitz**连续性条件：

$$ | f(x_1)-f(x_2) | ≤ | x_1-x_2 | $$

下面证明**W**距离也是一种散度，只需证明**W**距离满足散度的定义条件。

### ⚪ $$\mathcal{W}[p(x),q(x)] \geq 0$$

对任意$p,q$，不妨取$f(x)=0$则$$\Bbb{E}_{x \text{~} p(x)}[f(x)]-\Bbb{E}_{x \text{~} q(x)}[f(x)]=0$$。由于**W**距离定义为该表达式的上确界，因此至少不会小于$0$。

当$p \neq q$时，不妨取$$f_0(x) = \text{sign}(p(x)-q(x))$$，则有：

$$ \begin{aligned} &\Bbb{E}_{x \text{~} p(x)}[f_0(x)]-\Bbb{E}_{x \text{~} q(x)}[f_0(x)] \\ =& \Bbb{E}_{x \text{~} p(x)}[\text{sign}(p(x)-q(x))]-\Bbb{E}_{x \text{~} q(x)}[\text{sign}(p(x)-q(x))] \\ =& \int  (p(x)-q(x)) \text{sign}(p(x)-q(x)) dx > 0 \end{aligned}  $$

仍然由**W**距离定义为该表达式的上确界，可知当$p \neq q$时$$\mathcal{W}[p(x),q(x)] > 0$$

### ⚪ $$\mathcal{W}[p,q]=0 \leftrightarrow p=q$$

若$$\mathcal{W}[p,q]=0$$，根据上述讨论一定有$p=q$；若$p=q$，显然$$\mathcal{W}[p,p]=0$$。

### ⚪ 分布不重叠时的情况

不妨假设$p(x) = \delta(x-\alpha)$，$q(x) = \delta(x-\beta)$，即分布$p(x)$和$q(x)$没有重叠。此时**W**距离为：

$$    \mathcal{W}[p(x),q(x)]  = \mathop{\sup}_{f, ||f||_L \leq 1} \{ f(\alpha)-f(\beta) \} $$

其中**Lipschitz**约束$$\|f\|_L \leq 1$$使得$\|f(\alpha)-f(\beta)\| \leq \|\alpha-\beta\|$，因此有：

$$    \mathcal{W}[p(x),q(x)]  = \mathop{\sup}_{f, ||f||_L \leq 1} \{ |\alpha-\beta| \} $$

由此可知，即使分布不重叠，**W**距离仍然是有意义的，并且取值是光滑的。

# ⚪ 平方势散度 (Quadratic Potential Divergence)

- paper：[<font color=Blue>GAN-QP: A Novel GAN Framework without Gradient Vanishing and Lipschitz Constraint</font>](https://0809zheng.github.io/2022/02/22/ganqp.html)

## 1. 平方势散度的定义和性质

一般地，$p(x)$和$q(x)$之间的**平方势散度 (QP-div)**定义为：

$$ \begin{aligned} D_{QP}[p || q] &= \mathop{ \max}_{f} \Bbb{E}_{(x_r,x_f) \text{~} p(x_r)q(x_f)}[f(x_r,x_f)-f(x_f,x_r) -\frac{(f(x_r,x_f)-f(x_f,x_r))^2}{2 \lambda d(x_r,x_f)} ] \\ &= \mathop{ \max}_{f} \iint_{x_r,x_f} p(x_r)q(x_f)[f(x_r,x_f)-f(x_f,x_r) -\frac{(f(x_r,x_f)-f(x_f,x_r))^2}{2 \lambda d(x_r,x_f)} ]dx_rdx_f \end{aligned} $$

其中$\lambda > 0$，$d(x_r,x_f)$是任意距离。

### ⚪ 性质：无梯度消失

不妨假设$p(x) = \delta(x-\alpha)$，$q(x) = \delta(x-\beta)$，即分布$p(x)$和$q(x)$没有重叠。此时平方势散度为：

$$    D_{QP}[p(x),q(x)]  = \mathop{ \max}_{f}  \{ f(\alpha,\beta)-f(\beta,\alpha) - \frac{(f(\alpha,\beta)-f(\beta,\alpha))^2}{2 \lambda d(\alpha,\beta)} \} $$

令$g=f(\alpha,\beta)-f(\beta,\alpha)$，则上式转换为二次函数的最大值问题，对应的最大值为：

$$    D_{QP}[p(x),q(x)]  =  \frac{1}{2 }\lambda d(\alpha,\beta) $$

由此可知，即使分布不重叠，平方势散度仍然是有意义的，并且取值是光滑的，从而在优化时不会有梯度消失的风险。

### ⚪ 性质：Lipschitz连续

下面求平方势散度表达式的最优值：

$$ \begin{aligned} &\nabla_f p(x_r)q(x_f)[f(x_r,x_f)-f(x_f,x_r) -\frac{(f(x_r,x_f)-f(x_f,x_r))^2}{2 \lambda d(x_r,x_f)} ]  \\ &=  p(x_r)q(x_f)[\nabla_ff(x_r,x_f)-\nabla_ff(x_f,x_r) \\& \quad -\frac{(f(x_r,x_f)-f(x_f,x_r))(\nabla_ff(x_r,x_f)-\nabla_ff(x_f,x_r))}{ \lambda d(x_r,x_f)} ] \\ &=  p(x_r)q(x_f)\nabla_ff(x_r,x_f)-p(x_r)q(x_f)\nabla_ff(x_f,x_r) \\& \quad -p(x_r)q(x_f)\frac{f(x_r,x_f)-f(x_f,x_r)}{ \lambda d(x_r,x_f)}\nabla_ff(x_r,x_f) \\ &\quad +p(x_r)q(x_f)\frac{f(x_r,x_f)-f(x_f,x_r)}{ \lambda d(x_r,x_f)}\nabla_ff(x_f,x_r)  \\ &=  p(x_r)q(x_f)\nabla_ff(x_r,x_f)-p(x_f)q(x_r)\nabla_ff(x_r,x_f) \\& \quad -p(x_r)q(x_f)\frac{f(x_r,x_f)-f(x_f,x_r)}{ \lambda d(x_r,x_f)}\nabla_ff(x_r,x_f) \\ &\quad -p(x_f)q(x_r)\frac{f(x_r,x_f)-f(x_f,x_r)}{ \lambda d(x_r,x_f)}\nabla_ff(x_r,x_f) \end{aligned} $$

令上式为零得：

$$ \frac{p(x_r)q(x_f)-p(x_f)q(x_r)}{p(x_r)q(x_f)+p(x_f)q(x_r)} = \frac{f(x_r,x_f)-f(x_f,x_r)}{ \lambda d(x_r,x_f)} $$

注意到上式满足：

$$ -1 \leq \frac{f(x_r,x_f)-f(x_f,x_r)}{ \lambda d(x_r,x_f)} \leq 1 $$

因此最优值满足**Lipschitz**连续条件。
