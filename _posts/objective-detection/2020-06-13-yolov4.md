---
layout: post
title: 'YOLOv4: Optimal Speed and Accuracy of Object Detection'
date: 2020-06-13
author: 郑之杰
cover: 'https://pic.downk.cc/item/5ea3eea1c2a9a83be5996f6b.jpg'
tags: 目标检测
---

> Notes about YOLOv4.

- paper：YOLOv4: Optimal Speed and Accuracy of Object Detection
- arXiv：[https://arxiv.org/abs/2004.10934v1](https://arxiv.org/abs/2004.10934v1)
- pytorch：[github](https://github.com/0809zheng/yolov4-pytorch)

目录：
1. Related work
2. Methodology
3. 模型介绍

# 1. Related work

## (1)Object detection models
![](https://pic.downk.cc/item/5ea3dd27c2a9a83be5823a71.jpg)

一个典型的目标检测模型包含以下part：

### **1.Input**
- Image
- Patches
- Image Pyramid

### **2.Backbones**
- **GPU platform**：VGG、ResNet、ResNeXt、DenseNet、DetNet、DetNAS
- **CPU platform**：SqueezeNet、MobileNet、 ShuffleNet
- **new whole model**：SpineNet、HitDetector

### **3.Neck**
Collect feature maps from different stages.

- **Additional blocks**：SPP、ASPP、RFB、SAM
- **Path-aggregation blocks**：FPN、PAN、NAS-FPN、Fully-connected FPN、BiFPN、ASFF、SFAM

### **4.Heads**
- **one-stage（Dense Prediction）**

**anchor-base**：RPN、YOLO、SSD、RetinaNet

**anchor-free**：CenterNet、CornerNet、 MatrixNet、FCOS

- **two-stage（Sparse Prediction）**

**anchor-base**：R-CNN、faster R-CNN、R-FCN、Libra R-CNN、Mask R-CNN

**anchor-free**：RepPoints

## (2)Bag of freebies
Methods that only change the training strategy or only increase the training cost.

### 1.data augmentation
Increase the variability of the input images, model has higher robustness to images obtained from different environments.

**i. pixel-wise adjustments**

- **photometric distortion**：brightness、contrast、hue、saturation、noise
- **geometric distortions**：random scaling、cropping、flipping、rotating

**ii. simulating object occlusion**

- **To images**：random erase、CutOut、hide-and-seek、grid mask
- **To feature maps**：DropOut、DropConnect、DropBlock、Spatial DropOut

**iii. using multiple images together**

-  MixUp、CutMix

**iv. others**

- style transfer GAN

### 2.data imbalance
**semantic distribution bias**：the semantic distribution in the dataset has bias.

- one-stage：focal loss
- two-stage：hard negative example mining、 online hard example mining

### 3.representation scheme

- label smoothing、label refinement network

### 4.objective function of BBox regression

- **Mean Square Error**

1. traditional：$$\{x_{center},y_{center},w,h\}$$、$$\{x_{top-left},y_{top-left},x_{bottom-right},y_{bottom-right}\}$$
2. anchor-based method：$$\{x_{center-offset},y_{center-offset},w_{offset},h_{offset}\}$$、$$\{x_{top-left-offset},y_{top-left-offset},x_{bottom-right-offset},y_{bottom-right-offset}\}$$

- **IoU loss**、**GIoU loss**、**DIoU loss**、**CIoU loss**

## (3)Bag of specials
Only increase the inference cost by a small amount but can significantly improve the accuracy.

### 1.enhance receptive field

- SPP、ASPP、RFB

### 2.introducing attention mechanism

- **channel-wise attention**：Squeeze-and-Excitation
- **point-wise attention**： Spatial Attention Module

### 3.strengthening feature integration capability
Integrate low-level physical feature to high-level semantic feature.

- early practice：skip connection、hyper-column
- integrate different feature pyramid：SFAM、ASFF、BiFPN

### 4.activation function
A good activation function can make the gradient more efficiently propagated, and not cause too much extra computational cost.

- ReLU、LReLU、PReLU
- designed for quantization networks：ReLU6、hard-Swish
- self-normalizing a neural network：SELU
- continuously differentiable activation function：Swish、Mish

### 5.post-processing method

- NMS、soft NMS、DIoU NMS


# 2. Methodology

## YOLOv4 consists of:

- **Backbone**: CSPDarknet53
- **Neck**: SPP, PANet
- **Head**: YOLOv3

## YOLOv4 uses:

- **Bag of Freebies (BoF) for backbone**:
1. CutMix and Mosaic data augmentation
2. DropBlock regularization
3. Class label smoothing
- **Bag of Specials (BoS) for backbone**:
1. Mish activation
2. Cross-stage partial connections
3. Multi-input weighted residual connections
- **Bag of Freebies (BoF) for detector**:
1. CIoU-loss
2. CmBN
3. DropBlock regularization
4. Mosaic data augmentation、Self-Adversarial Training
5. Eliminate grid sensitivity
6. Using multiple anchors for a single ground truth
7. Cosine annealing scheduler
8. Optimal hyper-parameters
9. Random training shapes
- **Bag of Specials (BoS) for detector**:
1. Mish activation
2. SPP-block、SAM-block、PAN path-aggregation block
3. DIoU-NMS

## New method of data augmentation

**1. Mosaic**

Mixes 4 training images, batch norm calculates activation statistics from 4 different images on each layer.

![](https://pic.downk.cc/item/5ea3f60fc2a9a83be5a308f2.jpg)

**2. Self-Adversarial Training (SAT)**

1. In the 1st stage, network alters the original image, executes an adversarial attack on itself, altering the original image to create the deception that there is no desired object on the image.
2. In the 2nd stage, the neural network is trained to detect an object on this modified image in the normal way.

## Modification

**1. CmBN**

from **CBN** to **Cross mini-Batch Normalization(CmBN)**:

![](https://pic.downk.cc/item/5ea3f784c2a9a83be5a4e224.jpg)

**2. SAM**

from **spatial-wise attention** to **point-wise attention**:

![](https://pic.downk.cc/item/5ea3f7f3c2a9a83be5a579df.jpg)

**3. PAN**

from **shortcut connection** to **concatenation**:

![](https://pic.downk.cc/item/5ea3f807c2a9a83be5a590c0.jpg)

# 3. 模型介绍
**YOLOV4**继承了**YOLOV3**的单阶段检测器主体结构，使用三个特征层进行分类与回归预测。

## （1）网络结构
**YOLOV4**的主要结构包括：
- backbone：CSPDarknet53（**YOLOV3**是Darknet53）
- neck：SPP+PANet，提取三种尺寸的特征映射
- head：同**YOLOV3**

当输入图像尺寸是$416×416$时，网络结构如下：

![](https://pic.downk.cc/item/5ee340c9c2a9a83be5562e9f.jpg)

### backbone：CSPDarknet53
激活函数使用[Mish](https://0809zheng.github.io/2020/03/01/activation.html#17-mish)，表达式如下：

$$ mish(x) = x·tanh(ln(1+e^x)) $$

使用Darknet53中定义的**resblock_body**模块，其由一次下采样和多次残差结构的堆叠构成（下图左），**YOLOv4**使用了**CSPnet**结构，把Darknet53修改成CSPDarknet53，主干部分继续进行原来的残差块的堆叠，另一部分则引入了一个全局的残差连接（下图右）。

![](https://pic.downk.cc/item/5ee348b1c2a9a83be56097f8.jpg)

### neck：SPP+PANet
**SPP**结构对**CSPdarknet53**的最后一个**resblock_body**的$13×13$的输出特征进行三次**DarknetConv2D_BN_Mish**卷积后，分别用四个不同池化核（13×13、9×9、5×5、1×1）的最大池化。

**PANet**是一种反复提取和融合特征的网络结构。

### head
**YOLOV4**最后得到三种输出特征，其尺寸分别是$n×13×13×75$、$n×26×26×75$和$n×52×52×75$。

其中的$13×13$、$26×26$和$52×52$对应原图像的部分区域，分别在原图中检测大物体、中等物体和小物体。

![](https://pic.downk.cc/item/5ee34927c2a9a83be5611b06.jpg)

最后一个维度为$75$可以拆分成$3×\(1+4+20\)$:
- $3$是指每个特征映射的每个位置预先设定$3$个**anchor**先验框；
- $1$用来表示该位置是物体还是背景；
- $4$用来表示bbox的中心坐标和高宽；
- $20$是指**VOC**数据集的$20$类（条件概率）。

**anchor**先验框的尺寸是通过[k-means聚类](https://0809zheng.github.io/2020/05/02/kmeans.html)得到的。在实现时衡量不同标注框之间的距离使用**jaccard系数**：

$$ J(A,B) = \frac{\mid A∩B \mid}{\mid A∪B \mid} $$

## （2）训练过程

### 损失函数
经过**YOLOV4**主干网络得到的三个输出特征尺寸（在pytorch格式下）为$n×75×13×13$、$n×75×26×26$和$n×75×52×52$。

将其调整为：$\(n×3×h×w×25\)$，相当于把原图像划分成$h×w$个子区域，每个区域内设置了$3$个**anchor**，每个**anchor**具有$25$个参数$\(x_{pred},y_{pred},h_{pred},w_{pred},p_{pred},c_{pred}\)$

在计算loss的时候，实际上是y_pre和y_true之间的对比：
- y_pre就是一幅图像经过网络之后的输出，内部含有三个特征层的内容；其需要解码才能够在图上画出边界框；
- y_true就是一个真实图像的标签，它的每个真实框对应网格上的偏移位置、长宽与种类。其仍需要编码才能与y_pred的结构一致。

对于每一个输出特征层，损失函数的计算如下：
1. 利用y_true取出该特征层中真实存在目标的点的位置$\(n×3×h×w×1\)$及其对应的种类$\(n×3×h×w×20\)$;
2. 将预测值输出进行处理，得到reshape后的预测值y_pre，shape为$\(n×3×h×w×25\)$。还有解码后的$xywh$;
3. 对于每一幅图，计算其中所有真实框与预测框的IoU，如果某些预测框和真实框的重合程度小于0.5，则忽略；
4. 计算CIoU作为回归损失，这里只计算正样本的回归损失；
5. 计算置信度损失，其有两部分构成，第一部分是实际上存在目标的，预测结果中置信度的值与1对比；第二部分是实际上不存在目标的，在第四步中得到其最大IoU的值与0对比；
6. 计算预测种类损失，其计算的是实际上存在目标的，预测类与真实类的差距。

### 数据增强：Mosaic
**Mosaic**对四张图片进行融合，能丰富检测物体的背景，提高batch size使BN更稳定。
1. 每次读取四张图片；
2. 分别对四张图片进行翻转、缩放、色域变化等，并且按照左上、左下、右上、右下四个方向位置摆好；
3. 进行图片的组合和框的组合.

### 标签平滑
**Label Smoothing平滑**就是将标签进行一个平滑，如原始的标签是$0$、$1$(如果是二分类)，在平滑后变成$0.005$、$0.995$，也就是说对分类准确做了一点惩罚，让模型不可以分类的太准确，太准确容易过拟合。

### 学习率余弦退火
[余弦退火衰减法](https://0809zheng.github.io/2020/03/02/optimization.html#11-learning-rate-strategy)，学习率会先上升再下降，这是退火优化法的思想。上升的时候使用线性上升，下降的时候模拟cos函数下降。执行多次。

pytorch有现成的调用函数：

```
lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-5)
```
