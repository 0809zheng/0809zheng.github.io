---
layout: post
title: '生成对抗网络(Generative Adversarial Network)'
date: 2022-02-01
author: 郑之杰
cover: 'https://pic.downk.cc/item/5ebd0c69c2a9a83be54ed281.jpg'
tags: 深度学习
---

> Generative Adversarial Networks.

本文目录：
1. 理解生成对抗网络：博弈论视角、优化视角、概率论视角、能量模型视角、动力学视角
2. 使用Pytorch实现生成对抗网络：MMGAN、NSGAN、合并交替优化
3. 生成对抗网络的评估指标
4. 生成对抗网络的训练困难：纳什均衡、低维流形、梯度消失、模式崩溃
5. 生成对抗网络的各种变体：改进目标函数、改进网络结构、改进优化过程、其他应用

# 1. 理解生成对抗网络

**生成对抗网络(Generative Adversarial Network, GAN)**是一种生成模型，可以用来生成图像、文本、语音等结构化数据(**structured data**)。

假设真实数据具有概率分布$$P_{data}(x)$$，**GAN**使用一个**生成器generator**构造真实分布的一个近似分布$$P_G(x)$$，并使用一个**判别器discriminator**衡量生成分布和真实分布之间的差异。

- **生成器** $G$：生成器是一个神经网络，从一个简单的概率分布$P_Z(z)$中采样$z$，经过生成器$G$得到输入数据概率分布$$P_{data}$$的估计$$P_G(x)=G(z)$$；![](https://pic.downk.cc/item/5eb555dcc2a9a83be5b2b166.jpg)
- **判别器** $D$：判别器是一个二分类器，用于区分从输入数据分布$$P_{data}$$（标记为$1$）和生成分布$$P_G$$（标记为$0$）中采样得到的数据。![](https://pic.downk.cc/item/5eb55bb7c2a9a83be5b7fced.jpg)

对于判别器$D$，希望其能正确地区分真实数据与生成数据。若输入数据来自真实分布$$P_{data}$$，则希望其输出结果接近$1$；反之若数据来自生成分布$P_G$，则希望其输出结果接近$0$。优化目标采用二元交叉熵：

$$ D^* = \mathop{\arg \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-D(G(z)))] $$

对于生成器$G$，希望其能够成功地欺骗判别器，使其将生成样本误分类成真实样本：

$$ G^* = \mathop{\arg \min}_{G} \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-D(G(z)))] $$

### ⚪ 从博弈论视角理解生成对抗网络

若将判别器$D$和生成器$G$的目标函数合并，记为：

$$ \begin{aligned} \mathop{ \min}_{G} \mathop{\max}_{D} L(G,D) & =  \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-D(G(z)))] \\ & =  \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{x \text{~} P_{G}(x)}[\log(1-D(x))] \end{aligned} $$


上式表示判别器$D$和生成器$G$在进行**极小极大博弈(minimax game)**：这也是一个**零和博弈(zero-sum game)**，参与博弈的双方，在严格竞争下，一方的收益必然意味着另一方的损失，博弈各方的收益和损失相加总和永远为“零”。
在交替迭代的过程中，双方都极力优化自己的网络，从而形成竞争对抗，直到双方达到**纳什均衡 (Nash equilibrium)**。

观察目标可知博弈过程旨在寻找使得判别器$D$造成的损失$L(G,D)$最大值最小的生成器$G$。

![](https://pic.imgdb.cn/item/632eaa8a16f2c2beb1cc7ff3.jpg)

### ⚪ 从优化视角理解生成对抗网络

将目标函数写成积分形式：

$$ \begin{aligned}  L(G,D)  & =  \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{x \text{~} P_{G}(x)}[\log(1-D(x))]  \\ & =\int_x  (P_{data}(x)\log D(x) + P_{G}(x)\log(1-D(x))) dx \end{aligned} $$

下面先求判别器$D$的最优值$D^{\*}$，注意到积分不影响最优值的取得，因此计算被积表达式的极值$$\frac{\partial L(G,D)}{\partial D} = 0$$，得：

$$ D^*(x) = \frac{P_{data}(x)}{P_{data}(x)+P_{G}(x)} \in [0,1] $$

若生成器$G$也训练到最优值，此时有$$P_{data}(x)≈P_{G}(x)$$，则判别器退化为**常数** $D^{\*}(x)=\frac{1}{2}$，失去判别能力。

当判别器$D$取得最优值$D^{\*}$时，目标函数为：

$$ \begin{aligned}  L(G,D^*)  & =\int_x  (P_{data}(x)\log D^*(x) + P_{G}(x)\log(1-D^*(x))) dx \\ & =\int_x  (P_{data}(x)\log \frac{P_{data}(x)}{P_{data}(x)+P_{G}(x)} + P_{G}(x)\log\frac{P_{G}(x)}{P_{data}(x)+P_{G}(x)}) dx \\ & =\int_x  (P_{data}(x)\log \frac{P_{data}(x)}{\frac{P_{data}(x)+P_{G}(x)}{2}} + P_{G}(x)\log\frac{P_{G}(x)}{\frac{P_{data}(x)+P_{G}(x)}{2}}-2\log 2) dx \\ & = 2D_{JS}[P_{data}(x) || P_G(x)]-2\log 2 \end{aligned} $$

其中$D_{JS}$表示[<font color=blue>JS散度</font>](https://0809zheng.github.io/2020/02/03/kld.html#-js%E6%95%A3%E5%BA%A6-jenson-shannon-divergence)。因此当判别器$D$取得最优时，**GAN**的损失函数衡量了真实分布$$P_{data}(x)$$与生成分布$$P_G(x)$$之间的**JS散度**。若生成器$G$也取得最优值，则损失函数取得**最小值** $-2\log 2$。

另一方面，传统的基于**极大似然估计(maximum likelihood estimation)**的生成模型本质上是在最小化真实分布$$P_{data}(x)$$与生成分布$$P_G(x)$$之间的[<font color=blue>KL散度</font>](https://0809zheng.github.io/2020/02/03/kld.html#-kl%E6%95%A3%E5%BA%A6-kullback-leibler-divergence)：

$$ \begin{aligned} \mathop{ \max}_{G} L(G) & = \prod_{i=1}^{n} {P_G(x^i)} = \log(\prod_{i=1}^{n} {P_G(x^i)}) = \sum_{i=1}^{n} {\log P_G(x^i)} \\ & = \Bbb{E}_{x \text{~} P_{data}(x)}[\log P_G(x)] = \int_x P_{data}(x)\log P_G(x) dx \\ &  = \int_x P_{data}(x)\log \frac{P_G(x)}{P_{data}(x)} dx +  \int_x P_{data}(x)\log P_{data}(x) dx \\ & = -D_{KL}[P_{data}(x) || P_G(x)] + Const.  \end{aligned} $$

由于**JS**散度相比于**KL**散度具有对称性、平滑性等优点，因此通常认为**GAN**相比于传统的生成模型能够取得更好的表现。

值得一提的是，**GAN**的目标函数只有先精确完成$$\mathop{\max}_{D}$$，然后再进行$$\mathop{ \min}_{G}$$，才相当于优化两个分布的**JS**散度；在实际训练时常采用**交替优化**步骤，即先优化判别器再优化生成器，并且判别器的更新次数通常更多一些(或使用的学习率更大一些)；这样做的目的也是为了让判别器先取得局部最优，从而让生成器的优化过程趋近于**JS**散度的最小化过程(但理论上不可能精确逼近分布度量)。

### ⚪ 从概率论视角理解生成对抗网络


均匀分布到指定分布的变换

### ⚪ 从能量模型视角理解生成对抗网络

[能量模型](https://0809zheng.github.io/2020/04/12/energy.html)是指使用如下能量分布拟合一批真实数据$x_1,x_2,\cdots,x_n$~$$P_{data}(x)$$：

$$ q_{\theta}(x) = \frac{e^{-U_{\theta}(x)}}{Z_{\theta}},Z_{\theta} = \int e^{-U_{\theta}(x)}dx $$

其中$U_{\theta}(x)$是带参数的能量函数；$Z_{\theta}$是配分函数(归一化因子)。直观地，真实数据分布在能量函数中势最小的位置。我们希望通过对抗学习使得生成数据$\hat{x}_1,\hat{x}_2,\cdots \hat{x}_n$的势也尽可能小。

![](https://pic1.imgdb.cn/item/634e13f716f2c2beb1b9d59f.jpg)

使用判别器$D(x)$拟合能量函数$U_{\theta}(x)$，使用生成器构造生成分布$P_G(x)$。根据能量模型的[正相-负相分解](https://0809zheng.github.io/2020/04/12/energy.html#2-%E8%83%BD%E9%87%8F%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%AD%A3%E7%9B%B8-%E8%B4%9F%E7%9B%B8%E5%88%86%E8%A7%A3)构造目标函数：

$$ D^* \leftarrow \mathop{ \min}_{D} \Bbb{E}_{x \text{~} P_{data}(x)} [  D(x)]-  \Bbb{E}_{x \text{~} P_G(x)}[D(x) ] $$

上式表示判别器的目标为最小化真实数据分布的能量，最大化生成数据分布的能量。

与此同时，生成分布$P_G(x)$与能量分布$q_{\theta}(x)$应该足够接近，使用**KL**散度衡量两者差异：

$$ \begin{aligned} D_{KL}[P_G(x) || q_{\theta}(x)] &= \int P_G(x) \log \frac{P_G(x)}{q_{\theta}(x)} dx \\ &= \int P_G(x) \log P_G(x) dx - \int P_G(x) \log q_{\theta}(x) dx \\ &= -H(G(z))+\Bbb{E}_{x \text{~} P_G(x)}[D(x) ] + \log Z_{\theta} \end{aligned} $$

上式第一项$H(G(z))$表示生成样本的熵，希望熵越大越好(对应样本多样性越大)；第二项表示生成样本的势能，希望势越小越好(接近真实样本)；第三项是一个常数。通过上式可以构造生成器的目标函数：

$$ G^* \leftarrow \mathop{ \min}_{G} -H(G(z))+\Bbb{E}_{x \text{~} P_G(x)}[D(x) ] $$

上式表示生成器的目标为最小化生成数据分布的能量，并最大化生成数据分布的熵。





### ⚪ 从动力学视角理解生成对抗网络

# 2. 使用Pytorch实现生成对抗网络

### ⚪ GAN的算法流程

初始化生成器$G(z;\theta_G)$和判别器$D(x;\theta_D)$，在训练的每一次迭代中，先更新判别器$D$，再更新生成器$G$：
- **固定生成器$G$，更新判别器$D$**，重复$k$次：
1. 从训练数据集中采样$$\{x^1,x^2,...,x^n\}$$；
2. 从随机噪声中采样$$\{z^1,z^2,...,z^n\}$$；
3. 根据生成器$G$获得生成数据$$\{\tilde{x}^1,\tilde{x}^2,...,\tilde{x}^n\}$$；

$$ θ_D \leftarrow \mathop{\arg \max}_{\theta_D} \frac{1}{n} \sum_{i=1}^{n} {\log D(x^i)} + \frac{1}{n} \sum_{i=1}^{n} {\log (1-D(\tilde{x}^i))} $$

- **固定判别器$D$，更新生成器$G$**，进行$1$次：
1. 从随机噪声中采样$$\{z^1,z^2,...,z^n\}$$；
2. 根据生成器$G$获得生成数据$$\{\tilde{x}^1,\tilde{x}^2,...,\tilde{x}^n\}$$；

$$ \theta_G \leftarrow \mathop{\arg \min}_{\theta_G} \frac{1}{n} \sum_{i=1}^{n} {\log (1-D(\tilde{x}^i))} $$

上述即为**GAN**的标准训练流程，采用该流程的**GAN**也被称作**Minimax GAN (MMGAN)**。

### ⚪ 从MMGAN到NSGAN


由于上式在计算$θ_G$时，函数$\log (1-D(x))$在$D(x)$接近$0$（即训练的初始阶段）时梯度较小，在$D(x)$接近$1$（即优化后期）时梯度较大，会造成优化困难；因此在实践中采用下式代替：

$$ \theta_G \leftarrow \mathop{\arg \max}_{\theta_G} \frac{1}{n} \sum_{i=1}^{n} {\log D(\tilde{x}^i)} $$

使用上式的**GAN**被称作**Non-saturating GAN (NSGAN)**。

![](https://pic.imgdb.cn/item/632ec75916f2c2beb1f0b96d.jpg)




### ⚪ GAN的Pytorch实现

```python
import torch

# 定义生成器和判别器
generator = Generator()
discriminator = Discriminator()

# 定义损失函数
adversarial_loss = torch.nn.BCELoss()

# 定义优化器
optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))

for epoch in range(n_epochs):
    for i, real_imgs in enumerate(dataloader):
        # 构造对抗标签
        valid = torch.ones(real_imgs.shape[0], 1).requires_grad_.(False)
        fake = torch.zeros(real_imgs.shape[0], 1).requires_grad_.(False)

        # 从噪声中采样生成图像
        z = torch.randn(real_imgs.shape[0], latent_dim)
        gen_imgs = generator(z)

        # 训练判别器
        optimizer_D.zero_grad()
        # 计算判别器的损失
        real_loss = adversarial_loss(discriminator(real_imgs), valid)
        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake) # 此处不计算生成器的梯度
        d_loss = (real_loss + fake_loss) / 2
        # 更新判别器参数
        d_loss.backward()
        optimizer_D.step()

        # 训练生成器
        optimizer_G.zero_grad()
        g_loss = adversarial_loss(discriminator(gen_imgs), valid)
        g_loss.backward()
        optimizer_G.step()
```

### ⚪ 技巧：合并交替优化

**GAN**的训练过程采用交替优化的形式：

$$ \begin{aligned} D^* &\leftarrow \mathop{ \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_z}[\log(1-D(G(z)))]  \\ G^* & \leftarrow \mathop{ \max}_{G}  \Bbb{E}_{z \text{~} P_z}[\log D(G(z))] \end{aligned} $$

如果能把交替优化步骤合并为一个步骤，则可以节省优化所需的时间（代价是增大优化过程占用的显存）。

合并交替优化的关键是引入**stop gradient**操作（对应**pytorch**中的`.detach()`方法）。

对于判别器，目标是优化判别器的参数，因此把生成器的梯度置零：

$$ \begin{aligned} D^* \leftarrow \mathop{ \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_z}[\log(1-D(\text{StopGrad}(G(z))))] \end{aligned} $$

对于生成器，目标时优化生成器的参数。注意到反向传播中$D(G(z))$会产生生成器和判别器的联合梯度$(\nabla_G L,\nabla_D L)$，而生成器梯度置零后$$\text{StopGrad}(D(G(z)))$$会产生判别器的梯度$(0,\nabla_D L)$；两者作差即为生成器的梯度：

$$ \begin{aligned}  G^* & \leftarrow \mathop{ \max}_{G}  \Bbb{E}_{z \text{~} P_z}[\log D(G(z))-\log D(\text{StopGrad}(G(z)))] \end{aligned} $$

值得一提的是，上述形式的生成器目标数值将恒为零，然而梯度**不为零**；此时生成器损失不宜再作为指示训练过程的指标，但是参数更新过程仍然在正常进行。

因此**GAN**的目标函数可以合并写作：

$$ \begin{aligned} (D^*, G^*) \leftarrow \mathop{ \max}_{D,G} & \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_z}[\log(1-D(\text{StopGrad}(G(z))))] \\ &+ \lambda \Bbb{E}_{z \text{~} P_z}[\log D(G(z))-\log D(\text{StopGrad}(G(z)))] \end{aligned} $$

其中$\lambda$控制判别器和生成器的学习率之比为$1:\lambda$。至此**GAN**的更新过程可以被合并为一步：

```python
# 定义优化器
optimizer = torch.optim.Adam(
    itertools.chain(generator.parameters(), discriminator.parameters()), 
    lr=0.0002, betas=(0.5, 0.999)
)

for epoch in range(n_epochs):
    for i, real_imgs in enumerate(dataloader):
        # 构造对抗标签
        valid = torch.ones(real_imgs.shape[0], 1).requires_grad_.(False)
        fake = torch.zeros(real_imgs.shape[0], 1).requires_grad_.(False)

        # 从噪声中采样生成图像
        z = torch.randn(real_imgs.shape[0], latent_dim)
        gen_imgs = generator(z)

        # 构造损失函数
        d_real_loss = adversarial_loss(discriminator(real_imgs), valid)
        d_fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake) # 此处不计算生成器的梯度
        d_loss = (d_real_loss + d_fake_loss) / 2
        g_fake_loss = adversarial_loss(discriminator(gen_imgs), valid)
        g_fake_loss_sg = adversarial_loss(discriminator(gen_imgs.detach()), valid) # 此处不计算生成器的梯度
        g_loss = (g_fake_loss - g_fake_loss_sg) / 2
        loss = d_loss + lambda * g_loss

        # 更新网络参数
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```



# 3. 生成对抗网络的评估指标

**GAN**的训练过程是交替迭代的，其总目标函数无法同时有效地显示生成器和判别器的训练过程。缺乏有效的评估指标使得**GAN**的训练过程不知何时停止，也不方便对不同模型的性能进行比较。

本节介绍几种**GAN**的评估指标：
- FID
- Kernel Density Estimation
- Inception Score

### Kernel Density Estimation
从生成器$G$中随机采样一些样本，使用高斯混合模型拟合这些样本，得到生成样本概率分布的估计值$P_G$，

![](https://pic.downk.cc/item/5ed9e0abc2a9a83be5e94672.jpg)

选择一些真实样本$x^i$，计算其在估计的生成样本分布中的极大似然概率：

$$ L = \frac{1}{N} \sum_{i=1}^{N} {log(P_G(x^i))} $$

将这个似然值作为$GAN$模型的好坏。这种评估方法存在一些问题：
- 一些高质量样本的似然值较低：模型可能产生不同于已知样本集的高质量样本；
- 一些低质量样本的似然值较高

### Inception Score
把生成图像作为图像分类任务的样本集，进行进一步的实验；分成两步：
1. 对于一张生成的图像，将其喂入训练好的分类网络（如Inception），若分类置信度越高则图像质量越好；
![](https://pic.downk.cc/item/5ed9e362c2a9a83be5ecc47e.jpg)
2. 对于不同的生成图像，若分类网络分类后结果越平均（意味着$GAN$不会只生成某几个类）则图像质量越好；
![](https://pic.downk.cc/item/5ed9e388c2a9a83be5ecea9c.jpg)

**Inception Score**的定义是对某一具体的生成图像，分类网络的输出分布越集中越好（熵越小越好）；对于生成的所有图像，分类网络的输出分布越平均越好（熵越大越好）：

$$ \sum_{x}^{} {\sum_{y}^{} {P(y \mid x)logP(y \mid x)}} - \sum_{y}^{} {P(y)logP(y)} $$

![](https://pic.downk.cc/item/5ed9e3e9c2a9a83be5ed87a3.jpg)














# 4. 生成对抗网络的训练困难

众所周知，**GAN**模型的训练比较困难，训练速度较慢且训练过程不稳定。主要体现在以下几点：

## （1）难以实现纳什均衡 Hard to achieve Nash equilibrium

**GAN**使用梯度下降算法优化两人**非合作博弈(non-cooperative)**，通常梯度下降算法只有在目标函数为凸函数时才能保证实现纳什均衡。而判别器和生成器独立地优化各自的损失，在博弈中没有考虑到另一方。因此同时更新两个模型的梯度不能保证收敛。


## （2）低维流形 Low dimensional manifold

许多真实世界的数据集分布$$P_{data}(x)$$通常集中在高维空间中的一个低维流形上，这是因为真实图像要遵循主题或目标的限制；生成分布$$P_G(x)$$通常也位于一个低维流形上，因为它是由一个低维噪声变量$z$定义的。高维空间中的两个低维流形几乎不可能重叠；即使两个流形有重合的部分，对概率分布采样时也很难采集到重合的样本，从而导致**GAN**训练的不稳定性。


![](https://pic.imgdb.cn/item/633107ba16f2c2beb14cbe22.jpg)


## （3）梯度消失 Vanishing gradient

**GAN**的训练过程进退两难：如果判别器表现较差，则生成器没有准确的反馈，损失函数不能代表真实情况；如果判别器表现较好，则损失函数及其梯度趋近于$0$，训练过程变得非常慢甚至卡住。

![](https://pic.imgdb.cn/item/6331125716f2c2beb156fc56.jpg)

通常**GAN**的训练过程被视为最优化真实分布与生成分布的**f**散度(如**JS**散度)。这类散度在两个概率分布不重叠时始终为常数(或无穷大)，导致更新梯度为$0$。

## （4）模式崩溃 Mode collapse

**Mode Collapse**是指在训练过程中生成器可能会崩溃到一种始终产生相同输出的设置。由于生成器无法学习表示复杂的真实世界数据分布，所学习到的生成分布会陷入一个缺乏多样性的局部空间中，只能集中在真实数据分布的一小部分。

![](https://pic.imgdb.cn/item/633115cf16f2c2beb15a9918.jpg)

与之类似的一个问题是**Mode Dropping**，是指真实数据分布通常有多个簇，而生成器的每次迭代过程中只能生成其中某一个簇。

![](https://pic.imgdb.cn/item/6331164716f2c2beb15b5d28.jpg)

从能量的角度，真实数据分布在能量函数所有可能的极小值点附近。在生成数据时通常只需要找到附近的极小值点；若采用带**动量**的优化器，则有可能使其跳出附近的极小值点，集中在一些更小的极小值点附近，从而丧失了数据生成的多样性。因此在**GAN**的优化过程中，尽量使用不带动量的优化器，或者设置较小的动量和学习率。

### 解决措施①：生成数据的熵约束

**Mode Collapse**体现在生成数据太集中，因此可以向生成器的目标中加入约束项，使得生成数据更加分散。一种常用的约束是生成数据的熵$H(G(z))$：

$$ \begin{aligned} G^* & \leftarrow \mathop{ \max}_{G}  \Bbb{E}_{z \text{~} P_z}[\log D(G(z))] + H(G(z)) \end{aligned} $$

### 解决措施②：GAN的集成

**Ensemble**，即在**GAN**的训练过程中同时训练多个生成器，每次随机选择一个生成器来生成样本。


# 5. 生成对抗网络的各种变体

生成对抗网络的设计是集目标函数、网络结构、优化过程于一体的。本节分别从这三个方面介绍**GAN**的各种变体：
- 改进目标函数：选用**f**散度(如**f-GAN**, **BGAN**, **RGAN**)、均方误差(如**LSGAN**)、**Wasserstein**距离(如**WGAN**, **WGAN-GP**, **SN-GAN**, **GN-GAN**, **GraN-GAN**, **c-transform**)、**W**散度(如**WGAN-div**)、平方势散度(如**GAN-QP**)
- 改进网络结构：使用卷积神经网络(如**DCGAN**, **ResNet**)、引入编码器(如**VAE-GAN**, **BiGAN**)、使用能量模型(如**EBGAN**, **LSGAN**, **BEGAN**, **MAGAN**, **MEG**)、
- 改进优化过程：使训练更快收敛、解决低维流形中分布不匹配、
- 其他应用：条件生成(如**CGAN**, **InfoGAN**, **ACGAN**)、图像翻译(有配对数据, 如**CGAN**, **InfoGAN**, **ACGAN**; 无配对数据, 如**CGAN**, **InfoGAN**, **ACGAN**)、图像修补


近些年来$GAN$的发展：

![](https://pic.downk.cc/item/5ed86506c2a9a83be5b4e88c.jpg)

**GAN**的演化：
- 领域迁移：cycleGAN、GauGAN、GANILLA
- 图像生成：DCGAN、LAPGAN、SAGAN、BigGAN
- 特殊的模型：SinGAN（使用单张图像训练）、NICE-GAN（生成器是判别器的一部分）、StackGAN（文本生成图像）


## （1）改进目标函数

根据前述分析可知，当判别器$D$取得最优时，**GAN**的目标函数衡量了真实分布$$P_{data}(x)$$与生成分布$$P_G(x)$$之间的**JS**散度。

对于**JS**散度，当两个概率分布没有重合时，散度取值通常是固定值，从而导致优化梯度为0。在实践中可以选择具有更平滑的值空间的[分布度量指标](https://0809zheng.github.io/2020/02/03/kld.html)。






| 方法 | 目标函数 |
| :---: | :---: |
| Minimax GAN <br> (MMGAN) |  $$ \begin{aligned} (D^*, G^*) &\leftarrow \mathop{\min}_{G}\mathop{\max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{x \text{~} P_{G}(x)}[\log(1-D(x))] \end{aligned} $$ |
| Non-Saturating GAN <br> (NSGAN) |  $$ \begin{aligned} D^* &\leftarrow \mathop{ \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{x \text{~} P_{G}(x)}[\log(1-D(x))]  \\ G^* & \leftarrow \mathop{ \max}_{G}  \Bbb{E}_{x \text{~} P_{G}(x)}[\log D(x)] \end{aligned} $$ |
| [<font color=Blue>f-GAN</font>](https://0809zheng.github.io/2022/02/07/fgan.html) <br> 使用f散度构造目标函数 | $$ \begin{aligned} (D^*, G^*) & \leftarrow \mathop{\min}_{G} \mathop{\max}_{D \in f'(\Bbb{D})}  \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]- \Bbb{E}_{x \text{~} P_{G}(x)}[f^*(D(x))] \end{aligned} $$ |
| [<font color=Blue>Boundary-Seeking GAN <br> (BGAN) </font>](https://0809zheng.github.io/2022/02/28/bgan.html) <br> 边界搜索 | $$ \begin{aligned} D^* &\leftarrow \mathop{ \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{x \text{~} P_{G}(x)}[\log(1-D(x))]  \\ G^* & \leftarrow \mathop{ \min}_{G} \Bbb{E}_{x \text{~} P_G(x)}[ \frac{1}{2}(\log D(x) - \log (1-D(x)))^2 ] \end{aligned} $$ |
| [<font color=Blue>Relativistic GAN <br> (RGAN)</font>](https://0809zheng.github.io/2022/02/21/rgan.html) <br> 相对判别器 | $$ \begin{aligned} D^* &\leftarrow \mathop{ \min}_{D}  -\Bbb{E}_{x_r \text{~} P_{data}(x), x_f \text{~} P_{G}(x)}[\log \sigma(D(x_r)-D(x_f))] \\ G^* & \leftarrow \mathop{ \min}_{G}- \Bbb{E}_{x_r \text{~} P_{data}(x), x_f \text{~} P_{G}(x)}[\log \sigma(D(x_f)-D(x_r))] \end{aligned} $$ |
| [<font color=Blue>Least Squares GAN <br> (LSGAN) </font>](https://0809zheng.github.io/2022/02/15/lsgan.html) <br> 使用均方误差构造目标函数 | $$ \begin{aligned} D^* &\leftarrow \mathop{ \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[(D(x)-b)^2] + \Bbb{E}_{x \text{~} P_{G}(x)}[(D(x)-a)^2]  \\ G^* & \leftarrow \mathop{ \max}_{G}  \Bbb{E}_{x \text{~} P_{G}(x)}[(D(x)-c)^2] \end{aligned} $$ |
| [<font color=Blue>Wasserstein GAN <br> (WGAN)</font>](https://0809zheng.github.io/2022/02/04/wgan.html) <br> 使用W距离构造目标函数 | $$ \begin{aligned} (D^*, G^*) & \leftarrow \mathop{ \min}_{G} \mathop{ \max}_{D,\|D\|_L \leq K} \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]-\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)]  \end{aligned} $$ |
| [<font color=Blue>Wasserstein GAN <br> gradient penalty <br> (WGAN-GP)</font>](https://0809zheng.github.io/2022/02/06/wgangp.html) <br> 使用梯度惩罚约束Lipschitz连续性 | $$ \begin{aligned} D^* \leftarrow\mathop{ \max}_{D} & \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]-\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \\ & - λ \Bbb{E}_{x \text{~} \epsilon P_{data}(x) + (1-\epsilon)P_{G}(x) }[(\|\| \nabla_xD(x) \|\| -1)^2]  \\ G^* \leftarrow  \mathop{ \min}_{G}&  -\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \end{aligned} $$ |
| [<font color=Blue>Spectral Normalized GAN <br> (SN-GAN) </font>](https://0809zheng.github.io/2022/02/08/sngan.html) <br> 使用谱归一化约束Lipschitz连续性 | $$ \begin{aligned} D^* & \leftarrow \mathop{ \max}_{D} \Bbb{E}_{x \text{~} P_{data}(x)}[\max(0,1-D(x))]-\Bbb{E}_{x \text{~} P_{G}(x)}[\max(0,1+D(x))] \\  G^* & \leftarrow \mathop{ \min}_{G} -\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \end{aligned} $$   |
| [<font color=Blue>Gradient Normalized GAN <br> (GN-GAN) </font>](https://0809zheng.github.io/2022/02/10/gngan.html) <br> 使用梯度归一化约束Lipschitz连续性 | $$ \begin{aligned} D^* & \leftarrow\mathop{ \max}_{D}  \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]-\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \\ D^*  & \leftarrow \frac{D(x)}{\|\|\nabla_x D(x)\|\|+\|D(x)\|} \\ G^* & \leftarrow  \mathop{ \min}_{G}  -\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \end{aligned} $$ |
| [<font color=Blue>Gradient Normalized GAN v2 <br> (GraN-GAN)</font>](https://0809zheng.github.io/2022/02/11/grangan.html) <br> 使用梯度归一化约束Lipschitz连续性 | $$ \begin{aligned} D^* & \leftarrow\mathop{ \max}_{D}  \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]-\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \\ D^*  & \leftarrow \frac{D(x) \cdot \|\|\nabla_x D(x)\|\|}{\|\|\nabla_x D(x)\|\|^2+\epsilon} \\ G^* & \leftarrow  \mathop{ \min}_{G}  -\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \end{aligned} $$ |
| [<font color=Blue>c-transform WGAN</font>](https://0809zheng.github.io/2022/02/12/ctrans.html) <br> 更精确的W距离近似 | $$ \begin{aligned} (D^*, G^*) \leftarrow \mathop{ \min}_{G} \mathop{ \max}_{D,\|D\|_L \leq K} & \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)] \\ & +\Bbb{E}_{x \text{~} P_{G}(x)}[\mathop{\min}_{\tilde{x} \text{~} P_{data}(x)} \{ c(\tilde{x}-x)- D(\tilde{x})\}]  \end{aligned} $$ |
| [<font color=Blue>Wasserstein Divergence GAN <br> (WGAN-div)</font>](https://0809zheng.github.io/2022/02/09/wgandiv.html) <br> 使用W散度构造目标函数 | $$ \begin{aligned} (D^*, G^*) \leftarrow \mathop{ \min}_{G} \mathop{ \max}_{D} & \Bbb{E}_{x \text{~} P_{data}(x)}[D(x)]-\Bbb{E}_{x \text{~} P_{G}(x)}[D(x)] \\ & - k\Bbb{E}_{x \text{~} r(x)}[ \|\| \nabla_xD(x) \|\|^p ]   \end{aligned} $$ |
| [<font color=Blue>GAN-QP</font>](https://0809zheng.github.io/2022/02/22/ganqp.html) <br> 使用平方势散度构造目标函数 | $$ \begin{aligned} D^* &\leftarrow \mathop{ \max}_{D} \Bbb{E}_{x_r \text{~} P_{data}(x), x_f \text{~} P_{G}(x)}[D(x_r,x_f)-D(x_f,x_r) -\frac{(D(x_r,x_f)-D(x_f,x_r))^2}{2 \lambda d(x_r,x_f)} ]  \\ G^* & \leftarrow \mathop{ \min}_{G}\Bbb{E}_{x_r \text{~} P_{data}(x), x_f \text{~} P_{G}(x)}[D(x_r,x_f)-D(x_f,x_r)] \end{aligned} $$ |










## （2）改进网络结构

### a. 使用卷积神经网络

### ⚪ [<font color=Blue>Deep Convolutional GAN (DCGAN)</font>](https://0809zheng.github.io/2022/02/05/dcgan.html)

**DCGAN**使用卷积神经网络构造生成对抗网络，为了稳定卷积网络的训练过程，作者提出了以下几点设计思路：
1. 去掉网络中的**pooling**层，在判别器中使用**Strided convolution** (步幅卷积)进行下采样，在生成器中使用**transposed convolution**（转置卷积）进行上采样；
2. 在判别器和生成器中使用**batch norm**；
3. 移除网络中的所有全连接层；
4. 生成器的输出层使用**Tanh**激活函数，其他层使用**ReLU**激活函数；
5. 判别器的所有层使用**LeakyReLU**激活函数。

![](https://pic1.imgdb.cn/item/6343747916f2c2beb122d53e.jpg)

### ⚪ ResNet

为进一步增强卷积网络的非线性表示能力，将**ResNet**引入了**GAN**的模型构建，其典型代表为[PGGAN]()。这类网络的主要特点如下：
1. 由于**stride** $>1$的卷积操作存在[棋盘效应]()，因此移除了步幅卷积和转置卷积；在下采样时采用平均池化，在上采样时采用线性插值；
2. 通过增加残差模块的数量，能够同时增加网络的非线性表示能力和深度；
3. 卷积核设置为$3\times 3$；激活函数统一使用**ReLU**；**BatchNorm**可以替换为**InstanceNorm**等；
4. 由于**GAN**的模型初始化通常比较小，将残差形式调整为$x + \alpha f(x); \alpha < 1$具有更好的稳定性。

![](https://pic1.imgdb.cn/item/6349633716f2c2beb1785f6d.jpg)

### b. 引入编码器

### ⚪ [<font color=Blue>VAE-GAN</font>](https://0809zheng.github.io/2022/02/17/vaegan.html)

**VAE-GAN**是一种用**GAN**训练**VAE**（或用**VAE**训练**GAN**）的方法。该模型包括编码器、解码器（生成器）、判别器三部分。编码器把真实图像编码成正态分布$z$；解码器从$z$中采样生成重构图像；判别器区分真实图像和重构图像。

![](https://pic1.imgdb.cn/item/634b621e16f2c2beb18db84c.jpg)


### ⚪ [<font color=Blue>Bidirectional GAN (BiGAN)</font>](https://0809zheng.github.io/2022/02/18/bigan.html)

**BiGAN**既可以将隐空间的噪声分布映射到任意复杂的数据分布，又可以将数据映射回隐空间，以此学习有价值的特征表示。该模型包括编码器、解码器（生成器）、判别器三部分。编码器把真实图像$x$编码成$z$；生成器把$z$解码成重构图像；判别器区分图像$x$和编码$z$是编码器还是解码器提供的。

![](https://pic1.imgdb.cn/item/634b704916f2c2beb1a1df6e.jpg)

### c. 使用能量模型

根据前面的讨论，能量模型角度下**GAN**的目标函数写作：

$$ \begin{aligned} D^* &\leftarrow \mathop{ \min}_{D} \Bbb{E}_{x \text{~} P_{data}(x)} [ D(x)]-  \Bbb{E}_{x \text{~} P_G(x)}[D(x) ]  \\ G^* &\leftarrow \mathop{ \min}_{G} -H(G(z))+\Bbb{E}_{x \text{~} P_G(x)}[D(x) ] \end{aligned} $$

其中判别器$D(x)$用于近似能量函数$U(x)$，不同方法中能量函数的实现形式不同。

若直接优化上述目标，判别器倾向于对真实样本的能量$U(x)$~$D(x) \to -\infty$，对生成样本的能量$U(x)$~$D(x) \to +\infty$，从而导致训练不稳定。因此不同方法会对能量的上下限进行不同的约束。

此外，不同方法对生成图像的熵$H(G(z))$的实现形式也不同。

### ⚪ [<font color=Blue>Energy-based GAN (EBGAN)</font>](https://0809zheng.github.io/2022/02/16/ebgan.html)

判别器采用自编码器形式，能量函数为自编码器的**L2**重构误差：

$$ U(x) = ||D(x)-x||_2 = ||Dec(Enc(x))-x||_2 $$

能量的最小值是$0$，最大值设置为$m$。生成图像的熵$H(G(z))$采用判别器的编码器提取的编码特征的余弦相似度衡量：

$$ \begin{aligned} D^* &\leftarrow \mathop{ \min}_{D} \Bbb{E}_{x \text{~} P_{data}(x)} [  U(x)]+  \Bbb{E}_{x \text{~} P_G(x)}[\max(0, m-U(x)) ] \\ G^* &\leftarrow \mathop{ \min}_{G} \Bbb{E}_{(x_i,x_j) \text{~} P_G(x)}[(\frac{Enc(x_i)^TEnc(x_j)}{||Enc(x_i)|| \cdot ||Enc(x_j)||})^2]+\Bbb{E}_{x \text{~} P_G(x)}[U(x) ] \end{aligned} $$

### ⚪ [<font color=Blue>Loss-Sensitive GAN (LSGAN)</font>](https://0809zheng.github.io/2022/02/25/lsgan.html)

能量函数为判别器$U(x)=D(x)$。

如果生成图像与真实图像差异较大，则把生成图像的能量调整得大一些。

$$ \begin{aligned} D^* \leftarrow \mathop{ \min}_{D} &\Bbb{E}_{x \text{~} P_{data}(x)} [  D(x)] \\ &+  \Bbb{E}_{(x,z) \text{~}(P_{data}(x),P_z(z))}[\max \{ 0, \Delta(x,G(z))+D(x)-D(G(z)) ] \\ G^* \leftarrow \mathop{ \min}_{G} &\Bbb{E}_{x \text{~} P_G(x)}[D(x) ] \end{aligned} $$

### ⚪ [<font color=Blue>Boundary Equilibrium GAN (BEGAN)</font>](https://0809zheng.github.io/2022/02/27/began.html)

判别器采用自编码器形式，能量函数为自编码器的**L1**重构误差：

$$ U(x) = ||D(x)-x||_1 = ||Dec(Enc(x))-x||_1 $$

当生成图像的能量小于$\gamma$倍真实图像的能量时，判别器才会考虑增大生成图像的能量：

$$ \begin{aligned} D^* &\leftarrow \mathop{ \min}_{D} \Bbb{E}_{x \text{~} P_{data}(x)} [  U(x)]-  k_t \Bbb{E}_{x \text{~} P_G(x)}[U(x) ] \\ k_{t+1} &\leftarrow  k_t + \lambda (\gamma U(x)-U(G(z))) \\ G^* &\leftarrow \mathop{ \min}_{G} \Bbb{E}_{x \text{~} P_G(x)}[U(x) ]  \end{aligned} $$


### ⚪ [<font color=Blue>Margin Adaptation GAN (MAGAN)</font>](https://0809zheng.github.io/2022/02/24/magan.html)

判别器采用自编码器形式，能量函数为自编码器的**L2**重构误差：

$$ U(x) = ||D(x)-x||_2 = ||Dec(Enc(x))-x||_2 $$

能量的最小值是$0$，最大值设置为$m_t$；$m_t$随训练轮数增大而减小。

$$ \begin{aligned} D^* &\leftarrow \mathop{ \min}_{D} \Bbb{E}_{x \text{~} P_{data}(x)} [  U(x)]+  \Bbb{E}_{x \text{~} P_G(x)}[\max(0, m_t-U(x)) ] \\ G^* &\leftarrow \mathop{ \min}_{G} \Bbb{E}_{x \text{~} P_G(x)}[U(x) ] \end{aligned} $$

### ⚪ [<font color=Blue>Maximum Entropy Generator (MEG)</font>](https://0809zheng.github.io/2022/02/23/meg.html)

能量函数为判别器$U(x)=D(x)$。

对于判别器，真实样本应该落在能量函数的极小值点附近，因此引入以$0$为中心的梯度惩罚，形式上等价于[<font color=Blue>WGAN-GP</font>](https://0809zheng.github.io/2022/02/06/wgangp.html)的目标函数。

对于生成器，生成图像的熵$H(G(z))$用互信息的下界近似(引入一个编码器$E(x)$估计互信息)。

$$ \begin{aligned}  D^* &\leftarrow  \mathop{ \min}_{D} \Bbb{E}_{x \text{~} P_{data}(x)} [  D(x)]-  \Bbb{E}_{x \text{~} P_G(x)}[D(x) ] + \lambda \Bbb{E}_{x \text{~} P_{data}(x)} [ || \nabla_x D(x) ||^2 ] \\G^*,E^* &\leftarrow \mathop{ \min}_{G,E} \Bbb{E}_{z \text{~} q(z)}[||z-E(G(z))||^2]+\Bbb{E}_{x \text{~} P_G(x)}[D(x) ] \end{aligned} $$

## （3）改进优化过程


### ⚪ [<font color=Blue>Improved Techniques for Training GANs</font>](https://0809zheng.github.io/2022/02/02/improve.html)

本文提出了几点使得**GAN**训练更快收敛的方法：
- **feature matching**：检测生成器的输出是否与真实样本的预期统计值(如均值或中位数)相匹配。
- **minibatch discrimination**：使得判别器了解一批训练样本中的数据点之间的近似程度，而不是独立地处理每个样本。
- **historical averaging**：强迫生成器和判别器的参数接近过去训练过程中的历史平均参数。
- **label smoothing**：设置判别器的标签为软标签(如$0.1$和$0.9$)，以此降低模型的脆弱性。
- **virtual batch normalization**：在进行批归一化时使用一个固定批次(参考批次, 在训练开始时选定)的统计量进行归一化。

### ⚪ [<font color=Blue>Towards Principled Methods for Training Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/02/03/principle.html)

本文提出了几点解决低维流形中分布不匹配的方法：
- 增加噪声：通过在判别器的输入中增加连续噪声，可以人为地扩大真实分布$$P_{data}(x)$$与生成分布$$P_G(x)$$的范围，使得两个概率分布有更大的概率重叠。
- 使用更好的分布相似度度量：**GAN**的损失函数(在一定条件下)衡量真实分布$$P_{data}(x)$$与生成分布$$P_G(x)$$之间的**JS**散度，这在两个分布不相交时没有意义；可以选择具有更平滑值空间的分布度量。

### ⚪ [<font color=Blue>Wasserstein GANs Work Because They Fail (to Approximate the Wasserstein Distance)</font>](https://0809zheng.github.io/2022/02/13/fail.html)

本文提出效果比较好的**WGAN**在训练过程中并没有精确地近似**Wasserstein**距离；相反如果对**Wasserstein**距离做更好的近似，效果反而会变差。主要原因如下：
- 交替训练：**WGAN**的目标函数目标函数只有先精确完成$$\mathop{\max}_{D}$$，然后再进行$$\mathop{ \min}_{G}$$，才相当于优化两个分布的**Wasserstein**距离；在实际训练时采用交替优化，理论上不可能精确逼近分布度量。
- 批量训练：**WGAN**在训练时采用批量训练的方法，导致目标为最小化训练集中两个批量之间的**Wasserstein**距离，该目标仍然大于一个批量与训练集平均样本之间的**Wasserstein**距离。
- 成本函数：**WGAN**的成本函数一般选择欧氏距离$$\|x-y\|_2$$。欧氏距离在衡量两张图像的相似程度时在视觉效果上是不合理的；两张相似的图像对应的欧氏距离不一定小。

## （4）其他应用

### a. 条件生成 (Conditional Generation)

### ⚪ [<font color=Blue>Conditional GAN (CGAN)</font>](https://0809zheng.github.io/2022/03/02/cgan.html)

**CGAN**的生成器接收随机噪声$z$和随机标签$c$，生成给定标签$c$时的图像$x=G(z,c)$；判别器接收图像$x$和对应的标签$c$，判断图像$x$是否为给定标签$c$时的真实图像$D(x\|c)$。
目标函数如下：

$$ \begin{aligned} \mathop{ \min}_{G} \mathop{\max}_{D}  \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x|c)] + \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-D(G(z,c)))] \end{aligned} $$

### ⚪ [<font color=Blue>Information Maximizing GAN (InfoGAN)</font>](https://0809zheng.github.io/2022/03/06/infogan.html)

**InfoGAN**的生成器接收随机噪声$z$和条件编码$c$，生成给定条件$c$时的图像$x=G(z,c)$；判别器$D(x)$接收图像$x$，判断图像$x$是否为真实图像。与判别器共享参数的编码器预测生成图像$x$对应的条件编码$\hat{c}=E(G(c))$。损失函数额外引入条件编码的重构误差（实际上是条件编码$c$和生成图像$G(z,c)$的互信息$I(c;G(z,c))$的一个下界）：

$$ \begin{aligned} \mathop{ \min}_{G,E} \mathop{\max}_{D} & \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-D(G(z,c)))] \\ & +\lambda \Bbb{E}_{z \text{~} P_{Z}(z)}[||c-E(G(z,c))||^2]  \end{aligned} $$

### ⚪ [<font color=Blue>Auxiliary Classifier GAN (ACGAN)</font>](https://0809zheng.github.io/2022/03/03/acgan.html)

**ACGAN**的生成器接收随机噪声$z$和随机标签$c$，生成给定标签$c$时的图像$G(z,c)$；判别器$D(x)$接收图像$x$，判断图像$x$是否为真实图像(二分类)以及是否属于对应的标签$c$(多分类)。目标函数如下：

$$ \begin{aligned}  \mathop{\max}_{D} & \Bbb{E}_{x \text{~} P_{data}(x)}[\log D(x)] + \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(1-D(G(z,c)))] \\ & +  \Bbb{E}_{c,x \text{~} P_{data}(x)}[\log D_c(x)]+  \Bbb{E}_{z \text{~} P_{Z}(z)}[\log D_c(G(z,c))] \\ \mathop{ \min}_{G} & \Bbb{E}_{z \text{~} P_{Z}(z)}[\log(D(G(z,c))] -  \Bbb{E}_{z \text{~} P_{Z}(z)}[\log D_c(G(z,c))] \end{aligned} $$

### b. 图像翻译 (Image-to-Image Translation)


[<font color=Blue>Image-to-Image Translation with Conditional Adversarial Networks</font>](https://0809zheng.github.io/2022/03/10/p2p.html)：(arXiv1611)


[<font color=Blue>Coupled Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/03/08/cogan.html)：(arXiv1606)

[<font color=Blue>Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/03/15/pixelda.html)：(arXiv1612)

[<font color=Blue>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</font>](https://0809zheng.github.io/2022/02/14/cyclegan.html)：(arXiv1703)

[<font color=Blue>Learning to Discover Cross-Domain Relations with Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/03/16/discogan.html)：(arXiv1703)

[<font color=Blue>DualGAN: Unsupervised Dual Learning for Image-to-Image Translation</font>](https://0809zheng.github.io/2022/03/17/dualgan.html)：(arXiv1704)

**本文目录**：

1.  Self-attention GAN (SAGAN)
2.  BigGAN
3.  SinGAN
4.  GauGAN
5.  GANILLA
6.  NICE-GAN
7.  GAN的评估指标
8.  LAPGAN
9.  StackGAN






# 14. SAGAN
**SAGAN(self-attention GAN)**是一种应用于图像生成的$GAN$，主要特点有：
- 使用**自注意力（self-attention）**
- 对$G$和$D$都使用**Spectral Norm（SN）**
- 训练$G$和$D$的学习率不相同（**TTUR**）

### （1）self-attention

![](https://pic.downk.cc/item/5ed864eac2a9a83be5b4cee2.jpg)

### （2）Spectral Norm
在$SNGAN$中，**Spectral Norm**被用于判别器$D$；而在$SAGAN$中，**Spectral Norm**同时用于判别器$D$和生成器$G$。

### （3）TTUR
在$SAGAN$的训练中，判别器$D$和生成器$G$的学习率是不平衡的，这种方法被称作**Two-Timescale Update Rule（TTUR）**。
- 判别器$D$的学习率设置为$0.0004$;
- 生成器$G$的学习率设置为$0.0001$.

# 15. BigGAN
**BigGAN**是在$SAGAN$的基础上使用更大的模型。包括：
- 两到四倍的参数量
- 八倍的$batch$ $size$

$BigGAN$还提出了一种**truncation trick**，用于平衡模型生成某一具体的图像或生成不同类型的图像：

具体地，在对噪声采样时进行截断，若噪声$z$的取值范围越小则生成图像越相近；反之生成的图像越丰富：

![](https://pic.downk.cc/item/5ed868d3c2a9a83be5ba48c5.jpg)

# 16. SinGAN
**SinGAN**通过使用**单张**图像来训练$GAN$。

具体的做法是将单张图像进行不同尺度的裁剪，用来生成大量的子图像，

并在每次训练时采用**progressively training**：

![](https://pic.downk.cc/item/5ed86a5ac2a9a83be5bcfa93.jpg)

# 17. GauGAN
**GauGAN**给定一张绘制图像和一张实际图像，希望能够生成具有真实图像风格的绘制图像：

![](https://pic.downk.cc/item/5ed86e88c2a9a83be5c47fae.jpg)

网络结构如下图所示，其主要步骤如下：
1. 对输入的实际图像进行编码，产生均值和方差，用来构建噪声分别；
2. 生成器接收输入的绘制图像和采样噪声，用来生成图像；
3. 判别器判断实际图像与生成图像，以及生成图像是否与绘制图像相似。

![](https://pic.downk.cc/item/5ed86cd4c2a9a83be5c1735e.jpg)

网络还提出了一种新的Normalization方法：**SPADE**，在标准化过程中引入绘制图像：

![](https://pic.downk.cc/item/5ed86c8fc2a9a83be5c0f2eb.jpg)

# 18. GANNILLA
**GANNILLA**实现了图像的**domain transform**，即把一张现实图像转换成具有儿童绘本风格的插图。

该论文提出了一个儿童绘本图像数据集，网络结构如下：

![](https://pic.downk.cc/item/5ed870b3c2a9a83be5c7d6e8.jpg)

该网络加入了一些残差连接、实例标准化和局部级联。

# 19. NICE-GAN
**NICE-GAN**提出了一种不显式的使用生成器的方法，而是使用判别器的前半部分作为生成器：

![](https://pic.downk.cc/item/5ed8710fc2a9a83be5c85b3e.jpg)



# 21. LAPGAN
- LAPGAN：Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks.

在原始 GAN和CGAN中，还只能生成$16×16$,$32×32$这种低像素小尺寸的图片。

而**LAPGAN**首次实现$64×64$的图像生成。采用**coarse-to-fine**的思路，与其一下子生成这么大的（包含信息量这么多），不如一步步由小到大，这样每一步生成的时候，可以基于上一步的结果，而且还只需要“填充”和“补全”新图片所需要的那些信息。

![](https://pic.downk.cc/item/5ee7304bc2a9a83be50d3720.jpg)

# 22. StackGAN
解决文本到图像生成分辨率不高的问题，采用与LAPGAN相似的思路，采用coarse-to-fine的思路，构建两个GAN。
- 第一个GAN用于根据文本描述生成一张低分辨率的图像。
- 第二个GAN将低分辨率图像和文本作为输入，修正之前生成的图像，添加细节纹理，生成高分辨率图像。

![](https://pic.downk.cc/item/5ee7314fc2a9a83be50ed01c.jpg)



# ⚪ 参考文献



- [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)：(arXiv1406)GAN的原始论文。
- [From GAN to WGAN](https://lilianweng.github.io/posts/2017-08-20-gan/)：Blog by Lilian Weng.
- [互怼的艺术：从零直达WGAN-GP](https://spaces.ac.cn/archives/4439)：Blog by 苏剑林.
- [The GAN Zoo](https://github.com/AntixK/PyTorch-VAE)：(github)A list of all named GANs!
- [PyTorch-GAN: PyTorch implementations of Generative Adversarial Networks](https://github.com/eriklindernoren/PyTorch-GAN)：(github)GAN的PyTorch实现。
- [<font color=Blue>Conditional Generative Adversarial Nets</font>](https://0809zheng.github.io/2022/03/02/cgan.html)：(arXiv1411)CGAN：条件生成对抗网络。
- [<font color=Blue>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/02/05/dcgan.html)：(arXiv1511)DCGAN：使用深度卷积神经网络构造GAN。
- [<font color=Blue>Autoencoding beyond pixels using a learned similarity metric</font>](https://0809zheng.github.io/2022/02/17/vaegan.html)：(arXiv1512)VAE-GAN：结合VAE和GAN。
- [<font color=Blue>Adversarial Feature Learning</font>](https://0809zheng.github.io/2022/02/18/bigan.html)：(arXiv1605)BiGAN：使用双向GAN进行对抗特征学习。
- [<font color=Blue>Improved Techniques for Training GANs</font>](https://0809zheng.github.io/2022/02/02/improve.html)：(arXiv1606)训练生成对抗网络的改进技巧。
- [<font color=Blue>f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization</font>](https://0809zheng.github.io/2022/02/07/fgan.html)：(arXiv1606)fGAN：通过f散度构造GAN。
- [<font color=Blue>InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</font>](https://0809zheng.github.io/2022/03/06/infogan.html)：(arXiv1606)InfoGAN：通过最大化互信息实现可插值的表示学习。
- [<font color=Blue>Coupled Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/03/08/cogan.html)：(arXiv1606)CoGAN：耦合生成对抗网络。
- [<font color=Blue>Energy-based Generative Adversarial Network</font>](https://0809zheng.github.io/2022/02/16/ebgan.html)：(arXiv1609)EBGAN：基于能量的生成对抗网络。
- [<font color=Blue>Conditional Image Synthesis With Auxiliary Classifier GANs</font>](https://0809zheng.github.io/2022/03/03/acgan.html)：(arXiv1610)ACGAN：基于辅助分类器GAN的条件图像合成。
- [<font color=Blue>Least Squares Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/02/15/lsgan.html)：(arXiv1611)LSGAN：使用均方误差构造目标函数。
- [<font color=Blue>Image-to-Image Translation with Conditional Adversarial Networks</font>](https://0809zheng.github.io/2022/03/10/p2p.html)：(arXiv1611)Pix2Pix：通过UNet和PatchGAN实现图像转换。
- [<font color=Blue>Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/03/15/pixelda.html)：(arXiv1612)PixelDA：通过GAN实现像素级领域自适应。
- [<font color=Blue>Towards Principled Methods for Training Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/02/03/principle.html)：(arXiv1701)训练生成对抗网络的原则性方法。
- [<font color=Blue>Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities</font>](https://0809zheng.github.io/2022/02/25/lsgan.html)：(arXiv1701)LSGAN：损失敏感GAN。
- [<font color=Blue>BEGAN: Boundary Equilibrium Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/02/27/began.html)：(arXiv1703)BEGAN：边界平衡GAN。
- [<font color=Blue>Wasserstein GAN</font>](https://0809zheng.github.io/2022/02/04/wgan.html)：(arXiv1701)WGAN：使用Wasserstein距离构造GAN。
- [<font color=Blue>Boundary-Seeking Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/02/28/bgan.html)：(arXiv1702)BGAN：边界搜索GAN。
- [<font color=Blue>Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</font>](https://0809zheng.github.io/2022/02/14/cyclegan.html)：(arXiv1703)CycleGAN：使用循环一致损失实现无配对数据的图像转换。
- [<font color=Blue>Learning to Discover Cross-Domain Relations with Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/03/16/discogan.html)：(arXiv1703)DiscoGAN：使用GAN学习发现跨领域关系。
- [<font color=Blue>DualGAN: Unsupervised Dual Learning for Image-to-Image Translation</font>](https://0809zheng.github.io/2022/03/17/dualgan.html)：(arXiv1704)DualGAN：图像转换的无监督对偶学习。
- [<font color=Blue>Improved Training of Wasserstein GANs</font>](https://0809zheng.github.io/2022/02/06/wgangp.html)：(arXiv1704)WGAN-GP：在WGAN中引入梯度惩罚。
- [<font color=Blue>MAGAN: Margin Adaptation for Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/02/24/magan.html)：(arXiv1704)MAGAN：自适应调整EBGAN的能量边界。
- [<font color=Blue>Wasserstein Divergence for GANs</font>](https://0809zheng.github.io/2022/02/09/wgandiv.html)：(arXiv1712)WGAN-div：通过Wasserstein散度构造GAN。
- [<font color=Blue>Spectral Normalization for Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/02/08/sngan.html)：(arXiv1802)SN-GAN：在WGAN中引入谱归一化。
- [<font color=Blue>The relativistic discriminator: a key element missing from standard GAN</font>](https://0809zheng.github.io/2022/02/21/rgan.html)：(arXiv1807)RGAN：GAN中的相对判别器。
- [<font color=Blue>GAN-QP: A Novel GAN Framework without Gradient Vanishing and Lipschitz Constraint</font>](https://0809zheng.github.io/2022/02/22/ganqp.html)：(arXiv1811)GAN-QP：在对偶空间定义没有梯度消失且满足Lipschitz约束的目标。
- [<font color=Blue>Maximum Entropy Generators for Energy-Based Models</font>](https://0809zheng.github.io/2022/02/23/meg.html)：(arXiv1901)MEG：基于能量模型的最大熵生成器。
- [<font color=Blue>How Well Do WGANs Estimate the Wasserstein Metric?</font>](https://0809zheng.github.io/2022/02/12/ctrans.html)：(arXiv1910)讨论WGAN与Wasserstein距离的近似程度。
- [<font color=Blue>Wasserstein GANs Work Because They Fail (to Approximate the Wasserstein Distance)</font>](https://0809zheng.github.io/2022/02/13/fail.html)：(arXiv2103)WGAN的表现与Wasserstein距离的近似程度没有必然联系。
- [<font color=Blue>Gradient Normalization for Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/02/10/gngan.html)：(arXiv2109)GN-GAN：在WGAN中引入梯度归一化。
- [<font color=Blue>GraN-GAN: Piecewise Gradient Normalization for Generative Adversarial Networks</font>](https://0809zheng.github.io/2022/02/11/grangan.html)：(arXiv2111)GraN-GAN：在WGAN中引入分段线性的梯度归一化。