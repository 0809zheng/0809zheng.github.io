---
layout: post
title: '文本摘要'
date: 2020-05-13
author: 郑之杰
cover: 'https://pic.downk.cc/item/5ed5e96ec2a9a83be54a8149.jpg'
tags: 深度学习
---

> Text Summarization.

随着近几年文本信息的爆发式增长，从大量文本信息中提取重要的内容，已成为一个迫切需求，而**自动文本摘要（automatic text summarization）**则提供了一个高效的解决方案。

**文本摘要**是利用计算机按照某类应用自动地将文本（或文本集合）转换生成简短摘要的一种信息压缩技术。

要求：
- 足够的信息量
- 较低的冗余度
- 较高的可读性

本文目录：
1. 数据集和评价指标
2. 抽取式摘要
3. 生成式摘要


# 1. 数据集和评价指标

### Benchmarks
文本生成摘要技术最常用的公开数据集是**CNN/DailyMail**。

该数据集是2016年IBM Watson公开的，用于多句文本摘要任务，为此后大量的相关工作提供了数据保障。

论文：Abstractive text summarization using sequence-to-sequence rnns and beyond

### Evaluation
目前文本摘要生成领域应用最广泛的自动评价指标是**ROUGE(Recall-Oriented Understudy for Gisting Evaluation)**。

ROUGE是Lin提出的一个指标集合，包括一些衍生的指标，最常用的有ROUGE-n，ROUGE-L：
- **ROUGE-n**：该指标旨在通过比较生成的摘要和参考摘要的n-grams（连续的n个词）评价摘要的质量。常用的有ROUGE-1，ROUGE-2，ROUGE-3。
- **ROUGE-L**：不同于ROUGE-n，该指标基于最长公共子序列（LCS）评价摘要。如果生成的摘要和参考摘要的LCS越长，那么认为生成的摘要质量越高。

# 2. 抽取式摘要
**抽取式摘要**就是从原文中抽取一些句子组成摘要，这本质上是一种排序问题，通过一定的算法对原文中的句子进行重要性评分，抽取高分句子，去除冗余得到摘要。
- 优点：抽取式方法主要考虑句子的重要性，直接从原文中抽取已有的句子组成摘要，方法简单易实现，因此应用比较广泛。
- 缺点：缺乏语义信息，不符合摘要生成的本质。

### Text Rank排序算法
**Text Rank排序算法**的步骤：
1. 去除原文中的一些停用词，度量原文中每个句子的相似度，计算每一句相对于另一句的相似度得分，迭代传播，直至误差小于某一个范围。
2. 对关键句子进行排序，根据摘要长度选择一定数量的句子组成摘要。

# 3. 生成式摘要
**生成式摘要**是
改写或者重新组织原文生成摘要。具体地，生成式方法首先根据输入文本获得对原文本的语义理解，然后使用任意的单词或者其他表示来生成文本摘要。随着深度学习技术和序列到序列模型的发展，生成式文本摘要成为一种主流的摘要生成方法。
- 优点：对原文有更全面的把握，更符合摘要的本质。
- 缺点：句子的可读性、流畅度等不如抽取式方法。

生成式文本摘要生成的框架是**序列到序列模型（seq2seq）**，也就是**编解码(encoder-decoder)**结构。

编码器和解码器分别由循环神经网络构成，普遍的做法是：
- 使用**双向循环神经网络构成编码器**，将原文进行编码，
- 使用**单向循环神经网络构成解码器**，负责从编码器所生成的向量中提取语义信息，生成文本摘要。

生成式摘要的关键技术：
- **注意力机制（Attention mechanism）**
- **指针机制 （Pointer mechanism）**
- **覆盖机制 （Coverage mechanism）**

# 注意力机制
原理见[注意力机制](https://0809zheng.github.io/2020/04/22/attention.html)。

由于“长距离依赖”问题的存在，RNN到最后一个时间步输入单词时，已经丢失了相当一部分信息。

此时编码生成的语义向量C同样也丢失了大量信息，就可能导致生成摘要准确性不足。

与机器翻译相同，为了解决这个问题，在摘要生成的任务中同样使用了注意力机制。

基于注意力机制的生成式文本摘要模型图：

![](https://pic.downk.cc/item/5ed5e624c2a9a83be546cb9d.jpg)

# 指针机制
问题：难以准确复述原文的事实细节、无法处理原文中的未登录词(OOV)

**pointer-generator network**是 seq2seq 模型和 pointer network 的混合模型。一方面通过seq2seq模型保持抽象生成的能力，另一方面通过指针机制直接从原文中取词，提高摘要的准确度和缓解OOV问题。

在预测的每一步，通过动态计算一个生成概率$p_{gen}$把二者软性地结合起来。这样，在每一步单词的概率分布计算如下:

$$ P(w) = p_{gen}P_{vocab}(w)+(1-p_{gen})\sum_{i:w_i=w}^{} {a_i^t} $$

其中，$p_{gen}$表示使用序列到序列模型生成该单词的概率。除此之外，会有$1-p_{gen}$的概率，在每次摘要生成过程中，把原文动态地加入到词表中去，并且在每一步的预测过程中，相比于单纯的seq2seq模型，选原文中出现的词作为摘要的概率要更大一些。

![](https://pic.downk.cc/item/5ed5e834c2a9a83be5491e7b.jpg)

# 覆盖机制
问题：生成的摘要中存在重复的片段

文本生成问题通常面临着重复问题，see等人将用于机器翻译的覆盖机制应用到摘要生成问题上，取得了有效的结果。

覆盖机制就是在预测的过程中，维护一个$coverage$向量：

$$ c^t = \sum_{t'=0}^{t-1} {a^{t'}} $$

$coverage$向量表示过去每一步预测中$attention$分布的累积和，记录着模型已经关注过原文的那些词并且让这个$coverage$向量影响当前步的$attention$计算。这样能有效避免模型持续关注到某些特定的词上。