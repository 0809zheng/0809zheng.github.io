---
layout: post
title: '池化方法'
date: 2021-07-02
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60dec5105132923bf86c3fca.jpg'
tags: 深度学习
---

> Pooling Layers.

**池化(pooling)**是卷积神经网络中的重要组成部分。通过池化可以对特征图(**feature map**)进行降采样，从而降低网络的模型参数量和计算成本，也在一定程度上降低过拟合的风险。池化的作用包括：
1. 通过降采样增大网络的感受野
1. 通过信息提取抑制噪声，降低信息的冗余
1. 通过减小特征图降低模型计算量，降低网络优化难度，减少过拟合的风险
1. 使模型对输入图像中的特征位置变化更加鲁棒

本文介绍卷积神经网络中通用的池化方法，包括：

1. **Max Pooling** 最大池化
2. **Average Pooling** 平均池化
3. **Global Average Pooling** 全局平均池化
4. **Mix Pooling** 混合池化
5. **Stochastic Pooling** 随机池化
1. **S3 Pooling** 随机空间采样池化
6. **Power Average Pooling** 幂平均池化
1. **Covariance Pooling** 协方差池化
7. **Detail-Preserving Pooling** 细节保留池化
8. **Local Importance Pooling** 局部重要性池化
9. **Soft Pooling** 软池化

值得一提的是，为下游任务设计的**task-specific**池化方法，如空间金字塔池化,**RoI**池化，均不在本文中讨论。

# 1. Max Pooling
**最大池化(Max Pooling)**是将输入的图像特征划分为若干个可重叠的子区域，对每个子区域$R$输出其最大值：

$$ \tilde{a} = \mathop{\max}_{i \in R} a_i $$

最大池化只选择每个矩形子区域中的最大值，即提取特征图中响应最强烈的部分进入下一层，这种操作能够过滤网络中大量的冗余信息，但也会丢失特征图中的一些细节信息。

# 2. Average Pooling
**平均池化(Average Pooling)**是将输入的图像特征划分为若干个可重叠的子区域，对每个子区域$R$输出其平均值：

$$ \tilde{a} = \sum_{i \in R}^{} \frac{a_i}{|R|} $$

平均池化选择每个矩形子区域中的平均值，可以提取特征图中所有特征的信息，从而保留更多的图像背景信息。

# 3. Global Average Pooling
- 论文：[Network In Network](https://arxiv.org/abs/1312.4400)

早期的卷积神经网络设计中，在卷积层和池化层后通常将特征图展开成一维向量，并设置全连接层用于降维和后续的分类等任务。由于全连接层具有较多参数量，会降低网络的训练速度并引入过拟合的风险。**全局平均池化(Global Average Pooling,GAP)**是将整个特征图中每个通道的所有元素取平均输出到**Softmax**层。

![](https://pic.imgdb.cn/item/60defebc5132923bf88a15d7.jpg)

**GAP**相当于为每一个分类类别生成一个通道的特征图，使用该特征图的平均值作为该类别的概率。它使得卷积神经网络对输入尺寸不再有限制(全连接层需要输入尺寸是固定的)，并且没有引入额外的参数，因此也不存在过拟合的风险。**GAP**也可以看作一种正则化的方法，因为它显式地把最后一层特征图作为类别置信度图。

# 4. Mix Pooling
- 论文：[Mixed Pooling for Convolutional Neural Networks](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.678.7068&rep=rep1&type=pdf)

最大池化和平均池化各有利弊，**混合池化(Mix Pooling)**则通过引入一个取值为$0$或$1$的随机数$\lambda$，每次池化操作随机地选用其中之一：

$$ \tilde{a} = \lambda \cdot \mathop{\max}_{i \in R} a_i + (1-\lambda) \cdot \sum_{i \in R}^{} \frac{a_i}{|R|} $$

混合池化优于传统的最大池化和平均池化，缓解了过拟合的风险；并且该方法所需的计算开销可以忽略不计。

# 5. Stochastic Pooling
- 论文：[Stochastic Pooling for Regularization of Deep Convolutional Neural Networks](https://arxiv.org/abs/1301.3557v1)

**随机池化(Stochastic Pooling)**的思路是先对特征图子区域中的元素进行归一化，得到概率矩阵。再按其概率随机地选择一个方格(按照多项分布采样`multinomial()`)，将该方格对应的元素作为池化输出结果:

![](https://pic.imgdb.cn/item/60df081c5132923bf8b69c29.jpg)

在反向传播时，只需保留前向传播被选中方格的索引，将梯度反传到该位置，其实现过程与最大池化的反向传播类似。随机池化特征图中的元素按照其概率值大小随机选择，数值大的元素被选中的概率也大，它不像最大池化只取最大元素值，因此随机池化具有更强的泛化能力。

# 6. S3 Pooling
- 论文：[S3Pool: Pooling with Stochastic Spatial Sampling](https://arxiv.org/abs/1611.05138)

之前介绍的最大池化或随机池化可以被拆分成两步，第一步是使用给定尺寸为$k \times k$的对应池化核按步长为$1$沿特征图滑动，进行对应的最大或平均操作，生成尺寸不变的新特征图；第二步是使用给定步长$s$等间隔地从中采样，获取降采样后的特征图。

![](https://pic.imgdb.cn/item/60e3c6705132923bf8025c0b.jpg)

**S3 (Stochastic Spatial Sampling)**池化修改了第二步降采样过程。对于输入尺寸为$h \times w$的特征图，给定超参数**栅格尺寸(grid size)** $g$，将特征图划分为$\frac{h}{g} \times \frac{w}{g}$个子区域。在每个区域中随机采样$\frac{g}{s}$行$\frac{g}{s}$列，即可得到降采样后尺寸为$\frac{h}{s} \times \frac{w}{s}$的特征图。

![](https://pic.imgdb.cn/item/60dfc5ae5132923bf882c20c.jpg)

**S3**池化相比于随机池化增加了更多的随机性，相当于一个更强的正则化方法，能够提高模型的表达能力。此外**S3**池化可以看作一种隐式的针对特征图的数据增强方法。

# 7. Power Average Pooling
- 论文：[Signal recovery from Pooling Representations](https://arxiv.org/abs/1311.4025)

**幂平均池化(Power Average Pooling)**是将输入的图像特征划分为若干个可重叠的子区域，对每个子区域$R$输出其$L_p$范数：

$$ \tilde{a} = \sqrt[p]{\sum_{i \in R}^{} a_i^p} $$

当$p=1$时，该池化方法与平均池化仅相差一个常数；当$p→∞$时，该池化方法等价于最大池化。

# 8. Covariance Pooling
- 论文：[Skip-Connected Covariance Network for Remote Sensing Scene Classification](https://ieeexplore.ieee.org/document/8759970)

**协方差池化(Covariance Pooling)**是指计算特征图的协方差，从中提取高阶统计信息，提高模型的特征提取能力。流程如下：

![](https://pic.imgdb.cn/item/60e3eb325132923bf8ec40f3.jpg)

对于尺寸为$D \times H \times W$的特征图，首先将其变换为特征矩阵$X \in \Bbb{R}^{D \times N}$，其中$N = H \times W$。之后计算该矩阵的协方差矩阵：

$$ X → C, C = X \hat{I} X^T $$

其中散点矩阵$\hat{I}=\frac{1}{N}(1-\frac{1}{N}11^T)$。若协方差矩阵$C$的特征向量和特征值为$U$,$\Sigma$，即$C=U \Sigma U^T$；使用矩阵对数将协方差矩阵从流行空间投影到欧氏空间，获得池化后的特征：

$$ C → F, F = Ulog(\Sigma)U^T  $$

由于$F$是对称矩阵，只取其右上角元素。将其展开后作为输出特征向量$f$，并进一步执行后续任务。特征向量$f$的长度为$\frac{D(D+1)}{2}$。


# 9. Detail-Preserving Pooling
- 论文：[Detail-Preserving Pooling in Deep Networks](https://arxiv.org/abs/1804.04076)

最大池化只关注激活值最大的像素点，没有考虑像素之间的相关性；平均池化考虑邻域内的所有像素点，但采取平均的方法并不合适。**细节保留池化(Detail-Preserving Pooling,DPP)**认为不同像素对中心像素的影响是不同的，因此引入通过可学习参数$\alpha$,$\lambda$控制的权重$w$。对于图像$I$中像素点$p$处的池化计算，考虑其邻域$\Omega_{p}$中的任意像素点$q$对其影响：

$$ D_{\alpha,\lambda}(I)[p] =\frac{1}{\sum_{q' \in \Omega_{p}}^{} w_{\alpha,\lambda}[p,q']} \sum_{q \in \Omega_{p}}^{} w_{\alpha,\lambda}[p,q]I[q] $$

权重$w$受可学习参数$\alpha$和$\lambda$控制：

$$ w_{\alpha,\lambda}[p,q] = \alpha + ρ_{\lambda}(I[q]-\tilde{I}[p]) , \quad \tilde{I}_F[p]=\sum_{q \in \tilde{\Omega}_p}^{}F[q]I[q] $$

其中$\alpha$是一个全局的奖励参数，$\lambda$用于控制一个奖励函数$ρ(\cdot)$决定像素$q$对像素$p$的影响。$ρ(\cdot)$有两种形式：
- 对称(**symmetric**)形式:

$$ ρ_{\text{Sym}(x)} = (\sqrt{x^2+\epsilon^2})^{\lambda} $$

- 非对称(**asymmetric**)形式:

$$ ρ_{\text{Asym}(x)} = (\sqrt{max(0,x)^2+\epsilon^2})^{\lambda} $$

**DPP**池化可以放大空间变化并保留重要的图像结构细节，引入可学习的参数控制细节的保存量；此外，**DPP**池化可以与随机池化等方法结合使用，以进一步提高准确率。

# 10. Local Importance Pooling
- 论文：[LIP: Local Importance-based Pooling](https://arxiv.org/abs/1908.04156)

**局部重要性池化(Local Importance Pooling,LIP)**是一种通过学习基于输入的自适应重要性权重，从而在采样过程中自动增强特征提取能力的池化方法。具体地，通过一个子网络自动学习输入特征的重要性；这也可以看作是一种局部注意力方法，通过局部卷积产生注意力权值，并在局部归一化。

对于输入特征$I$，首先生成一个自适应重要性权重图$F(I)$(与输入特征尺寸相同，代表每个像素位置的重要性)。为了使重要性权重非负且易于优化，使用指数构造权重图：

$$ F(I)=exp(\mathcal{G}(I)) $$

其中$\mathcal{G}(\cdot)$被称为**对数模块(Logit Module)**，因为其为重要性权重的对数。在实践中，对数模块$\mathcal{G}(\cdot)$是由一个子网络实现的，作者给出了两种实现结构：

![](https://pic.imgdb.cn/item/60e406b75132923bf8bc8d2e.jpg)

在池化过程中，对于特征位置$(x,y)$，选取其邻域$\Omega$，对该邻域的重要性权重进行归一化后作用于原特征的局部滑动窗口，计算得到池化后的特征：

$$ O_{x',y'} = \frac{\sum_{(\Delta x,\Delta y) \in \Omega}^{} F(I)_{x+\Delta x,y+\Delta y}I_{x+\Delta x,y+\Delta y}}{\sum_{(\Delta x,\Delta y) \in \Omega}^{} F(I)_{x+\Delta x,y+\Delta y}} $$

**LIP**可以自适应地学习具有可判别性的特征图，汇总下采样特征同时丢弃无信息特征。这种池化机制能保留物体大部分细节，对于细节信息异常丰富的任务至关重要。

# 11. Soft Pooling
- 论文：[Refining activation downsampling with SoftPool](https://arxiv.org/abs/2101.00440)

**软池化(soft pooling)**可以在保持池化层功能的同时尽可能减少池化过程中带来的信息损失，更好地保留信息特征并改善网络的性能。与最大池化等不同，软池化是可微的，通过反向传播回传梯度，有助于网络进一步训练。

软池化的步骤如下：
1. 通过滑动窗口在特征图上选择局部区域$R$；
2. 通过**softmax**计算特征区域的每个像素点权重:
$$ w_i = \frac{e^{a_i}}{\sum_{j \in R}^{}e^{a_i}} $$
3. 将相应的特征数值与权重相乘后求和获得输出结果:
$$ \tilde{a} = \sum_{i \in R}^{}w_i \cdot a_i $$

![](https://pic.imgdb.cn/item/60e40b3a5132923bf8dbee8a.jpg)

软池化使得整体的局部数值都有所贡献，并且重要的特征占有较高的权重，能够保留更多信息。