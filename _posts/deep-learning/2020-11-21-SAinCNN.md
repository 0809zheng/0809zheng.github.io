---
layout: post
title: '卷积神经网络中的自注意力(Self-Attention)机制'
date: 2020-11-21
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63b80741be43e0d30edb9553.jpg'
tags: 深度学习
---

> Self-Attention Mechanism in Convolutional Neural Networks.

卷积神经网络中的**自注意力(Self-Attention)**机制表现为**非局部滤波(non-local filtering)**操作，其实现过程与[<font color=Blue>Seq2Seq模型的自注意力机制</font>](https://0809zheng.github.io/2020/04/24/self-attention.html)类似。

标准的卷积层是一种局部滤波操作，其输出特征上的任意位置是由对应输入特征的一个邻域构造的，只能捕捉局部特征之间的关系。而自注意力机制通过计算任意两个位置之间的关系直接捕捉远程依赖，而不用局限于相邻点，相当于构造了一个**和特征图尺寸一样大**的卷积核，从而可以捕捉更多信息。

![](https://pic.imgdb.cn/item/63b808dabe43e0d30edf6519.jpg)

在卷积网络的自注意力机制中，首先构造输入特征$x$的键特征$f(x)$, 查询特征$g(x)$和值特征$h(x)$；然后应用点积注意力构造自注意力特征图：

$$ \alpha_{i} = \text{softmax}\left(f(x_i)^Tg(x_j)\right) =\frac{e^{f(x_i)^Tg(x_j)}}{\sum_j e^{f(x_i)^Tg(x_j)}} $$

在计算输出位置$i$的响应$y_i$时，考虑所有输入值特征$h(x_j)$的加权：

$$ y_i=  \sum_{j}^{} \alpha_{j}h(x_j) =  \sum_{j}^{} \frac{e^{f(x_i)^Tg(x_j)}}{\sum_k e^{f(x_i)^Tg(x_k)}} h(x_j) $$

上式可以被写作更一般的形式：

$$
y_i=\frac{1}{C\left(x_i\right)} \sum_j f\left(x_i, x_j\right) h\left(x_j\right)
$$

其中相似度函数$f(\cdot,\cdot)$计算两个特征位置$x_i,x_j$的相似程度，输出被权重因子$C(x_i)$归一化。注意到当相似度函数取**Embedded Gaussian**函数：

$$
f\left(\mathbf{x}_i, \mathbf{x}_j\right)=e^{\theta\left(\mathbf{x}_i\right)^T \phi\left(\mathbf{x}_j\right)}
$$

此时自注意力机制等价于上述**query-key-value**形式。

![](https://pic.imgdb.cn/item/63b80c02be43e0d30ee766fd.jpg)


自注意力机制的实现可参考：

```python
class SelfAttention(nn.Module):
    def __init__(self, in_channels, k=8):
        super(SelfAttention, self).__init__()
        self.f = nn.Conv2d(in_channels, in_channels, 1)
        self.g = nn.Conv2d(in_channels, in_channels, 1)
        self.h = nn.Conv2d(in_channels, in_channels, 1)
        self.o = nn.Conv2d(in_channels, in_channels, 1)

    def forward(self, x):
        b, c, h, w = x.shape
        fx = self.f(x).view(b, c, -1) # [b, c, hw]
        fx = fx.permute(0, 2, 1) # [b, hw, c]
        gx = self.g(x).view(b, c, -1) # [b, c, hw]
        attn = torch.matmul(fx, gx) # [b, hw, hw]
        attn = F.softmax(attn, dim=2) # 按行归一化

        hx = self.h(x).view(b, c, -1) # [b, c, hw]
        hx = hx.permute(0, 2, 1) # [b, hw, c]
        y = torch.matmul(attn, hx) # [b, hw, c]
        y = y.permute(0, 2, 1).contiguous() # [b, c, hw]
        y = y.view(b, c, h, w)
        return self.o(y)
```

向卷积神经网络中引入自注意力机制的方法包括**Non-Local Net**。

## ⚪ [<font color=blue>Non-local Net</font>](https://0809zheng.github.io/2020/11/06/nonlocal.html)

**Non-Local Net**设计了卷积网络中自注意力机制的基本结构。

![](https://pic.imgdb.cn/item/63fc1688f144a010074cf050.jpg)

## ⚪ [<font color=blue>GCNet</font>](https://0809zheng.github.io/2020/11/06/nonlocal.html)

**GCNet**设计了**Global Context Block**，通过**query**和**key**权重共享简化了**Non-Local Net**，并且引入了通道注意力。

![](https://pic.imgdb.cn/item/63fd6c84f144a010075c40f9.jpg)




# 8. CCNet
- paper：[CCNet: Criss-Cross Attention for Semantic Segmentation](https://arxiv.org/abs/1811.11721)
- 分类：自注意力

**Pytorch**代码如下：

```
import functools
import time

import torch
import torch.autograd as autograd
import torch.cuda.comm as comm
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd.function import once_differentiable

class CA_Weight(autograd.Function):
    @staticmethod
    def forward(ctx, t, f):
        # Save context
        n, c, h, w = t.size()
        size = (n, h + w - 1, h, w)
        weight = torch.zeros(size,
                             dtype=t.dtype,
                             layout=t.layout,
                             device=t.device)
        _ext.ca_forward_cuda(t, f, weight)
        # Output
        ctx.save_for_backward(t, f)
        return weight

    @staticmethod
    @once_differentiable
    def backward(ctx, dw):
        t, f = ctx.saved_tensors
        dt = torch.zeros_like(t)
        df = torch.zeros_like(f)
        _ext.ca_backward_cuda(dw.contiguous(), t, f, dt, df)
        _check_contiguous(dt, df)
        return dt, df

class CA_Map(autograd.Function):
    @staticmethod
    def forward(ctx, weight, g):
        # Save context
        out = torch.zeros_like(g)
        _ext.ca_map_forward_cuda(weight, g, out)
        # Output
        ctx.save_for_backward(weight, g)
        return out

    @staticmethod
    @once_differentiable
    def backward(ctx, dout):
        weight, g = ctx.saved_tensors
        dw = torch.zeros_like(weight)
        dg = torch.zeros_like(g)
        _ext.ca_map_backward_cuda(dout.contiguous(), weight, g, dw, dg)
        _check_contiguous(dw, dg)
        return dw, dg

ca_weight = CA_Weight.apply
ca_map = CA_Map.apply

class CrissCrossAttention(nn.Module):
    """ Criss-Cross Attention Module"""
    def __init__(self, in_dim):
        super(CrissCrossAttention, self).__init__()
        self.chanel_in = in_dim
        self.query_conv = nn.Conv2d(in_channels=in_dim,
                                    out_channels=in_dim // 8,
                                    kernel_size=1)
        self.key_conv = nn.Conv2d(in_channels=in_dim,
                                  out_channels=in_dim // 8,
                                  kernel_size=1)
        self.value_conv = nn.Conv2d(in_channels=in_dim,
                                    out_channels=in_dim,
                                    kernel_size=1)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        proj_query = self.query_conv(x)
        proj_key = self.key_conv(x)
        proj_value = self.value_conv(x)
        energy = ca_weight(proj_query, proj_key)
        attention = F.softmax(energy, 1)
        out = ca_map(attention, proj_value)
        out = self.gamma * out + x
        return out
```




# 7. DANet
- paper：[]()
- 分类：自注意力

**Pytorch**代码如下：

```
import numpy as np
import torch
import math
from torch.nn import Module, Sequential, Conv2d, ReLU,AdaptiveMaxPool2d, AdaptiveAvgPool2d, \
    NLLLoss, BCELoss, CrossEntropyLoss, AvgPool2d, MaxPool2d, Parameter, Linear, Sigmoid, Softmax, Dropout, Embedding
from torch.nn import functional as F
from torch.autograd import Variable
torch_ver = torch.__version__[:3]

__all__ = ['PAM_Module', 'CAM_Module']

class PAM_Module(Module):
    """ Position attention module"""
    #Ref from SAGAN
    def __init__(self, in_dim):
        super(PAM_Module, self).__init__()
        self.chanel_in = in_dim

        self.query_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)
        self.key_conv = Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)
        self.value_conv = Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)
        self.gamma = Parameter(torch.zeros(1))

        self.softmax = Softmax(dim=-1)
    def forward(self, x):
        """
            inputs :
                x : input feature maps( B X C X H X W)
            returns :
                out : attention value + input feature
                attention: B X (HxW) X (HxW)
        """
        m_batchsize, C, height, width = x.size()
        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)
        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)
        energy = torch.bmm(proj_query, proj_key)
        attention = self.softmax(energy)
        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)

        out = torch.bmm(proj_value, attention.permute(0, 2, 1))
        out = out.view(m_batchsize, C, height, width)

        out = self.gamma*out + x
        return out


class CAM_Module(Module):
    """ Channel attention module"""
    def __init__(self, in_dim):
        super(CAM_Module, self).__init__()
        self.chanel_in = in_dim


        self.gamma = Parameter(torch.zeros(1))
        self.softmax  = Softmax(dim=-1)
    def forward(self,x):
        """
            inputs :
                x : input feature maps( B X C X H X W)
            returns :
                out : attention value + input feature
                attention: B X C X C
        """
        m_batchsize, C, height, width = x.size()
        proj_query = x.view(m_batchsize, C, -1)
        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)
        energy = torch.bmm(proj_query, proj_key)
        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy
        attention = self.softmax(energy_new)
        proj_value = x.view(m_batchsize, C, -1)

        out = torch.bmm(attention, proj_value)
        out = out.view(m_batchsize, C, height, width)

        out = self.gamma*out + x
        return out
```

## ⭐ 参考文献
- [<font color=blue>Non-local Neural Networks</font>](https://0809zheng.github.io/2020/11/06/nonlocal.html)：(arXiv1711)非局部神经网络。
- [<font color=blue>GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</font>](https://0809zheng.github.io/2020/11/06/nonlocal.html)：(arXiv1904)GCNet：结合非局部神经网络和通道注意力。
