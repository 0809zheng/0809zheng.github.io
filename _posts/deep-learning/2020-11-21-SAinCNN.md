---
layout: post
title: '卷积神经网络中的自注意力(Self-Attention)机制'
date: 2020-11-21
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63b80741be43e0d30edb9553.jpg'
tags: 深度学习
---

> Self-Attention Mechanism in Convolutional Neural Networks.

卷积神经网络中的**自注意力(Self-Attention)**机制表现为**非局部滤波(non-local filtering)**操作，其实现过程与[<font color=Blue>Seq2Seq模型的自注意力机制</font>](https://0809zheng.github.io/2020/04/24/self-attention.html)类似。

标准的卷积层是一种局部滤波操作，其输出特征上的任意位置是由对应输入特征的一个邻域构造的，只能捕捉局部特征之间的关系。而自注意力机制通过计算任意两个位置之间的关系直接捕捉远程依赖，而不用局限于相邻点，相当于构造了一个**和特征图尺寸一样大**的卷积核，从而可以捕捉更多信息。

![](https://pic.imgdb.cn/item/63b808dabe43e0d30edf6519.jpg)

在卷积网络的自注意力机制中，首先构造输入特征$x$的键特征$f(x)$, 查询特征$g(x)$和值特征$h(x)$；然后应用点积注意力构造自注意力特征图：

$$ \alpha_{i} = \text{softmax}\left(f(x_i)^Tg(x_j)\right) =\frac{e^{f(x_i)^Tg(x_j)}}{\sum_j e^{f(x_i)^Tg(x_j)}} $$

在计算输出位置$i$的响应$y_i$时，考虑所有输入值特征$h(x_j)$的加权：

$$ y_i=  \sum_{j}^{} \alpha_{j}h(x_j) =  \sum_{j}^{} \frac{e^{f(x_i)^Tg(x_j)}}{\sum_k e^{f(x_i)^Tg(x_k)}} h(x_j) $$

上式可以被写作更一般的形式：

$$
y_i=\frac{1}{C\left(x_i\right)} \sum_j f\left(x_i, x_j\right) h\left(x_j\right)
$$

其中相似度函数$f(\cdot,\cdot)$计算两个特征位置$x_i,x_j$的相似程度，输出被权重因子$C(x_i)$归一化。注意到当相似度函数取**Embedded Gaussian**函数：

$$
f\left(\mathbf{x}_i, \mathbf{x}_j\right)=e^{\theta\left(\mathbf{x}_i\right)^T \phi\left(\mathbf{x}_j\right)}
$$

此时自注意力机制等价于上述**query-key-value**形式。

![](https://pic.imgdb.cn/item/63b80c02be43e0d30ee766fd.jpg)


自注意力机制的实现可参考：

```python
class SelfAttention(nn.Module):
    def __init__(self, in_channels, k=8):
        super(SelfAttention, self).__init__()
        self.f = nn.Conv2d(in_channels, in_channels, 1)
        self.g = nn.Conv2d(in_channels, in_channels, 1)
        self.h = nn.Conv2d(in_channels, in_channels, 1)
        self.o = nn.Conv2d(in_channels, in_channels, 1)

    def forward(self, x):
        b, c, h, w = x.shape
        fx = self.f(x).view(b, c, -1) # [b, c, hw]
        fx = fx.permute(0, 2, 1) # [b, hw, c]
        gx = self.g(x).view(b, c, -1) # [b, c, hw]
        attn = torch.matmul(fx, gx) # [b, hw, hw]
        attn = F.softmax(attn, dim=2) # 按行归一化

        hx = self.h(x).view(b, c, -1) # [b, c, hw]
        hx = hx.permute(0, 2, 1) # [b, hw, c]
        y = torch.matmul(attn, hx) # [b, hw, c]
        y = y.permute(0, 2, 1).contiguous() # [b, c, hw]
        y = y.view(b, c, h, w)
        return self.o(y)
```

向卷积神经网络中引入自注意力机制的方法包括**Non-Local Net**。

## ⚪ [<font color=blue>Non-local Net</font>](https://0809zheng.github.io/2020/11/06/nonlocal.html)

**Non-Local Net**设计了卷积网络中自注意力机制的基本结构。

![](https://pic.imgdb.cn/item/63fc1688f144a010074cf050.jpg)

## ⚪ [<font color=blue>DANet</font>](https://0809zheng.github.io/2020/11/13/danet.html)

**DANet**设计了**Dual Attention**，同时引入了沿空间维度和和通道维度的自注意力。

![](https://pic.imgdb.cn/item/64099facf144a010078cd559.jpg)

## ⚪ [<font color=blue>CCNet</font>](https://0809zheng.github.io/2020/11/08/nonlocal.html)

**CCNet**设计了**Criss-Cross Attention**，计算一个点的横纵位置的**attention**信息，通过串行两个**Criss-Cross attention**模块间接与全图位置内的任意点进行注意力计算。

![](https://pic.imgdb.cn/item/64099a43f144a0100782dc4c.jpg)

## ⚪ [<font color=blue>GCNet</font>](https://0809zheng.github.io/2020/11/07/gcnet.html)

**GCNet**设计了**Global Context Block**，通过**query**和**key**权重共享简化了**attention map**的构造，并且引入了通道注意力。

![](https://pic.imgdb.cn/item/63fd6c84f144a010075c40f9.jpg)










## ⭐ 参考文献
- [<font color=blue>Non-local Neural Networks</font>](https://0809zheng.github.io/2020/11/06/nonlocal.html)：(arXiv1711)非局部神经网络。
- [<font color=blue>Dual Attention Network for Scene Segmentation</font>](https://0809zheng.github.io/2020/11/13/danet.html)：(arXiv1809)场景分割的对偶注意力网络。
- [<font color=blue>CCNet: Criss-Cross Attention for Semantic Segmentation</font>](https://0809zheng.github.io/2020/11/08/nonlocal.html)：(arXiv1811)CCNet：语义分割中的交叉注意力机制。
- [<font color=blue>GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond</font>](https://0809zheng.github.io/2020/11/07/gcnet.html)：(arXiv1904)GCNet：结合非局部神经网络和通道注意力。
