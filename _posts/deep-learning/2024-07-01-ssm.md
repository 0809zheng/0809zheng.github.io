---
layout: post
title: '状态空间模型(State Space Model)'
date: 2024-07-01
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/668d210ed9c307b7e9395fde.png'
tags: 深度学习
---

> State Space Model.

# 1. 状态空间模型的建模

**状态空间**包含完整描述系统的最小变量数，这些变量称为**状态向量**。**状态空间模型(State Space Model, SSM)**是用于描述这些状态向量的模型，并根据额外的输入预测它们的下一个状态。

**SSM**是一种描述动态系统行为的数学模型，它使用一组一阶微分方程（连续时间系统）或差分方程（离散时间系统）来表示系统内部状态的演化，这组方程被称为**状态方程**；同时用另一组方程来描述系统状态和输出之间的关系，这组方程被称为**观测方程**（也称为输出方程）。

![](https://pic1.imgdb.cn/item/6786021ad0e0a243d4f41bc6.png)

## （1）连续形式的状态空间模型

**状态方程**描述了系统内部状态$h(t)$随时间和输入的演化：

$$
h^\prime(t) = Ah(t) + Bx(t) + \omega(t)
$$

- $h(t)$是系统状态变量，包含所有必要的变量来描述系统在任意时刻的状态，是一个$n$维向量。
- $h'(t)$是$h(t)$关于时间的微分，表示系统状态的变化率。
- $A$是系统矩阵，描述了系统状态之间的关系，以及无控制输入时它们如何随时间自然演化，是一个$n \times n$矩阵。
- $x(t)$是控制输入向量，表示外部输入或控制信号的影响，是一个$m≤ n$维向量。
- $B$是输入矩阵，描述了控制输入如何影响系统状态，是一个$n \times m$矩阵。
- $\omega(t)$是过程噪声，代表系统内部的不确定性，一般假设为高斯噪声，可省略。

![](https://pic1.imgdb.cn/item/6786010dd0e0a243d4f41b24.png)

**观测方程**描述了系统的输出$y(t)$如何依赖于系统状态和控制输入：

$$
y(t) = Ch(t) + Dx(t) + e(t)
$$

- $y(t)$是输出向量或观测向量，包含所有的测量或观测到的变量，是一个$p≤n$维向量。
- $C$是输出矩阵，描述了系统状态如何影响输出，是一个$p \times n$矩阵。
- $D$是直达传递矩阵，表示控制输入直接对输出的影响，是一个$p \times m$矩阵（在很多实际系统中，这个矩阵通常是零或者不显著）。
- $e(t)$是测量噪声，代表了测量过程中的不确定性或误差，一般假设为高斯噪声，可省略。

![](https://pic1.imgdb.cn/item/67860154d0e0a243d4f41b3c.png)

**SSM**将一个$m$维输入向量$x(t)$映射为一个$n$维隐状态变量$h(t)$，然后再映射为一个$p$维输出向量$y(t)$。其中参数$A,B,C,D$可以人为指定或设置为可学习参数。

![](https://pic1.imgdb.cn/item/67860201d0e0a243d4f41bc0.png)

## （2）离散形式的状态空间模型

由于数字计算机通常只能处理离散时间信号，需要将状态空间模型从连续过程转换为离散过程。

连续形式的状态空间模型通常表示为线性常微分方程的形式：

$$
\dot{\mathbf{h}}(t) = \mathbf{A}(t) \mathbf{h}(t) + \mathbf{B}(t) \mathbf{x}(t) \\
\mathbf{y}(t) = \mathbf{C}(t) \mathbf{h}(t) + \mathbf{D}(t) \mathbf{x}(t)
$$

其中$\mathbf{h}(t)$是状态向量，$\mathbf{x}(t)$是输入向量，$\mathbf{y}(t)$是输出向量，$\mathbf{A}(t),\mathbf{B}(t),\mathbf{C}(t),\mathbf{D}(t)$是系统矩阵。
离散化过程涉及以下步骤：

**① 采样时间的选择**：

确定一个合适的采样时间间隔$\Delta$，这个时间间隔要基于系统动态特性以及所需的控制精度（通常设置为可学习参数）。引入**零阶保持 (Zero-order hold)**技术，每次收到离散信号时都会保持其值，直到收到新的离散信号。通过该技术，离散输入信号被连续化。

![](https://pic1.imgdb.cn/item/678603a8d0e0a243d4f41c6c.png)

**② 离散化系统矩阵**：

使用适当的数值方法将连续系统矩阵$\mathbf{A}(t),\mathbf{B}(t)$转换为离散矩阵$\overline{A},\overline{B}$。最常见的方法是指数映射（也称为矩阵指数）：

$$
\begin{aligned}
\overline{A} &= e^{\Delta \mathbf{A}} \\
\overline{B} &= \Delta\mathbf{B}
\end{aligned}
$$

下面进行推导。在长度为$\Delta$的区间$[t_{k-1},t_k)$内对状态方程进行积分：

$$
\int_{t_{k-1}}^{t_k}\dot{\mathbf{h}}(t) = \mathbf{A}\int_{t_{k-1}}^{t_k} \mathbf{h}(t) + \mathbf{B}\int_{t_{k-1}}^{t_k} \mathbf{x}(t) \\
$$

根据零阶保持和$e^x \approx 1+x$，上式可化简为：

$$
\begin{aligned}
\mathbf{h}(t_k)-\mathbf{h}(t_{k-1}) & = \Delta \mathbf{A} \mathbf{h}(t_{k-1}) + \Delta \mathbf{B} \mathbf{x}(t_{k-1}) \\
& \downarrow \\
\mathbf{h}(t_k) & = (I+\Delta \mathbf{A} )\mathbf{h}(t_{k-1}) + \Delta \mathbf{B} \mathbf{x}(t_{k-1}) \\
& \downarrow \\
\mathbf{h}(t_k) & = e^{\Delta \mathbf{A}} \mathbf{h}(t_{k-1}) + \Delta \mathbf{B} \mathbf{x}(t_{k-1}) \\
\end{aligned}
$$


**③ 离散化状态和输出方程**：

确定的离散时间点$t_k = k\Delta t$上的状态方程和观测方程可以表示为：

$$
\begin{aligned}
\mathbf{h}[k] &= \overline{A} \mathbf{h}[k-1] + \overline{B} \mathbf{x}[k] \\
\mathbf{y}[k] &= \overline{C} \mathbf{h}[k] + \overline{D} \mathbf{x}[k]
\end{aligned}
$$

其中$\overline{C},\overline{D}$通常可以直接等同于连续时间模型中的$\mathbf{C},\mathbf{D}$。

![](https://pic1.imgdb.cn/item/6786043ed0e0a243d4f41d3d.png)

# 2. 状态空间模型的实现

## （1）状态空间模型的循环表示

状态空间模型的简化离散表示为：

$$
\begin{aligned}
\mathbf{h}[k] &= \overline{A} \mathbf{h}[k-1] + \overline{B} \mathbf{x}[k] \\
\mathbf{y}[k] &= C \mathbf{h}[k]
\end{aligned}
$$

对于每个时间步，计算当前输入$x_k$如何影响之前的状态$h_{k-1}$，然后预测输出$y_k$。形式上**SSM**与[循环神经网络**RNN**](https://0809zheng.github.io/2020/03/07/RNN.html)类似，主要区别在于**SSM**直接采用了线性变换，而没有使用激活函数进行非线性化；因此**SSM**也被称为线性**RNN**。

![](https://pic1.imgdb.cn/item/67861929d0e0a243d4f426ad.png)


## （2）状态空间模型的卷积表示

状态空间模型计算效率最大的提升在于其序列运算可以**卷积化**。从0时刻开始向后推导状态空间模型几个时刻后的输出，可得到如下的形式：

$$
\begin{aligned}
\mathbf{h}_0 &= \overline{B} \mathbf{x}_0 \\
\mathbf{y}_0 &= C \mathbf{h}_0= C\overline{B} \mathbf{x}_0 \\
\mathbf{h}_1 &= \overline{A} \mathbf{h}_0 + \overline{B} \mathbf{x}_1 = \overline{A} \overline{B} \mathbf{x}_0 + \overline{B} \mathbf{x}_1 \\
\mathbf{y}_1 &= C \mathbf{h}_1 = C\overline{A} \overline{B} \mathbf{x}_0 + C\overline{B} \mathbf{x}_1 \\
\mathbf{h}_2 &= \overline{A} \mathbf{h}_1 + \overline{B} \mathbf{x}_2 = \overline{A} (\overline{A} \overline{B} \mathbf{x}_0 + \overline{B} \mathbf{x}_1) + \overline{B} \mathbf{x}_2= \overline{A}^2 \overline{B} \mathbf{x}_0 +\overline{A} \overline{B} \mathbf{x}_1 + \overline{B} \mathbf{x}_2 \\
\mathbf{y}_1 &= C \mathbf{h}_2 = C\overline{A}^2 \overline{B} \mathbf{x}_0 +C\overline{A} \overline{B} \mathbf{x}_1 + C\overline{B} \mathbf{x}_2  \\
& \cdots \\
\mathbf{h}_k &= \overline{A}^k \overline{B} \mathbf{x}_0 +\overline{A}^{k-1} \overline{B} \mathbf{x}_1  + \cdots +\overline{A} \overline{B} \mathbf{x}_{k-1} + \overline{B} \mathbf{x}_k \\
\mathbf{y}_k &= C \overline{A}^k \overline{B} \mathbf{x}_0 +C\overline{A}^{k-1} \overline{B} \mathbf{x}_1  + \cdots +C\overline{A} \overline{B} \mathbf{x}_{k-1} + C\overline{B} \mathbf{x}_k  \\
\end{aligned}
$$

形式上，构造下列形式的卷积核，即可将序列运算转化为卷积运算：

$$
\begin{aligned}
\overline{K} &= (C\overline{B}, C\overline{A} \overline{B},...,C \overline{A}^k \overline{B}, ...) \\
\mathbf{y} &= \mathbf{x} * \overline{K}
\end{aligned}
$$

卷积运算在计算机中可进行并行训练，这大大加速了状态空间模型的训练速度。而**RNN**由于在输出时使用了激活函数，因此无法进行并行训练，这也是状态空间模型对比**RNN**的一大优势所在。然而由于内核大小固定，它们的推理速度不如 **RNN** 那样快且不受限制。

## （3）状态空间模型的训练与推理

根据前面的讨论，循环形式的**SSM**具有高效的推理速度，但无法并行化训练；而卷积形式的**SSM**可以并行化训练，但无法进行无限长度上下文的推理。因此在实践中可以使用循环 **SSM** 进行有效推理，并使用卷积 **SSM** 进行可并行训练。

![](https://pic1.imgdb.cn/item/67861c94d0e0a243d4f4280d.png)

# 3. 深度学习中的状态空间模型

深度学习中的状态空间模型包括**HiPPO**。

### ⚪ [<font color=blue>HiPPO</font>](https://0809zheng.github.io/2024/07/02/hippo.html)
- (arXiv2008)HiPPO: Recurrent Memory with Optimal Polynomial Projections

**HiPPO**模型的出发点是用勒让德多项式构造的标准正交基函数的线性组合来近似当前时刻的输入$x(t)$，基于此导出了两种形式的**SSM**：

- **HiPPO-LegT（Translated Legendre）**：只保留最邻近窗口$[T-w,T]$的输入信息。

$$
\begin{aligned}
\frac{d}{dt}c_n(t) &= A_{n,k}c_n(t) + B_n \mathbf{u}(t) \\
A_{n,k} &= -\frac{1}{w}\begin{cases} \sqrt{(2n+1)(2k+1)}, & k < n \\ (-1)^{n-k} \sqrt{(2n+1)(2k+1)}, & k \geq n \end{cases} \\
B_n &= \frac{\sqrt{2(2n+1)}}{w} \\
\end{aligned}
$$

- **HiPPO-LegS（Scaled Legendre）**：保留所有历史输入信息，且随时间呈多项式衰减。

$$
\begin{aligned}
\frac{d}{dt}c_n(t) &= \frac{A_{n,k}}{t}c_n(t) + \frac{B_n}{t} \mathbf{u}(t) \\
A_{n,k} &= -\begin{cases} \sqrt{(2n+1)(2k+1)}, & k < n \\ n+1, & k = n\\ 0, & k > n \end{cases} \\
B_n &= \sqrt{2(2n+1)} \\
\end{aligned}
$$


### ⚪ Reference
- [A Visual Guide to Mamba and State Space Models](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state)