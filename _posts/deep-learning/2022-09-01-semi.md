---
layout: post
title: '半监督学习(Semi-Supervised Learning)'
date: 2022-09-01
author: 郑之杰
cover: ''
tags: 深度学习
---

> Semi-Supervised Learning.

**半监督学习(Semi-Supervised Learning)**是指同时从有标签数据和无标签数据中进行学习，适用于标注数据有限、标注成本较高的场合。值得一提的是，目前半监督学习方法主要是为视觉任务设计的，而在自然语言任务中通常采用预训练+微调的学习流程。

在半监督学习中，通过有标签数据构造监督损失$$\mathcal{L}_s$$，通过无标签数据构造无监督损失$$\mathcal{L}_u$$，总损失函数通过权重项$\mu$加权：

$$ \mathcal{L} = \mathcal{L}_s + \mu \mathcal{L}_u $$

其中权重项$\mu=\mu(t)$通常设置为随训练轮数$t$变化的斜坡函数(**ramp function**)，以逐渐增大无监督损失的重要性。

半监督学习的一些假设：
- **平滑性假设 (Smoothness Assumption)**：如果两个数据样本在特征空间的高密度区域中接近，则他们的标签应该相同或非常相似。
- **聚类假设 (Cluster Assumption)**：特征空间既有高密度区域又有稀疏区域。高密度分组的数据点自然形成一个聚类集群。同一集群中的样本应具有相同的标签。
- **低密度分离假设 (Low-density Separation Assumption)**：样本类别之间的决策边界倾向于位于稀疏的低密度区域；否则决策边界会将一个高密度集群划分为两个类，对应于两个集群，这会使平滑性假设和聚类假设无效。
- **流形假设 (Manifold Assumption)**：高维数据往往位于低维流形上。尽管真实世界数据通常在非常高的维度上观察到，它们实际上可以由较低维度的流形来捕获，其中某些属性相似的点被分为相近的分组。流形假设是表示学习的基础，能够学习更有效的表示，以便发现和测量无标签数据点之间的相似性。

常用的半监督学习方法包括：
- **一致性正则化**：假设神经网络的随机性或数据增强变换不会改变输入样本的模型预测结果，如$\Pi$**-Model**, **Temporal Ensembling**, **Mean Teacher**, **VAT**, **ICT**。

# 1. 一致性正则化 Consistency Regularization

**一致性正则化(Consistency Regularization)**也称为**一致性训练(Consistency Training)**，是假设神经网络的随机性或数据增强变换不会改变输入样本的模型预测结果，并可以基于此构造一致性正则化损失$$\mathcal{L}_u$$。

![](https://pic.imgdb.cn/item/63babe39be43e0d30e4bc733.jpg)

### ⚪ [<font color=blue>$\Pi$-Model</font>](https://0809zheng.github.io/2022/09/02/pimodel.html)

$\Pi$**-Model**的无监督损失旨在最小化一个数据样本两次经过同一个带随机变换(如随即增强或**dropout**)的网络后预测结果的差异：

$$ \mathcal{L}_u^{\Pi} = \sum_{x \in \mathcal{D}} \text{Dist}[f_{\theta}(x),f_{\theta}'(x)] $$

![](https://pic.imgdb.cn/item/63ba8bf9be43e0d30ee9ac61.jpg)

### ⚪ [<font color=blue>Temporal Ensembling</font>](https://0809zheng.github.io/2022/09/03/te.html)

**时序集成**对每个数据样本$x$的预测结果$y$存储一个指数滑动平均值$$\tilde{y}\leftarrow \beta \tilde{y} + (1-\beta)f_{\theta}(x)$$；则无监督损失旨在最小化当前预测与滑动平均的差异：

$$ \mathcal{L}_u^{TE} = \sum_{x \in \mathcal{D}} \text{Dist}[f_{\theta}(x),\tilde{y}] $$

![](https://pic.imgdb.cn/item/63ba8c12be43e0d30eea0654.jpg)

### ⚪ [<font color=blue>Mean Teacher</font>](https://0809zheng.github.io/2022/09/04/meanteacher.html)

**Mean Teacher**存储模型参数的滑动平均值$\theta'\leftarrow \beta \theta' + (1-\beta)\theta$作为教师模型，通过当前学生模型和教师模型的预测结果构造无监督损失。

$$ \mathcal{L}_u^{MT} = \sum_{x \in \mathcal{D}} \text{Dist}[f_{\theta}(x),f_{\theta'}(x)] $$

![](https://pic.imgdb.cn/item/63ba8c2ebe43e0d30eea62af.jpg)

### ⚪ [<font color=blue>Virtual Adversarial Training (VAT)</font>](https://0809zheng.github.io/2022/09/05/vat.html)

**虚拟对抗训练**把对抗训练的思想引入半监督学习：构造当前样本的攻击噪声$r$，则无监督损失旨在最小化引入噪声$r$前后模型预测结果的差异：

$$ \begin{aligned} r &= \mathop{\arg \max}_{||r|| \leq \epsilon} \text{Dist}[\text{sg}(f_{\theta}(x)),f_{\theta}(x+r)] \\ \mathcal{L}_u^{VAT} &= \sum_{x \in \mathcal{D}} \text{Dist}[\text{sg}(f_{\theta}(x)),f_{\theta}(x+r)]  \end{aligned} $$

### ⚪ [<font color=blue>Interpolation Consistency Training (ICT)</font>](https://0809zheng.github.io/2022/09/06/ict.html)

**插值一致性训练)**通过**mixup**构造插值样本进行一致性预测，则无监督损失旨在最小化插值样本的预测结果和预测结果的插值之间的差异：

$$ \mathcal{L}_u^{ICT} = \sum_{(x_i,x_j) \in \mathcal{D}} \text{Dist}[f_{\theta}(\lambda x_i + (1-\lambda) x_j),\lambda f_{\theta'}(x_i) + (1-\lambda) f_{\theta'}(x_j)] $$

其中$\theta'$是$\theta$的滑动平均值。

![](https://pic.imgdb.cn/item/63bba04bbe43e0d30ec1763a.jpg)

### ⚪ [<font color=blue>Unsupervised Data Augmentation (UDA)</font>](https://0809zheng.github.io/2022/09/07/uda.html)

**无监督数据增强**采用先进的数据增强策略生成噪声样本$$\hat{x}$$，并采用以下技巧：丢弃预测置信度低于阈值$\tau$的样本；在**softmax**中引入温度系数$T$；训练一个分类器预测域标签，并保留域内分类置信度高的样本。

$$ \mathcal{L}_u^{UDA} = \sum_{x \in \mathcal{D}} \Bbb{I} [\mathop{\max}_c f_{\theta}^c(x) > \tau] \cdot \text{Dist}[\text{sg}(f_{\theta}(x;T)),f_{\theta}(\hat{x})] $$

![](https://pic.imgdb.cn/item/63bbf895be43e0d30e769e1b.jpg)

# 2. 



### ⭐ 参考文献
- [Learning with not Enough Data Part 1: Semi-Supervised Learning](https://lilianweng.github.io/posts/2021-12-05-semi-supervised/)(**Lil'Log**)一篇介绍半监督学习的博客。
- [An Overview of Deep Semi-Supervised Learning](https://arxiv.org/abs/2006.05278)：(arXiv2006)一篇深度半监督学习的综述。
- [<font color=blue>Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning</font>](https://0809zheng.github.io/2022/09/02/pimodel.html)：(arXiv1606)深度半监督学习的随机变换和扰动正则化。
- [<font color=blue>Temporal Ensembling for Semi-Supervised Learning</font>](https://0809zheng.github.io/2022/09/03/te.html)：(arXiv1610)半监督学习的时序集成。
- [<font color=blue>Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</font>](https://0809zheng.github.io/2022/09/04/meanteacher.html)：(arXiv1703)加权平均一致性目标改进半监督深度学习。
- [<font color=blue>Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning</font>](https://0809zheng.github.io/2022/09/05/vat.html)：(arXiv1704)虚拟对抗训练：一种半监督学习的正则化方法。
- [<font color=blue>Interpolation Consistency Training for Semi-Supervised Learning</font>](https://0809zheng.github.io/2022/09/06/ict.html)：(arXiv1903)半监督学习的插值一致性训练。
- [<font color=blue>Unsupervised Data Augmentation for Consistency Training</font>](https://0809zheng.github.io/2022/09/07/uda.html)：(arXiv1904)一致性训练的无监督数据增强。

