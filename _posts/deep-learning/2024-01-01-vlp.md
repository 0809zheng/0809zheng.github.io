---
layout: post
title: '视觉-语言预训练(Vision-Language Pretraining)'
date: 2024-01-01
author: 郑之杰
cover: 'https://pic1.imgdb.cn/item/67b5a430d0e0a243d400c5f0.png'
tags: 深度学习
---

> Vision-Language Pretraining.

视觉-语言预训练（**Vision-Language Pretraining, VLP**）旨在从大规模的图像-文本对中学习通用的跨模态表示，使得模型能够理解图像和文本之间的语义关联。这些预训练模型通常通过图像-文本匹配、对比学习、掩码区域分类等任务来聚合和对齐视觉和语言信息。预训练完成后，模型可以直接在下游的视觉-语言任务上进行微调，如视觉问答（**VQA**）、视觉推理等。

**VLP**相比于单一模态的预训练模型的主要优势包括：
- 多模态融合特征相比于单模态特征对一些下游任务表现更优；
- 能够提供对多模态任务的处理能力；
- 多模态任务之间存在通用的特征。

根据视觉数据和语言数据的特征交互和对齐方式不同，视觉-语言预训练方法可以分为：
- 单塔结构模型；将文本和视觉特征连接到一起，然后使用**Transformer**编码器提取特征；如**VisualBERT**, **VL-BERT**, **UNITER**, **ImageBERT**, **Oscar**, **Pixel-BERT**, **VinVL**, **ViLT**, **Frozen**, **VLMo**, **VL-BEiT**, **BEiT-3**。
- 双塔结构模型；将文本和视觉特征分别编码，然后使用交叉注意力来实现不同模态之间的交互；如**ViLBERT**, **LXMERT**, **ALBEF**。
- 编解码器模型：通过完整的**Transformer**模型把视觉和语言任务统一为**Token**生成任务；如**VL-T5**, **SimVLM**, **GIT**, **CoCa**。
- 对比学习模型：通过使匹配的图像和文本在嵌入空间中彼此靠近来提取图像和文本的共享表示；如**ALIGN**, **CLIP**, **SLIP**, **GLIP**, **GLIPv2**, **BLIP**, **BLIP-2**, **MaskCLIP**, **Chinese CLIP**, **FLIP**, **A-CLIP**, **SigLIP**, **SigLIP 2**, **LaCLIP**。

![](https://pic1.imgdb.cn/item/67b5a430d0e0a243d400c5f0.png)

## 1. 单塔结构模型
单塔结构模型将文本和视觉特征连接到一起，然后输入到单个**Transformer**模型中（常用**BERT**）。这种结构利用注意力机制来融合多模态输入，因为对不同的模态都使用了相同形式的参数，所以在参数方面更具效率。

### ⚪ [<font color=blue>VisualBERT</font>](https://0809zheng.github.io/2024/01/03/visualbert.html)
- (arXiv1908)VisualBERT: A Simple and Performant Baseline for Vision and Language

**VisualBERT**在**BERT**的基础上进行了扩展，以处理图像和文本的联合输入。除了**BERT**的文本嵌入外，**VisualBERT**引入了一组视觉嵌入用于表示图像。每个视觉嵌入对应于图像中的一个边界框区域，由目标检测器提取。

**VisualBERT**的训练过程包括三个阶段：1. 任务无关预训练：包括掩码语言建模和句子-图像预测；2. 任务特定预训练：掩码语言建模；3. 微调：引入特定任务。

![](https://pic1.imgdb.cn/item/679dcb83d0e0a243d4f9724c.png)

### ⚪ [<font color=blue>VL-BERT</font>](https://0809zheng.github.io/2024/01/05/vlbert.html)
- (arXiv1908)VL-BERT: Pre-training of Generic Visual-Linguistic Representations

**VL-BERT**扩展了**BERT**模型，使其能够同时处理图像和文本输入。输入包括视觉和语言嵌入特征。每个**token**的嵌入特征是四种嵌入的总和：标记嵌入、视觉特征嵌入、段嵌入和序列位置嵌入。**VL-BERT**通过掩码语言建模和掩码**RoI**分类这两个预训练任务学习视觉与语言的联合表示。

![](https://pic1.imgdb.cn/item/679e0eefd0e0a243d4f977fd.png)

### ⚪ [<font color=blue>UNITER</font>](https://0809zheng.github.io/2024/01/07/uniter.html)
- (arXiv1909)UNITER: UNiversal Image-TExt Representation Learning

**UNITER**通过设计多种预训练任务来学习上下文化的表示，包括掩码语言建模（**MLM**）、掩码区域建模（**MRM**）、图像-文本匹配（**ITM**）和基于最优传输的词-区域对齐（**WRA**）。

![](https://pic1.imgdb.cn/item/679f66ead0e0a243d4f997fb.png)


### ⚪ [<font color=blue>ImageBERT</font>](https://0809zheng.github.io/2024/01/09/imagebert.html)
- (arXiv2001)ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data

**ImageBERT**收集了一个大规模弱监督图像-文本数据集（**LAIT**），并采用多阶段预训练策略：在**LAIT**数据集上进行第一阶段预训练，随后在**Conceptual Captions**和**SBU Captions**数据集上进行第二阶段预训练。预训练任务包括掩码语言模型、掩码目标分类、掩码区域特征回归、图像-文本匹配。

![](https://pic1.imgdb.cn/item/67a07ba5d0e0a243d4f9b56c.png)

### ⚪ [<font color=blue>Oscar</font>](https://0809zheng.github.io/2024/01/06/oscar.html)
- (arXiv2004)Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks

**Oscar**将输入的图像-文本对表示为一个三元组：单词序列、目标标签序列和图像区域特征。这种表示方式允许模型通过目标标签作为锚点，学习图像和文本之间的语义对齐。**Oscar**的预训练目标包括掩码标记损失和对比损失。

![](https://pic1.imgdb.cn/item/679e19bed0e0a243d4f97914.png)

### ⚪ [<font color=blue>Pixel-BERT</font>](https://0809zheng.github.io/2024/01/08/pixelbert.html)
- (arXiv2004)Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers

**Pixel-BERT**没有采用**Faster-RCNN**提取图像的区域特征，而是直接用**CNN**提取图像的像素特征。**CNN**提取的图像特征和文本特征连接之后作为**Transformer**的输入。为了增强视觉特征的鲁棒性，将图像特征随机采样$100$个视觉特征向量。预训练任务包括掩码语言模型和图像-文本匹配。

![](https://pic1.imgdb.cn/item/67a03bd3d0e0a243d4f9a60d.png)


### ⚪ [<font color=blue>VinVL</font>](https://0809zheng.github.io/2024/01/17/vinvl.html)
- (arXiv2101)VinVL: Revisiting Visual Representations in Vision-Language Models

**VinVL**通过改进目标检测模型来提供更丰富的图像视觉表示。**VinVL**基于**ResNeXt-152 C4**架构，使用改进的**Oscar**模型进行预训练，以学习图像和文本的联合表示。**OSCAR+**在预训练阶段使用掩码标记损失（**Masked Token Loss**）和三元组对比损失（**3-way Contrastive Loss**）。

### ⚪ [<font color=blue>ViLT</font>](https://0809zheng.github.io/2024/01/12/vilt.html)
- (arXiv2102)ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision

**ViLT**模型不依赖于卷积网络或区域监督，而是通过将输入图像划分为固定大小的图像块并进行简单的线性投影将图像和文本嵌入到同一个空间中，然后通过**Transformer**模块进行交互学习。**ViLT**在预训练阶段采用了图像-文本匹配和掩码语言建模。

![](https://pic1.imgdb.cn/item/67a0ace0d0e0a243d4f9bb20.png)

### ⚪ [<font color=blue>Frozen</font>](https://0809zheng.github.io/2024/01/11/frozen.html)
- (arXiv2106)Multimodal Few-Shot Learning with Frozen Language Models

**Frozen**通过将图像编码为预训练语言模型的词嵌入序列，使模型能够在仅通过少量示例提示的情况下快速适应新的视觉-语言任务。在训练过程中，仅更新视觉编码器的参数，冻结语言模型的参数。通过将图像嵌入作为前缀输入到语言模型中，模型能够生成与图像相关的文本（如标题）。训练目标是最大化给定图像的标题生成概率。

![](https://pic1.imgdb.cn/item/67a088ced0e0a243d4f9b6dd.png)


### ⚪ [<font color=blue>VLMo</font>](https://0809zheng.github.io/2024/01/14/vlmo.html)
- (arXiv2111)VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts

**VLMo**通过引入视觉专家（**V-FFN**）、语言专家（**L-FFN**）和视觉-语言专家（**VL-FFN**）来替代标准**Transformer**中的**FFN**。视觉专家用于编码图像，语言专家用于编码文本，而视觉-语言专家则用于处理图像-文本对，以捕捉更深层次的模态交互。

**VLMo**采用了分阶段预训练策略，充分利用大规模的纯图像和纯文本数据。首先使用掩码图像建模进行视觉预训练，其次使用掩码语言建模进行语言预训练，最后使用图像-文本对比学习、掩码语言建模、图像-文本匹配进行视觉-语言预训练。

![](https://pic1.imgdb.cn/item/67a1891dd0e0a243d4fbba48.png)

### ⚪ [<font color=blue>VL-BEiT</font>](https://0809zheng.github.io/2024/02/28/vlbeit.html)
- (arXiv2206)VL-BEiT: Generative Vision-Language Pretraining

**VL-BEiT**使用统一的双向**Transformer**架构，使用混合模态专家作为骨干网络（其形式与**VLMo**相似），通过掩码语言建模（**MLM**）、掩码图像建模（**MIM**）和掩码视觉-语言建模（**MVLM**）三种任务进行预训练。

![](https://pic1.imgdb.cn/item/67b59643d0e0a243d400c027.png)

### ⚪ [<font color=blue>BEiT-3</font>](https://0809zheng.github.io/2024/01/31/beit3.html)
- (arXiv2208)Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks

**BEiT-3**使用共享的**Multiway Transformer**作为骨干网络，其形式与**VLMo**相似；并通过掩码建模的方式在单模态（图像、文本）和多模态（图像-文本对）数据上进行预训练。这种方法不仅简化了预训练任务，还使得模型能够高效扩展。

![](https://pic1.imgdb.cn/item/67b58de5d0e0a243d400bc36.png)


## 2. 双塔结构模型
双塔结构模型中，文本和视觉特征没有连接在一起，而是分别输入到两个不同的**Transformer**模块中。这两个**Transformer**模块没有共享参数，为了达到更高的性能，双塔结构使用交叉注意力的方式来实现不同模态之间的交互。

### ⚪ [<font color=blue>ViLBERT</font>](https://0809zheng.github.io/2024/01/04/vilbert.html)
- (arXiv1908)ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks

**ViLBERT**扩展了**BERT**模型，使其能够同时处理图像和文本输入。**ViLBERT**包含两个并行的**BERT**风格的模型，分别处理图像区域和文本段落。这两个流通过共注意力**Transformer**层进行交互，允许两个模态之间的信息交换；并通过掩码多模态建模和多模态对齐预测两个预训练任务学习视觉与语言的联合表示。

![](https://pic1.imgdb.cn/item/679dd1fcd0e0a243d4f972bd.png)

### ⚪ [<font color=blue>LXMERT</font>](https://0809zheng.github.io/2024/01/02/lxmert.html)
- (arXiv1908)LXMERT: Learning Cross-Modality Encoder Representations from Transformers

**LXMERT**是一种用于学习视觉与语言之间联系的框架。它构建了一个包含三个编码器（目标关系编码器、语言编码器和跨模态编码器）的大规模**Transformer**模型，并通过五个预训练任务（掩码语言建模、掩码目标预测: **RoI**特征回归和检测标签分类、跨模态匹配、图像问答）学习视觉和语言之间的对齐关系。

![](https://pic1.imgdb.cn/item/679dc3fed0e0a243d4f971d5.png)

### ⚪ [<font color=blue>ALBEF</font>](https://0809zheng.github.io/2024/01/13/albef.html)
- (arXiv2107)Align before Fuse: Vision and Language Representation Learning with Momentum Distillation

**ALBEF**通过对比学习在融合之前对齐图像和文本表示，并引入动量蒸馏（**Momentum Distillation**）方法来提高模型在噪声数据上的学习能力。**ALBEF**的预训练目标包括图像-文本对比学习、掩码语言建模、图像-文本匹配。

![](https://pic1.imgdb.cn/item/67a0b4d5d0e0a243d4f9bc1d.png)



## 3. 编解码器模型

编解码器模型通过编解码器结构将视觉和语言任务统一为**Token**生成任务。编码器将视觉和语言特征编码为统一的表示，解码器则生成对应的输出。


### ⚪ [<font color=blue>VL-T5</font>](https://0809zheng.github.io/2024/01/10/vlt5.html)
- (arXiv2102)Unifying Vision-and-Language Tasks via Text Generation

本文提出的框架将视觉和语言问题统一为多模态条件文本生成任务。基于两个预训练的**Transformer**语言模型**T5Base**和**BARTBase**，分别扩展为**VL-T5**和**VL-BART**。预训练在多任务设置下进行，包括多模态语言建模、视觉问答、图像-文本匹配、视觉定位和基于区域的描述生成等任务。

![](https://pic1.imgdb.cn/item/67a0819bd0e0a243d4f9b5f0.png)

### ⚪ [<font color=blue>SimVLM</font>](https://0809zheng.github.io/2024/01/16/simvlm.html)
- (arXiv2108)SimVLM: Simple Visual Language Model Pretraining with Weak Supervision

**SimVLM**通过前缀语言建模训练了**Transformer**视觉-语言预训练模型。对于一个图像-文本对，将图像特征作为语言模型的前缀输入，和文本提示词一起作为语言模型编码器的输入，同时利用语言模型的解码器得到对应的文本输出。

![](https://pic1.imgdb.cn/item/67a1a635d0e0a243d4fbc2f7.png)

### ⚪ [<font color=blue>GIT</font>](https://0809zheng.github.io/2024/01/15/git.html)
- (arXiv2205)GIT: A Generative Image-to-text Transformer for Vision and Language

**GIT**包括一个图像编码器和一个文本解码器。图像编码器输出一个紧凑的二维特征图，然后被展平并投影到文本解码器的输入维度。文本解码器是一个自回归**Transformer**模块，用于预测文本描述。在预训练阶段，通过语言建模任务将输入图像映射到完整的相关文本描述。

![](https://pic1.imgdb.cn/item/67a1922ed0e0a243d4fbc10e.png)

### ⚪ [<font color=blue>CoCa</font>](https://0809zheng.github.io/2024/01/18/coca.html)
- (arXiv2205)CoCa: Contrastive Captioners are Image-Text Foundation Models

**CoCa** 是一个编码器-解码器架构，其中编码器负责提取图像特征，解码器负责生成文本描述。解码器被分为单模态解码器 和 多模态解码器；单模态解码器仅对输入文本进行编码，实现图像和文本的对齐。多模态解码器通过交叉注意力机制与图像编码器的输出进行交互。**CoCa** 的训练目标包括对比损失和标题生成损失。

![](https://pic1.imgdb.cn/item/67a1c352d0e0a243d4fbc77d.png)

## 4. 对比学习模型

图像文本对比学习模型通过鼓励相似的图像和文本在嵌入空间中彼此靠近来进行训练。它通常用于学习图像和文本的共享表示，以便在嵌入空间中相似的图像和文本能够彼此靠近。

### ⚪ [<font color=blue>ALIGN</font>](https://0809zheng.github.io/2024/01/29/align.html)
- (arXiv2102)Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision

**ALIGN**利用简单的双编码器架构和对比损失函数，将图像和文本表示对齐到一个共享嵌入空间，从超过18亿对图像和噪声文本数据中学习视觉和语言表示。

![](https://pic1.imgdb.cn/item/67b5837bd0e0a243d400b8cf.png)

### ⚪ [<font color=blue>CLIP</font>](https://0809zheng.github.io/2021/01/06/dalle.html)
- (arXiv2103)Learning Transferable Visual Models From Natural Language Supervision

**CLIP**通过在图像和文本数据集中进行匹配来进行视觉语言预训练。具体地，训练一个文本编码器和图像编码器，分别得到文本和图像的编码，并计算两者的匹配程度。

![](https://pic.imgdb.cn/item/63e2f48f4757feff3376d5d7.jpg)


### ⚪ [<font color=blue>SLIP</font>](https://0809zheng.github.io/2024/01/25/slip.html)
- (arXiv2112)SLIP: Self-supervision meets Language-Image Pre-training

**SLIP**在预训练阶段同时使用图像的自监督学习（通过**SimCLR**将同一图像的不同增强特征拉近，将不同图像的特征推远）和图像-文本对的语言监督学习（通过**CLIP**将图像和文本嵌入到一个共享的嵌入空间中）。

![](https://pic1.imgdb.cn/item/67ab1c4bd0e0a243d4fe57e3.png)



### ⚪ [<font color=blue>GLIP</font>](https://0809zheng.github.io/2023/11/03/glip.html)
- (arXiv2112)Grounded Language-Image Pre-training

**GLIP**把**CLIP**中的图像块-文本匹配扩展到图像区域-文本匹配，使用**DyHead**作为**Visual Encoder**来编码图像区域特征，融合后文本和区域特征会分别经过**BERTLayer**和**DyHead Module**进一步提取特征，最后通过对比学习进行特征对齐。

![](https://pic.imgdb.cn/item/655c6111c458853aef2d9091.jpg)

### ⚪ [<font color=blue>GLIPv2</font>](https://0809zheng.github.io/2024/01/21/glipv2.html)
- (arXiv2206)GLIPv2: Unifying Localization and Vision-Language Understanding

**GLIPv2** 包括双编码器（视觉编码器和文本编码器）和融合编码器。模型输入图像-文本对，输出经过跨模态融合后的特征，用于后续任务的处理。**GLIPv2** 的预训练包括三个任务：短语定位任务、区域-单词对比学习任务和掩码语言建模任务。

![](https://pic1.imgdb.cn/item/67a1cbfcd0e0a243d4fbc8da.png)

### ⚪ [<font color=blue>BLIP</font>](https://0809zheng.github.io/2024/01/22/blip.html)
- (arXiv2201)BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation

**BLIP** 采用多模态混合编码器-解码器：包括单模态编码器、图像引导的文本编码器和图像引导的文本解码器。
- 单模态编码器：分别对图像和文本进行编码，训练目标是图像-文本对比学习。
- 图像引导的文本编码器：通过交叉注意力层将视觉信息注入文本编码器，训练目标是图像-文本匹配。
- 图像引导的文本解码器：将双向自注意力层替换为因果自注意力层，用于给定图像生成文本描述，训练目标是语言建模。

![](https://pic1.imgdb.cn/item/67a1d9c1d0e0a243d4fbcad7.png)

### ⚪ [<font color=blue>BLIP-2</font>](https://0809zheng.github.io/2024/01/23/blip2.html)
- (arXiv2301)BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models

**BLIP-2** 利用现成的冻结预训练图像编码器和大型语言模型，使用一个轻量级的 **Querying Transformer（Q-Former）**桥接视觉和语言模态，通过一组可学习的查询向量从冻结的图像编码器中提取视觉特征并传递给大型语言模型。

**BLIP-2** 分为两个阶段进行预训练：第一阶段从冻结的图像编码器中引导视觉语言表示学习，使用的预训练任务包括图像-文本对比学习、图像引导的文本生成、图像-文本匹配；第二阶段从冻结的 **LLM** 中引导视觉到语言的生成学习。

![](https://pic1.imgdb.cn/item/67a1f75fd0e0a243d4fbce96.png)

### ⚪ [<font color=blue>MaskCLIP</font>](https://0809zheng.github.io/2024/01/27/maskclip.html)
- (arXiv2208)MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining

**MaskCLIP** 在 **CLIP** 的基础上引入掩码自蒸馏来提升对比语言-图像预训练的效果。其中图像的掩码自蒸馏是从完整图像中提取的表征来指导从掩码图像中预测的表征，文本的掩码自蒸馏是通过随机掩码文本中的部分单词来学习文本的局部语义。

![](https://pic1.imgdb.cn/item/67aeb305d0e0a243d4ff0547.png)

### ⚪ [<font color=blue>Chinese CLIP</font>](https://0809zheng.github.io/2024/01/24/cclip.html)
- (arXiv2211)Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese

**Chinese CLIP**是一个专门针对中文的视觉-语言对比预训练模型。与传统的多模态模型不同，**Chinese CLIP** 通过两阶段预训练方法（锁定图像微调和对比微调）在大规模中文图文数据上进行训练，能够有效提升模型在中文场景下的性能。

![](https://pic1.imgdb.cn/item/67a2044dd0e0a243d4fbd04f.png)

### ⚪ [<font color=blue>FLIP</font>](https://0809zheng.github.io/2024/01/26/flip.html)
- (arXiv2212)Scaling Language-Image Pre-training via Masking

**FLIP** 的核心思想是在训练过程中随机遮盖图像块，从而减少计算量并提高训练速度。在训练时，随机遮盖掉大部分（如 **50%** 或 **75%**）的块，仅对可见块进行编码。这种遮盖方式将图像编码的时间复杂度从 $O(n)$ 降低到 $O(n×(1−m))$。

![](https://pic1.imgdb.cn/item/67adddbed0e0a243d4fee94b.png)

### ⚪ [<font color=blue>A-CLIP</font>](https://0809zheng.github.io/2024/01/28/aclip.html)
- (arXiv2212)Attentive Mask CLIP

**A-CLIP** 通过保留与文本描述语义相关性高的图像 **token**，解决了随机掩码导致的语义信息丢失问题，并通过多视图对比学习进一步优化性能。此外，**A-CLIP** 还引入了辅助自监督学习任务(**SimCLR+BYOL**)，以进一步提升模型性能。

![](https://pic1.imgdb.cn/item/67aee856d0e0a243d4ff1aa5.png)

### ⚪ [<font color=blue>SigLIP</font>](https://0809zheng.github.io/2024/02/29/siglip.html)
- (arXiv2303)Sigmoid Loss for Language Image Pre-Training

**SigLIP**使用**Sigmoid**损失代替**CLIP**中基于**Softmax**的**InfoNCE**损失，将图像-文本对的相似性评分转化为一个二分类问题：匹配的对为正样本（标签为$1$），不匹配的对为负样本（标签为$-1$）。

$$
L = -\frac{1}{\mid B \mid} \sum_{i=1}^{\mid B \mid} \sum_{j=1}^{\mid B \mid} \log\frac{1}{ 1 + e^{z_{ij} (-t x_i \cdot y_j + b)} }
$$

### ⚪ [<font color=blue>SigLIP 2</font>](https://0809zheng.github.io/2025/02/20/siglip2.html)
- (arXiv2502)SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features

**SigLIP 2**在**SigLIP**的基础上引入了基于描述的预训练和自监督损失（如自蒸馏和掩码预测）：
- 基于描述的预训练：引入了 **LocCa** 的描述预训练方法，通过在图像编码器上附加一个标准的 **Transformer** 解码器，模型能够学习生成图像描述、自动指代表达预测和基于区域的描述生成。这些任务不仅提升了模型的 **OCR** 能力，还显著增强了定位任务的性能。
- 自蒸馏和掩码预测：自蒸馏通过让学生网络匹配教师网络（学生网络的滑动平均）的全局表示，而掩码预测则通过让学生网络预测被掩码的图像块的特征。这些技术显著提升了模型在密集预测任务（如分割和深度估计）中的性能。

![](https://pic1.imgdb.cn/item/67c6e8e1d0e0a243d40bde53.png)


### ⚪ [<font color=blue>LaCLIP</font>](https://0809zheng.github.io/2024/01/30/laclip.html)
- (arXiv2305)Improving CLIP Training with Language Rewrites

**LaCLIP**通过语言重写为**CLIP**训练引入文本增强。具体来说，**LaCLIP**利用大型语言模型对每个图像的文本描述进行重写。在训练过程中，**LaCLIP**随机选择原始文本或重写版本作为文本增强，从而显著提升了**CLIP**的迁移性能。

![](https://pic1.imgdb.cn/item/67b58869d0e0a243d400ba7b.png)
