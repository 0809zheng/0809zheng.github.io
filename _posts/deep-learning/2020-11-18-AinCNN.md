---
layout: post
title: '卷积神经网络中的注意力机制(Attention Mechanism)'
date: 2020-11-18
author: 郑之杰
cover: ''
tags: 深度学习
---

> Attention Mechanism in Convolutional Neural Networks.

卷积神经网络中的**注意力机制(Attention Mechanism)**表现为在特征的某个维度上计算相应**统计量**，并根据所计算的统计量对该维度上的每一个元素赋予不同的权重，用以增强网络的特征表达能力。

```python
class Attention(nn.Module):
    def __init__(self, ):
        super(Attention, self).__init__()
        self.layer() = nn.Sequential()
    
    def forward(self, x):
        b, c, h, w = x.size()
        w = self.layer(x)         # 在某特征维度上计算权重
        return x * w.expand_as(x) # 对特征进行加权
```

卷积层的特征维度包括通道维度$C$和空间维度$H,W$，因此注意力机制可以应用在不同维度上：
- 通道注意力(**Channel Attention**)：**SENet**, **GSoP**, **SKNet**, 
- 空间注意力(**Spatial Attention**)：
- 融合注意力(通道+空间)：


## 1. 通道注意力

### ⚪ [<font color=blue>Squeeze-and-Excitation Network (SENet)</font>](https://0809zheng.github.io/2020/10/01/senet.html)

**SENet**对输入特征沿着通道维度计算一阶统计量(全局平均池化)，然后通过带有瓶颈层的全连接层学习通道之间的相关性。

![](https://pic.imgdb.cn/item/63a41844b1fccdcd36de13f2.jpg)

### ⚪ [<font color=blue>Global Second-order Pooling (GSoP)</font>](https://0809zheng.github.io/2020/10/03/gsopnet.html)

**GSoP**对输入特征沿着通道维度进行降维后，计算通道之间的协方差矩阵(二阶统计量)，然后通过按行卷积和全连接层学习通道之间的相关性。

![](https://pic.imgdb.cn/item/63a441b408b6830163cd168d.jpg)

### ⚪ [<font color=blue>Selective Kernel Network (SKNet)</font>](https://0809zheng.github.io/2020/10/02/sknet.html)

**SKNet**首先同时使用不同大小的卷积核作为不同的分支提取特征，然后通过通道注意力机制融合这些特征，以获得不同感受野的信息。

![](https://pic.imgdb.cn/item/63a4269bb1fccdcd36f4c6b3.jpg)

## 2. 空间注意力

### ⚪ 

## 3. 融合注意力

### ⚪ 





# 3. scSE
- paper：[Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)
- 分类：空间注意力+通道注意力

![](https://pic.downk.cc/item/5fbb916ab18d627113f3e062.jpg)

**scSE**包括空间注意力模块**sSE**模块和通道注意力模块**cSE**，分别应用一阶统计量（全局平均池化）后将特征**逐元素相加**。

**Pytorch**代码如下：

```
import torch
import torch.nn as nn

class sSE(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.Conv1x1 = nn.Conv2d(in_channels, 1, kernel_size=1, bias=False)
        self.norm = nn.Sigmoid()

    def forward(self, U):
        q = self.Conv1x1(U)  # U:[bs,c,h,w] to q:[bs,1,h,w]
        q = self.norm(q)
        return U * q  # 广播机制

class cSE(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.Conv_Squeeze = nn.Conv2d(in_channels, in_channels // 2, kernel_size=1, bias=False)
        self.Conv_Excitation = nn.Conv2d(in_channels//2, in_channels, kernel_size=1, bias=False)
        self.norm = nn.Sigmoid()

    def forward(self, U):
        z = self.avgpool(U)# shape: [bs, c, h, w] to [bs, c, 1, 1]
        z = self.Conv_Squeeze(z) # shape: [bs, c/2]
        z = self.Conv_Excitation(z) # shape: [bs, c]
        z = self.norm(z)
        return U * z.expand_as(U)

class csSE(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.cSE = cSE(in_channels)
        self.sSE = sSE(in_channels)

    def forward(self, U):
        U_sse = self.sSE(U)
        U_cse = self.cSE(U)
        return U_cse+U_sse
		
if __name__ == "__main__":
    bs, c, h, w = 10, 3, 64, 64
    in_tensor = torch.ones(bs, c, h, w)
    cs_se = csSE(c)
    print("in shape:",in_tensor.shape)
    out_tensor = cs_se(in_tensor)
    print("out shape:", out_tensor.shape)
```

# 4. BAM
- paper：[BAM: Bottleneck Attention Module](https://arxiv.org/abs/1807.06514)
- 分类：空间注意力+通道注意力

![](https://pic.downk.cc/item/5fbb9582b18d627113f4d5db.jpg)

**BAM**将空间注意力和通道注意力**并联**使用，将两种注意力生成的**mask**通过广播相乘得到对所有元素的**mask**。

**Pytorch**代码如下：

```
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)

class ChannelGate(nn.Module):
    def __init__(self, gate_channel, reduction_ratio=16, num_layers=1):
        super(ChannelGate, self).__init__()
        self.gate_c = nn.Sequential()
        self.gate_c.add_module('flatten', Flatten())

        gate_channels = [gate_channel]  # eg 64
        gate_channels += [gate_channel // reduction_ratio] * num_layers  # eg 4
        gate_channels += [gate_channel]  # 64
        # gate_channels: [64, 4, 64]

        for i in range(len(gate_channels) - 2):
            self.gate_c.add_module(
                'gate_c_fc_%d' % i,
                nn.Linear(gate_channels[i], gate_channels[i + 1]))
            self.gate_c.add_module('gate_c_bn_%d' % (i + 1),
                                   nn.BatchNorm1d(gate_channels[i + 1]))
            self.gate_c.add_module('gate_c_relu_%d' % (i + 1), nn.ReLU())

        self.gate_c.add_module('gate_c_fc_final',
                               nn.Linear(gate_channels[-2], gate_channels[-1]))

    def forward(self, x):
        avg_pool = F.avg_pool2d(x, x.size(2), stride=x.size(2))
        return self.gate_c(avg_pool).unsqueeze(2).unsqueeze(3).expand_as(x)

class SpatialGate(nn.Module):
    def __init__(self,
                 gate_channel,
                 reduction_ratio=16,
                 dilation_conv_num=2,
                 dilation_val=4):
        super(SpatialGate, self).__init__()
        self.gate_s = nn.Sequential()

        self.gate_s.add_module(
            'gate_s_conv_reduce0',
            nn.Conv2d(gate_channel,
                      gate_channel // reduction_ratio,
                      kernel_size=1))
        self.gate_s.add_module('gate_s_bn_reduce0',
                               nn.BatchNorm2d(gate_channel // reduction_ratio))
        self.gate_s.add_module('gate_s_relu_reduce0', nn.ReLU())

        # 进行多个空洞卷积，丰富感受野
        for i in range(dilation_conv_num):
            self.gate_s.add_module(
                'gate_s_conv_di_%d' % i,
                nn.Conv2d(gate_channel // reduction_ratio,
                          gate_channel // reduction_ratio,
                          kernel_size=3,
                          padding=dilation_val,
                          dilation=dilation_val))
            self.gate_s.add_module(
                'gate_s_bn_di_%d' % i,
                nn.BatchNorm2d(gate_channel // reduction_ratio))
            self.gate_s.add_module('gate_s_relu_di_%d' % i, nn.ReLU())

        self.gate_s.add_module(
            'gate_s_conv_final',
            nn.Conv2d(gate_channel // reduction_ratio, 1, kernel_size=1))

    def forward(self, x):
        return self.gate_s(x).expand_as(x)

class BAM(nn.Module):
    def __init__(self, gate_channel):
        super(BAM, self).__init__()
        self.channel_att = ChannelGate(gate_channel)
        self.spatial_att = SpatialGate(gate_channel)

    def forward(self, x):
        att = 1 + F.sigmoid(self.channel_att(x) * self.spatial_att(x))
        return att * x
```

# 5. CBAM
- paper：[CBAM: Convolutional Block Attention Module](https://arxiv.org/abs/1807.06521)
- 分类：空间注意力+通道注意力

![](https://pic.downk.cc/item/5fbb9b16b18d627113f63c65.jpg)

**CBAM**将通道注意力和空间注意力**串联**使用。每种注意力机制使用两个一阶统计量（**全局最大**和**全局平均**）。

**Pytorch**代码如下：

```
import torch
import torch.nn as nn

def conv3x3(in_planes, out_planes, stride=1):
    "3x3 convolution with padding"
    return nn.Conv2d(in_planes,
                     out_planes,
                     kernel_size=3,
                     stride=stride,
                     padding=1,
                     bias=False)

class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=4):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.sharedMLP = nn.Sequential(
            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False), nn.ReLU(),
            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False))
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avgout = self.sharedMLP(self.avg_pool(x))
        maxout = self.sharedMLP(self.max_pool(x))
        return self.sigmoid(avgout + maxout)


class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        assert kernel_size in (3, 7), "kernel size must be 3 or 7"
        padding = 3 if kernel_size == 7 else 1
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avgout = torch.mean(x, dim=1, keepdim=True)
        maxout, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avgout, maxout], dim=1)
        x = self.conv(x)
        return self.sigmoid(x)

class BasicBlock(nn.Module):
    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm2d(planes)
        self.ca = ChannelAttention(planes)
        self.sa = SpatialAttention()
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.ca(out) * out  # 广播机制
        out = self.sa(out) * out  # 广播机制
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out
		
if __name__ == "__main__":
    downsample = nn.Sequential(
        nn.Conv2d(16, 32, kernel_size=1, stride=1, bias=False),
        nn.BatchNorm2d(32))
    x = torch.ones(3, 16, 32, 32)
    model = BasicBlock(16, 32, stride=1, downsample=downsample)
    print(model(x).shape)
```


# 6. Others
一些新的注意力机制模型：
- [ResNeSt](https://0809zheng.github.io/2020/09/09/resnest.html)：通道注意力，为网络多路径**cardinality**中引入注意力。
- [Attentional Feature Fusion](https://0809zheng.github.io/2020/12/01/aff.html)：通道注意力，为特征融合引入注意力。
- [Shuffle Attention: SANet](https://0809zheng.github.io/2021/01/30/sanet.html)：通道注意力+空间注意力，通过特征分组与通道置换实现轻量型注意力计算。
- [Coordinate Attention](https://0809zheng.github.io/2021/03/06/ca.html)：通道注意力，适用于轻量性网络的坐标注意力机制。



### ⚪ 参考文献

- [<font color=blue>Squeeze-and-Excitation Networks</font>](https://0809zheng.github.io/2020/10/01/senet.html)：(arXiv1709)SENet：卷积神经网络的通道注意力机制。
- [<font color=blue>Global Second-order Pooling Convolutional Networks</font>](https://0809zheng.github.io/2020/10/03/gsopnet.html)：(arXiv1811)GSoP-Net：全局二阶池化卷积网络。
- [<font color=blue>Selective Kernel Networks</font>](https://0809zheng.github.io/2020/10/02/sknet.html)：(arXiv1903)SKNet：通过注意力机制实现卷积核尺寸选择。

- [Selective Kernel Networks](https://0809zheng.github.io/2020/09/07/generalize.html)：(arXiv1903)SKNet：卷积核选择网络。

- [ResNeSt: Split-Attention Networks](https://0809zheng.github.io/2020/09/09/resnest.html)：(arXiv2004)ResNeSt：拆分注意力网络。

- [Attentional Feature Fusion](https://0809zheng.github.io/2020/12/01/aff.html)：(arXiv2009)AFF：特征通道注意力融合。

- [SA-Net: Shuffle Attention for Deep Convolutional Neural Networks](https://0809zheng.github.io/2021/01/30/sanet.html)：(arXiv2102)SANet：通过特征分组和通道置换实现轻量型置换注意力。

- [Coordinate Attention for Efficient Mobile Network Design](https://0809zheng.github.io/2021/03/06/ca.html)：(arXiv2103)为轻量型网络设计的坐标注意力机制。