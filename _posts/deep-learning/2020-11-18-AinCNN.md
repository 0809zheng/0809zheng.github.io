---
layout: post
title: '卷积神经网络中的注意力机制'
date: 2020-11-18
author: 郑之杰
cover: 'https://pic.downk.cc/item/5fbb8efbb18d627113f340fe.jpg'
tags: 深度学习
---

> Attention Mechanism in Convolutional Neural Networks.

近些年**注意力机制(Attention Mechanism)**在卷积神经网络中应用广泛。研究者们提出了若干种不同的注意力模块，用以增强模型的特征表达能力。

卷积神经网络中的注意力机制表现为在特征的某个维度上计算**统计量**，并根据所计算的统计量对该维度上的每一个位置或通道赋予不同的权重。根据选择维度不同，可以分为**通道注意力(Channel Attention)**和**空间注意力(Spatial Attention)**。

本文目录：
1. [SENet](https://0809zheng.github.io/2020/11/18/AinCNN.html#1-senet)：通道注意力
2. [SKNet](https://0809zheng.github.io/2020/11/18/AinCNN.html#2-sknet)：通道注意力
3. [scSE](https://0809zheng.github.io/2020/11/18/AinCNN.html#3-scse)：空间注意力+通道注意力
4. [BAM](https://0809zheng.github.io/2020/11/18/AinCNN.html#4-bam)：空间注意力+通道注意力
5. [CBAM](https://0809zheng.github.io/2020/11/18/AinCNN.html#5-cbam)：空间注意力+通道注意力
6. [其他注意力模型](https://0809zheng.github.io/2020/11/18/AinCNN.html#6-others)


# 1. SENet
- paper：[Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)
- 分类：通道注意力

![](https://pic.downk.cc/item/5fbb8efbb18d627113f340fe.jpg)

最早应用在卷积神经网络中的注意力机制。对通道选取一阶统计量（**全局平均池化**），并引入了$r=16$倍降采样的瓶颈层(**bottleneck**)。

下面说明瓶颈层的用途。对于一个$1 \times 1$卷积层，若输入通道数和输出通道数分别为$C_{in}$和$C_{out}$，则该层的参数量为$C_{in} \times C_{out}$。中间引入$r$倍降采样的瓶颈层，则两层的参数总量为$C_{in} \times \frac{C_{in}}{r} + \frac{C_{in}}{r} \times C_{out} = \frac{C_{in}}{r} \times (C_{in}+C_{out})$，该参数量通常远小于之前的参数量。

**Pytorch**代码如下：

```
import torch.nn as nn

class SELayer(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel//reduction,bias=False),
            nn.ReLU(inplace=True),
            nn.Linear(channel//reduction,channel, bias=False),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        b,c,h,w = x.size()
        y = self.avgpool(x).view(b,c)
        y = self.fc(y).view(b,c,1,1)
        return x * y.expand_as(x)
```


# 2. SKNet
- paper：[Selective Kernel Networks](https://arxiv.org/abs/1903.06586?context=cs)
- 分类：通道注意力

![](https://pic.downk.cc/item/5f559bd0160a154a6741aace.jpg)

**SENet**的改进版本，同时使用不同大小的卷积核作为不同的分支提取特征，再根据通道注意力融合这些特征。

**Pytorch**代码如下：

```
import torch
import torch.nn as nn

class SKLayer(nn.Module):
    def __init__(self, features, WH, M, G, r, stride=1, L=32):
        super(SKLayer, self).__init__()

        """ Constructor
        Args:
            features: input channel dimensionality.
            WH: input spatial dimensionality, used for GAP kernel size.
            M: the number of branchs.
            G: num of convolution groups.
            r: the radio for compute d, the length of z.
            stride: stride, default 1.
            L: the minimum dim of the vector z in paper, default 32.
        """

        d = max(int(features / r), L)
        self.M = M
        self.features = features
        self.convs = nn.ModuleList([])
        for i in range(M):
            self.convs.append(
                nn.Sequential(
                    nn.Conv2d(features,
                              features,
                              kernel_size=3 + i * 2,
                              stride=stride,
                              padding=1 + i,
                              groups=G),
                    nn.BatchNorm2d(features),
                    nn.ReLU(inplace=False)))
        self.fc = nn.Linear(features, d)
        self.fcs = nn.ModuleList([])
        for i in range(M):
            self.fcs.append(nn.Linear(d, features))
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        for i, conv in enumerate(self.convs):
            fea = conv(x).unsqueeze_(dim=1)
            if i == 0:
                feas = fea
            else:
                feas = torch.cat([feas, fea], dim=1)
        fea_U = torch.sum(feas, dim=1)
        fea_s = fea_U.mean(-1).mean(-1)
        fea_z = self.fc(fea_s)
        for i, fc in enumerate(self.fcs):
            vector = fc(fea_z).unsqueeze_(dim=1)
            if i == 0:
                attention_vectors = vector
            else:
                attention_vectors = torch.cat([attention_vectors, vector],
                                              dim=1)
        attention_vectors = self.softmax(attention_vectors)
        attention_vectors = attention_vectors.unsqueeze(-1).unsqueeze(-1)
        fea_v = (feas * attention_vectors).sum(dim=1)
        return fea_v
		
if __name__ == "__main__":
    t = torch.ones((32, 256, 24,24))
    sk = SKConv(256,WH=1,M=2,G=1,r=2)
    out = sk(t)
    print(out.shape)
```

# 3. scSE
- paper：[Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)
- 分类：空间注意力+通道注意力

![](https://pic.downk.cc/item/5fbb916ab18d627113f3e062.jpg)

**scSE**包括空间注意力模块**sSE**模块和通道注意力模块**cSE**，分别应用一阶统计量（全局平均池化）后将特征**逐元素相加**。

**Pytorch**代码如下：

```
import torch
import torch.nn as nn

class sSE(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.Conv1x1 = nn.Conv2d(in_channels, 1, kernel_size=1, bias=False)
        self.norm = nn.Sigmoid()

    def forward(self, U):
        q = self.Conv1x1(U)  # U:[bs,c,h,w] to q:[bs,1,h,w]
        q = self.norm(q)
        return U * q  # 广播机制

class cSE(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.avgpool = nn.AdaptiveAvgPool2d(1)
        self.Conv_Squeeze = nn.Conv2d(in_channels, in_channels // 2, kernel_size=1, bias=False)
        self.Conv_Excitation = nn.Conv2d(in_channels//2, in_channels, kernel_size=1, bias=False)
        self.norm = nn.Sigmoid()

    def forward(self, U):
        z = self.avgpool(U)# shape: [bs, c, h, w] to [bs, c, 1, 1]
        z = self.Conv_Squeeze(z) # shape: [bs, c/2]
        z = self.Conv_Excitation(z) # shape: [bs, c]
        z = self.norm(z)
        return U * z.expand_as(U)

class csSE(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.cSE = cSE(in_channels)
        self.sSE = sSE(in_channels)

    def forward(self, U):
        U_sse = self.sSE(U)
        U_cse = self.cSE(U)
        return U_cse+U_sse
		
if __name__ == "__main__":
    bs, c, h, w = 10, 3, 64, 64
    in_tensor = torch.ones(bs, c, h, w)
    cs_se = csSE(c)
    print("in shape:",in_tensor.shape)
    out_tensor = cs_se(in_tensor)
    print("out shape:", out_tensor.shape)
```

# 4. BAM
- paper：[BAM: Bottleneck Attention Module](https://arxiv.org/abs/1807.06514)
- 分类：空间注意力+通道注意力

![](https://pic.downk.cc/item/5fbb9582b18d627113f4d5db.jpg)

**BAM**将空间注意力和通道注意力**并联**使用，将两种注意力生成的**mask**通过广播相乘得到对所有元素的**mask**。

**Pytorch**代码如下：

```
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)

class ChannelGate(nn.Module):
    def __init__(self, gate_channel, reduction_ratio=16, num_layers=1):
        super(ChannelGate, self).__init__()
        self.gate_c = nn.Sequential()
        self.gate_c.add_module('flatten', Flatten())

        gate_channels = [gate_channel]  # eg 64
        gate_channels += [gate_channel // reduction_ratio] * num_layers  # eg 4
        gate_channels += [gate_channel]  # 64
        # gate_channels: [64, 4, 64]

        for i in range(len(gate_channels) - 2):
            self.gate_c.add_module(
                'gate_c_fc_%d' % i,
                nn.Linear(gate_channels[i], gate_channels[i + 1]))
            self.gate_c.add_module('gate_c_bn_%d' % (i + 1),
                                   nn.BatchNorm1d(gate_channels[i + 1]))
            self.gate_c.add_module('gate_c_relu_%d' % (i + 1), nn.ReLU())

        self.gate_c.add_module('gate_c_fc_final',
                               nn.Linear(gate_channels[-2], gate_channels[-1]))

    def forward(self, x):
        avg_pool = F.avg_pool2d(x, x.size(2), stride=x.size(2))
        return self.gate_c(avg_pool).unsqueeze(2).unsqueeze(3).expand_as(x)

class SpatialGate(nn.Module):
    def __init__(self,
                 gate_channel,
                 reduction_ratio=16,
                 dilation_conv_num=2,
                 dilation_val=4):
        super(SpatialGate, self).__init__()
        self.gate_s = nn.Sequential()

        self.gate_s.add_module(
            'gate_s_conv_reduce0',
            nn.Conv2d(gate_channel,
                      gate_channel // reduction_ratio,
                      kernel_size=1))
        self.gate_s.add_module('gate_s_bn_reduce0',
                               nn.BatchNorm2d(gate_channel // reduction_ratio))
        self.gate_s.add_module('gate_s_relu_reduce0', nn.ReLU())

        # 进行多个空洞卷积，丰富感受野
        for i in range(dilation_conv_num):
            self.gate_s.add_module(
                'gate_s_conv_di_%d' % i,
                nn.Conv2d(gate_channel // reduction_ratio,
                          gate_channel // reduction_ratio,
                          kernel_size=3,
                          padding=dilation_val,
                          dilation=dilation_val))
            self.gate_s.add_module(
                'gate_s_bn_di_%d' % i,
                nn.BatchNorm2d(gate_channel // reduction_ratio))
            self.gate_s.add_module('gate_s_relu_di_%d' % i, nn.ReLU())

        self.gate_s.add_module(
            'gate_s_conv_final',
            nn.Conv2d(gate_channel // reduction_ratio, 1, kernel_size=1))

    def forward(self, x):
        return self.gate_s(x).expand_as(x)

class BAM(nn.Module):
    def __init__(self, gate_channel):
        super(BAM, self).__init__()
        self.channel_att = ChannelGate(gate_channel)
        self.spatial_att = SpatialGate(gate_channel)

    def forward(self, x):
        att = 1 + F.sigmoid(self.channel_att(x) * self.spatial_att(x))
        return att * x
```

# 5. CBAM
- paper：[CBAM: Convolutional Block Attention Module](https://arxiv.org/abs/1807.06521)
- 分类：空间注意力+通道注意力

![](https://pic.downk.cc/item/5fbb9b16b18d627113f63c65.jpg)

**CBAM**将通道注意力和空间注意力**串联**使用。每种注意力机制使用两个一阶统计量（**全局最大**和**全局平均**）。

**Pytorch**代码如下：

```
import torch
import torch.nn as nn

def conv3x3(in_planes, out_planes, stride=1):
    "3x3 convolution with padding"
    return nn.Conv2d(in_planes,
                     out_planes,
                     kernel_size=3,
                     stride=stride,
                     padding=1,
                     bias=False)

class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=4):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.sharedMLP = nn.Sequential(
            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False), nn.ReLU(),
            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False))
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avgout = self.sharedMLP(self.avg_pool(x))
        maxout = self.sharedMLP(self.max_pool(x))
        return self.sigmoid(avgout + maxout)


class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        assert kernel_size in (3, 7), "kernel size must be 3 or 7"
        padding = 3 if kernel_size == 7 else 1
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avgout = torch.mean(x, dim=1, keepdim=True)
        maxout, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avgout, maxout], dim=1)
        x = self.conv(x)
        return self.sigmoid(x)

class BasicBlock(nn.Module):
    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm2d(planes)
        self.ca = ChannelAttention(planes)
        self.sa = SpatialAttention()
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.ca(out) * out  # 广播机制
        out = self.sa(out) * out  # 广播机制
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out
		
if __name__ == "__main__":
    downsample = nn.Sequential(
        nn.Conv2d(16, 32, kernel_size=1, stride=1, bias=False),
        nn.BatchNorm2d(32))
    x = torch.ones(3, 16, 32, 32)
    model = BasicBlock(16, 32, stride=1, downsample=downsample)
    print(model(x).shape)
```


# 6. Others
一些新的注意力机制模型：
- [ResNeSt](https://0809zheng.github.io/2020/09/09/resnest.html)：通道注意力，为网络多路径**cardinality**中引入注意力。
- [Attentional Feature Fusion](https://0809zheng.github.io/2020/12/01/aff.html)：通道注意力，为特征融合引入注意力。
- [SANet](https://0809zheng.github.io/2021/01/30/sa.html)：通道注意力+空间注意力，通过特征分组与通道置换实现轻量型注意力计算。


