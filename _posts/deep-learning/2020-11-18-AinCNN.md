---
layout: post
title: '卷积神经网络中的注意力机制(Attention Mechanism)'
date: 2020-11-18
author: 郑之杰
cover: ''
tags: 深度学习
---

> Attention Mechanism in Convolutional Neural Networks.

卷积神经网络中的**注意力机制(Attention Mechanism)**表现为在特征的某个维度上计算相应**统计量**，并根据所计算的统计量对该维度上的每一个元素赋予不同的权重，用以增强网络的特征表达能力。

```python
class Attention(nn.Module):
    def __init__(self, ):
        super(Attention, self).__init__()
        self.layer() = nn.Sequential()
    
    def forward(self, x):
        b, c, h, w = x.size()
        w = self.layer(x)         # 在某特征维度上计算权重
        return x * w.expand_as(x) # 对特征进行加权
```

卷积层的特征维度包括通道维度$C$和空间维度$H,W$，因此注意力机制可以应用在不同维度上：
- 通道注意力(**Channel Attention**)：**SENet**, **GSoP**, **SKNet**, 
- 空间注意力(**Spatial Attention**)：
- 融合注意力(通道+空间)：(并联)**BAM**; (串联)**CBAM**


## 1. 通道注意力

### ⚪ [<font color=blue>Squeeze-and-Excitation Network (SENet)</font>](https://0809zheng.github.io/2020/10/01/senet.html)

**SENet**对输入特征沿着通道维度计算一阶统计量(全局平均池化)，然后通过带有瓶颈层的全连接层学习通道之间的相关性。

![](https://pic.imgdb.cn/item/63a41844b1fccdcd36de13f2.jpg)

### ⚪ [<font color=blue>Global Second-order Pooling (GSoP)</font>](https://0809zheng.github.io/2020/10/03/gsopnet.html)

**GSoP**对输入特征沿着通道维度进行降维后，计算通道之间的协方差矩阵(二阶统计量)，然后通过按行卷积和全连接层学习通道之间的相关性。

![](https://pic.imgdb.cn/item/63a441b408b6830163cd168d.jpg)

### ⚪ [<font color=blue>Selective Kernel Network (SKNet)</font>](https://0809zheng.github.io/2020/10/02/sknet.html)

**SKNet**首先同时使用不同大小的卷积核作为不同的分支提取特征，然后通过通道注意力机制融合这些特征，以获得不同感受野的信息。

![](https://pic.imgdb.cn/item/63a4269bb1fccdcd36f4c6b3.jpg)

## 2. 空间注意力

### ⚪ 

## 3. 融合注意力

### ⚪ [<font color=blue>Concurrent Spatial and Channel Squeeze & Excitation (scSE)</font>](https://0809zheng.github.io/2020/10/06/scse.html)

**scSE**模块通过并联使用通道注意力和空间注意力增强特征的表达能力。通道注意力通过全局平均池化和全连接层实现；空间注意力通过$1\times 1$卷积实现。

![](https://pic.imgdb.cn/item/63a51eee08b6830163cb93b0.jpg)

### ⚪ [<font color=blue>Bottleneck Attention Module (BAM)</font>](https://0809zheng.github.io/2020/10/04/bam.html)

**BAM**模块通过并联使用通道注意力和空间注意力增强特征的表达能力。通道注意力通过全局平均池化和全连接层实现；空间注意力通过$1\times 1$卷积和空洞卷积实现。

![](https://pic.imgdb.cn/item/63a50e2908b6830163b5939c.jpg)

### ⚪ [<font color=blue>Convolutional Block Attention Module (CBAM)</font>](https://0809zheng.github.io/2020/10/05/cbam.html)

**CBAM**模块通过串联使用通道注意力和空间注意力增强特征的表达能力，每种注意力机制构造两个一阶统计量（全局最大池化和全局平均池化）。

![](https://pic.imgdb.cn/item/63a516a408b6830163c0658b.jpg)








# 6. Others
一些新的注意力机制模型：
- [ResNeSt](https://0809zheng.github.io/2020/09/09/resnest.html)：通道注意力，为网络多路径**cardinality**中引入注意力。
- [Attentional Feature Fusion](https://0809zheng.github.io/2020/12/01/aff.html)：通道注意力，为特征融合引入注意力。
- [Shuffle Attention: SANet](https://0809zheng.github.io/2021/01/30/sanet.html)：通道注意力+空间注意力，通过特征分组与通道置换实现轻量型注意力计算。
- [Coordinate Attention](https://0809zheng.github.io/2021/03/06/ca.html)：通道注意力，适用于轻量性网络的坐标注意力机制。



### ⚪ 参考文献

- [<font color=blue>Squeeze-and-Excitation Networks</font>](https://0809zheng.github.io/2020/10/01/senet.html)：(arXiv1709)SENet：卷积神经网络的通道注意力机制。
- [<font color=blue>Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks</font>](https://0809zheng.github.io/2020/10/06/scse.html)：(arXiv1803)scSE：全卷积网络中的并行空间和通道注意力模块。
- [<font color=blue>BAM: Bottleneck Attention Module</font>](https://0809zheng.github.io/2020/10/04/bam.html)：(arXiv1807)BAM：瓶颈注意力模块。
- [<font color=blue>CBAM: Convolutional Block Attention Module</font>](https://0809zheng.github.io/2020/10/05/cbam.html)：(arXiv1807)CBAM：卷积块注意力模块。
- [<font color=blue>Global Second-order Pooling Convolutional Networks</font>](https://0809zheng.github.io/2020/10/03/gsopnet.html)：(arXiv1811)GSoP-Net：全局二阶池化卷积网络。
- [<font color=blue>Selective Kernel Networks</font>](https://0809zheng.github.io/2020/10/02/sknet.html)：(arXiv1903)SKNet：通过注意力机制实现卷积核尺寸选择。





- [ResNeSt: Split-Attention Networks](https://0809zheng.github.io/2020/09/09/resnest.html)：(arXiv2004)ResNeSt：拆分注意力网络。

- [Attentional Feature Fusion](https://0809zheng.github.io/2020/12/01/aff.html)：(arXiv2009)AFF：特征通道注意力融合。

- [SA-Net: Shuffle Attention for Deep Convolutional Neural Networks](https://0809zheng.github.io/2021/01/30/sanet.html)：(arXiv2102)SANet：通过特征分组和通道置换实现轻量型置换注意力。

- [Coordinate Attention for Efficient Mobile Network Design](https://0809zheng.github.io/2021/03/06/ca.html)：(arXiv2103)为轻量型网络设计的坐标注意力机制。