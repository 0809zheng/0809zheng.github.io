---
layout: post
title: '卷积神经网络中的注意力机制(Attention Mechanism)'
date: 2020-11-18
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63aaba3608b6830163a51dfb.jpg'
tags: 深度学习
---

> Attention Mechanism in Convolutional Neural Networks.

卷积神经网络中的**注意力机制(Attention Mechanism)**表现为在特征的某个维度上计算相应**统计量**，并根据所计算的统计量对该维度上的每一个元素赋予不同的权重，用以增强网络的特征表达能力。

```python
class Attention(nn.Module):
    def __init__(self, ):
        super(Attention, self).__init__()
        self.layer() = nn.Sequential()
    
    def forward(self, x):
        b, c, h, w = x.size()
        w = self.layer(x)         # 在某特征维度上计算权重
        return x * w.expand_as(x) # 对特征进行加权
```

卷积层的特征维度包括通道维度$C$和空间维度$H,W$，因此注意力机制可以应用在不同维度上：
- 通道注意力(**Channel Attention**)：**SENet**, **GSoP**, **SKNet**, **ECA-Net**, **FcaNet**
- 空间注意力(**Spatial Attention**)：**SGE**,
- 融合注意力(通道+空间)：(并联)**scSE**, **BAM**, **SA-Net**; (串联)**CBAM**


## 1. 通道注意力

### ⚪ [<font color=blue>Squeeze-and-Excitation Network (SENet)</font>](https://0809zheng.github.io/2020/10/01/senet.html)

**SENet**对输入特征沿着通道维度计算一阶统计量(全局平均池化)，然后通过带有瓶颈层的全连接层学习通道之间的相关性。

![](https://pic.imgdb.cn/item/63a41844b1fccdcd36de13f2.jpg)

### ⚪ [<font color=blue>Global Second-order Pooling (GSoP)</font>](https://0809zheng.github.io/2020/10/03/gsopnet.html)

**GSoP**对输入特征沿着通道维度进行降维后，计算通道之间的协方差矩阵(二阶统计量)，然后通过按行卷积和全连接层学习通道之间的相关性。

![](https://pic.imgdb.cn/item/63a441b408b6830163cd168d.jpg)

### ⚪ [<font color=blue>Selective Kernel Network (SKNet)</font>](https://0809zheng.github.io/2020/10/02/sknet.html)

**SKNet**首先同时使用不同大小的卷积核作为不同的分支提取特征，然后通过通道注意力机制融合这些特征，以获得不同感受野的信息。

![](https://pic.imgdb.cn/item/63a4269bb1fccdcd36f4c6b3.jpg)

### ⚪ [<font color=blue>Efficient Channel Attention (ECA-Net)</font>](https://0809zheng.github.io/2020/10/07/ecanet.html)

**ECA-Net**通过把通道注意力模块中的全连接层替换为一维卷积层，实现了轻量级的通道注意力。

![](https://pic.imgdb.cn/item/63a5570808b6830163258973.jpg)

### ⚪ [<font color=blue>Multi-Spectral Channel Attention (FcaNet)</font>](https://0809zheng.github.io/2020/10/09/fcanet.html)

**FcaNet**首先选择应用离散余弦变换后**Top-n**个性能最佳的频率分量标号，然后把输入特征沿通道划分为$n$等份，对每份计算其对应的**DCT**频率分量，并与对应的特征分组相乘。

![](https://pic.imgdb.cn/item/63a6645a08b6830163bc8227.jpg)

## 2. 空间注意力

### ⚪ [<font color=blue>Spatial Group-wise Enhance (SGE)</font>](https://0809zheng.github.io/2020/10/07/ecanet.html)

**SGE**把输入特征进行分组，对每组特征在空间维度上与其全局平均池化特征做点积后进行标准化，然后通过学习两个仿射参数(缩放和偏移)实现空间注意力。

![](https://pic.imgdb.cn/item/63a55f3d08b683016332fdc6.jpg)

## 3. 融合注意力：并联

### ⚪ [<font color=blue>Concurrent Spatial and Channel Squeeze & Excitation (scSE)</font>](https://0809zheng.github.io/2020/10/06/scse.html)

**scSE**通过并联使用通道注意力和空间注意力增强特征的表达能力。通道注意力通过全局平均池化和全连接层实现；空间注意力通过$1\times 1$卷积实现。

![](https://pic.imgdb.cn/item/63a51eee08b6830163cb93b0.jpg)

### ⚪ [<font color=blue>Bottleneck Attention Module (BAM)</font>](https://0809zheng.github.io/2020/10/04/bam.html)

**BAM**通过并联使用通道注意力和空间注意力增强特征的表达能力。通道注意力通过全局平均池化和全连接层实现；空间注意力通过$1\times 1$卷积和空洞卷积实现。

![](https://pic.imgdb.cn/item/63a50e2908b6830163b5939c.jpg)


### ⚪ [<font color=blue>Shuffle Attention (SA-Net)</font>](https://0809zheng.github.io/2021/01/30/sanet.html)

**SA-Net**把输入特征沿通道维度拆分为$g$组，对每组特征再次沿通道平均拆分后应用并行的通道注意力和空间注意力，之后集成所有特征，并通过通道置换操作进行不同通道间的交互。

![](https://img.imgdb.cn/item/601cb04a3ffa7d37b3c0b9c2.jpg)

## 3. 融合注意力：串联

### ⚪ [<font color=blue>Convolutional Block Attention Module (CBAM)</font>](https://0809zheng.github.io/2020/10/05/cbam.html)

**CBAM**通过串联使用通道注意力和空间注意力增强特征的表达能力，每种注意力机制构造两个一阶统计量（全局最大池化和全局平均池化）。

![](https://pic.imgdb.cn/item/63a516a408b6830163c0658b.jpg)

### ⭐ 参考文献
- [Attention Mechanisms in Computer Vision: A Survey](https://arxiv.org/abs/2111.07624)(arXiv1709)一篇卷积神经网络中的注意力机制综述。
- [<font color=blue>Squeeze-and-Excitation Networks</font>](https://0809zheng.github.io/2020/10/01/senet.html)：(arXiv1709)SENet：卷积神经网络的通道注意力机制。
- [<font color=blue>Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks</font>](https://0809zheng.github.io/2020/10/06/scse.html)：(arXiv1803)scSE：全卷积网络中的并行空间和通道注意力模块。
- [<font color=blue>BAM: Bottleneck Attention Module</font>](https://0809zheng.github.io/2020/10/04/bam.html)：(arXiv1807)BAM：瓶颈注意力模块。
- [<font color=blue>CBAM: Convolutional Block Attention Module</font>](https://0809zheng.github.io/2020/10/05/cbam.html)：(arXiv1807)CBAM：卷积块注意力模块。
- [<font color=blue>Global Second-order Pooling Convolutional Networks</font>](https://0809zheng.github.io/2020/10/03/gsopnet.html)：(arXiv1811)GSoP-Net：全局二阶池化卷积网络。
- [<font color=blue>Selective Kernel Networks</font>](https://0809zheng.github.io/2020/10/02/sknet.html)：(arXiv1903)SKNet：通过注意力机制实现卷积核尺寸选择。
- [<font color=blue>Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks</font>](https://0809zheng.github.io/2020/10/07/ecanet.html)：(arXiv1905)通过空间分组增强模块提高卷积网络的语义特征学习能力。
- [<font color=blue>ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks</font>](https://0809zheng.github.io/2020/10/07/ecanet.html)：(arXiv1910)ECA-Net：卷积神经网络的高效通道注意力机制。
- [<font color=blue>FcaNet: Frequency Channel Attention Networks</font>](https://0809zheng.github.io/2020/10/09/fcanet.html)：(arXiv2012)FcaNet：频域通道注意力网络。
- [<font color=blue>SA-Net: Shuffle Attention for Deep Convolutional Neural Networks</font>](https://0809zheng.github.io/2021/01/30/sanet.html)：(arXiv2102)SANet：通过特征分组和通道置换实现轻量型置换注意力。




- [ResNeSt: Split-Attention Networks](https://0809zheng.github.io/2020/09/09/resnest.html)：(arXiv2004)ResNeSt：拆分注意力网络。

- [Attentional Feature Fusion](https://0809zheng.github.io/2020/12/01/aff.html)：(arXiv2009)AFF：特征通道注意力融合。



- [Coordinate Attention for Efficient Mobile Network Design](https://0809zheng.github.io/2021/03/06/ca.html)：(arXiv2103)为轻量型网络设计的坐标注意力机制。



### Residual Attention Network for Image Classification
- intro:Residual Attention Network
- keypoint:trunk brunch + mask brunch
- arXiv:[https://arxiv.org/abs/1704.06904](https://arxiv.org/abs/1704.06904)

![](https://pic.downk.cc/item/5e82b31f504f4bcb04d14747.png)




### Competitive Inner-Imaging Squeeze and Excitation for Residual Network
- intro:Competitive SENet
- arXiv:[https://arxiv.org/abs/1807.08920v4](https://arxiv.org/abs/1807.08920v4)

![](https://pic.downk.cc/item/5e82bbfd504f4bcb04d8bf5a.png)
![](https://pic.downk.cc/item/5e82bc12504f4bcb04d8d3ee.png)