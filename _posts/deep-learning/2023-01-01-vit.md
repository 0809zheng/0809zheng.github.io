---
layout: post
title: '视觉Transformer(Vision Transformer)'
date: 2023-01-01
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63f48f89f144a0100755912f.jpg'
tags: 深度学习
---

> Vision Transformer.

[<font color=blue>Transformer</font>](https://0809zheng.github.io/2020/04/25/transformer.html)是基于**自注意力机制(self-attention mechanism)**的深度神经网络，该模型在$2017$年$6$月被提出，并逐渐在自然语言处理任务上取得最好的性能。

![](https://pic.downk.cc/item/5fe9916d3ffa7d37b3c174be.jpg)

**Transformer**最近被扩展到计算机视觉任务上。由于**Transformer**缺少**CNN**的**inductive biases**如平移等变性 (**Translation equivariance**)，通常认为**Transformer**在图像领域需要大量的数据或较强的数据增强才能完成训练。随着结构设计不断精细，也有一些视觉**Transformer**只依赖小数据集就能取得较好的表现。

本文主要介绍视觉**Transformer**在基础视觉任务(即图像分类)上的应用，这些模型训练完成后正如图像识别的**CNN**模型一样，可以作为**backbone**迁移到不同的下游视觉任务上，如目标检测、图像分割或**low-level**视觉任务。

# 1. 视觉Transformer的基本架构

[<font color=Blue>ViT</font>](https://0809zheng.github.io/2020/12/30/vit.html)是最早把**Transformer**引入图像分类任务的工作之一。在**ViT**中，输入图像被划分为一系列图像块(**patch**)；使用嵌入层把每个图像块编码为序列向量；再使用**Transformer**编码器进行特征提取；在输入序列中额外引入一个分类**token**，则对应的输出特征用于分类任务。

## (1) Patch Tokenization

把输入图像划分成若干图像块，对每个图像块通过线性映射转换为嵌入向量，并在起始位置增加一个类别嵌入；最后加入可学习的位置编码。

```python
from torch import nn
from einops.layers.torch import Rearrange

self.to_patch_embedding = nn.Sequential(
    Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),
    nn.LayerNorm(patch_dim),
    nn.Linear(patch_dim, dim),
    nn.LayerNorm(dim),
)
self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))

x = self.to_patch_embedding(img)
b, n, _ = x.shape
cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)
x = torch.cat((cls_tokens, x), dim=1)
x += self.pos_embedding[:, :(n + 1)]
```

## (2) Transformer Encoder

为了充分利用**Transformer**在自然语言处理领域的优化技巧，**ViT**最大程度地保留了**Transformer**编码器的原始结构。

**Transformer**编码器采用**pre-norm**的形式，即**layer norm**放置在自注意力机制或**MLP**之前。

![](https://pic.imgdb.cn/item/63f48f89f144a0100755912f.jpg)

```python
import torch
from einops import rearrange

class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn
    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)

class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout = 0.):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout)
        )
    def forward(self, x):
        return self.net(x)

class Attention(nn.Module):
    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
        super().__init__()
        inner_dim = dim_head *  heads
        project_out = not (heads == 1 and dim_head == dim)

        self.heads = heads
        self.scale = dim_head ** -0.5

        self.attend = nn.Softmax(dim = -1)
        self.dropout = nn.Dropout(dropout)

        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)

        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        ) if project_out else nn.Identity()

    def forward(self, x):
        qkv = self.to_qkv(x).chunk(3, dim = -1)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)

        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale

        attn = self.attend(dots)
        attn = self.dropout(attn)

        out = torch.matmul(attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')
        return self.to_out(out)

class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):
        super().__init__()
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),
                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))
            ]))
    def forward(self, x):
        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x
        return x
```


# 2. 改进视觉Transformer

## （1）改进训练策略

### ⚪ [<font color=Blue>SimpleViT</font>](https://0809zheng.github.io/2023/01/02/simplevit.html)

**SimpleViT**对**ViT**模型结构进行如下改动：位置编码采用**sincos2d**；分类特征采用全局平均池化；分类头采用单层线性层；移除了**Dropout**。在训练方面采用**RandAug**和**MixUP**数据增强，训练**batch size**采用$1024$。

### ⚪ [<font color=Blue>DeiT (Data-efficient Image Transformer)</font>](https://0809zheng.github.io/2023/01/03/deit.html)

**DeiT**在输入图像块序列尾部添加了一个蒸馏**token**，以教师网络输出作为目标进行学习。蒸馏方式包括硬蒸馏(通过交叉熵学习教师网络决策结果)和软蒸馏(通过**KL**散度学习教师网络预测概率)。

![](https://pic.imgdb.cn/item/63f6c8f2f144a01007973b06.jpg)

### ⚪ [<font color=Blue>Token Labeling</font>](https://0809zheng.github.io/2023/01/22/lvvit.html)

**Token Labeling**是指使用预训练的强分类模型为每个**patch token**分配一个软标签，作为额外的训练目标。本文还设计了**MixToken**，在**Patch embedding**后对两个样本的**token**序列进行混合。

![](https://pic.imgdb.cn/item/643bb4910d2dde5777acd9bc.jpg)

### ⚪ [<font color=Blue>Suppressing Over-smoothing</font>](https://0809zheng.github.io/2023/01/24/oversmooth.html)

作者把视觉**Transformer**训练不稳定的原因归结为**Over-smoothing**问题，即不同**token**之间的相似性随着模型的加深而增加。基于此提出了三种提高训练稳定性的损失函数：相似度惩罚项减小输出**token**之间的相似度、**Patch Contrastive Loss**使得深层**token**与浅层对应**token**更加接近、**Patch Mixing Loss**给每个**Patch**提供一个监督信息。

![](https://pic.imgdb.cn/item/643bcc650d2dde5777c9a2b7.jpg)

## （2）改进Patch Tokenization

### ⚪ [<font color=Blue>Visual Transformer</font>](https://0809zheng.github.io/2023/01/10/vt.html)

**Visual Transformer**把卷积**backbone**提取的特征图通过空间注意力转换为一组视觉语义**tokens**，再通过**Transformer**处理高级概念和语义信息。

![](https://pic.imgdb.cn/item/63fc51c0f144a01007a5a4ee.jpg)

### ⚪ [<font color=Blue>CCT (Compact Convolutional Transformer)</font>](https://0809zheng.github.io/2023/01/08/cct.html)

**CCT**引入了**序列池化**，对输出的序列特征进行加权平均用于后续的分类任务；并且使用卷积层编码图像**patch**。

![](https://pic.imgdb.cn/item/63fab73af144a0100760db0a.jpg)


### ⚪ [<font color=Blue>T2T-ViT (Tokens-to-Token ViT)</font>](https://0809zheng.github.io/2023/01/07/t2tvit.html)

**T2T-ViT**通过 **Tokens-to-Token module** 来建模一张图片的局部信息，和更高效的 **Transformer Backbone** 架构设计来提升中间特征的丰富程度减少冗余以提升性能。

![](https://pic.imgdb.cn/item/63f9cccaf144a0100731574f.jpg)

### ⚪ [<font color=Blue>CPVT (Conditional Position encoding Vision Transformer)</font>](https://0809zheng.github.io/2023/01/11/cpvt.html)

**CPVT**提出了**Positional Encoding Generator (PEG)**代替位置编码，通过带零填充的深度卷积为**tokens**引入灵活的位置表示和更高效的位置信息编码。

![](https://pic.imgdb.cn/item/63fda563f144a01007aee896.jpg)


### ⚪ [<font color=Blue>PiT (Pooling-based Vision Transformer)</font>](https://0809zheng.github.io/2023/01/16/pit.html)

**PiT**在视觉**Transformer**体系结构中引入了池化层。池化层将**patch token**重塑为具有空间结构的**3D**张量，通过深度卷积来执行空间大小的减小和通道的增加。

![](https://pic.imgdb.cn/item/6415779aa682492fcc51735c.jpg)




## （3）改进Self-Attention

### ⚪ [<font color=Blue>DeepViT</font>](https://0809zheng.github.io/2023/01/04/deepvit.html)

**DeepViT**设计了**Re-attention**模块，采用一个可学习的变换矩阵$\Theta$把不同**head**的信息结合起来重新构造**attention map**，以此缓解不同层的注意力坍塌。

$$
\operatorname{Re-}\operatorname{Attention}(Q, K, V)=\operatorname{Norm}\left(\Theta^{\top}\left(\operatorname{Softmax}\left(\frac{Q K^{\top}}{\sqrt{d}}\right)\right)\right) V
$$

![](https://pic.imgdb.cn/item/63f811a8f144a010079e26e2.jpg)


### ⚪ [<font color=Blue>CaiT (Class-Attention in Image Transformer)</font>](https://0809zheng.github.io/2023/01/06/cait.html)

**CaiT**引入**LayerScale**使得深层**ViT**更易于训练；并提出**Class-Attention Layer**，在网络前半部分，**Patch Token**相互交互计算注意力；而在网络最后几层，**Patch Token**不再改变，**Class Token**与其交互计算注意力。

![](https://pic.imgdb.cn/item/63f81fc9f144a01007b3c99e.jpg)

### ⚪ [<font color=Blue>Twins-SVT</font>](https://0809zheng.github.io/2023/01/23/twins.html)

**Twins-SVT**设计了**Spatially Separable Self-Attention（SSSA）**。**SSSA**由两个部分组成：**Locally-Grouped Self-Attention（LSA）**和**Global Sub-Sampled Attention（GSA）**。**LSA**将**2D feature map**划分为多个**Sub-Windows**，并仅在**Window**内部进行**Self-Attention**计算；**GSA**将每个**Window**提取一个维度较低的特征作为各个**window**的表征，然后基于这个表征再去与各个**window**进行交互。

![](https://pic.imgdb.cn/item/643b6b250d2dde5777530ef2.jpg)

### ⚪ [<font color=Blue>Refiner</font>](https://0809zheng.github.io/2023/01/25/refiner.html)

**Refiner**首先通过 **Linear Expansion** 来对 **attention map** 的**head**数量进行扩展；再进行 **Head-wise** 的卷积操作，以建模**tokens** 之间的 **local relationship**。

![](https://pic.imgdb.cn/item/643fd0520d2dde577730d6db.jpg)

## （4）多尺度输入

### ⚪ [<font color=Blue>PVT</font>](https://0809zheng.github.io/2023/01/18/pvt.html)

**PVT**在每个阶段进行**patch embedding**时划分**2x2**大小(第一阶段为**4x4**)，以提取不同尺度的特征。为进一步减少计算量，设计了**spatial-reduction attention (SRA)**，将**K**和**V**的空间分辨率都降低了**R**倍。

![](https://pic.imgdb.cn/item/6418163ca682492fccc8c1dc.jpg)

### ⚪ [<font color=Blue>TNT</font>](https://0809zheng.github.io/2021/03/08/tnt.html)

**TNT**使用**outer transformer**处理**patch embedding**，使用**inner transformer**处理每个**patch**的**pixel embedding**，从而融合了**Patch**内部信息与不同**Patch**之间的信息。

![](https://pic.imgdb.cn/item/63fdb1faf144a01007c33c80.jpg)

### ⚪ [<font color=Blue>CrossViT</font>](https://0809zheng.github.io/2023/01/12/crossvit.html)

**CrossViT**采用双分支结构来处理不同大小的图像**patch**，以提取多尺度特征表示；不同分支之间通过基于交叉注意的**token**融合模块交换信息，该模块使用一个分支的**CLS token**作为一个查询与另一个分支的**patch token**进行交互。


![](https://pic.imgdb.cn/item/64156e7ba682492fcc414425.jpg)


## （5）引入卷积层

### ⚪ [<font color=Blue>BotNet (Bottleneck Transformer)</font>](https://0809zheng.github.io/2021/01/31/botnet.html)

**BotNet**把**ResNet**中的$3×3$卷积替换为自注意力层。

![](https://pic.imgdb.cn/item/643b634c0d2dde57774bec45.jpg)

### ⚪ [<font color=Blue>CvT (Convolutional vision Transformer)</font>](https://0809zheng.github.io/2023/01/20/cvt.html)

**CvT**使用三个阶段的卷积层对图像和特征进行嵌入和下采样，并使用深度可分离卷积构造$Q,K,V$。

![](https://pic.imgdb.cn/item/6427d8e2a682492fcc6535b7.jpg)

### ⚪ [<font color=Blue>CeiT (Convolution-enhanced image Transformer)</font>](https://0809zheng.github.io/2023/01/21/ceit.html)

**CeiT**使用卷积层对图像进行嵌入，并使用深度卷积替换**Transformer**模块中的**FFN**层。

![](https://pic.imgdb.cn/item/6427e138a682492fcc72d524.jpg)

### ⚪ [<font color=Blue>LeViT</font>](https://0809zheng.github.io/2023/01/09/levit.html)

**LeViT**把视觉**Transformer**中的所有线性变换替换为卷积操作，并采用卷积+下采样结构。

![](https://pic.imgdb.cn/item/63fb031ef144a01007de797e.jpg)



## ⚪ 视觉Transformer

- [Image Transformer](https://0809zheng.github.io/2021/02/04/it.html)：(arXiv1802)基于Transformer的图像生成自回归模型。



- [On the Relationship between Self-Attention and Convolutional Layers](https://0809zheng.github.io/2021/01/04/SAandConv.html)：(arXiv1911)理解自注意力和卷积层的关系。

- [Generative Pretraining from Pixels](https://0809zheng.github.io/2020/12/29/igpt.html)：(ICML2020)iGPT：像素级的图像预训练模型。

- [DETR：End-to-End Object Detection with Transformers](https://0809zheng.github.io/2020/06/20/detr.html)：(arXiv2005)DETR：使用Transformer进行目标检测。

- [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://0809zheng.github.io/2020/12/31/ddetr.html)：(arXiv2010)Deformable DETR：使用多尺度可变形的注意力模块进行目标检测。



- [Pre-Trained Image Processing Transformer](https://0809zheng.github.io/2021/02/09/ipt.html)：(arXiv2012)IPT：使用Transformer解决超分辨率、去噪和去雨等底层视觉任务。






- [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://0809zheng.github.io/2021/12/10/swint.html)：(arXiv2103)Swin Transformer: 基于移动窗口的分层视觉Transformer。



# ⚪ 参考文献
- [Vision Transformer](https://github.com/lucidrains/vit-pytorch)：(知乎) 通用 Vision Backbone 超详细解读 (原理分析+代码解读)。
- [vit-pytorch](https://github.com/lucidrains/vit-pytorch)：(github) Implementation of Vision Transformer in Pytorch。
- [<font color=Blue>A Survey on Visual Transformer</font>](https://0809zheng.github.io/2021/02/10/visual-transformer.html)：(arXiv2012)一篇关于视觉Transformer的综述。
- [<font color=Blue>Visual Transformers: Token-based Image Representation and Processing for Computer Vision</font>](https://0809zheng.github.io/2023/01/10/vt.html)：(arXiv2006)VT：基于Token的图像表示和处理。
- [<font color=Blue>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</font>](https://0809zheng.github.io/2020/12/30/vit.html)：(arXiv2010)ViT：使用图像块序列的Transformer进行图像分类。
- [<font color=Blue>Training data-efficient image transformers & distillation through attention</font>](https://0809zheng.github.io/2023/01/03/deit.html)：(arXiv2012)DeiT：通过注意力蒸馏训练数据高效的视觉Transformer。
- [<font color=Blue>Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</font>](https://0809zheng.github.io/2023/01/07/t2tvit.html)：(arXiv2101)T2T-ViT：在ImageNet上从头开始训练视觉Transformer。
- [<font color=Blue>Bottleneck Transformers for Visual Recognition</font>](https://0809zheng.github.io/2021/01/31/botnet.html)：(arXiv2101)BotNet：CNN与Transformer结合的backbone。
- [<font color=Blue>Do We Really Need Explicit Position Encodings for Vision Transformers?</font>](https://0809zheng.github.io/2023/01/11/cpvt.html)：(arXiv2102)视觉Transformer真的需要显式位置编码吗？
- [<font color=Blue>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</font>](https://0809zheng.github.io/2023/01/18/pvt.html)：(arXiv2102)PVT：一种无卷积密集预测的通用骨干。
- [<font color=Blue>Transformer in Transformer</font>](https://0809zheng.github.io/2021/03/08/tnt.html)：(arXiv2103)TNT：对图像块与图像像素同时建模的Transformer。
- [<font color=Blue>DeepViT: Towards Deeper Vision Transformer</font>](https://0809zheng.github.io/2023/01/04/deepvit.html)：(arXiv2103)DeepViT：构建更深的视觉Transformer。
- [<font color=Blue>Going deeper with Image Transformers</font>](https://0809zheng.github.io/2023/01/06/cait.html)：(arXiv2103)CaiT：更深的视觉Transformer。
- [<font color=Blue>CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification</font>](https://0809zheng.github.io/2023/01/12/crossvit.html)：(arXiv2103)CrossViT：图像分类的交叉注意力多尺度视觉Transformer。
- [<font color=Blue>Rethinking Spatial Dimensions of Vision Transformers</font>](https://0809zheng.github.io/2023/01/16/pit.html)：(arXiv2103)PiT：重新思考视觉Transformer的空间维度。
- [<font color=Blue>CvT: Introducing Convolutions to Vision Transformers</font>](https://0809zheng.github.io/2023/01/20/cvt.html)：(arXiv2103)CvT：向视觉Transformer中引入卷积。
- [<font color=Blue>Incorporating Convolution Designs into Visual Transformers</font>](https://0809zheng.github.io/2023/01/21/ceit.html)：(arXiv2103)CeiT：将卷积设计整合到视觉Transformers中。
- [<font color=Blue>Escaping the Big Data Paradigm with Compact Transformers</font>](https://0809zheng.github.io/2023/01/08/cct.html)：(arXiv2104)CCT：使用紧凑的Transformer避免大数据依赖。
- [<font color=Blue>LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference</font>](https://0809zheng.github.io/2023/01/09/levit.html)：(arXiv2104)LeViT：以卷积网络的形式进行快速推理的视觉Transformer。
- [<font color=Blue>Twins: Revisiting the Design of Spatial Attention in Vision Transformers</font>](https://0809zheng.github.io/2023/01/23/twins.html)：(arXiv2104)Twins：重新思考视觉Transformer中的空间注意力设计。
- [<font color=Blue>All Tokens Matter: Token Labeling for Training Better Vision Transformers</font>](https://0809zheng.github.io/2023/01/22/lvvit.html)：(arXiv2104)LV-ViT：使用标志标签更好地训练视觉Transformers。
- [<font color=Blue>Improve Vision Transformers Training by Suppressing Over-smoothing</font>](https://0809zheng.github.io/2023/01/24/oversmooth.html)：(arXiv2104)通过抑制过度平滑改进视觉Transformer。
- [<font color=Blue>Refiner: Refining Self-attention for Vision Transformers</font>](https://0809zheng.github.io/2023/01/25/refiner.html)：(arXiv2106)Refiner：精炼视觉Transformer中的自注意力机制。
- [<font color=Blue>Better plain ViT baselines for ImageNet-1k</font>](https://0809zheng.github.io/2023/01/02/simplevit.html)：(arXiv2205)在ImageNet-1k数据集上更好地训练视觉Transformer。

