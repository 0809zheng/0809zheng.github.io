---
layout: post
title: '视觉Transformer(Vision Transformer)'
date: 2023-01-01
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63f48f89f144a0100755912f.jpg'
tags: 深度学习
---

> Vision Transformer.

[<font color=blue>Transformer</font>](https://0809zheng.github.io/2020/04/25/transformer.html)是基于**自注意力机制(self-attention mechanism)**的深度神经网络，该模型在$2017$年$6$月被提出，并逐渐在自然语言处理任务上取得最好的性能。

![](https://pic.downk.cc/item/5fe9916d3ffa7d37b3c174be.jpg)

**Transformer**最近被扩展到计算机视觉任务上。由于**Transformer**缺少**CNN**的**inductive biases**如平移等变性 (**Translation equivariance**)，通常认为**Transformer**在图像领域需要大量的数据或较强的数据增强才能完成训练。随着结构设计不断精细，也有一些视觉**Transformer**只依赖小数据集就能取得较好的表现。

本文主要介绍视觉**Transformer**在基础视觉任务(即图像分类)上的应用，这些模型训练完成后正如图像识别的**CNN**模型一样，可以作为**backbone**迁移到不同的下游视觉任务上，如目标检测、图像分割或**low-level**视觉任务。

# 1. 视觉Transformer的基本架构

[<font color=Blue>ViT</font>](https://0809zheng.github.io/2020/12/30/vit.html)是最早把**Transformer**引入图像分类任务的工作之一。在**ViT**中，输入图像被划分为一系列图像块(**patch**)；使用嵌入层把每个图像块编码为序列向量；再使用**Transformer**编码器进行特征提取；在输入序列中额外引入一个分类**token**，则对应的输出特征用于分类任务。

## (1) Patch Tokenization

把输入图像划分成若干图像块，对每个图像块通过线性映射转换为嵌入向量，并在起始位置增加一个类别嵌入；最后加入可学习的位置编码。

```python
from torch import nn
from einops.layers.torch import Rearrange

self.to_patch_embedding = nn.Sequential(
    Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),
    nn.LayerNorm(patch_dim),
    nn.Linear(patch_dim, dim),
    nn.LayerNorm(dim),
)
self.cls_token = nn.Parameter(torch.randn(1, 1, dim))
self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))

x = self.to_patch_embedding(img)
b, n, _ = x.shape
cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)
x = torch.cat((cls_tokens, x), dim=1)
x += self.pos_embedding[:, :(n + 1)]
```

## (2) Transformer Encoder

为了充分利用**Transformer**在自然语言处理领域的优化技巧，**ViT**最大程度地保留了**Transformer**编码器的原始结构。

**Transformer**编码器采用**pre-norm**的形式，即**layer norm**放置在自注意力机制或**MLP**之前。

![](https://pic.imgdb.cn/item/63f48f89f144a0100755912f.jpg)

```python
import torch
from einops import rearrange

class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.norm = nn.LayerNorm(dim)
        self.fn = fn
    def forward(self, x, **kwargs):
        return self.fn(self.norm(x), **kwargs)

class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout = 0.):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(dim, hidden_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, dim),
            nn.Dropout(dropout)
        )
    def forward(self, x):
        return self.net(x)

class Attention(nn.Module):
    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
        super().__init__()
        inner_dim = dim_head *  heads
        project_out = not (heads == 1 and dim_head == dim)

        self.heads = heads
        self.scale = dim_head ** -0.5

        self.attend = nn.Softmax(dim = -1)
        self.dropout = nn.Dropout(dropout)

        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)

        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        ) if project_out else nn.Identity()

    def forward(self, x):
        qkv = self.to_qkv(x).chunk(3, dim = -1)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)

        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale

        attn = self.attend(dots)
        attn = self.dropout(attn)

        out = torch.matmul(attn, v)
        out = rearrange(out, 'b h n d -> b n (h d)')
        return self.to_out(out)

class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):
        super().__init__()
        self.layers = nn.ModuleList([])
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),
                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))
            ]))
    def forward(self, x):
        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x
        return x
```


# 2. 改进视觉Transformer


## （1）改进Patch Tokenization

### ⚪ [<font color=Blue>Visual Transformer</font>](https://0809zheng.github.io/2023/01/10/vt.html)

**Visual Transformer**把卷积**backbone**提取的特征图通过空间注意力转换为一组视觉语义**tokens**，再通过**Transformer**处理高级概念和语义信息。

![](https://pic.imgdb.cn/item/63fc51c0f144a01007a5a4ee.jpg)

### ⚪ [<font color=Blue>CCT</font>](https://0809zheng.github.io/2023/01/08/cct.html)

**CCT**引入了**序列池化**，对输出的序列特征进行加权平均用于后续的分类任务；并且使用卷积层编码图像**patch**。

![](https://pic.imgdb.cn/item/63fab73af144a0100760db0a.jpg)


### ⚪ [<font color=Blue>T2T-ViT</font>](https://0809zheng.github.io/2023/01/07/t2tvit.html)

**T2T-ViT**通过 **Tokens-to-Token module** 来建模一张图片的局部信息，和更高效的 **Transformer Backbone** 架构设计来提升中间特征的丰富程度减少冗余以提升性能。

![](https://pic.imgdb.cn/item/63f9cccaf144a0100731574f.jpg)

### ⚪ [<font color=Blue>CPVT</font>](https://0809zheng.github.io/2023/01/11/cpvt.html)

**CPVT**提出了**Positional Encoding Generator (PEG)**代替位置编码，通过带零填充的深度卷积为**tokens**引入灵活的位置表示和更高效的位置信息编码。

![](https://pic.imgdb.cn/item/63fda563f144a01007aee896.jpg)

### ⚪ [<font color=Blue>TNT</font>](https://0809zheng.github.io/2021/03/08/tnt.html)

**TNT**使用**outer transformer**处理**patch embedding**，使用**inner transformer**处理每个**patch**的**pixel embedding**，从而融合了**Patch**内部信息与不同**Patch**之间的信息。

![](https://pic.imgdb.cn/item/63fdb1faf144a01007c33c80.jpg)

### ⚪ [<font color=Blue>PiT</font>](https://0809zheng.github.io/2023/01/16/pit.html)

**PiT**在视觉**Transformer**体系结构中引入了池化层。池化层将**patch token**重塑为具有空间结构的**3D**张量，通过深度卷积来执行空间大小的减小和通道的增加。

![](https://pic.imgdb.cn/item/6415779aa682492fcc51735c.jpg)

### ⚪ [<font color=Blue>PVT</font>](https://0809zheng.github.io/2023/01/18/pvt.html)

**PVT**在每个阶段进行**patch embedding**时划分**2x2**大小(第一阶段为**4x4**)，以提取不同尺度的特征。为进一步减少计算量，设计了**spatial-reduction attention (SRA)**，将**K**和**V**的空间分辨率都降低了**R**倍。

![](https://pic.imgdb.cn/item/6418163ca682492fccc8c1dc.jpg)


## （2）改进Self-Attention

### ⚪ [<font color=Blue>DeepViT</font>](https://0809zheng.github.io/2023/01/04/deepvit.html)

**DeepViT**设计了**Re-attention**模块，采用一个可学习的变换矩阵$\Theta$把不同**head**的信息结合起来重新构造**attention map**，以此缓解不同层的注意力坍塌。

$$
\operatorname{Re-}\operatorname{Attention}(Q, K, V)=\operatorname{Norm}\left(\Theta^{\top}\left(\operatorname{Softmax}\left(\frac{Q K^{\top}}{\sqrt{d}}\right)\right)\right) V
$$

![](https://pic.imgdb.cn/item/63f811a8f144a010079e26e2.jpg)


### ⚪ [<font color=Blue>CaiT</font>](https://0809zheng.github.io/2023/01/06/cait.html)

**CaiT**引入**LayerScale**使得深层**ViT**更易于训练；并提出**Class-Attention Layer**，在网络前半部分，**Patch Token**相互交互计算注意力；而在网络最后几层，**Patch Token**不再改变，**Class Token**与其交互计算注意力。

![](https://pic.imgdb.cn/item/63f81fc9f144a01007b3c99e.jpg)

### ⚪ [<font color=Blue>CrossViT</font>](https://0809zheng.github.io/2023/01/12/crossvit.html)

**CrossViT**采用双分支结构来处理不同大小的图像**patch**，以提取多尺度特征表示；不同分支之间通过基于交叉注意的**token**融合模块交换信息，该模块使用一个分支的**CLS token**作为一个查询与另一个分支的**patch token**进行交互。


![](https://pic.imgdb.cn/item/64156e7ba682492fcc414425.jpg)


### ⚪ [<font color=Blue>LeViT</font>](https://0809zheng.github.io/2023/01/09/levit.html)

**LeViT**把视觉**Transformer**中的所有线性变换替换为卷积操作，并采用卷积+下采样结构。

![](https://pic.imgdb.cn/item/63fb031ef144a01007de797e.jpg)


## （3）改进训练策略

### ⚪ [<font color=Blue>SimpleViT</font>](https://0809zheng.github.io/2023/01/02/simplevit.html)

**SimpleViT**对**ViT**模型结构进行如下改动：位置编码采用**sincos2d**；分类特征采用全局平均池化；分类头采用单层线性层；移除了**Dropout**。在训练方面采用**RandAug**和**MixUP**数据增强，训练**batch size**采用$1024$。

### ⚪ [<font color=Blue>DeiT</font>](https://0809zheng.github.io/2023/01/03/deit.html)

**DeiT**在输入图像块序列尾部添加了一个蒸馏**token**，以教师网络输出作为目标进行学习。蒸馏方式包括硬蒸馏(通过交叉熵学习教师网络决策结果)和软蒸馏(通过**KL**散度学习教师网络预测概率)。

![](https://pic.imgdb.cn/item/63f6c8f2f144a01007973b06.jpg)








## ⚪ 视觉Transformer

- [Image Transformer](https://0809zheng.github.io/2021/02/04/it.html)：(arXiv1802)基于Transformer的图像生成自回归模型。



- [On the Relationship between Self-Attention and Convolutional Layers](https://0809zheng.github.io/2021/01/04/SAandConv.html)：(arXiv1911)理解自注意力和卷积层的关系。

- [Generative Pretraining from Pixels](https://0809zheng.github.io/2020/12/29/igpt.html)：(ICML2020)iGPT：像素级的图像预训练模型。

- [DETR：End-to-End Object Detection with Transformers](https://0809zheng.github.io/2020/06/20/detr.html)：(arXiv2005)DETR：使用Transformer进行目标检测。

- [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://0809zheng.github.io/2020/12/31/ddetr.html)：(arXiv2010)Deformable DETR：使用多尺度可变形的注意力模块进行目标检测。



- [Pre-Trained Image Processing Transformer](https://0809zheng.github.io/2021/02/09/ipt.html)：(arXiv2012)IPT：使用Transformer解决超分辨率、去噪和去雨等底层视觉任务。



- [Bottleneck Transformers for Visual Recognition](https://0809zheng.github.io/2021/01/31/botnet.html)：(arXiv2101)BotNet：CNN与Transformer结合的backbone。

- [TransGAN: Two Transformers Can Make One Strong GAN](https://0809zheng.github.io/2021/03/02/transgan.html)：(arXiv2102)TransGAN：用Transformer实现GAN。


- [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://0809zheng.github.io/2021/12/10/swint.html)：(arXiv2103)Swin Transformer: 基于移动窗口的分层视觉Transformer。



# ⚪ 参考文献
- [Vision Transformer](https://github.com/lucidrains/vit-pytorch)：(知乎) 通用 Vision Backbone 超详细解读 (原理分析+代码解读)。
- [vit-pytorch](https://github.com/lucidrains/vit-pytorch)：(github) Implementation of Vision Transformer in Pytorch。
- [<font color=Blue>A Survey on Visual Transformer</font>](https://0809zheng.github.io/2021/02/10/visual-transformer.html)：(arXiv2012)一篇关于视觉Transformer的综述。
- [<font color=Blue>Visual Transformers: Token-based Image Representation and Processing for Computer Vision</font>](https://0809zheng.github.io/2023/01/10/vt.html)：(arXiv2006)VT：基于Token的图像表示和处理。
- [<font color=Blue>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</font>](https://0809zheng.github.io/2020/12/30/vit.html)：(arXiv2010)ViT：使用图像块序列的Transformer进行图像分类。
- [<font color=Blue>Training data-efficient image transformers & distillation through attention</font>](https://0809zheng.github.io/2023/01/03/deit.html)：(arXiv2012)DeiT：通过注意力蒸馏训练数据高效的视觉Transformer。
- [<font color=Blue>Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</font>](https://0809zheng.github.io/2023/01/07/t2tvit.html)：(arXiv2101)T2T-ViT：在ImageNet上从头开始训练视觉Transformer。
- [<font color=Blue>Do We Really Need Explicit Position Encodings for Vision Transformers?</font>](https://0809zheng.github.io/2023/01/11/cpvt.html)：(arXiv2102)视觉Transformer真的需要显式位置编码吗？
- [<font color=Blue>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions</font>](https://0809zheng.github.io/2023/01/18/pvt.html)：(arXiv2102)PVT：一种无卷积密集预测的通用骨干。
- [<font color=Blue>Transformer in Transformer</font>](https://0809zheng.github.io/2021/03/08/tnt.html)：(arXiv2103)TNT：对图像块与图像像素同时建模的Transformer。
- [<font color=Blue>DeepViT: Towards Deeper Vision Transformer</font>](https://0809zheng.github.io/2023/01/04/deepvit.html)：(arXiv2103)DeepViT：构建更深的视觉Transformer。
- [<font color=Blue>Going deeper with Image Transformers</font>](https://0809zheng.github.io/2023/01/06/cait.html)：(arXiv2103)CaiT：更深的视觉Transformer。
- [<font color=Blue>CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification</font>](https://0809zheng.github.io/2023/01/12/crossvit.html)：(arXiv2103)CrossViT：图像分类的交叉注意力多尺度视觉Transformer。
- [<font color=Blue>Rethinking Spatial Dimensions of Vision Transformers</font>](https://0809zheng.github.io/2023/01/16/pit.html)：(arXiv2103)PiT：重新思考视觉Transformer的空间维度。
- [<font color=Blue>Escaping the Big Data Paradigm with Compact Transformers</font>](https://0809zheng.github.io/2023/01/08/cct.html)：(arXiv2104)CCT：使用紧凑的Transformer避免大数据依赖。
- [<font color=Blue>LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference</font>](https://0809zheng.github.io/2023/01/09/levit.html)：(arXiv2104)LeViT：以卷积网络的形式进行快速推理的视觉Transformer。
- [<font color=Blue>Better plain ViT baselines for ImageNet-1k</font>](https://0809zheng.github.io/2023/01/02/simplevit.html)：(arXiv2205)在ImageNet-1k数据集上更好地训练视觉Transformer。

