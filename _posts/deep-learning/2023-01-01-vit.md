---
layout: post
title: '视觉Transformer(Vision Transformer)'
date: 2023-01-01
author: 郑之杰
cover: ''
tags: 深度学习
---

> Vision Transformer.

[<font color=blue>Transformer</font>](https://0809zheng.github.io/2020/04/25/transformer.html)是基于**自注意力机制(self-attention mechanism)**的深度神经网络，该模型在$2017$年$6$月被提出，并逐渐在自然语言处理任务上取得最好的性能。

![](https://pic.downk.cc/item/5fe9916d3ffa7d37b3c174be.jpg)

**Transformer**最近被扩展到计算机视觉任务上，并在这些任务上表现出比**CNN**等更具有竞争力的结果。按照应用场合对视觉**Transformer**进行分类，包括：
- 图像分类：
- 下游任务：

patch tokenization

# 1. 图像分类

### ⚪ [<font color=Blue>ViT</font>](https://0809zheng.github.io/2020/12/30/vit.html)

在**ViT**中，输入图像被划分为一系列图像块(**patch**)；使用嵌入层把每个图像块编码为序列向量，再使用**Transformer**的编码器进行特征提取。**Transformer**采用**pre-norm**的形式。

![](https://pic.imgdb.cn/item/63f48f89f144a0100755912f.jpg)

### ⚪ [<font color=Blue>SimpleViT</font>](https://0809zheng.github.io/2023/01/02/simplevit.html)

**SimpleViT**对**ViT**模型结构进行如下改动：位置编码采用**sincos2d**；分类特征采用全局平均池化；分类头采用单层线性层；移除了**Dropout**。在训练方面采用**RandAug**和**MixUP**数据增强，训练**batch size**采用$1024$。

### ⚪ [<font color=Blue>DeiT</font>](https://0809zheng.github.io/2023/01/03/deit.html)

**DeiT**在输入图像块序列尾部添加了一个蒸馏**token**，蒸馏**token**的输出特征以教师网络输出作为目标进行学习。蒸馏方式包括硬蒸馏(通过交叉熵学习教师网络决策结果)和软蒸馏(通过**KL**散度学习教师网络预测概率)。

![](https://pic.imgdb.cn/item/63f6c8f2f144a01007973b06.jpg)

### ⚪ [<font color=Blue>DeepViT</font>](https://0809zheng.github.io/2023/01/04/deepvit.html)

**DeepViT**设计了**Re-attention**模块，使得**ViT**能够堆叠更深的层（如大于$12$层）。**Re-attention**模块采用一个可学习的变换矩阵和**multi-head attention maps**相乘来得到新的**attention map**，以此解耦不同层之间的相似性。

$$
\operatorname{Re-}\operatorname{Attention}(Q, K, V)=\operatorname{Norm}\left(\Theta^{\top}\left(\operatorname{Softmax}\left(\frac{Q K^{\top}}{\sqrt{d}}\right)\right)\right) V
$$

![](https://pic.imgdb.cn/item/63f811a8f144a010079e26e2.jpg)


### ⚪ [<font color=Blue>CaiT</font>](https://0809zheng.github.io/2023/01/06/cait.html)

**CaiT**引入**LayerScale**，即对残差模块的输出进行按特征维度的乘法；并提出**Class-Attention Layer**，在网络前半部分，**Patch Token**相互交互计算注意力；而在网络最后几层，**Patch Token**不再改变，**Class Token**与其交互计算注意力。

![](https://pic.imgdb.cn/item/63f81fc9f144a01007b3c99e.jpg)


### ⚪ [<font color=Blue>T2T-ViT</font>](https://0809zheng.github.io/2023/01/07/t2tvit.html)

**T2T-ViT**设计了**Tokens-to-Token (T2T)**操作，使得图像块嵌入**Token**时具有重叠的部分，每个**token**能够捕捉到更加精细的局部结构。**T2T**用到了**Pytorch**提供的**Unfold**操作，这个操作的具体作用是在输入图像中滑动地提取出局部区域块，然后把区域块内每个像素的特征拼接起来作为一个新的**Token**。

![](https://pic.imgdb.cn/item/63f9cccaf144a0100731574f.jpg)



## ⚪ 视觉Transformer

- [Image Transformer](https://0809zheng.github.io/2021/02/04/it.html)：(arXiv1802)基于Transformer的图像生成自回归模型。



- [On the Relationship between Self-Attention and Convolutional Layers](https://0809zheng.github.io/2021/01/04/SAandConv.html)：(arXiv1911)理解自注意力和卷积层的关系。

- [Generative Pretraining from Pixels](https://0809zheng.github.io/2020/12/29/igpt.html)：(ICML2020)iGPT：像素级的图像预训练模型。

- [DETR：End-to-End Object Detection with Transformers](https://0809zheng.github.io/2020/06/20/detr.html)：(arXiv2005)DETR：使用Transformer进行目标检测。

- [Deformable DETR: Deformable Transformers for End-to-End Object Detection](https://0809zheng.github.io/2020/12/31/ddetr.html)：(arXiv2010)Deformable DETR：使用多尺度可变形的注意力模块进行目标检测。



- [Pre-Trained Image Processing Transformer](https://0809zheng.github.io/2021/02/09/ipt.html)：(arXiv2012)IPT：使用Transformer解决超分辨率、去噪和去雨等底层视觉任务。



- [Bottleneck Transformers for Visual Recognition](https://0809zheng.github.io/2021/01/31/botnet.html)：(arXiv2101)BotNet：CNN与Transformer结合的backbone。

- [TransGAN: Two Transformers Can Make One Strong GAN](https://0809zheng.github.io/2021/03/02/transgan.html)：(arXiv2102)TransGAN：用Transformer实现GAN。

- [Transformer in Transformer](https://0809zheng.github.io/2021/03/08/tnt.html)：(arXiv2103)TNT：对图像块与图像像素同时建模的Transformer。

- [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://0809zheng.github.io/2021/12/10/swint.html)：(arXiv2103)Swin Transformer: 基于移动窗口的分层视觉Transformer。



# ⚪ 参考文献
- [vit-pytorch](https://github.com/lucidrains/vit-pytorch)：(github) Implementation of Vision Transformer in Pytorch。
- [<font color=Blue>A Survey on Visual Transformer</font>](https://0809zheng.github.io/2021/02/10/visual-transformer.html)：(arXiv2012)一篇关于视觉Transformer的综述。
- [<font color=Blue>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</font>](https://0809zheng.github.io/2020/12/30/vit.html)：(arXiv2010)ViT：使用图像块序列的Transformer进行图像分类。
- [<font color=Blue>Training data-efficient image transformers & distillation through attention</font>](https://0809zheng.github.io/2023/01/03/deit.html)：(arXiv2012)DeiT：通过注意力蒸馏训练数据高效的视觉Transformer。
- [<font color=Blue>Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</font>](https://0809zheng.github.io/2023/01/07/t2tvit.html)：(arXiv2101)T2T-ViT：在ImageNet上从头开始训练视觉Transformer。
- [<font color=Blue>DeepViT: Towards Deeper Vision Transformer</font>](https://0809zheng.github.io/2023/01/04/deepvit.html)：(arXiv2103)DeepViT：构建更深的视觉Transformer。
- [<font color=Blue>Going deeper with Image Transformers</font>](https://0809zheng.github.io/2023/01/06/cait.html)：(arXiv2103)CaiT：更深的视觉Transformer。
- [<font color=Blue>Better plain ViT baselines for ImageNet-1k</font>](https://0809zheng.github.io/2023/01/02/simplevit.html)：(arXiv2205)在ImageNet-1k数据集上更好地训练视觉Transformer。

