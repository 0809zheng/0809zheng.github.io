---
layout: post
title: 'Seq2Seq'
date: 2020-04-21
author: 郑之杰
cover: 'https://pic.downk.cc/item/5e9fbd2bc2a9a83be53b5563.jpg'
tags: 深度学习
---

> Sequence to sequence.

**序列到序列(Seq2Seq)模型**是一种序列生成模型，输入一个序列，生成另一个序列。

Seq2Seq模型可用于语音识别、文本摘要、对话系统、图像标题生成、机器翻译、阅读理解等任务。

**本文目录：**
1. 模型介绍
2. 学习问题
3. 生成过程
4. 条件生成
5. 指针网络


### 1. 模型介绍
可以用RNN实现Seq2Seq模型。

根据序列的基本单位不同。可以分为**词(word)**级别的模型和**字符(character)**级别的模型。

**训练**时将每一个单词的one-hot向量或词嵌入向量作为输入，对应的下一个单词作为输出：

![](https://pic.downk.cc/item/5e9e9762c2a9a83be55a3ed2.jpg)

训练时可以最小化预测值和真实值的交叉熵，但这种方法并不是最好的。

**生成**时对于每一个输入，随机采样一个输出，作为下一个输入：

![](https://pic.downk.cc/item/5e9e9781c2a9a83be55a5f9e.jpg)

其中$$<BOS>$$标志序列起始(begin of sequence),$$<EOS>$$标志序列结束(end of sequence)。


### 2. 学习问题
训练时，输入是固定的序列(真实数据)；生成时，每一步输入时上一步输出采样的结果(模型生成数据)。这会造成训练和测试(生成)数据的**不匹配(mismatch)**。

在生成时，如果采集到训练时不存在的序列，会导致错误传播，使得后续生成的序列也会偏离真实分布. 这个问题称为**曝光偏差（Exposure Bias）**。

![](https://pic.downk.cc/item/5e9ed452c2a9a83be592308a.jpg)

若在训练时，输入也进行采样，即完全使用模型生成数据，会增大训练难度。

为了缓解曝光偏差问题，可以在训练时混合使用**真实数据**和**模型生成数据**，这种策略称为[**计划采样（Scheduled Sampling）**](https://arxiv.org/abs/1506.03099)。

在每一步，以$ε$的概率使用真实数据，以$1-ε$的概率使用模型生成数据。

![](https://pic.downk.cc/item/5e9ed635c2a9a83be594301e.jpg)

如果一开始训练时的$ε$过小，模型相当于在噪声很大的数据上训练，会导致模型性能变差，并且难以收敛。

因此，一个较好的策略是在训练初期赋予$ε$较大的值，随着训练次数的增加逐步减小$ε$的取值。

计划采样的一个缺点是**过度纠正**，即在每一步中不管输入如何选择，目标输出依然是来自于真实数据。这可能使得模型预测一些不正确的序列。

### 3. 生成过程
训练好的Seq2Seq模型可用于序列生成。

**(1).贪婪搜索**

当使用Seq2Seq模型生成一个最可能的序列时，生成过程是一种从左到右的贪婪式搜索过程。在每一步都生成最可能的词，直到生成符号$$<EOS>$$。

这种生成方法可能不是最优的; 如下图，最优的序列是绿色序列，但却生成了红色序列：

![](https://pic.downk.cc/item/5e9e997ac2a9a83be55b8d72.jpg)

**(2).束搜索**

一种常用的减少搜索错误的启发式方法是**束搜索（Beam Search）**。

在每一步中，生成$K$个最可能的前缀序列，其中$K$为**束的大小（Beam Size）**，是一个超参数。

如下图，若选定$2$，则每次在生成的序列中选择概率最大的前2个：

![](https://pic.downk.cc/item/5e9e9a10c2a9a83be55c3610.jpg)

束的大小$K$越大，束搜索的复杂度越高，但越有可能生成最优序列。

在实际应用中，束搜索可以通过调整束大小$K$来平衡计算复杂度和搜索质量之间的优先级。

### 4. 条件生成(Conditional Generation)
有时候不希望Seq2Seq模型生成随机的序列，因此在序列生成时引入一些**条件conditions**。如在图像标注时把图像内容作为条件，在对话系统中把提问者的问题作为条件。

这时可以把模型分解为**Encoder**和**Decoder**。
![](https://pic.downk.cc/item/5e9fbde1c2a9a83be53bf7e1.jpg)

**Encoder**把输入文本、图像喂入一个神经网络(FNN、RNN、CNN)中，生成一个**上下文向量**$c$作为输入的条件信息。

**Decoder**把生成的向量作为RNN每一次时间步的输入。

上述条件Seq2Seq模型的缺点是：
1. 编码向量的容量问题，输入序列的信息很难全部保存在一个**固定维度的向量**中；
2. 当序列很长时，由于循环神经网络的**长程依赖问题**，容易丢失输入序列的信息。

为解决上述问题，引入[注意力机制](https://0809zheng.github.io/2020/04/22/attention.html)。

### 5. 指针网络

- paper：Pointer Networks
- arXiv：[https://arxiv.org/abs/1506.03134](https://arxiv.org/abs/1506.03134)

问题引入：如下图所示，输入10个点的坐标序列，输出边界点的坐标。

![](https://pic.downk.cc/item/5ea1160fc2a9a83be59362d9.jpg)

若直接使用Seq2Seq模型，则由于输入序列长度的不确定，输出维度不确定。

**指针网络(pointer network)**是一种特殊的序列到序列模型,输出序列是输入序列的下标（索引）。

![](https://pic.downk.cc/item/5ea1136ac2a9a83be591465d.jpg)

输入是长度为$N$的**向量**序列$$X=x_1,...,x_N$$；输出是长度为$M$的**下标**序列$$C_{1:M}=c_1,...,c_M$$，$$c_m \in [1, N]$$。

指针网络在实现时，要在输入序列首端加上一个**END标志**。

可以用注意力机制实现指针网络。
