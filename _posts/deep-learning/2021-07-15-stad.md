---
layout: post
title: '时空动作检测'
date: 2021-07-15
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60ef8d035132923bf85c3681.jpg'
tags: 深度学习
---

> Spatio-Temporal Action Detection.

**时空动作检测(Spatio-Temporal Action Detection)**是视频检测领域的任务，视频领域常见的任务有动作识别、时序动作检测、时空动作检测。
- **动作识别(action recognition)**： 对每个输入视频进行分类，识别出视频中人物做的动作。即输入**视频序列**，得到视频对应的**类别**；
- **时序动作检测(temporal action detection)**：任务的输入是一个未经裁剪的视频(**untrimmed video**)，即视频中有些帧没有动作发生，因此需要检测出动作开始和结束的区间，并判断区间内动作的类别。即输入未经裁剪的视频序列，得到动作出现的**区间**和对应的**类别**；
- **时空动作检测(spatio-temporal action detection)**：不仅需要识别动作出现的**区间**和对应的**类别**，还要在空间范围内用一个包围框(**bounding box**)标记出人物的**空间位置**。

# 1. 时空动作检测的常用数据集
目前常用的**时空动作检测(Spatio-Temporal Action Detection)**数据集可以被分为两类：
1. 密集标注的数据集 **Densely annotated**：每一帧都进行标注
2. 稀疏标注的数据集 **Sparsely annotated**：隔一段时间标注一帧

## (1) 密集标注的数据集
密集标注的数据集由于每一帧都进行标注，因此标注了人物目标的**id**，这些数据集通常带有任务跟踪功能。由于数据集标注量大，因此会有标签质量低、动作类别过于粗糙等问题。常用的数据集包括：
- [**J-HMDB**](http://jhmdb.is.tue.mpg.de/)：对**HMDB**数据集的二次标注，共有$21$类动作，标注了$31838$张图像。每个视频包括$14$-$40$帧，包含动作的起止时间。每个视频最多只有一类动作。
- [**UCF101-24**](https://github.com/gurkirt/corrected-UCF101-Annots)：**UCF101**数据集的子集，每个视频最多只有一类动作，共有$24$类动作。
- [**MultiSports**](https://0809zheng.github.io/2021/07/16/multisports.html)：篮球,足球,体操,排球赛事的数据集。

## (1) 稀疏标注的数据集
稀疏标注的数据集由于每隔一段时间(如$1$s)进行一帧标注，因此动作没有明确的时间边界，也不具有人物跟踪功能。常用的数据集包括：
- [**AVA**](https://arxiv.org/abs/1705.08421)：每$1$s进行一帧标注，共有$80$类动作。
- **DALY**

# 2. 时空动作检测的方法
按照处理方式不同，时空动作检测可以分为帧级别的检测器(**frame-level detector**)和**tubelet**级别的检测器(**tubelet-level detector**)。
- **frame-level detector**：先逐帧处理视频，得到每帧内人物的包围框和动作类别，再沿时间维度将这些框连接起来，形成时空动作检测结果。这种逐帧处理的方式导致沿时间连接比较困难，且因为缺乏时序信息的利用导致动作识别精度不高。
- **tubelet-level detector**：每次输入连续$K$帧的视频并产生连续$K$帧的检测结果**tubelet**(**tubelet**内相邻帧的框已连接)，再将这些**tubelet**在时序上进行连接，得到视频级别的时空动作检测结果。

## (1) Frame-Level Detector
**frame**级别的检测器是指每次检测时输入单帧图像，得到单帧图像上的检测结果；之后把检测结果沿时间维度进行连接，得到视频检测结果。常用的**frame**级别检测方法包括：
- **T-CNN**

### ⚪ T-CNN
- 论文：[Object Detection from Video Tubelets with Convolutional Neural Networks](https://0809zheng.github.io/2021/06/10/tcnn.html)

**T-CNN**扩展了目前流行的静态目标检测方法，并将其用于视频检测中。为增强方法的时间一致性，首先使用静态目标检测方法**DeepID-Net**和**CRAFT**两种框架对视频中的每一帧进行检测，
在相邻帧上保持高置信度类别的检测分数不变，降低低置信度类别的检测分数，从而降低**误检率**(即减少**false positive**)。
将每帧的检测框和它们的分数传播到其相邻帧以增加检测结果，从而降低**漏检率**(即减少**false negative**)。
使用跟踪算法生成长期的检测序列，与其他检测结果进行融合获得最终检测框。

![](https://pic.imgdb.cn/item/60f0f8575132923bf85fd65a.jpg)


## (2) Tubelet-Level Detector
**tubelet**级别的检测器在检测视频中的动作时引入了**tubelet**的概念(**-let**后缀表示小型的，此处是指预测结果在时间-空间上像一个小管道)。
即每次检测时输入多帧连续视频帧，在这些帧上预先设定**anchor cuboids**(同一个**anchor**在不同帧的空间维度上具有相同的初始位置和形状，在时间维度上构成立方体，因此得名，用于检测某一个具体的目标及其动作类型)，通过模型对每一帧上的检测框进行修正，得到待预测的动作在连续帧上的多个**bbox**构成的**tubelets**。因此**tubelets**实际上就是同时含有时序信息和空间信息的检测框序列。

![](https://pic.imgdb.cn/item/60ef97855132923bf894bd2e.jpg)

常用的**tubelet**级别的检测方法包括：
- **ACT-detector**
- **MOC-detector**

### ⚪ ACT-detector
- 论文：[Action Tubelet Detector for Spatio-Temporal Action Localization](https://0809zheng.github.io/2021/06/09/act.html)

**ACT-detector**将**tubelet**的概念引入视频检测。
给定$K$帧的连续帧视频序列，使用共享权重的卷积神经网络**backbone**(本文采用**SSD**)从每帧图像上提取不同层次的特征图。将$K$帧连续图像的属于同一个**anchor**的特征(构成**anchor cuboids**)堆叠起来，通过分类层预测$C+1$个动作类别的得分，通过回归层输出$4K$个坐标用于对**tubelet**进行调整。

![](https://pic.imgdb.cn/item/60ef98805132923bf89a578a.jpg)

### ⚪ MOC-detector
- 论文：[Actions as Moving Points](https://0809zheng.github.io/2021/07/17/mocdetector.html)

将视频中的动作建模为**每一帧动作中心点沿时序的运动轨迹**。
**MOC-detector**采用**anchor-free**的结构。首先将连续$K$帧图像分别输入共享权重的**2D Backbone**提取每一帧的高层语义特征，获得$K$张特征图；然后由三个分支共同协作来完成时空动作检测任务。**中心点预测分支(Center Branch)**用于检测**关键帧中心点**的空间位置和所属**类别**。**运动估计分支(Movement Branch)**用于估计每一帧上的**动作中心点**距离**关键帧中心点**的运动矢量，形成中心点运动轨迹，将**关键帧中心点**移动到当前帧的对应中心点，可以形成连续$K$帧内每个**动作中心点**的运动轨迹。**包围框回归分支(Box Branch)**：逐帧处理，输入单帧特征图，在每一帧的**动作中心点**直接回归**bbox**大小来得到动作实例在空间上的包围框。

![](https://pic.imgdb.cn/item/60efb4c75132923bf8380225.jpg)

