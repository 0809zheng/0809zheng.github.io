---
layout: post
title: '自监督学习(Self-Supervised Learning)'
date: 2022-10-01
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/63bfffa1be43e0d30e3eda4f.jpg'
tags: 深度学习
---

> Self-Supervised Learning.

**自监督学习(Self-Supervised Learning)**是一种无监督表示学习方法，旨在根据无标签数据集中的一部分信息预测剩余的信息，并以有监督的方式来训练该数据集。

自监督学习的优势包括：
- 能够充分利用大型无标签数据集，利用数据集本身以较低成本构造大量标签；
- 能够学习携带语义或结构信息的数据特征表示，从而有益于下游任务。

自监督学习已被广泛应用在自然语言处理任务中，比如语言模型默认的[预训练任务](https://0809zheng.github.io/2020/04/27/elmo-bert-gpt.html)就是根据过去的序列预测下一个单词。本文主要关注计算机视觉领域的自监督学习方法，即如何构造适用于图像数据集的自监督任务，包括：
- **前置任务(pretext task)**：通过从数据集中自动构造伪标签而设计的对目标任务有帮助的辅助任务，如**Exemplar-CNN**, **Context Prediction**, **Jigsaw Puzzle**, **Image Colorization**, **Learning to Count**, **Image Rotation**。
- **生成模型(generative modeling)**：
- **对比学习(contrastive learning)**：


# 1. 基于前置任务的自监督学习方法

**前置任务(pretext task)**也叫**代理任务(surrogate task)**，是指通过从数据集中自动构造伪标签而设计的对目标任务有帮助的辅助任务。

### ⚪ [<font color=blue>Exemplar-CNN</font>](https://0809zheng.github.io/2022/10/02/exemplarcnn.html)

**Exemplar-CNN**从图像数据集的梯度较大区域（通常覆盖边缘并包含目标的一部分）中采样$32 \times 32$大小的图像块；对每一个图像块应用不同的随机图像增强，同一个图像块的增强样本属于同一个代理类别；自监督学习的前置任务旨在区分不同的代理类别。

![](https://pic.imgdb.cn/item/63c0d8a9be43e0d30e7cc5b5.jpg)

### ⚪ [<font color=blue>Context Prediction</font>](https://0809zheng.github.io/2022/10/04/context.html)

随机在图像中选取一个图像块；然后考虑以该图像块为中心的$3\times 3$网格，随机选择其$8$个邻域图像块中的一个；则自监督学习的前置任务是预测后者属于哪个邻域的八分类任务。

![](https://pic.imgdb.cn/item/63c104d3be43e0d30ec7e6ce.jpg)

### ⚪ [<font color=blue>Jigsaw Puzzle</font>](https://0809zheng.github.io/2022/10/05/jigsaw.html)

随机打乱图像中的九个图像块，通过共享权重的模型分别处理每一个图像块，并根据预定义的排列集合输出图像块排列的索引概率，则自监督学习的前置任务是一种多分类任务。

![](https://pic.imgdb.cn/item/63c10983be43e0d30ecfac9e.jpg)

### ⚪ [<font color=blue>Image Colorization</font>](https://0809zheng.github.io/2022/10/07/colorization.html)

**着色**是指把输入灰度图像转化为彩色图像，即将灰度图像映射到量化颜色值输出的分布上。彩色图像设置在**Lab\***颜色空间，其中取值$0$-$100$的整数值*L*匹配人眼对亮度的感知，*ab*值控制不同的颜色取值，量化为$313$种颜色对。则自监督学习的前置任务构造为在量化颜色值上预测概率分布的交叉熵损失。

![](https://pic.imgdb.cn/item/63c129fdbe43e0d30e0b29d3.jpg)

### ⚪ [<font color=blue>Learning to Count</font>](https://0809zheng.github.io/2022/10/06/count.html)

把图像的特征看作一种标量属性，如果把一幅图像划分成$2\times 2$的图像块，则四个图像块中特征的数量之和应该与原始图像的特征数量相同。把模型看作特征计数器$\phi(\cdot)$，对于输入图像$x$定义$2\times$下采样操作$D(\cdot)$和$2\times 2$图像块划分操作$T_i(\cdot),i=1,2,3,4$，则自监督学习的前置任务定义为如下目标函数：

$$ \mathcal{L} = ||\phi(D \circ x)  - \sum_{i=1}^4 \phi(T_i \circ x)||_2^2 + \max(0,c-||\phi(D \circ y)  - \sum_{i=1}^4 \phi(T_i \circ x)||_2^2) $$

![](https://pic.imgdb.cn/item/63c11ef0be43e0d30ef8c190.jpg)

### ⚪ [<font color=blue>Image Rotation</font>](https://0809zheng.github.io/2022/10/03/rotation.html)

对输入图像随机地旋转四种不同的角度：$$[0^{\circ},90^{\circ},180^{\circ},270^{\circ}]$$，则自监督学习的前置任务是预测图像旋转哪种角度的四分类任务。

![](https://pic.imgdb.cn/item/63c0e754be43e0d30e91f5aa.jpg)


# 2. 基于生成模型的自监督学习方法



# 3. 基于对比学习的自监督学习方法

**对比学习(Contrastive Learning)**旨在学习一个特征嵌入空间，使得相似的样本对(正样本对)彼此靠近，不相似的样本对(负样本对)相互远离。在无监督形式的对比学习中，可以通过**数据增强**等方法构造样本对，从而实现有意义的特征表示学习。

## （1）对比损失函数

对比学习中的损失函数可以追溯到监督学习中的[深度度量学习](https://0809zheng.github.io/2022/11/01/metric.html)，通过给定类别标签构造正负样本对，最小化正样本对$(x,x^+)$的嵌入距离，最大化负样本对$(x,x^-)$的嵌入距离。

### ⚪ [Noise Contrastive Estimation (NCE)](http://proceedings.mlr.press/v9/gutmann10a.html)

**噪声对比估计**是一种统计模型的参数估计方法，其想法是运行[逻辑回归](https://0809zheng.github.io/2020/03/13/logistic-regression.html)来区分目标样本$x$和噪声$$\tilde{x}$$。逻辑回归模型$f(\cdot)$通过**Sigmoid**激活建模属于目标而不是噪声的概率，进而建立二元交叉熵损失：

$$ \mathcal{L}_{NCE} =  \Bbb{E}_{x \text{~} p_{\text{data}}} [-\log f(x) - \log(1-f(\tilde{x}))] $$

### ⚪ [<font color=blue>Contrastive Predictive Coding (CPC)</font>](https://0809zheng.github.io/2022/10/08/cpc.html)

**对比预测编码**把二元**NCE**损失扩展为多元**InfoNCE**损失。给定上下文向量$c$，正样本通过$p(x\|c)$构造，$N-1$个负样本通过$p(x)$构造；使用类别交叉熵损失区分正样本和噪声样本：

$$ \begin{aligned} \mathcal{L}_{InfoNCE} &= - \Bbb{E}_{x \text{~} p_{\text{data}}} [\log\frac{f(x,c)}{\sum_{x' \text{~} p_{\text{data}}} f(x',c)}] \\ &= - \Bbb{E}_{x,x^+,\{x^-_i\}_{i=1}^N} [\log\frac{e^{f(x)^Tf(x^+)}}{e^{f(x)^Tf(x^+)}+\sum_{i=1}^N e^{f(x)^Tf(x^-_i)}}] \end{aligned} $$

### ⚪ [<font color=blue>CPC v2</font>](https://0809zheng.github.io/2022/10/09/cpcv2.html)

把**InfoNCE**应用到图像数据集中，把输入图像$x$的每个图像块压缩为潜在表示$z_{i,j}$，从中构造上下文特征$c_{i,j}=g_{\phi}(z_{\leq i,\leq j})$，并进一步预测潜在表示$$\hat{z}_{i+k,j} = W_kc_{i,j}$$。

$$ \begin{aligned} \mathcal{L}_N = - \sum_{i,j,k} \log\frac{\exp(\hat{z}_{i+k,j}^Tz_{i+k,j})}{\exp(\hat{z}_{i+k,j}^Tz_{i+k,j}) + \sum_l \exp(\hat{z}_{i+k,j}^Tz_{l})} \end{aligned} $$

### ⚪ [<font color=blue>Alignment and Uniformity</font>](https://0809zheng.github.io/2022/10/12/hypersphere.html)

对比学习的损失函数具有两种性质：**对齐性(Alignment)**和**一致性(Uniformity)**。对齐性用于衡量正样本对之间的相似程度；一致性用于衡量归一化的特征在超球面上分布的均匀性。

$$ \begin{aligned} \mathcal{L}_{align}(f;\alpha) &=  \Bbb{E}_{(x,y)\text{~}p_{pos}} [||f(x)-f(y)||_2^{\alpha}] \\ \mathcal{L}_{uniform}(f;t) &= \log \Bbb{E}_{(x,y)\text{~}p_{data}} [e^{-t||f(x)-f(y)||_2^2}] \end{aligned} $$

### ⚪ [<font color=blue>Debiased Contrastive Loss</font>](https://0809zheng.github.io/2022/10/13/debiased.html)

由于样本的真实标签是未知的，因此负样本可能采样到假阴性样本。在构造对比损失时，对负样本项进行偏差修正：

$$ g(x,\{u_i\}_{i=1}^N,\{v_i\}_{i=1}^M) = \max(\frac{1}{\eta^-}(\frac{1}{N}\sum_{i=1}^N \exp(f(x)^Tf(u_i))-\frac{\eta^+}{M}\sum_{i=1}^M \exp(f(x)^Tf(v_i))),\exp(-1/\tau)) \\ \mathcal{L}_{unbiased} = \Bbb{E}_{x,\{u_i\}_{i=1}^N\text{~}p;x^+,\{v_i\}_{i=1}^M\text{~}p_x^+} [-\log \frac{\exp(f(x)^Tf(x^+))}{\exp(f(x)^Tf(x^+))+Ng(x,\{u_i\}_{i=1}^N,\{v_i\}_{i=1}^M)}] $$

### ⚪ [<font color=blue>Hard Negative Samples</font>](https://0809zheng.github.io/2022/10/14/hardnegtive.html)

对对比损失中的负样本对项$$\exp(f(x)^Tf(x^-))$$进行加权，权重正比于负样本与**anchor**样本的相似度，设置为：

$$ \frac{\beta \exp(f(x)^Tf(x^-))}{\sum_{x^-} \exp(f(x)^Tf(x^-))} $$

## （2）并行数据增强 Parallel Augmentation

基于**并行数据增强**的对比学习方法为**anchor**样本同时生成两个数据增强样本，并使得它们共享相同的特征表示。

### ⚪ [<font color=blue>Invariant and Spreading Instance Feature (InvaSpread)</font>](https://0809zheng.github.io/2022/10/18/invaspread.html)

**InvaSpread**对于一批样本进行数据增强，把样本$x$的增强样本$$\hat{x}$$视为正样本，其余所有样本视为负样本；正样本特征应具有不变性，负样本特征应尽可能地分开。

$$ \mathcal{L}_{\text{InvaSpread}} = -\sum_i \log \frac{\exp(f_i^T\hat{f}_i/\tau)}{\sum_{k=1}^N\exp(f_k^T\hat{f}_i/\tau)}-\sum_i \sum_{j\neq i} \log(1- \frac{\exp(f_i^Tf_j/\tau)}{\sum_{k=1}^N\exp(f_k^Tf_j/\tau)}) $$

![](https://pic.imgdb.cn/item/63d882dbface21e9efa1f99b.jpg)

### ⚪ [<font color=blue>Simple Framework for Contrastive Learning of Visual Representation (SimCLR)</font>](https://0809zheng.github.io/2022/10/15/simclr.html)

**SimCLR**随机采样$N$个数据样本，对每个样本应用两次同一类的不同数据增强，构造$2N$个增强样本；对于任意样本$$\tilde{x}_i$$，$$\tilde{x}_j$$为正样本，其余$2(N-1)$个样本为负样本。通过编码网络$f(\cdot)$和映射层$g(\cdot)$提取特征表示，并构造对比损失：

$$ \mathcal{L}^{(i,j)}_{\text{SimCLR}} = -\log \frac{\exp(\text{sim}(z_i,z_j)/\tau)}{\sum_{k=1,...,2N;k\neq i}\exp(\text{sim}(z_i,z_k)/\tau)} $$

![](https://pic.imgdb.cn/item/63d5e959face21e9efe0214a.jpg)

### ⚪ [<font color=blue>SimCLRv2</font>](https://0809zheng.github.io/2022/09/16/selfsemi.html)

**SimCLRv2**在**SimCLR**的基础上采用更大的卷积网络和更深的映射头，并通过微调和数据蒸馏实现半监督学习：

![](https://pic.imgdb.cn/item/63bfd38fbe43e0d30ee98443.jpg)

### ⚪ [<font color=blue>Bootstrap your own latent (BYOL)</font>](https://0809zheng.github.io/2022/10/17/byol.html)

**BYOL**没有构建负样本对，而是使用参数为$\theta$的在线网络和参数为$\xi$的目标网络分别从图像$x$的两个增强版本中提取特征$z,z'$，根据$z$预测$z'$(或交换顺序后根据$z'$预测$z$)。损失函数设置为归一化特征的均方误差损失，更新参数$\theta$，参数$\xi$是参数$\theta$的滑动平均：$$\xi \leftarrow \tau \xi + (1-\tau)\theta$$。

$$ \begin{aligned} \mathcal{L}_{\text{BYOL}} \propto -2(\frac{<q_{\theta}(z_{\theta}),z'_{\xi}>}{||q_{\theta}(z_{\theta})||_2 \cdot ||z'_{\xi}||_2}+\frac{<q_{\theta}(z'_{\theta}),z_{\xi}>}{||q_{\theta}(z'_{\theta})||_2 \cdot ||z_{\xi}||_2}) \end{aligned}  $$

![](https://pic.imgdb.cn/item/63d8c180face21e9ef442d3e.jpg)


### ⚪ [<font color=blue>Barlow Twins</font>](https://0809zheng.github.io/2022/10/16/barlow.html)

**Barlow Twins**把数据样本$x$的两个增强版本$x^A,x^B$喂入同一个神经网络以提取特征表示$z^A,z^B$，并使得两组输出特征的互相关矩阵$$\mathcal{C}$$接近单位矩阵。

$$ \mathcal{L}_{\text{BT}} = \sum_i (1-\mathcal{C}_{ii})^2 + \lambda \sum_i \sum_{j\neq i} \mathcal{C}_{ij}^2 , \quad \mathcal{C}_{ij} = \frac{\sum_bz^A_{b,i}z^B_{b,j}}{\sqrt{\sum_b(z_{b,i}^A)^2}\sqrt{\sum_b(z^B_{b,j})^2}} $$

![](https://pic.imgdb.cn/item/63d797e3face21e9ef0c7486.jpg)


## （3）



# ⭐ 参考文献
- [Self-Supervised Representation Learning](https://lilianweng.github.io/posts/2019-11-10-self-supervised/)(Lil'Log)一篇介绍自监督学习的博客。
- [Awesome Self-Supervised Learning](https://github.com/jason718/awesome-self-supervised-learning)：(github) A curated list of awesome self-supervised methods.
- [An Overview of Deep Semi-Supervised Learning](https://arxiv.org/abs/2006.05278)：(arXiv2006)一篇深度半监督学习的综述。
- [<font color=blue>Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks</font>](https://0809zheng.github.io/2022/10/02/exemplarcnn.html)：(arXiv1406)通过Exemplar-CNN实现判别无监督特征学习。
- [<font color=blue>Unsupervised Visual Representation Learning by Context Prediction</font>](https://0809zheng.github.io/2022/10/04/context.html)：(arXiv1505)通过上下文预测实现无监督视觉表示学习。
- [<font color=blue>Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles</font>](https://0809zheng.github.io/2022/10/05/jigsaw.html)：(arXiv1603)通过解决拼图问题实现无监督视觉表示学习。
- [<font color=blue>Colorful Image Colorization</font>](https://0809zheng.github.io/2022/10/07/colorization.html)：(arXiv1603)通过彩色图像着色实现无监督特征学习。
- [<font color=blue>Representation Learning by Learning to Count</font>](https://0809zheng.github.io/2022/10/06/count.html)：(arXiv1708)通过学习计数实现无监督表示学习。
- [<font color=blue>Unsupervised Representation Learning by Predicting Image Rotations</font>](https://0809zheng.github.io/2022/10/03/rotation.html)：(arXiv1803)通过预测图像旋转角度实现无监督表示学习。
- [<font color=blue>Representation Learning with Contrastive Predictive Coding</font>](https://0809zheng.github.io/2022/10/08/cpc.html)：(arXiv1807)通过对比预测编码进行表示学习。
- [<font color=blue>Unsupervised Embedding Learning via Invariant and Spreading Instance Feature</font>](https://0809zheng.github.io/2022/10/18/invaspread.html)：(arXiv1904)通过不变和扩散的实例特征实现无监督嵌入学习。
- [<font color=blue>Data-Efficient Image Recognition with Contrastive Predictive Coding</font>](https://0809zheng.github.io/2022/10/09/cpcv2.html)：(arXiv1905)通过对比预测编码实现数据高效的图像识别。
- [<font color=blue>A Simple Framework for Contrastive Learning of Visual Representations</font>](https://0809zheng.github.io/2022/10/15/simclr.html)：(arXiv2002)SimCLR：一种视觉对比表示学习的简单框架。
- [<font color=blue>Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere</font>](https://0809zheng.github.io/2022/10/12/hypersphere.html)：(arXiv2005)通过超球面上的对齐和一致性理解对比表示学习。
- [<font color=blue>Big Self-Supervised Models are Strong Semi-Supervised Learners</font>](https://0809zheng.github.io/2022/09/16/selfsemi.html)：(arXiv2006)SimCLRv2：自监督大模型是强半监督学习器。
- [<font color=blue>Bootstrap your own latent: A new approach to self-supervised Learning</font>](https://0809zheng.github.io/2022/10/17/byol.html)：(arXiv2006)BYOL：通过在隐空间应用自举法实现自监督学习。
- [<font color=blue>Debiased Contrastive Learning</font>](https://0809zheng.github.io/2022/10/13/debiased.html)：(arXiv2007)偏差修正的对比学习。
- [<font color=blue>Contrastive Learning with Hard Negative Samples</font>](https://0809zheng.github.io/2022/10/14/hardnegtive.html)：(arXiv2010)使用难例负样本进行对比学习。
- [<font color=blue>Barlow Twins: Self-Supervised Learning via Redundancy Reduction</font>](https://0809zheng.github.io/2022/10/16/barlow.html)：(arXiv2103)Barlow Twins：通过冗余度消除实现自监督学习。