---
layout: post
title: '图像数据集中的长尾分布问题'
date: 2021-05-20
author: 郑之杰
cover: 'https://pic.imgdb.cn/item/60a4730d6ae4f77d353fd1fe.jpg'
tags: 深度学习
---

> Long-tail distribution problem in image datasets.


在**ImageNet**、**COCO**等常用视觉数据集中，由于经过人工预筛选，图像中的不同目标类别的数量是接近的。而在实际的视觉应用中，数据集大多服从**长尾分布(long-tail distribution)**，即少数类别(称为**head class**)占据绝大多数样本，多数类别(称为**tail class**)仅有少量样本。一个典型的长尾分布数据集(**Open Brands**商标数据集)如下图所示。

![](https://pic.imgdb.cn/item/60a4730d6ae4f77d353fd1fe.jpg)

目前常用的一些长尾分布数据集如下：
- **Long-tailed CIFAR**：训练集是通过对**CIFAR10**或**CIFAR100**的每类训练样本下采样得到的，其类别的最大数量和最小数量之比可以取$10$-$200$；测试集保持不变。
- **iNaturalist 2018**：自然物种分类数据集，包含$8142$类别的$437513$张图像，面临长尾分布和细粒度检测问题。
- **Long-tailed ImageNet(ImageNet-LT)**：从**ImageNet**中按照**Pareto**分布采样得到，包含$1000$类别的$19k$张图像，最多的类别具有$1280$张图像，而最少的仅有$5$张图像。
- **LVIS**：大规模实例分割数据集，对超过$1000$类物体进行了约$200$万个高质量的实例分割标注，包含$164$**k**张图像。

本文介绍一些解决图像数据集中长尾分布问题的方法：
1. 重采样 **Re-sampling**
2. 重加权 **Re-weighting**
3. 其他方法 **Others**



# 1. Re-sampling
**重采样(re-sampling)**的思想是通过对**head class**进行欠采样或对**tail class**进行过采样，人为地让模型学习时接触到的训练样本是类别均衡的，从而一定程度上减少对**head class**的过拟合。不过由于**tail class**的少量数据往往被反复学习，缺少足够多的样本从而容易过拟合；而**head class**又往往得不到充分学习。

![](https://pic.imgdb.cn/item/60a4da286ae4f77d35753c60.jpg)

常用的重采样方法包括：
- **Random over-sampling**：对**tail class**进行过采样，这种方法容易过拟合。
- **Random under-sampling**：对**head class**进行欠采样，会使数据集减小。
- [<font color=Blue>Class-balanced sampling</font>](https://0809zheng.github.io/2021/01/17/decouple.html)：控制重采样时每个类别$j$(而不是每个样本)被采样的概率相同：

$$ p_j^{CB} = \frac{1}{C} $$

- [<font color=Blue>Progressively-balanced sampling</font>](https://0809zheng.github.io/2021/01/17/decouple.html)：训练前期偏向类别不平衡采样，训练后期偏向类别平衡采样；缺点是每轮训练都要重新采样构成数据集：

$$ p_j^{PB}(t) = (1-\frac{t}{T})\frac{N_j}{N} +\frac{t}{T}\frac{1}{C} $$

- [<font color=Blue>PReversed sampling</font>](https://0809zheng.github.io/2021/07/19/bbn.html)：控制重采样时每个类别$j$被采样的概率与该类别样本数量成反比例：

$$ p_j^{Re} = \frac{w_j}{\sum_{j=1}^{C}w_j}, \quad w_j=\frac{n_{max}}{n_j} $$



# 2. Re-weighting
**重加权(re-weighting)**的思想是在损失函数中对不同数据的损失设置不同的权重，通常是对**tail class**对应的损失设置更大的权重。但是这类方法需要针对不同的数据集和模型等条件设置不同的超参数，泛化性较差。

![](https://pic.imgdb.cn/item/60a4da0d6ae4f77d3574999b.jpg)

假设共有$C$个类别，类别$c$共有$n_c$个样本，总样本数为$n$；模型的输出**logits**(**softmax**前的输出)为$z=[z_1,z_2,...,z_C]^T$，则属于类别$c$的样本$x$的**交叉熵**损失函数计算为：

$$ \mathcal{L}(x,c) = -log(\frac{exp(z_c)}{\sum_{i=1}^{C} exp(z_i)}) $$

下面介绍几种对损失函数进行重加权的方法：
- **Inverse Class Frequency Weighting**：根据类别出现频率的倒数进行加权：

$$ \mathcal{L}_{\text{ICFW}}(x,c) = -\frac{n}{n_c}log(\frac{exp(z_c)}{\sum_{i=1}^{C} exp(z_i)}) $$

- **Cost-Sensitive Cross-Entropy Loss**：根据类别出现(与最少类别的)倍数的倒数进行加权：

$$ \mathcal{L}_{\text{CS}}(x,c) = -\frac{n_{min}}{n_c}log(\frac{exp(z_c)}{\sum_{i=1}^{C} exp(z_i)}) $$

- [**Class-Balanced Loss**](https://0809zheng.github.io/2021/01/18/classbalanced.html)：使用不同类别的有效样本数量$E_{n_c}$进行加权：

$$ \mathcal{L}_{\text{CB}}(x,c) = -\frac{1}{E_{n_c}} log(\frac{exp(z_c)}{\sum_{i=1}^{C} exp(z_i)}) = -\frac{1-\beta}{1-\beta^{n_c}} log(\frac{exp(z_c)}{\sum_{i=1}^{C} exp(z_i)}) $$

- [**Equalization Loss**](https://0809zheng.github.io/2021/01/19/equalization.html)：减轻对其余属于**tail**类别的梯度抑制：

$$ \mathcal{L}_{\text{EQ}}(x,c) = -log(\frac{exp(z_c)}{\sum_{i=1}^{C} \tilde{w}_i exp(z_i)}), \quad \tilde{w}_i=1-\beta \Bbb{I}(f_i<\lambda)(1-y_i) $$

- [**Seesaw Loss**](https://0809zheng.github.io/2021/01/20/seesaw.html)：通过平衡系数$\mathcal{S}_{ij}$(由缓解因子$\mathcal{M}_{ij}$和补偿因子$\mathcal{C}_{ij}$控制)控制施加在其余类别上的负样本梯度：

$$ \mathcal{L}_{\text{SS}}(x,c) = -log(\frac{exp(z_c)}{\sum_{j≠c}^{C} \mathcal{S}_{ij} e^{z_j} + e^{z_c}}), \quad \mathcal{S}_{ij} = \mathcal{M}_{ij} \cdot \mathcal{C}_{ij} $$

- [**Equalization Loss v2**](https://0809zheng.github.io/2021/07/14/eqlossv2.html)：根据梯度引导重加权机制动态调整每轮训练中正梯度和负梯度的权重：

$$ \nabla_{z_j}^{pos'}(\mathcal{L}^{(t)}) = 1+\alpha(1-\frac{1}{1+e^{-\gamma(g_j^{(t)}-\mu)}}) \nabla_{z_j}^{pos}(\mathcal{L}^{(t)}) $$

$$ \nabla_{z_j}^{neg'}(\mathcal{L}^{(t)}) = \frac{1}{1+e^{-\gamma(g_j^{(t)}-\mu)}} \nabla_{z_j}^{neg}(\mathcal{L}^{(t)}) $$

# 3. Others

### (1) 解耦特征表示与分类

- [Decoupling Representation and Classifier for Long-Tailed Recognition](https://0809zheng.github.io/2021/01/17/decouple.html)：将长尾分布的图像分类问题解耦为特征的表示学习和特征的分类。采用两阶段的训练方法，首先在原始数据集上进行特征学习，然后在构造的类别平衡数据集上进行微调。

- [BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition](https://0809zheng.github.io/2021/07/19/bbn.html)：采用双分支的网络结构同时进行特征学习和分类器学习，通过累积学习在训练过程中调整两个分支的权重。


### (2) 类别分组分类

- [Overcoming Classifier Imbalance for Long-tail Object Detection with Balanced Group Softmax](https://0809zheng.github.io/2021/06/14/groupsoftmax.html)：提出了**BAGS**方法，即按照不同类别的样本数量级对类别进行分组，每一组额外增加**others**类别，训练时分组分类训练，测试时分组测试并合并结果。


# 6. Balanced Meta-Softmax
- paper：Balanced Meta-Softmax for Long-Tailed Visual Recognition

- Reference：[Bag of Tricks for Long-Tailed Visual Recognition with Deep Convolutional Neural Networks](http://www.lamda.nju.edu.cn/zhangys/papers/AAAI_tricks.pdf)