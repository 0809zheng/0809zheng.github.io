---
layout: post
title: '循环神经网络'
date: 2020-03-07
author: 郑之杰
cover: 'https://pic.downk.cc/item/5e9fda52c2a9a83be551d194.jpg'
tags: 深度学习
---

> Recurrent Neural Networks.

**循环神经网络(Recurrent Neural Networks，RNN)**可以建模时间序列数据之间的相关性，其特点：
1. 每一时刻的输出不仅和当前时刻的输入相关，也和其过去一段时间的输出相关；
2. 可以处理输入长度不固定的文本等时序数据。

**本文目录**：
1. vanilla RNN
2. 参数学习
3. 长程依赖问题
4. 门控机制
5. 深层RNN

# 1. vanilla RNN
一个简单的循环神经网络包括输入层、一层隐藏层和输出层。

![](https://pic.downk.cc/item/5e9fdc29c2a9a83be5533395.jpg)

令向量$$x_t \in \Bbb{R}^M$$表示t时刻网络的输入，$$h_t \in \Bbb{R}^D$$表示**隐藏层状态(hidden state)**，则：

$$ z_t = W_{hh}h_{t-1}+W_{xh}x_t+b $$

$$ h_t = f(z_t) $$

$$ y_t = W_{hy}h_t $$

其中$f$是激活函数，常用Sigmoid或Tanh函数；参数$W_{hh}$、$W_{xh}$、$W_{hy}$、$b$在时间维度上**权值共享**。

**性质1：通用近似定理（Universal Approximation Theory）**

如果一个完全连接的循环神经网络有足够数量的sigmoid神经元，它可以以任意的准确率去近似任何一个非线性动力系统。

**性质2：图灵完备性（Turing Completeness）**

图灵完备是指一种数据操作规则，比如一种计算机编程语言，可以实现图灵机（Turing Machine）的所有功能，解决所有的可计算问题。目前主流的编程语言（比如C++、Java、Python等）都是图灵完备的。

RNN的图灵完备性：所有的图灵机都可以被一个由使用Sigmoid神经元构成的全连接循环网络来进行模拟。

# 2. 参数学习
RNN的参数可以通过梯度下降方法来进行学习，在RNN中主要有两种计算梯度的方式：
1. 随时间反向传播（BPTT）算法
2. 实时循环学习（RTRL）算法

### (1).随时间反向传播
**随时间反向传播（BackPropagation Through Time，BPTT）**算法将循环神经网络看作一个展开的多层前馈网络，其中“每一层”对应循环网络中的“每个时刻”。

定义误差项$$δ_{t,k} = \frac{\partial L_t}{\partial z_k}$$，则误差的反向传播：

$$ δ_{t,k} = \frac{\partial L_t}{\partial z_k} = \frac{\partial L_t}{\partial z_{k+1}} \frac{\partial z_{k+1}}{\partial z_k} = \frac{\partial L_t}{\partial z_{k+1}} \frac{\partial z_{k+1}}{\partial h_k} \frac{\partial h_k}{\partial z_k} = δ_{t,k+1}W_{hh}f'(z_k) $$

![](https://pic.downk.cc/item/5e9fe01dc2a9a83be5561566.jpg)

RNN所有层的参数是共享的，因此参数的真实梯度是所有“展开层”的参数梯度之和：

$$ \frac{\partial L_t}{\partial W_{hh}} = \sum_{t=1}^{T} {\sum_{k=1}^{K} {δ_{t,k}h_{k-1}}} $$

$$ \frac{\partial L_t}{\partial W_{xh}} = \sum_{t=1}^{T} {\sum_{k=1}^{K} {δ_{t,k}x_{k}}} $$

$$ \frac{\partial L_t}{\partial b} = \sum_{t=1}^{T} {\sum_{k=1}^{K} {δ_{t,k}}} $$


### (2).实时循环学习
**实时循环学习（Real-Time Recurrent Learning，RTRL）**是通过前向传播的方式来计算梯度，以$W_{hh}$为例：

$$ \frac{\partial h_{t+1}}{\partial W_{hh}} = \frac{\partial h_{t+1}}{\partial z_{t+1}} \frac{\partial z_{t+1}}{\partial W_{hh}} = \frac{\partial h_{t+1}}{\partial z_{t+1}} \frac{\partial W_{hh}h_t}{\partial W_{hh}} $$

$$ \frac{\partial L_{t}}{\partial W_{hh}} = \frac{\partial h_{t}}{\partial W_{hh}} \frac{\partial L_{t+1}}{\partial h_{t}} $$

RTRL算法和BPTT算法都是基于梯度下降的算法，分别通过前向模式和反向模式应用链式法则来计算梯度。

在循环神经网络中，一般网络输出维度远低于输入维度，因此BPTT算法的计算量会更小，但是BPTT算法需要保存所有时刻的中间梯度，空间复杂度较高。

RTRL算法不需要梯度回传，因此非常适合用于需要在线学习或无限序列的任务。

# 3. 长程依赖问题
RNN反向传播中的误差项$$δ_{t,k}$$满足：

$$ δ_{t,k} = δ_{t,k+1}W_{hh}f'(z_k) $$

若记$$γ ≈ W_{hh}f'(z_k)$$，则：

$$ δ_{t,k} = γ^{t-k}δ_{t,t} $$

- 若$γ<1$，当$t-k → ∞$时，$γ^{t-k} → 0$，出现**梯度消失(Vanishing Gradient)**问题;
- 若$γ>1$，当$t-k → ∞$时，$γ^{t-k} → ∞$，出现**梯度爆炸(Exploding Gradient)**问题;

值得注意的是，梯度消失并不是$$ \frac{\partial L_{t}}{\partial W}$$消失了，而是$$ \frac{\partial L_{t}}{\partial h_{k}}$$消失了。也就说参数$W$的更新主要靠$t$时刻的几个相邻状态更新，长距离的状态则没有影响。

由于RNN经常使用Logistic函数或Tanh函数作为非线性激活函数，其导数值都小于1，因而经常会出现梯度消失问题。

虽然RNN理论上可以建立长时间间隔的状态之间的依赖关系，但是由于梯度爆炸或消失问题，实际上只能学习到短期的依赖关系。这个问题称作**长程依赖问题（Long-Term Dependencies Problem）**。

为了减缓上述问题，可以采取以下措施：
- **梯度爆炸**：权重衰减、梯度截断
- **梯度消失**（主要问题）：引入门控机制

# 4. 门控机制
为了改善循环神经网络的长程依赖问题，引入了**门控机制(Gated Mechanism)**。

### (1).LSTM
**长短期记忆网络（Long Short-Term Memory Network，LSTM）**可以有效地解决RNN的梯度爆炸或消失问题。

LSTM网络引入了门控机制，来控制信息传递的路径，包括输入门$i$、遗忘门$f$和输出门$o$；

LSTM网络引入了一个新的内部状态$c$（cell state）进行线性的循环信息控制。

$$ \begin{pmatrix} i_t \\ f_t \\ o_t \\ g_t \\ \end{pmatrix} = \begin{pmatrix} sigmoid \\ sigmoid \\ sigmoid \\ tanh \\ \end{pmatrix} (W_xx_t+W_hh_{t-1}+b) $$

$$ c_t = c_{t-1} \bigodot f_t + i_t \bigodot g_t $$

$$ h_{t} = o_t \bigodot tanh(c_t) $$

![](https://pic.downk.cc/item/5ea12bdbc2a9a83be5a8834d.jpg)

- **短期记忆（Short-Term Memory）**：隐状态$h$每个时刻都会重写；
- **长期记忆（Long-Term Memory）**：网络参数更新周期要远远慢于短期记忆；
- **长短期记忆（Long Short-Term Memory）**：记忆单元$c$中保存信息的生命周期要长于短期记忆$h$，但又远远短于长期记忆。

LSTM的一些改进：
1. 遗忘门参数的初始化比较小会丢弃前一时刻的大部分信息，很难捕捉到长距离的依赖信息。因此遗忘门的参数初始值一般都设得比较大；
2. **peephole**：输入门$i$、遗忘门$f$和输出门$o$不但依赖于输入$x_t$和上一时刻的隐状态$h_{t-1}$，也依赖于记忆单元$c$；
![](https://pic.downk.cc/item/5ea13528c2a9a83be5b242d2.jpg)
3. 输入门和遗忘门有些互补关系，可以耦合输入门和遗忘门：$$c_t = c_{t-1} \bigodot (1-i_t) + i_t \bigodot g_t$$

### (2).GRU
**门控循环单元（Gated Recurrent Unit，GRU）**比LSTM更加简单。

GRU没有引入新的记忆单元，而是引入了更新门$z$和重置门$r$:

$$ \begin{pmatrix} z_t \\ r_t \\ \end{pmatrix} = \begin{pmatrix} sigmoid \\ sigmoid \\ \end{pmatrix} (W_xx_t+W_hh_{t-1}+b) $$

$$ \overline{h}_t = tanh(W'_xx_t+W'_hh_{t-1} \bigodot r_t+b') $$

$$ h_t = z_t \bigodot h_{t-1} + (1-z_t) \bigodot \overline{h}_t $$

![](https://pic.downk.cc/item/5ea12d0ac2a9a83be5a9da10.jpg)

# 5. 深层RNN
可以增加循环神经网络的深度从而增强循环神经网络的能力，即增加同一时刻网络输入到输出之间的路径。

### (1)堆叠循环神经网络
**堆叠循环神经网络（Stacked RNN）**是将多个循环网络堆叠起来。

![](https://pic.downk.cc/item/5ea1325ac2a9a83be5af5547.jpg)

### (2)双向循环神经网络
**双向循环神经网络（Bidirectional RNN）**由两层循环神经网络组成，它们的输入相同，只是信息传递的方向不同。

![](https://pic.downk.cc/item/5ea132bbc2a9a83be5afa1c3.jpg)